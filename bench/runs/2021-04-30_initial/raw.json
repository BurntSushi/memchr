{
  "name": "2021-04-30",
  "benchmarks": {
    "memchr1/fallback/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/fallback/empty/never",
        "directory_name": "memchr1/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7115506357112948,
            "upper_bound": 1.7141132529258285
          },
          "point_estimate": 1.7127839884217402,
          "standard_error": 0.0006566575500923613
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7110420437987113,
            "upper_bound": 1.7142685663000965
          },
          "point_estimate": 1.7125328442436114,
          "standard_error": 0.0007994741756884028
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003747988552083476,
            "upper_bound": 0.0038617998499625382
          },
          "point_estimate": 0.0018535090641434767,
          "standard_error": 0.0008239754430197158
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7119366272395733,
            "upper_bound": 1.7141679871634197
          },
          "point_estimate": 1.7131621018277745,
          "standard_error": 0.0005772530644544568
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001114116426989721,
            "upper_bound": 0.0028812301494004432
          },
          "point_estimate": 0.002189321217954425,
          "standard_error": 0.00046282561052330806
        }
      }
    },
    "memchr1/fallback/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/common",
        "directory_name": "memchr1/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 640991.0745989452,
            "upper_bound": 643021.6792026421
          },
          "point_estimate": 641918.0938659146,
          "standard_error": 522.4959164708763
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 640968.3453947369,
            "upper_bound": 642898.4993734336
          },
          "point_estimate": 641488.6467836257,
          "standard_error": 459.14103314919606
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.1718089650312,
            "upper_bound": 2704.371596198353
          },
          "point_estimate": 897.5208307763721,
          "standard_error": 636.0828968344292
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 641086.7828772041,
            "upper_bound": 642441.946760253
          },
          "point_estimate": 641574.6564137617,
          "standard_error": 354.6739878950436
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 546.9181044308116,
            "upper_bound": 2404.2784648208076
          },
          "point_estimate": 1745.3743131154429,
          "standard_error": 486.50330154360097
        }
      }
    },
    "memchr1/fallback/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/never",
        "directory_name": "memchr1/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32157.62219789953,
            "upper_bound": 32197.66740183052
          },
          "point_estimate": 32176.251260491797,
          "standard_error": 10.30661475950644
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32150.981532329497,
            "upper_bound": 32201.6272512548
          },
          "point_estimate": 32171.07748745202,
          "standard_error": 10.75191156501044
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.779663688393384,
            "upper_bound": 58.37176004429726
          },
          "point_estimate": 21.235366531763816,
          "standard_error": 13.326093398059133
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32155.894038368628,
            "upper_bound": 32181.278035119445
          },
          "point_estimate": 32168.080392946293,
          "standard_error": 6.303620051905042
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.1232582629549,
            "upper_bound": 44.81803154156712
          },
          "point_estimate": 34.38419512976583,
          "standard_error": 7.990201762245178
        }
      }
    },
    "memchr1/fallback/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/rare",
        "directory_name": "memchr1/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32825.81482589196,
            "upper_bound": 32895.090590943335
          },
          "point_estimate": 32858.35027368754,
          "standard_error": 17.65813710927906
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32817.83069117942,
            "upper_bound": 32886.39660940325
          },
          "point_estimate": 32850.68877551021,
          "standard_error": 14.457265039216578
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.983881840725074,
            "upper_bound": 98.20652390447133
          },
          "point_estimate": 33.561474404164464,
          "standard_error": 26.593721383570927
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32831.31748800387,
            "upper_bound": 32860.04688607595
          },
          "point_estimate": 32847.49474883164,
          "standard_error": 7.270736389553877
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.36013404884248,
            "upper_bound": 79.6431379551423
          },
          "point_estimate": 58.78691605636772,
          "standard_error": 14.57131064397672
        }
      }
    },
    "memchr1/fallback/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/uncommon",
        "directory_name": "memchr1/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156210.09056698345,
            "upper_bound": 156412.8281204782
          },
          "point_estimate": 156309.2089129028,
          "standard_error": 51.85605140792108
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156179.09806866953,
            "upper_bound": 156427.25785135227
          },
          "point_estimate": 156316.36131974251,
          "standard_error": 53.21886769711327
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.86631443640012,
            "upper_bound": 298.10071759750394
          },
          "point_estimate": 125.40528926505664,
          "standard_error": 76.07180187017248
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156238.3736726744,
            "upper_bound": 156418.3486751043
          },
          "point_estimate": 156323.56853018227,
          "standard_error": 47.583386764666
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.55922996648995,
            "upper_bound": 226.30944830318637
          },
          "point_estimate": 172.97039938384492,
          "standard_error": 36.216580831662256
        }
      }
    },
    "memchr1/fallback/huge/verycommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/verycommon",
        "directory_name": "memchr1/fallback_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1139480.95840563,
            "upper_bound": 1141641.847168899
          },
          "point_estimate": 1140547.4991654265,
          "standard_error": 555.5473849422675
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1139225.2726934524,
            "upper_bound": 1142021.921875
          },
          "point_estimate": 1140473.777560764,
          "standard_error": 680.3301323509841
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 385.42329136052825,
            "upper_bound": 3222.585480287786
          },
          "point_estimate": 1997.108495794237,
          "standard_error": 732.1189404815025
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1139363.7532800024,
            "upper_bound": 1140775.6879303982
          },
          "point_estimate": 1139993.1863636365,
          "standard_error": 362.0098457092678
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1027.4001477159643,
            "upper_bound": 2339.1021158126487
          },
          "point_estimate": 1848.3255761486444,
          "standard_error": 335.91980511447036
        }
      }
    },
    "memchr1/fallback/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/common",
        "directory_name": "memchr1/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 260.2588724443061,
            "upper_bound": 260.68347882774333
          },
          "point_estimate": 260.4688255803334,
          "standard_error": 0.1089395972171322
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 260.1641002383048,
            "upper_bound": 260.7559496973396
          },
          "point_estimate": 260.43683491129764,
          "standard_error": 0.15648996905029455
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12849953077003176,
            "upper_bound": 0.626208756782253
          },
          "point_estimate": 0.4387379961933337,
          "standard_error": 0.1282502831355837
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 260.18727213513677,
            "upper_bound": 260.5809746718344
          },
          "point_estimate": 260.34851534176124,
          "standard_error": 0.0998008526875174
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.21264662881184324,
            "upper_bound": 0.4576537175228093
          },
          "point_estimate": 0.36453957502974665,
          "standard_error": 0.06264533503601952
        }
      }
    },
    "memchr1/fallback/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/never",
        "directory_name": "memchr1/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.12665439104264,
            "upper_bound": 41.41746045806904
          },
          "point_estimate": 41.24075332305148,
          "standard_error": 0.08009726612422537
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.12058566125722,
            "upper_bound": 41.25757114656239
          },
          "point_estimate": 41.13799013912896,
          "standard_error": 0.03884379236278904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00430912256666705,
            "upper_bound": 0.18061814674663293
          },
          "point_estimate": 0.027721007186866892,
          "standard_error": 0.04786021673937861
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.103258272829386,
            "upper_bound": 41.15585964293492
          },
          "point_estimate": 41.12273070909142,
          "standard_error": 0.013686874098312212
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025503014614337825,
            "upper_bound": 0.4037539634412
          },
          "point_estimate": 0.2674299523277201,
          "standard_error": 0.11887711794536808
        }
      }
    },
    "memchr1/fallback/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/rare",
        "directory_name": "memchr1/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.9672185946892,
            "upper_bound": 50.545717092619995
          },
          "point_estimate": 50.261406888723485,
          "standard_error": 0.14794983251695368
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.83759403267571,
            "upper_bound": 50.70176277209639
          },
          "point_estimate": 50.33681752591115,
          "standard_error": 0.22052523311117267
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10050385076529296,
            "upper_bound": 0.8040534213097909
          },
          "point_estimate": 0.6249307858567608,
          "standard_error": 0.18984161449897805
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.1274435125282,
            "upper_bound": 50.63681567045709
          },
          "point_estimate": 50.41173872211214,
          "standard_error": 0.12876417343351312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.29027673824873584,
            "upper_bound": 0.6193273733374466
          },
          "point_estimate": 0.49446336343828895,
          "standard_error": 0.08509694687364516
        }
      }
    },
    "memchr1/fallback/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/uncommon",
        "directory_name": "memchr1/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.67959688154336,
            "upper_bound": 92.93768716544352
          },
          "point_estimate": 92.80331988232106,
          "standard_error": 0.06563794946030858
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.70839136297468,
            "upper_bound": 92.8769775178415
          },
          "point_estimate": 92.79627017408468,
          "standard_error": 0.03979282483533177
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013184627298688951,
            "upper_bound": 0.3304086228755367
          },
          "point_estimate": 0.10443942358210236,
          "standard_error": 0.08049308068991935
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.70979628870076,
            "upper_bound": 92.85073791967932
          },
          "point_estimate": 92.79578078639724,
          "standard_error": 0.03546329007702986
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06646054942069807,
            "upper_bound": 0.30911786232944755
          },
          "point_estimate": 0.21883869708043063,
          "standard_error": 0.06316481735693014
        }
      }
    },
    "memchr1/fallback/small/verycommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/verycommon",
        "directory_name": "memchr1/fallback_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 440.45381759679213,
            "upper_bound": 441.3888635804545
          },
          "point_estimate": 440.9348073975777,
          "standard_error": 0.23823398155547415
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 440.5953712012806,
            "upper_bound": 441.3609388335868
          },
          "point_estimate": 440.9441015255512,
          "standard_error": 0.16913619291085066
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03095716881318373,
            "upper_bound": 1.2775913446691631
          },
          "point_estimate": 0.38945263042181905,
          "standard_error": 0.3120006787085118
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 440.8046331316651,
            "upper_bound": 441.42426840285543
          },
          "point_estimate": 441.10570302826613,
          "standard_error": 0.16153658968533033
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2852183493122645,
            "upper_bound": 1.1052642501176553
          },
          "point_estimate": 0.7957166744835322,
          "standard_error": 0.20836942578473996
        }
      }
    },
    "memchr1/fallback/tiny/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/common",
        "directory_name": "memchr1/fallback_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.05314485329633,
            "upper_bound": 43.102128585873515
          },
          "point_estimate": 43.07758561185935,
          "standard_error": 0.012458962012418588
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.054744134757705,
            "upper_bound": 43.1044246211803
          },
          "point_estimate": 43.074269109980264,
          "standard_error": 0.011851990677798012
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007497717010116416,
            "upper_bound": 0.07259213942262242
          },
          "point_estimate": 0.032749691352118325,
          "standard_error": 0.016920299114983137
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.05068791199757,
            "upper_bound": 43.12169897754781
          },
          "point_estimate": 43.087091083553894,
          "standard_error": 0.019078355933586175
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01876117052431304,
            "upper_bound": 0.055640972435952445
          },
          "point_estimate": 0.04155680162473113,
          "standard_error": 0.00923648729194901
        }
      }
    },
    "memchr1/fallback/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/never",
        "directory_name": "memchr1/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.117023719207749,
            "upper_bound": 6.133344429748947
          },
          "point_estimate": 6.124779207156964,
          "standard_error": 0.0041777831300099846
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.117234427960778,
            "upper_bound": 6.13314430139755
          },
          "point_estimate": 6.122414990083284,
          "standard_error": 0.003803413015090558
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012048935870721054,
            "upper_bound": 0.022943332872656232
          },
          "point_estimate": 0.008712630652217009,
          "standard_error": 0.005391290392062037
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.11946008214745,
            "upper_bound": 6.126443183480819
          },
          "point_estimate": 6.122671320967811,
          "standard_error": 0.0017673212846806563
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005664259266456618,
            "upper_bound": 0.019173871592042688
          },
          "point_estimate": 0.013929368318857896,
          "standard_error": 0.0035846640328344695
        }
      }
    },
    "memchr1/fallback/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/rare",
        "directory_name": "memchr1/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.051566966618596,
            "upper_bound": 9.06434818586632
          },
          "point_estimate": 9.05790558253784,
          "standard_error": 0.0032766151644563124
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.049573854041784,
            "upper_bound": 9.067290445517967
          },
          "point_estimate": 9.05671393876355,
          "standard_error": 0.004256540585907582
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001641982454186745,
            "upper_bound": 0.019900031130377686
          },
          "point_estimate": 0.01161111407042009,
          "standard_error": 0.004492201318476372
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.050683682350456,
            "upper_bound": 9.061116893801511
          },
          "point_estimate": 9.055045574765446,
          "standard_error": 0.002651275759682251
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006317538761491311,
            "upper_bound": 0.013525582780188892
          },
          "point_estimate": 0.010937138350593505,
          "standard_error": 0.0018494550544845452
        }
      }
    },
    "memchr1/fallback/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/uncommon",
        "directory_name": "memchr1/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.86612921556041,
            "upper_bound": 34.90283000375458
          },
          "point_estimate": 34.88429230073983,
          "standard_error": 0.009362902845100872
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.86575200651987,
            "upper_bound": 34.905120160354876
          },
          "point_estimate": 34.88171316645945,
          "standard_error": 0.011189179195278062
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003906448009768187,
            "upper_bound": 0.054727070767784776
          },
          "point_estimate": 0.025160450060069967,
          "standard_error": 0.012083846062600706
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.863538732634744,
            "upper_bound": 34.91008729470516
          },
          "point_estimate": 34.88821627562295,
          "standard_error": 0.011899371672396662
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01512616298377913,
            "upper_bound": 0.04152760742983302
          },
          "point_estimate": 0.03124891252399289,
          "standard_error": 0.006863309378174758
        }
      }
    },
    "memchr1/krate/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/krate/empty/never",
        "directory_name": "memchr1/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2445091675520373,
            "upper_bound": 0.24508199433342784
          },
          "point_estimate": 0.24479228854762175,
          "standard_error": 0.00014667906857351783
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24433305789863088,
            "upper_bound": 0.2451228902199085
          },
          "point_estimate": 0.24482742119222203,
          "standard_error": 0.0002085525072464238
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00009566877090200904,
            "upper_bound": 0.0008827483364248804
          },
          "point_estimate": 0.0004815493351266143,
          "standard_error": 0.00019291413158417612
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24451010947510332,
            "upper_bound": 0.24512089393936495
          },
          "point_estimate": 0.2448557708959651,
          "standard_error": 0.00016045363325392214
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00028424974170894565,
            "upper_bound": 0.0006221389012496031
          },
          "point_estimate": 0.0004901122974252686,
          "standard_error": 0.00008767999310190202
        }
      }
    },
    "memchr1/krate/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/common",
        "directory_name": "memchr1/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219934.08735903288,
            "upper_bound": 220265.49896565435
          },
          "point_estimate": 220096.01868700032,
          "standard_error": 84.64992508437798
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219909.92700729927,
            "upper_bound": 220272.4720194647
          },
          "point_estimate": 220069.47189781023,
          "standard_error": 92.2565495713119
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.319025898958664,
            "upper_bound": 464.871746856363
          },
          "point_estimate": 257.2939977131386,
          "standard_error": 107.25938151741288
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219807.1704448423,
            "upper_bound": 220329.04898838975
          },
          "point_estimate": 220016.65731348944,
          "standard_error": 133.79012771898635
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.7543767115371,
            "upper_bound": 368.8903315952368
          },
          "point_estimate": 281.7870211777251,
          "standard_error": 58.12759186329968
        }
      }
    },
    "memchr1/krate/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/never",
        "directory_name": "memchr1/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8553.590396414082,
            "upper_bound": 8565.157492872098
          },
          "point_estimate": 8559.226871931498,
          "standard_error": 2.962042365881864
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8550.155182567727,
            "upper_bound": 8570.526266195524
          },
          "point_estimate": 8557.298442612224,
          "standard_error": 5.557671395634366
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7973122296615327,
            "upper_bound": 16.371210433734426
          },
          "point_estimate": 10.953781758809772,
          "standard_error": 4.82573460622481
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8554.344419959843,
            "upper_bound": 8565.462942513408
          },
          "point_estimate": 8560.039842748536,
          "standard_error": 2.7866563966203555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.612811889706403,
            "upper_bound": 11.40557493174549
          },
          "point_estimate": 9.906617863134548,
          "standard_error": 1.3927419885608885
        }
      }
    },
    "memchr1/krate/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/rare",
        "directory_name": "memchr1/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10226.874044448025,
            "upper_bound": 10249.44847500078
          },
          "point_estimate": 10237.497448353435,
          "standard_error": 5.801647746268062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10223.951651929674,
            "upper_bound": 10252.176070724148
          },
          "point_estimate": 10229.7410397295,
          "standard_error": 7.150655302223488
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6629692650212149,
            "upper_bound": 30.912668977816928
          },
          "point_estimate": 12.93066876874475,
          "standard_error": 8.098765797666093
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10225.965531152871,
            "upper_bound": 10242.915640184126
          },
          "point_estimate": 10232.28641175674,
          "standard_error": 4.326790641166177
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.495022060164237,
            "upper_bound": 23.723537006843895
          },
          "point_estimate": 19.272384813034712,
          "standard_error": 3.9029157842873095
        }
      }
    },
    "memchr1/krate/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/uncommon",
        "directory_name": "memchr1/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78996.04614848673,
            "upper_bound": 79143.9984040905
          },
          "point_estimate": 79077.07287625245,
          "standard_error": 38.18539785748549
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79004.85374186552,
            "upper_bound": 79178.75271149675
          },
          "point_estimate": 79112.67640223118,
          "standard_error": 39.116962691114914
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.297308207981565,
            "upper_bound": 187.5669869628605
          },
          "point_estimate": 100.39178162332028,
          "standard_error": 43.998675616302116
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78930.39231029512,
            "upper_bound": 79127.11671510013
          },
          "point_estimate": 79041.14441783812,
          "standard_error": 50.643054180573266
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.53561450109239,
            "upper_bound": 170.98082004097662
          },
          "point_estimate": 127.34913670179378,
          "standard_error": 33.36569135288069
        }
      }
    },
    "memchr1/krate/huge/verycommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/verycommon",
        "directory_name": "memchr1/krate_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 459506.7545265219,
            "upper_bound": 460619.4078553847
          },
          "point_estimate": 460001.1417048423,
          "standard_error": 287.05662237446273
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 459421.99829214386,
            "upper_bound": 460457.8971518987
          },
          "point_estimate": 459596.2784810127,
          "standard_error": 349.23631096586394
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.742027086171504,
            "upper_bound": 1395.6310747795922
          },
          "point_estimate": 715.2971434191105,
          "standard_error": 363.251441702964
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 459436.7187167637,
            "upper_bound": 460085.4788689261
          },
          "point_estimate": 459698.38379089267,
          "standard_error": 168.8550083764881
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398.5616298723511,
            "upper_bound": 1336.6731048530614
          },
          "point_estimate": 953.9322151154712,
          "standard_error": 277.9730706075154
        }
      }
    },
    "memchr1/krate/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/common",
        "directory_name": "memchr1/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.5463116940863,
            "upper_bound": 204.86574821789745
          },
          "point_estimate": 204.6995656244691,
          "standard_error": 0.08148017670160325
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.49893162994704,
            "upper_bound": 204.8713750021114
          },
          "point_estimate": 204.67319586846867,
          "standard_error": 0.1144097526250248
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016740573582205053,
            "upper_bound": 0.4899260472852176
          },
          "point_estimate": 0.28619751868865445,
          "standard_error": 0.1069267467869022
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.54242720528785,
            "upper_bound": 204.79653746401115
          },
          "point_estimate": 204.68087505911015,
          "standard_error": 0.06606813685636087
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14533236801008534,
            "upper_bound": 0.3574159003670366
          },
          "point_estimate": 0.2716435015248239,
          "standard_error": 0.05631399867531241
        }
      }
    },
    "memchr1/krate/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/never",
        "directory_name": "memchr1/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.548151152279089,
            "upper_bound": 7.57377177474004
          },
          "point_estimate": 7.558964133836804,
          "standard_error": 0.00670294784484791
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.546075397030961,
            "upper_bound": 7.563983580559053
          },
          "point_estimate": 7.554002553021743,
          "standard_error": 0.0047495193705088075
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003026585422788243,
            "upper_bound": 0.02298591385014476
          },
          "point_estimate": 0.013275336213690009,
          "standard_error": 0.005133642734358926
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.5479679053024515,
            "upper_bound": 7.561973012160755
          },
          "point_estimate": 7.555140832426793,
          "standard_error": 0.003684970467869483
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006538754067046674,
            "upper_bound": 0.03288674223918954
          },
          "point_estimate": 0.022267914890759268,
          "standard_error": 0.008263420219765309
        }
      }
    },
    "memchr1/krate/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/rare",
        "directory_name": "memchr1/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.11208441043354,
            "upper_bound": 12.177418490076452
          },
          "point_estimate": 12.136918619094322,
          "standard_error": 0.01834916846977085
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.108251745129612,
            "upper_bound": 12.136823278812368
          },
          "point_estimate": 12.113240678454016,
          "standard_error": 0.008169146853806635
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018941062044580256,
            "upper_bound": 0.031844899318125314
          },
          "point_estimate": 0.009290698619902938,
          "standard_error": 0.009426786601504093
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.110341568082257,
            "upper_bound": 12.127011246820391
          },
          "point_estimate": 12.115599849749149,
          "standard_error": 0.004422199256331977
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00620441601014283,
            "upper_bound": 0.09359219264463448
          },
          "point_estimate": 0.06128482731520968,
          "standard_error": 0.029100378982878896
        }
      }
    },
    "memchr1/krate/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/uncommon",
        "directory_name": "memchr1/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.056311663458395,
            "upper_bound": 46.11129207285981
          },
          "point_estimate": 46.08482887403254,
          "standard_error": 0.014090711169386476
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.03271158994102,
            "upper_bound": 46.13355879932386
          },
          "point_estimate": 46.093405084552565,
          "standard_error": 0.02040340858720236
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028036495699763973,
            "upper_bound": 0.06727948189179389
          },
          "point_estimate": 0.06065241625762703,
          "standard_error": 0.02187043181706422
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.04720156026229,
            "upper_bound": 46.11230485916977
          },
          "point_estimate": 46.07851151472201,
          "standard_error": 0.01714333285424188
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02208242526259015,
            "upper_bound": 0.0565255359342203
          },
          "point_estimate": 0.04707156252533805,
          "standard_error": 0.007814669130209678
        }
      }
    },
    "memchr1/krate/small/verycommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/verycommon",
        "directory_name": "memchr1/krate_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 493.8158496444433,
            "upper_bound": 494.76979517049983
          },
          "point_estimate": 494.31823815275686,
          "standard_error": 0.24316841076309048
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 493.8624737759856,
            "upper_bound": 494.9383071547285
          },
          "point_estimate": 494.3079697542133,
          "standard_error": 0.2801754799065467
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.213014428553489,
            "upper_bound": 1.339701282534692
          },
          "point_estimate": 0.6837509838059749,
          "standard_error": 0.29094321117597444
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 494.0307174843507,
            "upper_bound": 494.5341207120272
          },
          "point_estimate": 494.2821946328744,
          "standard_error": 0.12784603068723835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.38319941506853633,
            "upper_bound": 1.0938292183276654
          },
          "point_estimate": 0.8079945831941404,
          "standard_error": 0.1921657070894684
        }
      }
    },
    "memchr1/krate/tiny/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/common",
        "directory_name": "memchr1/krate_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.840146329709114,
            "upper_bound": 51.895642590435976
          },
          "point_estimate": 51.86966072054785,
          "standard_error": 0.014126025641865644
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.85004199575657,
            "upper_bound": 51.897715809124485
          },
          "point_estimate": 51.87648200689188,
          "standard_error": 0.012970323112281716
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032186130437186636,
            "upper_bound": 0.06993635156992802
          },
          "point_estimate": 0.03662416476396803,
          "standard_error": 0.016897537602406622
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.855539847656296,
            "upper_bound": 51.8904488276441
          },
          "point_estimate": 51.869834682125955,
          "standard_error": 0.008962349475422475
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017312548468997976,
            "upper_bound": 0.06616382118755215
          },
          "point_estimate": 0.04709423537399779,
          "standard_error": 0.013036366347315776
        }
      }
    },
    "memchr1/krate/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/never",
        "directory_name": "memchr1/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6779542882469607,
            "upper_bound": 3.6892142526877905
          },
          "point_estimate": 3.683792279991251,
          "standard_error": 0.0028889397292521835
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6777411072032495,
            "upper_bound": 3.691798173262925
          },
          "point_estimate": 3.6849885768640167,
          "standard_error": 0.003466774621250016
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022445861293772457,
            "upper_bound": 0.01623741833194242
          },
          "point_estimate": 0.010372185402947844,
          "standard_error": 0.0035263309523996134
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.681998695658119,
            "upper_bound": 3.691406492006739
          },
          "point_estimate": 3.687328891949876,
          "standard_error": 0.0023805995075971208
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004997579523343588,
            "upper_bound": 0.01252459389769567
          },
          "point_estimate": 0.009651082790995706,
          "standard_error": 0.001979256432432881
        }
      }
    },
    "memchr1/krate/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/rare",
        "directory_name": "memchr1/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.998415890037241,
            "upper_bound": 6.0456095937456205
          },
          "point_estimate": 6.0214921711721505,
          "standard_error": 0.012051566536828285
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.992250759415746,
            "upper_bound": 6.060989632493111
          },
          "point_estimate": 6.012794061533834,
          "standard_error": 0.016941448542663298
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006970597779389833,
            "upper_bound": 0.0698273947838889
          },
          "point_estimate": 0.0398674897253757,
          "standard_error": 0.016039973580920512
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.990305053517662,
            "upper_bound": 6.03607254240466
          },
          "point_estimate": 6.00959860176416,
          "standard_error": 0.011698903871237312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02301059982789697,
            "upper_bound": 0.04938930003594367
          },
          "point_estimate": 0.040178048353597104,
          "standard_error": 0.006643030589637811
        }
      }
    },
    "memchr1/krate/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/uncommon",
        "directory_name": "memchr1/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.93973000060741,
            "upper_bound": 17.979738176763174
          },
          "point_estimate": 17.95896838674786,
          "standard_error": 0.010234361741468176
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.934651849518623,
            "upper_bound": 17.984060273523333
          },
          "point_estimate": 17.948122253777136,
          "standard_error": 0.01262279270143227
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0031281708383005826,
            "upper_bound": 0.05503890726933264
          },
          "point_estimate": 0.03371569008046139,
          "standard_error": 0.014306603605707275
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.92384659552377,
            "upper_bound": 17.96661751034645
          },
          "point_estimate": 17.943575299939564,
          "standard_error": 0.011298290133199477
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017400115321655255,
            "upper_bound": 0.04380114244970285
          },
          "point_estimate": 0.03406900840453778,
          "standard_error": 0.00681019490197513
        }
      }
    },
    "memchr1/libc/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/libc/empty/never",
        "directory_name": "memchr1/libc_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.614731071629502,
            "upper_bound": 2.647327732562166
          },
          "point_estimate": 2.6315220981504037,
          "standard_error": 0.008342541604655819
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.611831966615803,
            "upper_bound": 2.6552860121925868
          },
          "point_estimate": 2.63221756935866,
          "standard_error": 0.011208233362814467
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00863425694922444,
            "upper_bound": 0.04836775461945536
          },
          "point_estimate": 0.027586702357282725,
          "standard_error": 0.009988518716315266
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.613708973327853,
            "upper_bound": 2.652105124659456
          },
          "point_estimate": 2.6313485755458546,
          "standard_error": 0.010074879502867316
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015469648999186748,
            "upper_bound": 0.03516276142592036
          },
          "point_estimate": 0.02775450414294797,
          "standard_error": 0.005111801894153743
        }
      }
    },
    "memchr1/libc/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/common",
        "directory_name": "memchr1/libc_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 281028.36323568376,
            "upper_bound": 281434.8346886447
          },
          "point_estimate": 281215.32930647134,
          "standard_error": 104.68460539648274
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280969.31871794874,
            "upper_bound": 281496.9841880342
          },
          "point_estimate": 281113.96038461535,
          "standard_error": 113.92561381152407
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.19260322129621,
            "upper_bound": 522.7556089243535
          },
          "point_estimate": 232.38899241272424,
          "standard_error": 114.63860860053738
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 281039.03547297296,
            "upper_bound": 281445.1761141305
          },
          "point_estimate": 281210.25298701297,
          "standard_error": 104.10046516803904
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120.67320682236311,
            "upper_bound": 437.402307498885
          },
          "point_estimate": 348.684192571706,
          "standard_error": 81.77152370031546
        }
      }
    },
    "memchr1/libc/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/never",
        "directory_name": "memchr1/libc_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9147.389098995283,
            "upper_bound": 9174.738562916382
          },
          "point_estimate": 9159.536991969464,
          "standard_error": 7.072885484329643
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9140.581270481473,
            "upper_bound": 9172.21111671288
          },
          "point_estimate": 9153.197031762036,
          "standard_error": 7.632379665904629
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3923078377526176,
            "upper_bound": 32.03836129659395
          },
          "point_estimate": 18.20381992637633,
          "standard_error": 8.597272208111717
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9143.269930047894,
            "upper_bound": 9164.50387394994
          },
          "point_estimate": 9151.933911261413,
          "standard_error": 5.447976393092868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.21552452768267,
            "upper_bound": 32.581622987233665
          },
          "point_estimate": 23.54141116995746,
          "standard_error": 6.80648893316193
        }
      }
    },
    "memchr1/libc/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/rare",
        "directory_name": "memchr1/libc_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10147.022916949089,
            "upper_bound": 10158.947039724626
          },
          "point_estimate": 10153.190275540584,
          "standard_error": 3.054908294931282
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10145.77772816076,
            "upper_bound": 10161.86613871058
          },
          "point_estimate": 10153.340828216578,
          "standard_error": 4.507607321951178
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6116367739588666,
            "upper_bound": 17.534724450068218
          },
          "point_estimate": 10.13167739827384,
          "standard_error": 4.066694308733427
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10152.056071340903,
            "upper_bound": 10162.796137080571
          },
          "point_estimate": 10158.469212841304,
          "standard_error": 2.769316581846536
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.643631538031962,
            "upper_bound": 12.964049846901412
          },
          "point_estimate": 10.147921474657982,
          "standard_error": 1.9367146171357996
        }
      }
    },
    "memchr1/libc/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/uncommon",
        "directory_name": "memchr1/libc_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73138.88376260882,
            "upper_bound": 73262.01490471391
          },
          "point_estimate": 73198.23176739312,
          "standard_error": 31.518427695479197
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73110.01052867384,
            "upper_bound": 73298.47849462366
          },
          "point_estimate": 73179.04111463134,
          "standard_error": 44.90990660265851
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.25358809455949,
            "upper_bound": 178.27285749028164
          },
          "point_estimate": 108.18524309781291,
          "standard_error": 39.94066042996062
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73090.75515232974,
            "upper_bound": 73179.6022647197
          },
          "point_estimate": 73123.43603896104,
          "standard_error": 22.805773417764467
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.46033217365681,
            "upper_bound": 130.76705068434904
          },
          "point_estimate": 105.39428048581898,
          "standard_error": 18.55507207827652
        }
      }
    },
    "memchr1/libc/huge/verycommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/verycommon",
        "directory_name": "memchr1/libc_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 586910.3586461532,
            "upper_bound": 587808.7394060419
          },
          "point_estimate": 587353.4972747056,
          "standard_error": 230.07377856408783
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 586751.1205645162,
            "upper_bound": 588104.6290322581
          },
          "point_estimate": 587139.7652329749,
          "standard_error": 382.1327055702293
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.93778947798643,
            "upper_bound": 1260.7787631005615
          },
          "point_estimate": 921.9296850840952,
          "standard_error": 307.77889184813813
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 586796.1081870781,
            "upper_bound": 587723.7226847034
          },
          "point_estimate": 587194.5586510263,
          "standard_error": 236.74429038043743
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 466.19516960972373,
            "upper_bound": 935.9297260375392
          },
          "point_estimate": 770.1791078912771,
          "standard_error": 119.1007295988686
        }
      }
    },
    "memchr1/libc/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/common",
        "directory_name": "memchr1/libc_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.60319169236269,
            "upper_bound": 220.2511522830889
          },
          "point_estimate": 219.90478056889756,
          "standard_error": 0.16615035733445294
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.45027635437816,
            "upper_bound": 220.30587112056463
          },
          "point_estimate": 219.7113247637291,
          "standard_error": 0.2166639341366596
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07148621029910536,
            "upper_bound": 0.890368786099872
          },
          "point_estimate": 0.393221857761386,
          "standard_error": 0.2124293732900731
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.55704265385603,
            "upper_bound": 220.2064596160012
          },
          "point_estimate": 219.79212244054892,
          "standard_error": 0.16644428532128824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.17982331685401748,
            "upper_bound": 0.6692355237513049
          },
          "point_estimate": 0.5509278096057111,
          "standard_error": 0.11316397447492887
        }
      }
    },
    "memchr1/libc/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/never",
        "directory_name": "memchr1/libc_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.398680111694999,
            "upper_bound": 7.430855756987978
          },
          "point_estimate": 7.415535348338589,
          "standard_error": 0.008245496250315384
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.398975830527558,
            "upper_bound": 7.441613149479792
          },
          "point_estimate": 7.413268992527531,
          "standard_error": 0.01381576277779542
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00240094049819733,
            "upper_bound": 0.05542187521833837
          },
          "point_estimate": 0.03104767961105181,
          "standard_error": 0.01315491122836858
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.406081338374364,
            "upper_bound": 7.44124547659256
          },
          "point_estimate": 7.427857888049383,
          "standard_error": 0.008998044345747946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016173479914866794,
            "upper_bound": 0.035310602227117294
          },
          "point_estimate": 0.02743747744147761,
          "standard_error": 0.005371199938254093
        }
      }
    },
    "memchr1/libc/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/rare",
        "directory_name": "memchr1/libc_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.215323013287271,
            "upper_bound": 11.238080208529952
          },
          "point_estimate": 11.225505762076889,
          "standard_error": 0.0058911356627716835
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.212590289348556,
            "upper_bound": 11.236695127568122
          },
          "point_estimate": 11.220267441011671,
          "standard_error": 0.0052641058808031976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002504599676098419,
            "upper_bound": 0.02841526876864558
          },
          "point_estimate": 0.009008585217988462,
          "standard_error": 0.006304927827784064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.21388833658932,
            "upper_bound": 11.22938052360688
          },
          "point_estimate": 11.218845661978412,
          "standard_error": 0.004033259008205664
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005792895050888118,
            "upper_bound": 0.02662919182037578
          },
          "point_estimate": 0.0196564915020513,
          "standard_error": 0.005595945358464154
        }
      }
    },
    "memchr1/libc/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/uncommon",
        "directory_name": "memchr1/libc_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.31112141414616,
            "upper_bound": 47.434181886421946
          },
          "point_estimate": 47.37172687465969,
          "standard_error": 0.031488438454876574
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.278872646779185,
            "upper_bound": 47.477143607464434
          },
          "point_estimate": 47.3374709668265,
          "standard_error": 0.05552426342051331
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015796718053939684,
            "upper_bound": 0.16597837030287416
          },
          "point_estimate": 0.12964879488078185,
          "standard_error": 0.04091485055668791
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.318353818059734,
            "upper_bound": 47.476184827644616
          },
          "point_estimate": 47.40723468711675,
          "standard_error": 0.04131049965767422
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06637613012367889,
            "upper_bound": 0.12531314707165153
          },
          "point_estimate": 0.10515924862466544,
          "standard_error": 0.015174404225004353
        }
      }
    },
    "memchr1/libc/small/verycommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/verycommon",
        "directory_name": "memchr1/libc_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 537.1850455197398,
            "upper_bound": 545.8464804657817
          },
          "point_estimate": 540.989354995545,
          "standard_error": 2.2250724104540875
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 536.4918743084753,
            "upper_bound": 546.0976555776843
          },
          "point_estimate": 538.138859445305,
          "standard_error": 2.0430824110941344
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5213555644329609,
            "upper_bound": 10.282664326555974
          },
          "point_estimate": 2.6451398367525947,
          "standard_error": 2.066446537200278
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 537.0483956627572,
            "upper_bound": 548.3917725163598
          },
          "point_estimate": 541.2046168893436,
          "standard_error": 2.930345163958614
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.392399301793258,
            "upper_bound": 9.497093674638467
          },
          "point_estimate": 7.411836452322636,
          "standard_error": 2.168498040392528
        }
      }
    },
    "memchr1/libc/tiny/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/common",
        "directory_name": "memchr1/libc_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.41861910162651,
            "upper_bound": 49.495159141061954
          },
          "point_estimate": 49.45767054770478,
          "standard_error": 0.019566798257501766
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.422498916073906,
            "upper_bound": 49.50188410526245
          },
          "point_estimate": 49.45779780963841,
          "standard_error": 0.0273619308918446
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003656346477218008,
            "upper_bound": 0.1125767747206137
          },
          "point_estimate": 0.04984588175192301,
          "standard_error": 0.030206088205995643
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.42189149507951,
            "upper_bound": 49.52310826709243
          },
          "point_estimate": 49.470263805902256,
          "standard_error": 0.028166184369129096
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0328029597359613,
            "upper_bound": 0.08596686566624726
          },
          "point_estimate": 0.06478853036127653,
          "standard_error": 0.013946963190583478
        }
      }
    },
    "memchr1/libc/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/never",
        "directory_name": "memchr1/libc_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.703993952472116,
            "upper_bound": 3.7168547289866902
          },
          "point_estimate": 3.7104180825858153,
          "standard_error": 0.0033030346480483934
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.7017314427490686,
            "upper_bound": 3.7207544845564726
          },
          "point_estimate": 3.7085524722797727,
          "standard_error": 0.005239946284383618
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0027990331328092825,
            "upper_bound": 0.018890363790507295
          },
          "point_estimate": 0.01409284896154198,
          "standard_error": 0.00417353335366702
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.700388475774566,
            "upper_bound": 3.717592795475304
          },
          "point_estimate": 3.708824185195144,
          "standard_error": 0.0045913546220944955
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006800580037584111,
            "upper_bound": 0.013170884414265395
          },
          "point_estimate": 0.010955854633361968,
          "standard_error": 0.001626278376283163
        }
      }
    },
    "memchr1/libc/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/rare",
        "directory_name": "memchr1/libc_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.100429670660762,
            "upper_bound": 6.1273841916457314
          },
          "point_estimate": 6.113245078952973,
          "standard_error": 0.006918952829328883
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.096363204279518,
            "upper_bound": 6.1287963654259014
          },
          "point_estimate": 6.1108947881189435,
          "standard_error": 0.007589400447560051
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003948810478359845,
            "upper_bound": 0.03741890467883457
          },
          "point_estimate": 0.024042701930970807,
          "standard_error": 0.009045538286643211
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.096062866610364,
            "upper_bound": 6.134885937419669
          },
          "point_estimate": 6.113144865765706,
          "standard_error": 0.01003378485362083
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011309363875510218,
            "upper_bound": 0.03046172202936666
          },
          "point_estimate": 0.02301979030329113,
          "standard_error": 0.00510694356238395
        }
      }
    },
    "memchr1/libc/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/libc/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/uncommon",
        "directory_name": "memchr1/libc_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.74750865172007,
            "upper_bound": 18.790869088366293
          },
          "point_estimate": 18.76888998720384,
          "standard_error": 0.01107332831678323
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.73390348494457,
            "upper_bound": 18.80281609830528
          },
          "point_estimate": 18.76328173359691,
          "standard_error": 0.01699248512117059
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0111918544069627,
            "upper_bound": 0.06274262784928636
          },
          "point_estimate": 0.04502313810890488,
          "standard_error": 0.01359121158808187
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.747568743633547,
            "upper_bound": 18.787815669802743
          },
          "point_estimate": 18.762881147494696,
          "standard_error": 0.01032311405161527
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021861834992709283,
            "upper_bound": 0.04463235110374396
          },
          "point_estimate": 0.036758554461884245,
          "standard_error": 0.005778997352530015
        }
      }
    },
    "memchr1/naive/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/naive/empty/never",
        "directory_name": "memchr1/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4889965595147712,
            "upper_bound": 0.49061021453024206
          },
          "point_estimate": 0.4897056177698841,
          "standard_error": 0.0004177051708177243
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4889476370331125,
            "upper_bound": 0.4904289825163044
          },
          "point_estimate": 0.4891701058507024,
          "standard_error": 0.00035433959010702836
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00005731569608649752,
            "upper_bound": 0.001899691157281546
          },
          "point_estimate": 0.0004698236148736876,
          "standard_error": 0.0004614065385144984
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.48901444247999937,
            "upper_bound": 0.4910742033214116
          },
          "point_estimate": 0.4900272129659935,
          "standard_error": 0.0005387971118233334
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00034439269154287587,
            "upper_bound": 0.0019187339993187616
          },
          "point_estimate": 0.0013927310376277491,
          "standard_error": 0.0004259255857546205
        }
      }
    },
    "memchr1/naive/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/common",
        "directory_name": "memchr1/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 455283.3170982142,
            "upper_bound": 458849.0936319445
          },
          "point_estimate": 456600.1253551587,
          "standard_error": 1023.898963730523
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 455084.2359375,
            "upper_bound": 456413.6475
          },
          "point_estimate": 455405.33697916666,
          "standard_error": 432.6975156448854
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.9697429482239,
            "upper_bound": 1559.838232911536
          },
          "point_estimate": 637.5271446191458,
          "standard_error": 445.47852121652113
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 455133.6297826562,
            "upper_bound": 456319.70812655846
          },
          "point_estimate": 455604.7366883117,
          "standard_error": 312.217415759291
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 383.30371940224944,
            "upper_bound": 5231.7276902764015
          },
          "point_estimate": 3405.911480759806,
          "standard_error": 1699.033689183983
        }
      }
    },
    "memchr1/naive/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/never",
        "directory_name": "memchr1/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145411.8532002624,
            "upper_bound": 145587.09989220896
          },
          "point_estimate": 145501.34912982988,
          "standard_error": 45.069028645172715
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145350.79316069058,
            "upper_bound": 145644.57414785304
          },
          "point_estimate": 145530.0920318725,
          "standard_error": 76.10281156992096
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.77471108304485,
            "upper_bound": 255.45998242351857
          },
          "point_estimate": 180.56573172129052,
          "standard_error": 58.32142733676842
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145414.4036382771,
            "upper_bound": 145593.81469019764
          },
          "point_estimate": 145511.63488384127,
          "standard_error": 45.3815670216288
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.57253703754115,
            "upper_bound": 178.8507414860448
          },
          "point_estimate": 150.27272240905134,
          "standard_error": 22.371111423571143
        }
      }
    },
    "memchr1/naive/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/rare",
        "directory_name": "memchr1/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146859.25811304324,
            "upper_bound": 147074.86719148423
          },
          "point_estimate": 146964.23171546977,
          "standard_error": 55.02409654840597
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146835.6468894009,
            "upper_bound": 147080.06088709677
          },
          "point_estimate": 146960.09495967743,
          "standard_error": 65.83536131339262
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.499967991658735,
            "upper_bound": 307.38372967390796
          },
          "point_estimate": 177.151940171459,
          "standard_error": 69.61010220878319
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146868.35697857226,
            "upper_bound": 147063.15172421432
          },
          "point_estimate": 146978.33566191874,
          "standard_error": 50.15905831644984
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.72085634461176,
            "upper_bound": 243.56326236578147
          },
          "point_estimate": 183.7836886257735,
          "standard_error": 39.20709579359552
        }
      }
    },
    "memchr1/naive/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/uncommon",
        "directory_name": "memchr1/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 197972.25770445136,
            "upper_bound": 198071.07488989717
          },
          "point_estimate": 198022.2311333247,
          "standard_error": 25.34693335331487
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 197916.948757764,
            "upper_bound": 198079.17934782608
          },
          "point_estimate": 198034.55480072464,
          "standard_error": 35.727036216320414
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.0063456719766375,
            "upper_bound": 170.5038315381644
          },
          "point_estimate": 82.67492468710172,
          "standard_error": 43.11707930763982
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 197973.0181428428,
            "upper_bound": 198105.1372458226
          },
          "point_estimate": 198049.88627893844,
          "standard_error": 33.420642824350594
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.271534211938786,
            "upper_bound": 104.75741695787283
          },
          "point_estimate": 84.58019160588391,
          "standard_error": 14.423576537130344
        }
      }
    },
    "memchr1/naive/huge/verycommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/verycommon",
        "directory_name": "memchr1/naive_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 913664.1038888888,
            "upper_bound": 915212.0096874998
          },
          "point_estimate": 914395.7201319444,
          "standard_error": 395.85283458150025
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 913197.575,
            "upper_bound": 915036.8109375
          },
          "point_estimate": 914422.5975,
          "standard_error": 461.5343824993851
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 336.9282570183346,
            "upper_bound": 2342.5594375779615
          },
          "point_estimate": 1079.6330073327897,
          "standard_error": 513.5516968098764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 913484.6536073826,
            "upper_bound": 914668.7222958663
          },
          "point_estimate": 914081.1356493508,
          "standard_error": 295.95682996286837
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 663.4131620607016,
            "upper_bound": 1775.9195065258136
          },
          "point_estimate": 1317.5217492139486,
          "standard_error": 309.2636216732584
        }
      }
    },
    "memchr1/naive/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/common",
        "directory_name": "memchr1/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 267.0444084761863,
            "upper_bound": 270.1845393206712
          },
          "point_estimate": 268.59548263771575,
          "standard_error": 0.805204712608974
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265.97363050295985,
            "upper_bound": 270.81092365997324
          },
          "point_estimate": 268.83345900801874,
          "standard_error": 1.3265688523304144
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6433993584086014,
            "upper_bound": 4.599610640115196
          },
          "point_estimate": 3.5077850005637945,
          "standard_error": 1.050543909846336
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266.8902802896381,
            "upper_bound": 269.3523736301929
          },
          "point_estimate": 267.88883907050246,
          "standard_error": 0.6279957092352251
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.662331921244521,
            "upper_bound": 3.2303889370536703
          },
          "point_estimate": 2.6835536731557688,
          "standard_error": 0.4023127370134059
        }
      }
    },
    "memchr1/naive/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/never",
        "directory_name": "memchr1/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.54529124551783,
            "upper_bound": 173.74797057351873
          },
          "point_estimate": 173.6379066554254,
          "standard_error": 0.05216353215019773
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.51219460738417,
            "upper_bound": 173.7438093509372
          },
          "point_estimate": 173.57782406567057,
          "standard_error": 0.05871028411566904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025624048114923255,
            "upper_bound": 0.2664411992697694
          },
          "point_estimate": 0.11804493265407896,
          "standard_error": 0.06104061165578957
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.53413834366629,
            "upper_bound": 173.7173536857574
          },
          "point_estimate": 173.61312373928305,
          "standard_error": 0.04693992921014088
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06419398290895036,
            "upper_bound": 0.23029765021699547
          },
          "point_estimate": 0.1736975392075313,
          "standard_error": 0.04370989671709051
        }
      }
    },
    "memchr1/naive/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/rare",
        "directory_name": "memchr1/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 183.09058999752884,
            "upper_bound": 183.40948892582347
          },
          "point_estimate": 183.24589777428275,
          "standard_error": 0.08175217132601827
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 183.00297583625,
            "upper_bound": 183.50744041615212
          },
          "point_estimate": 183.21419666542263,
          "standard_error": 0.12788198592541647
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08108368162433685,
            "upper_bound": 0.47121919066992807
          },
          "point_estimate": 0.2802520217309122,
          "standard_error": 0.09975587283344774
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 183.0113991061488,
            "upper_bound": 183.39711239734632
          },
          "point_estimate": 183.16984050717244,
          "standard_error": 0.0996588125064224
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1606756189137658,
            "upper_bound": 0.33092589653102517
          },
          "point_estimate": 0.2732377648146216,
          "standard_error": 0.04327509368583716
        }
      }
    },
    "memchr1/naive/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/uncommon",
        "directory_name": "memchr1/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205.1986737967313,
            "upper_bound": 207.6562883591748
          },
          "point_estimate": 206.3370305784269,
          "standard_error": 0.6316881547792155
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.8843619506975,
            "upper_bound": 207.6166750573679
          },
          "point_estimate": 206.0351906009062,
          "standard_error": 0.5858221523728258
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.224646118546136,
            "upper_bound": 3.525239969544304
          },
          "point_estimate": 1.4057110087685998,
          "standard_error": 0.8446771656623756
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.9279305414811,
            "upper_bound": 206.46572641712137
          },
          "point_estimate": 205.5967119421705,
          "standard_error": 0.3979182873332868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8698355452202964,
            "upper_bound": 2.7754660677033565
          },
          "point_estimate": 2.1019571652947224,
          "standard_error": 0.5051533349714381
        }
      }
    },
    "memchr1/naive/small/verycommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/verycommon",
        "directory_name": "memchr1/naive_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.5287419940313,
            "upper_bound": 340.32445230654
          },
          "point_estimate": 337.8183560974325,
          "standard_error": 1.224781370807655
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.2176831506938,
            "upper_bound": 341.4877481279065
          },
          "point_estimate": 336.63014961256886,
          "standard_error": 2.098737789723429
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7003270896635666,
            "upper_bound": 6.502554584386483
          },
          "point_estimate": 4.508799495536789,
          "standard_error": 1.6217025673659022
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.7503455342247,
            "upper_bound": 340.9775524471113
          },
          "point_estimate": 337.52758867228374,
          "standard_error": 1.5857850556139033
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.393604095500037,
            "upper_bound": 4.971884033865189
          },
          "point_estimate": 4.087155781390729,
          "standard_error": 0.6709141542851267
        }
      }
    },
    "memchr1/naive/tiny/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/common",
        "directory_name": "memchr1/naive_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.99011482110715,
            "upper_bound": 40.0627126940022
          },
          "point_estimate": 40.0231388739556,
          "standard_error": 0.01864090213134691
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.96926675675408,
            "upper_bound": 40.0461115387877
          },
          "point_estimate": 40.02375679188964,
          "standard_error": 0.018207381319863426
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006384624344393535,
            "upper_bound": 0.0919000796453505
          },
          "point_estimate": 0.04349160431461663,
          "standard_error": 0.02210269033399017
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.993766618328415,
            "upper_bound": 40.04450745708246
          },
          "point_estimate": 40.02427466268031,
          "standard_error": 0.01300272320769005
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026075232970081946,
            "upper_bound": 0.08742036977785692
          },
          "point_estimate": 0.06220722008530628,
          "standard_error": 0.01780991720618061
        }
      }
    },
    "memchr1/naive/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/never",
        "directory_name": "memchr1/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.10532122096738,
            "upper_bound": 27.4051619531948
          },
          "point_estimate": 27.2740555061584,
          "standard_error": 0.07790737026252095
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.164509153046332,
            "upper_bound": 27.445800106126633
          },
          "point_estimate": 27.37244798767769,
          "standard_error": 0.08501927312842479
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016611311812093275,
            "upper_bound": 0.347119446770385
          },
          "point_estimate": 0.16037507269283854,
          "standard_error": 0.08642913240564122
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.33095715357978,
            "upper_bound": 27.455596834120342
          },
          "point_estimate": 27.406609968184465,
          "standard_error": 0.032311090874802925
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09121220417591028,
            "upper_bound": 0.36926575472549306
          },
          "point_estimate": 0.2597579690336854,
          "standard_error": 0.08185520160096967
        }
      }
    },
    "memchr1/naive/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/rare",
        "directory_name": "memchr1/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.375993586848825,
            "upper_bound": 27.701511998431325
          },
          "point_estimate": 27.50514790424213,
          "standard_error": 0.08853820489123242
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.34234214097428,
            "upper_bound": 27.531693288104293
          },
          "point_estimate": 27.42488089744843,
          "standard_error": 0.05526042507924638
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010438740579384384,
            "upper_bound": 0.22213136317157667
          },
          "point_estimate": 0.12316925665612125,
          "standard_error": 0.05471149476422397
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.3372171730173,
            "upper_bound": 27.50432838160891
          },
          "point_estimate": 27.402618025541184,
          "standard_error": 0.04313614312527927
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06072719188692339,
            "upper_bound": 0.4474294831451824
          },
          "point_estimate": 0.2952443462812011,
          "standard_error": 0.1275579340854043
        }
      }
    },
    "memchr1/naive/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr1/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/uncommon",
        "directory_name": "memchr1/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.66899001104972,
            "upper_bound": 27.09717620695327
          },
          "point_estimate": 26.858836951254695,
          "standard_error": 0.11051114103104225
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.600045797686075,
            "upper_bound": 27.000001156038053
          },
          "point_estimate": 26.7931663938004,
          "standard_error": 0.11716626457831458
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027508419976378668,
            "upper_bound": 0.5197220258946786
          },
          "point_estimate": 0.2880309395456092,
          "standard_error": 0.1120934318045718
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.618162779331357,
            "upper_bound": 26.906094837067997
          },
          "point_estimate": 26.759235185857076,
          "standard_error": 0.07387779629238976
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14858042873367874,
            "upper_bound": 0.5250487640701137
          },
          "point_estimate": 0.36921905113598985,
          "standard_error": 0.113372171089461
        }
      }
    },
    "memchr2/fallback/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/fallback/empty/never",
        "directory_name": "memchr2/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.209470047935028,
            "upper_bound": 2.215828382744524
          },
          "point_estimate": 2.212699526718347,
          "standard_error": 0.0016218468065512038
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.2093205574280623,
            "upper_bound": 2.2171511923799017
          },
          "point_estimate": 2.2122960396403615,
          "standard_error": 0.0021849263800168025
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013583552411970785,
            "upper_bound": 0.009603401530187312
          },
          "point_estimate": 0.0055171176151633744,
          "standard_error": 0.0020341898753973224
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.209831878313843,
            "upper_bound": 2.21507416035014
          },
          "point_estimate": 2.2123157266871383,
          "standard_error": 0.00137870119346499
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002927192850014824,
            "upper_bound": 0.007049668107396969
          },
          "point_estimate": 0.005398832499059525,
          "standard_error": 0.0010747837900379616
        }
      }
    },
    "memchr2/fallback/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/common",
        "directory_name": "memchr2/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1222496.7236111113,
            "upper_bound": 1224975.7865728177
          },
          "point_estimate": 1223597.7447936507,
          "standard_error": 642.064956755293
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1222002.298611111,
            "upper_bound": 1224760.4261904762
          },
          "point_estimate": 1223087.9727777778,
          "standard_error": 635.6752224841534
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 336.89778135233405,
            "upper_bound": 2972.991833885365
          },
          "point_estimate": 1284.7387705245303,
          "standard_error": 702.4459690205771
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1222309.246922586,
            "upper_bound": 1223701.656060606
          },
          "point_estimate": 1222988.9242424243,
          "standard_error": 350.0635941025661
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 747.6255845977322,
            "upper_bound": 2974.625299965862
          },
          "point_estimate": 2139.474037251989,
          "standard_error": 627.1068442608329
        }
      }
    },
    "memchr2/fallback/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/never",
        "directory_name": "memchr2/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83605.17774457217,
            "upper_bound": 83774.1272610404
          },
          "point_estimate": 83675.20173827768,
          "standard_error": 44.67124190337744
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83584.07305236271,
            "upper_bound": 83721.22003284072
          },
          "point_estimate": 83622.83999042146,
          "standard_error": 39.75657592453552
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.420298985873997,
            "upper_bound": 160.596162367251
          },
          "point_estimate": 74.58246623145199,
          "standard_error": 37.77735091726753
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83598.1091574333,
            "upper_bound": 83704.3366730546
          },
          "point_estimate": 83653.08213763249,
          "standard_error": 27.99396116187396
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.28490106630059,
            "upper_bound": 220.55161300402264
          },
          "point_estimate": 148.89350864923094,
          "standard_error": 56.47798671542999
        }
      }
    },
    "memchr2/fallback/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/rare",
        "directory_name": "memchr2/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89684.51107553366,
            "upper_bound": 89869.85740496714
          },
          "point_estimate": 89770.28026409415,
          "standard_error": 47.61214324314858
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89659.13601532567,
            "upper_bound": 89864.14963054188
          },
          "point_estimate": 89716.47136699507,
          "standard_error": 53.22415231200492
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.089959988164175,
            "upper_bound": 259.8476064500124
          },
          "point_estimate": 112.44297472788404,
          "standard_error": 57.8577546520725
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89673.43716412898,
            "upper_bound": 89855.33384257238
          },
          "point_estimate": 89741.0829953298,
          "standard_error": 46.68192151499171
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.54976682599457,
            "upper_bound": 212.3921486872671
          },
          "point_estimate": 158.9808303360169,
          "standard_error": 38.98295813739168
        }
      }
    },
    "memchr2/fallback/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/uncommon",
        "directory_name": "memchr2/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326120.8180507459,
            "upper_bound": 327166.45882511337
          },
          "point_estimate": 326579.76337620465,
          "standard_error": 270.4434930409613
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326004.703125,
            "upper_bound": 326874.96507936507
          },
          "point_estimate": 326374.9893973214,
          "standard_error": 254.2725094199733
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.3833820476042,
            "upper_bound": 1135.3446083405886
          },
          "point_estimate": 622.6825335880103,
          "standard_error": 244.60272589796708
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326149.1204493893,
            "upper_bound": 326601.3861937215
          },
          "point_estimate": 326351.21869202226,
          "standard_error": 115.53804388143956
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332.64776179761793,
            "upper_bound": 1287.3704325987585
          },
          "point_estimate": 901.4293076135134,
          "standard_error": 286.9982541255638
        }
      }
    },
    "memchr2/fallback/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/common",
        "directory_name": "memchr2/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 448.4540839136989,
            "upper_bound": 449.4642124531319
          },
          "point_estimate": 448.9251388984492,
          "standard_error": 0.2571147229922978
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 448.3177760068968,
            "upper_bound": 449.280374726646
          },
          "point_estimate": 448.8697012565018,
          "standard_error": 0.2671026570321789
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017171018919488884,
            "upper_bound": 1.3521465791572078
          },
          "point_estimate": 0.5952928454712065,
          "standard_error": 0.32833646058599036
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 448.362740759978,
            "upper_bound": 449.1634421317337
          },
          "point_estimate": 448.8046599467324,
          "standard_error": 0.21183748707491756
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3393173038871635,
            "upper_bound": 1.1879995462703816
          },
          "point_estimate": 0.8569941214820049,
          "standard_error": 0.22616994208884564
        }
      }
    },
    "memchr2/fallback/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/never",
        "directory_name": "memchr2/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.16715831865064,
            "upper_bound": 103.2789251530326
          },
          "point_estimate": 103.2189100805526,
          "standard_error": 0.02860274289893391
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.16240605671275,
            "upper_bound": 103.25719980784972
          },
          "point_estimate": 103.20714491454214,
          "standard_error": 0.02633593803724554
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020800738954444453,
            "upper_bound": 0.13931592544756163
          },
          "point_estimate": 0.06510792838237124,
          "standard_error": 0.030095850699392183
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.18640881499375,
            "upper_bound": 103.27900604599036
          },
          "point_estimate": 103.23189212268085,
          "standard_error": 0.02339222325616083
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03740708425580873,
            "upper_bound": 0.13389020991395095
          },
          "point_estimate": 0.09531925592885192,
          "standard_error": 0.026938564840240178
        }
      }
    },
    "memchr2/fallback/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/rare",
        "directory_name": "memchr2/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.67462887350877,
            "upper_bound": 104.78116388464062
          },
          "point_estimate": 104.72768088788288,
          "standard_error": 0.02732649340949108
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.63886769633488,
            "upper_bound": 104.80182124805503
          },
          "point_estimate": 104.74209340842538,
          "standard_error": 0.05170001900515093
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008725519980777174,
            "upper_bound": 0.15513473902625596
          },
          "point_estimate": 0.12617280689859822,
          "standard_error": 0.03967483420127822
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.6913071012951,
            "upper_bound": 104.81921449065683
          },
          "point_estimate": 104.76636747323884,
          "standard_error": 0.03239666107289191
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05945094667495067,
            "upper_bound": 0.10854329799894416
          },
          "point_estimate": 0.09091618310286796,
          "standard_error": 0.012811815771034312
        }
      }
    },
    "memchr2/fallback/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/uncommon",
        "directory_name": "memchr2/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166.3036577764892,
            "upper_bound": 166.53502238751815
          },
          "point_estimate": 166.4260858158778,
          "standard_error": 0.05926530622847455
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166.26742548008357,
            "upper_bound": 166.57413932757035
          },
          "point_estimate": 166.47550460988714,
          "standard_error": 0.06864555224652812
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030341068397501895,
            "upper_bound": 0.340184700350278
          },
          "point_estimate": 0.17987009590469238,
          "standard_error": 0.07728016015109952
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166.44358753030164,
            "upper_bound": 166.55409644203198
          },
          "point_estimate": 166.50298901404287,
          "standard_error": 0.02863570173534012
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08173519925420372,
            "upper_bound": 0.24613277463454236
          },
          "point_estimate": 0.19736648974055868,
          "standard_error": 0.04093568504609483
        }
      }
    },
    "memchr2/fallback/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/never",
        "directory_name": "memchr2/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.222381339632957,
            "upper_bound": 13.246424012917206
          },
          "point_estimate": 13.2342121188394,
          "standard_error": 0.006136623189669751
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.216864868534618,
            "upper_bound": 13.251260750988417
          },
          "point_estimate": 13.235915989289126,
          "standard_error": 0.008210618365389083
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003134145229781351,
            "upper_bound": 0.037578838838259826
          },
          "point_estimate": 0.023756806899456664,
          "standard_error": 0.009001531895501425
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.218622235406716,
            "upper_bound": 13.239927333050453
          },
          "point_estimate": 13.230522687002004,
          "standard_error": 0.005484319078701824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011588814933155145,
            "upper_bound": 0.02551540703716821
          },
          "point_estimate": 0.0205248211892378,
          "standard_error": 0.003579178373701241
        }
      }
    },
    "memchr2/fallback/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/rare",
        "directory_name": "memchr2/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.510217423479297,
            "upper_bound": 21.574579195388175
          },
          "point_estimate": 21.539397437788796,
          "standard_error": 0.01647106796948936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.502091458206344,
            "upper_bound": 21.55676577285666
          },
          "point_estimate": 21.53855269504134,
          "standard_error": 0.016080050591103564
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0048433595428370065,
            "upper_bound": 0.07778070344876664
          },
          "point_estimate": 0.034084129075256105,
          "standard_error": 0.01825689179365144
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.518214921821546,
            "upper_bound": 21.561655585487607
          },
          "point_estimate": 21.536075821892,
          "standard_error": 0.01111191802066267
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021418564188935344,
            "upper_bound": 0.07791357179156028
          },
          "point_estimate": 0.05494727607210357,
          "standard_error": 0.01626706867307524
        }
      }
    },
    "memchr2/fallback/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/uncommon",
        "directory_name": "memchr2/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.38549556053528,
            "upper_bound": 56.561302419670426
          },
          "point_estimate": 56.46994341676045,
          "standard_error": 0.04481527142215011
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.308517145447546,
            "upper_bound": 56.550534359203645
          },
          "point_estimate": 56.476986437203145,
          "standard_error": 0.054899509240546304
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020579390632817933,
            "upper_bound": 0.2719894427930429
          },
          "point_estimate": 0.13882943990459878,
          "standard_error": 0.06646595766446874
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.46285292074539,
            "upper_bound": 56.57627120975251
          },
          "point_estimate": 56.5111495491232,
          "standard_error": 0.02880193111199958
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08036740991650512,
            "upper_bound": 0.19721861604565688
          },
          "point_estimate": 0.14966369395482537,
          "standard_error": 0.03211228378500875
        }
      }
    },
    "memchr2/krate/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/krate/empty/never",
        "directory_name": "memchr2/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.48881272976551865,
            "upper_bound": 0.4899532785734485
          },
          "point_estimate": 0.48930552693794416,
          "standard_error": 0.0002979798392280125
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4887231604234195,
            "upper_bound": 0.4897091230484394
          },
          "point_estimate": 0.489009072346744,
          "standard_error": 0.0002070135708943555
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00008190805262343454,
            "upper_bound": 0.001199772067038809
          },
          "point_estimate": 0.00042693425934306267,
          "standard_error": 0.0002907174243369908
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4888247644490122,
            "upper_bound": 0.4891563872338721
          },
          "point_estimate": 0.48901998830457183,
          "standard_error": 0.00008426858565982379
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000250679187155383,
            "upper_bound": 0.001406738374241268
          },
          "point_estimate": 0.0009939715157666574,
          "standard_error": 0.0003282510755049455
        }
      }
    },
    "memchr2/krate/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/common",
        "directory_name": "memchr2/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409666.73529962543,
            "upper_bound": 410228.4681112895
          },
          "point_estimate": 409939.16188336007,
          "standard_error": 143.91287473038213
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409559.2228464419,
            "upper_bound": 410442.5056179775
          },
          "point_estimate": 409813.7777153558,
          "standard_error": 198.26191939488743
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.7535864731014,
            "upper_bound": 804.7909425098605
          },
          "point_estimate": 477.5686344990204,
          "standard_error": 207.4529108796416
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409734.7539052598,
            "upper_bound": 410423.20687433874
          },
          "point_estimate": 410044.0862396031,
          "standard_error": 176.53726737346454
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.30987345801503,
            "upper_bound": 590.0666622389583
          },
          "point_estimate": 479.4855714640714,
          "standard_error": 83.17739812950757
        }
      }
    },
    "memchr2/krate/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/never",
        "directory_name": "memchr2/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11519.186263638914,
            "upper_bound": 11591.437252068705
          },
          "point_estimate": 11557.482277855464,
          "standard_error": 18.69017273936628
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11499.373144932111,
            "upper_bound": 11603.716079009228
          },
          "point_estimate": 11593.062113762462,
          "standard_error": 29.778962787657115
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7756458719277146,
            "upper_bound": 103.25171236015991
          },
          "point_estimate": 21.493198784821303,
          "standard_error": 30.60366596832588
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11570.885760405265,
            "upper_bound": 11603.107774982253
          },
          "point_estimate": 11594.208107964028,
          "standard_error": 8.54663207239044
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.381810367026915,
            "upper_bound": 74.49458213610023
          },
          "point_estimate": 62.10328585206186,
          "standard_error": 12.022618340839074
        }
      }
    },
    "memchr2/krate/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/rare",
        "directory_name": "memchr2/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15763.1620116207,
            "upper_bound": 15787.966747839408
          },
          "point_estimate": 15776.368977980925,
          "standard_error": 6.342992153576336
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15762.383947939265,
            "upper_bound": 15791.461373222464
          },
          "point_estimate": 15780.47899493854,
          "standard_error": 6.518444100985983
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9466091733608628,
            "upper_bound": 33.88494989191217
          },
          "point_estimate": 20.277449086857153,
          "standard_error": 8.594213176754746
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15765.805966584529,
            "upper_bound": 15788.815624711058
          },
          "point_estimate": 15776.70760796687,
          "standard_error": 5.916951405315116
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.641564493347811,
            "upper_bound": 28.761992620726566
          },
          "point_estimate": 21.221101669523247,
          "standard_error": 5.340626140909038
        }
      }
    },
    "memchr2/krate/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/uncommon",
        "directory_name": "memchr2/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161359.09223697148,
            "upper_bound": 161779.54827064896
          },
          "point_estimate": 161552.85188667648,
          "standard_error": 108.49278567087268
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161304.40034414944,
            "upper_bound": 161823.7319321534
          },
          "point_estimate": 161472.12914823007,
          "standard_error": 110.78030339709544
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.85548650248809,
            "upper_bound": 589.0714924732899
          },
          "point_estimate": 200.74174037417473,
          "standard_error": 132.63872730826776
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161311.61589877904,
            "upper_bound": 161690.6939102564
          },
          "point_estimate": 161464.84237443973,
          "standard_error": 97.0066107169098
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134.65405060982715,
            "upper_bound": 465.99447571738887
          },
          "point_estimate": 361.2168032271148,
          "standard_error": 86.41924257443195
        }
      }
    },
    "memchr2/krate/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/common",
        "directory_name": "memchr2/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 405.77578145475934,
            "upper_bound": 406.4024062078072
          },
          "point_estimate": 406.069848931032,
          "standard_error": 0.16027267011842453
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 405.733607710865,
            "upper_bound": 406.2640953855929
          },
          "point_estimate": 406.0488383629675,
          "standard_error": 0.12080660645336222
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10074107526429964,
            "upper_bound": 0.827514526498815
          },
          "point_estimate": 0.23498178527606864,
          "standard_error": 0.18402923403757832
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 405.77723717288035,
            "upper_bound": 406.13833191524793
          },
          "point_estimate": 405.9576272302572,
          "standard_error": 0.0910522898884466
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1745087280433475,
            "upper_bound": 0.7576481553533471
          },
          "point_estimate": 0.5350883047066289,
          "standard_error": 0.15260397603790088
        }
      }
    },
    "memchr2/krate/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/never",
        "directory_name": "memchr2/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.010817063610258,
            "upper_bound": 12.024757860759554
          },
          "point_estimate": 12.017621908775205,
          "standard_error": 0.0035616760357261006
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.008107081002969,
            "upper_bound": 12.02551028512805
          },
          "point_estimate": 12.017449209973783,
          "standard_error": 0.0047680633318781035
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028552205370585147,
            "upper_bound": 0.021243661195253193
          },
          "point_estimate": 0.012552158518922055,
          "standard_error": 0.004644793121311421
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.012685126414732,
            "upper_bound": 12.02624329021708
          },
          "point_estimate": 12.01909275203732,
          "standard_error": 0.0035309148381500125
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006554448319347456,
            "upper_bound": 0.01541714654682549
          },
          "point_estimate": 0.011899112605310664,
          "standard_error": 0.0023441945602661676
        }
      }
    },
    "memchr2/krate/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/rare",
        "directory_name": "memchr2/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.617371276023555,
            "upper_bound": 23.6780498945434
          },
          "point_estimate": 23.6500358418313,
          "standard_error": 0.015666845826341983
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.6086525111959,
            "upper_bound": 23.68509297234915
          },
          "point_estimate": 23.65935992182913,
          "standard_error": 0.014734056744433757
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018402285163812387,
            "upper_bound": 0.07910147281422933
          },
          "point_estimate": 0.024448941491358712,
          "standard_error": 0.019187284423588816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.613638718948284,
            "upper_bound": 23.67767610042506
          },
          "point_estimate": 23.65085702332844,
          "standard_error": 0.016025871022654705
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01779652009015726,
            "upper_bound": 0.06694945839224387
          },
          "point_estimate": 0.05232825566492121,
          "standard_error": 0.012670269068351564
        }
      }
    },
    "memchr2/krate/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/uncommon",
        "directory_name": "memchr2/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.5985952187524,
            "upper_bound": 106.75722246284252
          },
          "point_estimate": 106.67967549877147,
          "standard_error": 0.040457369077518665
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.59594583351684,
            "upper_bound": 106.78841449986052
          },
          "point_estimate": 106.68222566431592,
          "standard_error": 0.04976874602557568
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02662937710207687,
            "upper_bound": 0.23531132247789735
          },
          "point_estimate": 0.11424361431128262,
          "standard_error": 0.05369087556731237
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.63884891685144,
            "upper_bound": 106.73695665881348
          },
          "point_estimate": 106.69194040209764,
          "standard_error": 0.02502464170276368
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.066899421453729,
            "upper_bound": 0.17899830579727968
          },
          "point_estimate": 0.1350368305553985,
          "standard_error": 0.028816994603566383
        }
      }
    },
    "memchr2/krate/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/never",
        "directory_name": "memchr2/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.680684825455132,
            "upper_bound": 4.718201258149705
          },
          "point_estimate": 4.696642008859201,
          "standard_error": 0.009835717467582472
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.677333560838248,
            "upper_bound": 4.708077948334921
          },
          "point_estimate": 4.687661716671757,
          "standard_error": 0.0067755815542506715
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0038035966386949734,
            "upper_bound": 0.03660384190301444
          },
          "point_estimate": 0.01375362826819955,
          "standard_error": 0.008759956347046191
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.682332214112994,
            "upper_bound": 4.694800214555736
          },
          "point_estimate": 4.687841496695799,
          "standard_error": 0.0031784408629089967
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007938746843836204,
            "upper_bound": 0.04752466999507524
          },
          "point_estimate": 0.03289899820340169,
          "standard_error": 0.011570180485506818
        }
      }
    },
    "memchr2/krate/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/rare",
        "directory_name": "memchr2/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.189919229063264,
            "upper_bound": 11.205410277101484
          },
          "point_estimate": 11.19758385395895,
          "standard_error": 0.003953330785153049
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.188991780033916,
            "upper_bound": 11.202558588775972
          },
          "point_estimate": 11.19856259657272,
          "standard_error": 0.003318187294614424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001218714977763914,
            "upper_bound": 0.021494897945850792
          },
          "point_estimate": 0.006648433218777836,
          "standard_error": 0.005264422428544427
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.195848429865531,
            "upper_bound": 11.211629426780265
          },
          "point_estimate": 11.202929094160586,
          "standard_error": 0.004149731729541574
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0048191585499040065,
            "upper_bound": 0.018116296092650107
          },
          "point_estimate": 0.013166538899975455,
          "standard_error": 0.003472605480044757
        }
      }
    },
    "memchr2/krate/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/uncommon",
        "directory_name": "memchr2/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.63043610139863,
            "upper_bound": 55.69012094000491
          },
          "point_estimate": 55.65899753100771,
          "standard_error": 0.015329822927415864
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.621024596112086,
            "upper_bound": 55.71034682475831
          },
          "point_estimate": 55.63677114338068,
          "standard_error": 0.02363891480798927
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0056554627030926926,
            "upper_bound": 0.08230813404847016
          },
          "point_estimate": 0.048943888609679014,
          "standard_error": 0.02203241621541987
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.61621615648142,
            "upper_bound": 55.65440400017
          },
          "point_estimate": 55.6317999407498,
          "standard_error": 0.009721031356124562
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026918255945328533,
            "upper_bound": 0.06190800674782856
          },
          "point_estimate": 0.05113145904171528,
          "standard_error": 0.00876638517657708
        }
      }
    },
    "memchr2/naive/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/naive/empty/never",
        "directory_name": "memchr2/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6122489852770888,
            "upper_bound": 0.6142103072013537
          },
          "point_estimate": 0.6130576615318225,
          "standard_error": 0.0005173604736884102
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6122636350603055,
            "upper_bound": 0.6132501671950771
          },
          "point_estimate": 0.6126262307765089,
          "standard_error": 0.0003163540088512729
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000049838066006605434,
            "upper_bound": 0.0015999500425860416
          },
          "point_estimate": 0.0005441560217970832,
          "standard_error": 0.0003846263129849503
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6122804481359058,
            "upper_bound": 0.6130028679088569
          },
          "point_estimate": 0.6126236555622612,
          "standard_error": 0.00018130201738872972
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003937113239920578,
            "upper_bound": 0.0025793401605031492
          },
          "point_estimate": 0.001724397106582397,
          "standard_error": 0.000689342796828769
        }
      }
    },
    "memchr2/naive/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/common",
        "directory_name": "memchr2/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 849068.6022554908,
            "upper_bound": 854530.0245994832
          },
          "point_estimate": 851832.084250646,
          "standard_error": 1399.845142400647
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 848307.0245478037,
            "upper_bound": 854402.8023255814
          },
          "point_estimate": 852403.4738372093,
          "standard_error": 1217.876563066207
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 484.7441065104245,
            "upper_bound": 8299.286064983828
          },
          "point_estimate": 2651.662808039904,
          "standard_error": 2026.300416473888
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 848570.480988778,
            "upper_bound": 854610.1912194196
          },
          "point_estimate": 851874.2769556026,
          "standard_error": 1612.3979992954853
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1651.1106682036595,
            "upper_bound": 6264.075865723931
          },
          "point_estimate": 4685.012391367616,
          "standard_error": 1078.7356067974886
        }
      }
    },
    "memchr2/naive/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/never",
        "directory_name": "memchr2/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218129.5435575278,
            "upper_bound": 218433.33026880765
          },
          "point_estimate": 218279.30449553276,
          "standard_error": 78.18609948644055
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217987.9622754491,
            "upper_bound": 218478.59153122327
          },
          "point_estimate": 218294.1994760479,
          "standard_error": 99.1288191920906
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.00431940533354,
            "upper_bound": 488.715625495237
          },
          "point_estimate": 307.2413232280169,
          "standard_error": 147.42709875525327
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218057.09515822507,
            "upper_bound": 218308.99544662176
          },
          "point_estimate": 218179.59312543744,
          "standard_error": 66.46248782456085
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.75209327516276,
            "upper_bound": 324.1354160363816
          },
          "point_estimate": 260.5809560171384,
          "standard_error": 45.3673716614518
        }
      }
    },
    "memchr2/naive/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/rare",
        "directory_name": "memchr2/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226173.98855083704,
            "upper_bound": 228526.72344270832
          },
          "point_estimate": 227281.59052306548,
          "standard_error": 602.5766512091933
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226140.01279761904,
            "upper_bound": 228472.418125
          },
          "point_estimate": 226956.83671875,
          "standard_error": 491.3339757195293
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157.97971430467874,
            "upper_bound": 3231.636598689479
          },
          "point_estimate": 1294.1917316172694,
          "standard_error": 876.4961105388589
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226418.3532849709,
            "upper_bound": 228050.88279272153
          },
          "point_estimate": 227323.88430194804,
          "standard_error": 420.9272225653147
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 802.8874138226552,
            "upper_bound": 2779.3293551833913
          },
          "point_estimate": 2000.9953702295336,
          "standard_error": 523.3944423381198
        }
      }
    },
    "memchr2/naive/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/uncommon",
        "directory_name": "memchr2/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355594.1502588997,
            "upper_bound": 356204.07486515644
          },
          "point_estimate": 355927.5553667745,
          "standard_error": 156.69342635474968
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355694.38624595467,
            "upper_bound": 356339.28883495147
          },
          "point_estimate": 355929.6274271845,
          "standard_error": 166.07169404712624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.683617145398664,
            "upper_bound": 749.3427318421349
          },
          "point_estimate": 519.3724036433227,
          "standard_error": 191.81453519206588
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355867.7953235174,
            "upper_bound": 356314.04985976266
          },
          "point_estimate": 356097.933551885,
          "standard_error": 115.9259727185648
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223.66726229238589,
            "upper_bound": 722.5842064360971
          },
          "point_estimate": 522.3726943241151,
          "standard_error": 145.06486876720817
        }
      }
    },
    "memchr2/naive/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/common",
        "directory_name": "memchr2/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.61013587901726,
            "upper_bound": 334.7828834054505
          },
          "point_estimate": 334.6928462476704,
          "standard_error": 0.044229200074336235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.5614149185722,
            "upper_bound": 334.80404111984393
          },
          "point_estimate": 334.68447299477873,
          "standard_error": 0.05789145666442492
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02830854760250525,
            "upper_bound": 0.2593022890609944
          },
          "point_estimate": 0.15558838526872196,
          "standard_error": 0.06072682016690758
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.56314362512876,
            "upper_bound": 334.6933785030765
          },
          "point_estimate": 334.6173685432166,
          "standard_error": 0.03339790228755526
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07490316668069186,
            "upper_bound": 0.1837361828010698
          },
          "point_estimate": 0.14740683176149025,
          "standard_error": 0.0275859774674745
        }
      }
    },
    "memchr2/naive/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/never",
        "directory_name": "memchr2/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.53069143805106,
            "upper_bound": 278.82694855338116
          },
          "point_estimate": 278.6773548005011,
          "standard_error": 0.07572842686258172
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.45487542625415,
            "upper_bound": 278.8707609255903
          },
          "point_estimate": 278.7083982954953,
          "standard_error": 0.1043488814926358
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05040406708051329,
            "upper_bound": 0.442370556491564
          },
          "point_estimate": 0.30353869054210497,
          "standard_error": 0.10401251396883585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.56736211496434,
            "upper_bound": 278.766041788483
          },
          "point_estimate": 278.6941161861088,
          "standard_error": 0.05090743148951134
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14537681466311214,
            "upper_bound": 0.31790178754216286
          },
          "point_estimate": 0.25273292060433855,
          "standard_error": 0.044549232835765194
        }
      }
    },
    "memchr2/naive/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/rare",
        "directory_name": "memchr2/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295.3098902238454,
            "upper_bound": 296.90592591959717
          },
          "point_estimate": 295.9215583974602,
          "standard_error": 0.4453374853038411
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295.21797994152666,
            "upper_bound": 295.95660987640247
          },
          "point_estimate": 295.42867329219746,
          "standard_error": 0.1941129497434282
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06866140377002891,
            "upper_bound": 0.8497142521450601
          },
          "point_estimate": 0.3771520706008985,
          "standard_error": 0.22433483516712227
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295.23667561214825,
            "upper_bound": 295.5518030294459
          },
          "point_estimate": 295.363147855394,
          "standard_error": 0.07955586407958395
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.19690748595526583,
            "upper_bound": 2.266163800782778
          },
          "point_estimate": 1.485094309012886,
          "standard_error": 0.6917471191451394
        }
      }
    },
    "memchr2/naive/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/uncommon",
        "directory_name": "memchr2/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 371.973151565771,
            "upper_bound": 372.549555053107
          },
          "point_estimate": 372.25921072366594,
          "standard_error": 0.1478513323163593
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 371.7631622827699,
            "upper_bound": 372.6554318633782
          },
          "point_estimate": 372.31251976927064,
          "standard_error": 0.2880077156623293
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.048655552983267535,
            "upper_bound": 0.8357040734118141
          },
          "point_estimate": 0.6518016994122336,
          "standard_error": 0.22913487922852485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 372.0459696965555,
            "upper_bound": 372.799546100931
          },
          "point_estimate": 372.4903394430364,
          "standard_error": 0.19488575797947424
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3211341313224758,
            "upper_bound": 0.59126130510103
          },
          "point_estimate": 0.49264390705850464,
          "standard_error": 0.07046385905072197
        }
      }
    },
    "memchr2/naive/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/never",
        "directory_name": "memchr2/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.17514374536793,
            "upper_bound": 36.241144414840434
          },
          "point_estimate": 36.20641744764996,
          "standard_error": 0.01697370170460863
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.17351095244918,
            "upper_bound": 36.27182990372829
          },
          "point_estimate": 36.18013435853352,
          "standard_error": 0.02461247969440231
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0029885156652260953,
            "upper_bound": 0.08467016057475336
          },
          "point_estimate": 0.02053102255392932,
          "standard_error": 0.025239707753903114
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.17870197161334,
            "upper_bound": 36.238348330106824
          },
          "point_estimate": 36.20376181875657,
          "standard_error": 0.015318841012880856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02420613756040984,
            "upper_bound": 0.06909927714551692
          },
          "point_estimate": 0.056565235911345615,
          "standard_error": 0.010644220223475446
        }
      }
    },
    "memchr2/naive/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/rare",
        "directory_name": "memchr2/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.889973542954365,
            "upper_bound": 37.94383565952299
          },
          "point_estimate": 37.917766801501045,
          "standard_error": 0.013748657689758076
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.887390256525535,
            "upper_bound": 37.94584038643504
          },
          "point_estimate": 37.92589384305026,
          "standard_error": 0.012964188941795952
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004625359728009926,
            "upper_bound": 0.07752323642866042
          },
          "point_estimate": 0.02977265383197817,
          "standard_error": 0.01949325319948102
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.91464596412554,
            "upper_bound": 37.93586176038141
          },
          "point_estimate": 37.92583457005754,
          "standard_error": 0.0053623393742297685
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021016230234481896,
            "upper_bound": 0.06083688792217585
          },
          "point_estimate": 0.04594244084814864,
          "standard_error": 0.0101864731219262
        }
      }
    },
    "memchr2/naive/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr2/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/uncommon",
        "directory_name": "memchr2/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.45768258375288,
            "upper_bound": 35.49856451072275
          },
          "point_estimate": 35.47675806693504,
          "standard_error": 0.010523347054378024
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.447846384482474,
            "upper_bound": 35.50369520324229
          },
          "point_estimate": 35.473752762889106,
          "standard_error": 0.0128497028142698
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0069141228142281935,
            "upper_bound": 0.05986144715092013
          },
          "point_estimate": 0.03326095986318192,
          "standard_error": 0.014548526338871796
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.452964916547664,
            "upper_bound": 35.50102954060153
          },
          "point_estimate": 35.47882627079192,
          "standard_error": 0.01230225357077259
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015955157508846046,
            "upper_bound": 0.044793429195804334
          },
          "point_estimate": 0.035154974179291504,
          "standard_error": 0.0074869370118985645
        }
      }
    },
    "memchr3/fallback/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/fallback/empty/never",
        "directory_name": "memchr3/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.47905418514821,
            "upper_bound": 2.4844999279649
          },
          "point_estimate": 2.481781643868537,
          "standard_error": 0.0013937549395929294
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.478166818316367,
            "upper_bound": 2.485527073668181
          },
          "point_estimate": 2.4817442145240536,
          "standard_error": 0.0016755831144104854
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008896657972584062,
            "upper_bound": 0.00793050575850453
          },
          "point_estimate": 0.004351024561101894,
          "standard_error": 0.001842275015437009
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.477920904214247,
            "upper_bound": 2.484625893317654
          },
          "point_estimate": 2.4819164233371636,
          "standard_error": 0.0016935064474270657
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002531309424490357,
            "upper_bound": 0.0059401735620096836
          },
          "point_estimate": 0.0046450747083049415,
          "standard_error": 0.0008721115375577561
        }
      }
    },
    "memchr3/fallback/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/common",
        "directory_name": "memchr3/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1726239.632164502,
            "upper_bound": 1728895.1406060606
          },
          "point_estimate": 1727543.9994372297,
          "standard_error": 679.9178504011131
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1725546.7272727273,
            "upper_bound": 1729322.7348484849
          },
          "point_estimate": 1727745.945454545,
          "standard_error": 806.7293725498517
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 352.94447646127946,
            "upper_bound": 4381.5768013024635
          },
          "point_estimate": 2045.9239423137365,
          "standard_error": 1048.3179020292737
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1726635.2313452396,
            "upper_bound": 1728438.4032323232
          },
          "point_estimate": 1727609.19527745,
          "standard_error": 450.1925435847247
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1246.2673345362334,
            "upper_bound": 2873.9979160779244
          },
          "point_estimate": 2274.4568509004507,
          "standard_error": 421.12342512745823
        }
      }
    },
    "memchr3/fallback/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/never",
        "directory_name": "memchr3/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127645.77597462753,
            "upper_bound": 127823.55494430522
          },
          "point_estimate": 127740.82748384852,
          "standard_error": 45.76024455584584
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127629.95614035089,
            "upper_bound": 127826.2927631579
          },
          "point_estimate": 127776.96925647452,
          "standard_error": 42.255219539313686
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.68080997118159,
            "upper_bound": 261.0156269450141
          },
          "point_estimate": 73.83896980021159,
          "standard_error": 58.97853584193635
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127696.49003813884,
            "upper_bound": 127844.4026482874
          },
          "point_estimate": 127778.16943267258,
          "standard_error": 37.22557473753131
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.352964091601805,
            "upper_bound": 200.86502705187215
          },
          "point_estimate": 152.6022146988405,
          "standard_error": 37.33934148301982
        }
      }
    },
    "memchr3/fallback/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/rare",
        "directory_name": "memchr3/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135087.12897505827,
            "upper_bound": 135261.5481001062
          },
          "point_estimate": 135171.09151191948,
          "standard_error": 44.531902589054425
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135049.56836018176,
            "upper_bound": 135292.46050185873
          },
          "point_estimate": 135138.37881040893,
          "standard_error": 66.55725083233477
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.423071295489116,
            "upper_bound": 246.72337480193409
          },
          "point_estimate": 171.10036548775392,
          "standard_error": 53.5077448641891
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135056.07534576964,
            "upper_bound": 135244.10326891005
          },
          "point_estimate": 135148.14904649253,
          "standard_error": 47.61207721932658
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.33840069962643,
            "upper_bound": 186.22220622066655
          },
          "point_estimate": 148.9149688036043,
          "standard_error": 26.027722211679674
        }
      }
    },
    "memchr3/fallback/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/uncommon",
        "directory_name": "memchr3/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 513865.39040764025,
            "upper_bound": 514886.92892577697
          },
          "point_estimate": 514354.423894478,
          "standard_error": 261.96885553258676
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 513726.6220657277,
            "upper_bound": 514943.1710261569
          },
          "point_estimate": 514254.8555555555,
          "standard_error": 319.0865345210021
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236.72980044509947,
            "upper_bound": 1473.7716592813254
          },
          "point_estimate": 901.82772835553,
          "standard_error": 306.5665982470173
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 513926.6635329217,
            "upper_bound": 514587.930279184
          },
          "point_estimate": 514271.5535028352,
          "standard_error": 166.78149029648677
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 444.9452185907905,
            "upper_bound": 1152.4660890800603
          },
          "point_estimate": 873.2926889119307,
          "standard_error": 188.50340245920137
        }
      }
    },
    "memchr3/fallback/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/common",
        "directory_name": "memchr3/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 758.9552340230571,
            "upper_bound": 760.2528365338944
          },
          "point_estimate": 759.5615174103496,
          "standard_error": 0.3330622875384121
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 758.9150756183155,
            "upper_bound": 760.3500838545646
          },
          "point_estimate": 759.3848322088068,
          "standard_error": 0.30853812752239124
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10341169915832713,
            "upper_bound": 1.912166168947252
          },
          "point_estimate": 0.5930494095392923,
          "standard_error": 0.4432390816773537
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 758.8873771363725,
            "upper_bound": 759.6183124114704
          },
          "point_estimate": 759.2682231079415,
          "standard_error": 0.1893079864083959
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3635426055395816,
            "upper_bound": 1.4647262268316723
          },
          "point_estimate": 1.1115535449567424,
          "standard_error": 0.2678559490090979
        }
      }
    },
    "memchr3/fallback/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/never",
        "directory_name": "memchr3/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151.52520483450397,
            "upper_bound": 151.75821814013875
          },
          "point_estimate": 151.6260059064849,
          "standard_error": 0.060513637023746784
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151.5024411338353,
            "upper_bound": 151.68988868119763
          },
          "point_estimate": 151.59247506314188,
          "standard_error": 0.04639261215411872
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023676739040256963,
            "upper_bound": 0.24022575110079075
          },
          "point_estimate": 0.10381721685575888,
          "standard_error": 0.05533757200155614
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151.53377467377783,
            "upper_bound": 151.66658646919498
          },
          "point_estimate": 151.6015930104476,
          "standard_error": 0.03363935259873225
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06615938141392741,
            "upper_bound": 0.29187715752425225
          },
          "point_estimate": 0.2019730876654919,
          "standard_error": 0.06820942582426064
        }
      }
    },
    "memchr3/fallback/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/rare",
        "directory_name": "memchr3/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175.46828428755924,
            "upper_bound": 175.7858509555669
          },
          "point_estimate": 175.61699105277643,
          "standard_error": 0.08148693662798397
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175.40144224958544,
            "upper_bound": 175.78850696408455
          },
          "point_estimate": 175.57671786930007,
          "standard_error": 0.08899108496132384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024135913298399876,
            "upper_bound": 0.43937210508173286
          },
          "point_estimate": 0.2473611656399413,
          "standard_error": 0.1021905507588963
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175.47961848888124,
            "upper_bound": 175.6819425706331
          },
          "point_estimate": 175.57024877977756,
          "standard_error": 0.050988199494691154
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11504849141833429,
            "upper_bound": 0.3524425469104147
          },
          "point_estimate": 0.2725045709773776,
          "standard_error": 0.06103437855696518
        }
      }
    },
    "memchr3/fallback/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/uncommon",
        "directory_name": "memchr3/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280.662497395491,
            "upper_bound": 280.97683230750994
          },
          "point_estimate": 280.8198535108678,
          "standard_error": 0.08055317119400329
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280.53508258136355,
            "upper_bound": 281.05261005049215
          },
          "point_estimate": 280.87236153316786,
          "standard_error": 0.14855154396591266
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06356882255093903,
            "upper_bound": 0.446816756238837
          },
          "point_estimate": 0.3683973246433886,
          "standard_error": 0.10547666611123252
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280.57446619759077,
            "upper_bound": 280.9683165461066
          },
          "point_estimate": 280.7641551331825,
          "standard_error": 0.1038791328269502
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.17476208138211435,
            "upper_bound": 0.3155121535496499
          },
          "point_estimate": 0.2685085587327163,
          "standard_error": 0.03601707744575608
        }
      }
    },
    "memchr3/fallback/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/never",
        "directory_name": "memchr3/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.57063798443169,
            "upper_bound": 18.60306358291288
          },
          "point_estimate": 18.586418850311883,
          "standard_error": 0.008258372450734262
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.5681243845859,
            "upper_bound": 18.6051204177715
          },
          "point_estimate": 18.584646062812133,
          "standard_error": 0.007579531248102156
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001503298957197209,
            "upper_bound": 0.04804865480121968
          },
          "point_estimate": 0.01918959863637703,
          "standard_error": 0.012812607540731888
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.578240784287363,
            "upper_bound": 18.602605764657643
          },
          "point_estimate": 18.591300101828807,
          "standard_error": 0.006089213007841153
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01304830002649196,
            "upper_bound": 0.03595376588219756
          },
          "point_estimate": 0.027379010834424577,
          "standard_error": 0.005881185261071202
        }
      }
    },
    "memchr3/fallback/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/rare",
        "directory_name": "memchr3/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.79485704470656,
            "upper_bound": 32.82443659585344
          },
          "point_estimate": 32.80942340041732,
          "standard_error": 0.007573810854886029
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.7904612202838,
            "upper_bound": 32.827638687617714
          },
          "point_estimate": 32.80775344677508,
          "standard_error": 0.012657321881117914
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000703378369243695,
            "upper_bound": 0.04297979114011172
          },
          "point_estimate": 0.02563801195918253,
          "standard_error": 0.011036740983062275
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.783954416045425,
            "upper_bound": 32.83020139108772
          },
          "point_estimate": 32.80502519874313,
          "standard_error": 0.012148601564866772
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015048617532930944,
            "upper_bound": 0.03168789875535377
          },
          "point_estimate": 0.025218620170878742,
          "standard_error": 0.004348508904594864
        }
      }
    },
    "memchr3/fallback/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/uncommon",
        "directory_name": "memchr3/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.97989888798853,
            "upper_bound": 103.22887018354584
          },
          "point_estimate": 103.09586472125618,
          "standard_error": 0.06417977102422578
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.96337197441773,
            "upper_bound": 103.20099202172548
          },
          "point_estimate": 103.0527248832274,
          "standard_error": 0.06219227248727481
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03966120184013064,
            "upper_bound": 0.3459764046582286
          },
          "point_estimate": 0.17216068506258808,
          "standard_error": 0.080075018965971
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.90508816080688,
            "upper_bound": 103.14183663752225
          },
          "point_estimate": 103.018594269205,
          "standard_error": 0.06241855111339992
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.089381738006416,
            "upper_bound": 0.2867696028311266
          },
          "point_estimate": 0.2135913976155968,
          "standard_error": 0.05232266184806248
        }
      }
    },
    "memchr3/krate/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/krate/empty/never",
        "directory_name": "memchr3/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.48897251519704066,
            "upper_bound": 0.48950667116042673
          },
          "point_estimate": 0.4892350028480007,
          "standard_error": 0.0001360665499110154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4890063120102384,
            "upper_bound": 0.4895131269406219
          },
          "point_estimate": 0.4891623368415007,
          "standard_error": 0.00011739163148347992
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000029113052495443956,
            "upper_bound": 0.0008011128486757922
          },
          "point_estimate": 0.0002645341212888344,
          "standard_error": 0.00020810085725399816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4890554853701818,
            "upper_bound": 0.48948465102376415
          },
          "point_estimate": 0.4892354441285464,
          "standard_error": 0.00010798375066371348
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00019684617015381853,
            "upper_bound": 0.0006136078262079422
          },
          "point_estimate": 0.00045370595124940584,
          "standard_error": 0.00010737921161727778
        }
      }
    },
    "memchr3/krate/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/common",
        "directory_name": "memchr3/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629239.1610307198,
            "upper_bound": 629888.8605528187
          },
          "point_estimate": 629532.3392802408,
          "standard_error": 168.16109764810608
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629233.3261494252,
            "upper_bound": 629809.5189655173
          },
          "point_estimate": 629318.103448276,
          "standard_error": 156.92007099470484
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.409391683763044,
            "upper_bound": 798.064759452273
          },
          "point_estimate": 291.4544448257068,
          "standard_error": 215.8597207255334
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629345.4072300198,
            "upper_bound": 629981.9170746952
          },
          "point_estimate": 629667.4652933273,
          "standard_error": 163.9035672990696
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 197.19409905994252,
            "upper_bound": 775.8941947707981
          },
          "point_estimate": 560.5651505517758,
          "standard_error": 159.90244347325992
        }
      }
    },
    "memchr3/krate/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/never",
        "directory_name": "memchr3/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15280.704952095475,
            "upper_bound": 15303.239169821903
          },
          "point_estimate": 15291.677546326904,
          "standard_error": 5.785524042185137
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15278.222453272432,
            "upper_bound": 15304.275978123684
          },
          "point_estimate": 15288.514969849952,
          "standard_error": 6.094238243990385
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.44897067263485,
            "upper_bound": 32.64561020292314
          },
          "point_estimate": 18.947070467154997,
          "standard_error": 8.207349222387144
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15280.251260243554,
            "upper_bound": 15299.49234000619
          },
          "point_estimate": 15291.107498811663,
          "standard_error": 4.992236695136279
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.68247430912444,
            "upper_bound": 26.18600328638144
          },
          "point_estimate": 19.324238147476574,
          "standard_error": 4.408697396893694
        }
      }
    },
    "memchr3/krate/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/rare",
        "directory_name": "memchr3/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20152.952214087632,
            "upper_bound": 20179.77193912923
          },
          "point_estimate": 20164.63663682223,
          "standard_error": 6.966505263279999
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20150.761508596785,
            "upper_bound": 20173.56818001743
          },
          "point_estimate": 20159.601580698836,
          "standard_error": 5.739283956324238
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.133600558895178,
            "upper_bound": 27.42933295729256
          },
          "point_estimate": 16.152897932862963,
          "standard_error": 6.116099591378558
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20153.970631324013,
            "upper_bound": 20168.165200910826
          },
          "point_estimate": 20160.03609856588,
          "standard_error": 3.634445895358653
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.838316767958915,
            "upper_bound": 33.73743356841931
          },
          "point_estimate": 23.275024667827942,
          "standard_error": 7.900211821357215
        }
      }
    },
    "memchr3/krate/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/uncommon",
        "directory_name": "memchr3/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217341.9271062335,
            "upper_bound": 217781.72439780564
          },
          "point_estimate": 217563.94505574455,
          "standard_error": 112.27173386268308
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217290.7160714286,
            "upper_bound": 217803.01211734692
          },
          "point_estimate": 217573.72127976193,
          "standard_error": 118.76863502502184
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.171653411691445,
            "upper_bound": 653.7394415187997
          },
          "point_estimate": 336.27639840488797,
          "standard_error": 142.13094196888304
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217387.0247785161,
            "upper_bound": 217719.57397746775
          },
          "point_estimate": 217543.34993815707,
          "standard_error": 86.91905893115418
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174.57774320371436,
            "upper_bound": 500.3511980646818
          },
          "point_estimate": 373.7231251189787,
          "standard_error": 83.89296379864687
        }
      }
    },
    "memchr3/krate/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/common",
        "directory_name": "memchr3/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629.4789224253348,
            "upper_bound": 630.5714723900929
          },
          "point_estimate": 630.0130082519892,
          "standard_error": 0.27860020967484767
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629.2964434565157,
            "upper_bound": 630.517040943913
          },
          "point_estimate": 630.020559458067,
          "standard_error": 0.23223053202075844
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09889034741542396,
            "upper_bound": 1.6840733224217832
          },
          "point_estimate": 0.5411923184691794,
          "standard_error": 0.4169112046789588
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629.9198303354873,
            "upper_bound": 630.5080641418951
          },
          "point_estimate": 630.1495267895483,
          "standard_error": 0.14947410259355215
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.319575813014999,
            "upper_bound": 1.269762196853483
          },
          "point_estimate": 0.9300304013827376,
          "standard_error": 0.22590591219389208
        }
      }
    },
    "memchr3/krate/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/never",
        "directory_name": "memchr3/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.308017826120658,
            "upper_bound": 15.342957517332408
          },
          "point_estimate": 15.325648017936691,
          "standard_error": 0.00894160088361806
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.301305540448991,
            "upper_bound": 15.347345806706697
          },
          "point_estimate": 15.326651707295223,
          "standard_error": 0.010566689559246696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005601236802745163,
            "upper_bound": 0.05133377325932119
          },
          "point_estimate": 0.03016925127608527,
          "standard_error": 0.01170181660367591
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.309582870288338,
            "upper_bound": 15.333440830220155
          },
          "point_estimate": 15.320992194377808,
          "standard_error": 0.006041407844370542
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016232115063903294,
            "upper_bound": 0.038463901830828075
          },
          "point_estimate": 0.029866509589592853,
          "standard_error": 0.005736092718592334
        }
      }
    },
    "memchr3/krate/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/rare",
        "directory_name": "memchr3/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.70924316878661,
            "upper_bound": 33.76009718121792
          },
          "point_estimate": 33.73555766737662,
          "standard_error": 0.01299376159232114
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.709616390487106,
            "upper_bound": 33.76691929027974
          },
          "point_estimate": 33.73897974533081,
          "standard_error": 0.02092287857569886
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0037121235304791024,
            "upper_bound": 0.07870392574066547
          },
          "point_estimate": 0.04247863886213112,
          "standard_error": 0.02070598595795159
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.734818884495674,
            "upper_bound": 33.77569515101102
          },
          "point_estimate": 33.76091157971201,
          "standard_error": 0.010416082459007369
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02436804328179819,
            "upper_bound": 0.05775142735282074
          },
          "point_estimate": 0.04362850345295276,
          "standard_error": 0.009150118220073378
        }
      }
    },
    "memchr3/krate/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/uncommon",
        "directory_name": "memchr3/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165.71817575837176,
            "upper_bound": 166.25947434877327
          },
          "point_estimate": 165.97700645903998,
          "standard_error": 0.13839049760121772
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165.68991141732283,
            "upper_bound": 166.31084050540193
          },
          "point_estimate": 165.88628068021296,
          "standard_error": 0.14083794786163717
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08218712711930629,
            "upper_bound": 0.7821318923586262
          },
          "point_estimate": 0.3236259510504527,
          "standard_error": 0.1838525963288032
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165.73475852057575,
            "upper_bound": 166.21814252908834
          },
          "point_estimate": 165.90355388980183,
          "standard_error": 0.12473975240227927
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.19633784715576005,
            "upper_bound": 0.6056030183949817
          },
          "point_estimate": 0.4616034578810177,
          "standard_error": 0.10210812940229584
        }
      }
    },
    "memchr3/krate/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/never",
        "directory_name": "memchr3/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.38023625817444,
            "upper_bound": 5.393090914636422
          },
          "point_estimate": 5.38551003792648,
          "standard_error": 0.0034294306490661483
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.378896273406409,
            "upper_bound": 5.3880367636418836
          },
          "point_estimate": 5.381727861137088,
          "standard_error": 0.0021724609009458546
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000511781884252262,
            "upper_bound": 0.009404468307257928
          },
          "point_estimate": 0.004176682373715922,
          "standard_error": 0.002613653102597902
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.379811276664027,
            "upper_bound": 5.386196636085656
          },
          "point_estimate": 5.382139532335178,
          "standard_error": 0.0016462193780401291
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016977436929313922,
            "upper_bound": 0.01684426798548346
          },
          "point_estimate": 0.011448389922533542,
          "standard_error": 0.00449630438575733
        }
      }
    },
    "memchr3/krate/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/rare",
        "directory_name": "memchr3/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.111746160835217,
            "upper_bound": 20.13528029422491
          },
          "point_estimate": 20.124313194904673,
          "standard_error": 0.006031201020192487
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.112650231144706,
            "upper_bound": 20.14012536576577
          },
          "point_estimate": 20.126874286463742,
          "standard_error": 0.007483376310386404
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005002474665679516,
            "upper_bound": 0.03327131323190204
          },
          "point_estimate": 0.0188715015641323,
          "standard_error": 0.006793814542360535
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.116674511067625,
            "upper_bound": 20.137723529274083
          },
          "point_estimate": 20.12914204196752,
          "standard_error": 0.005433578787168704
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009772705306550946,
            "upper_bound": 0.02682246076161406
          },
          "point_estimate": 0.020104870419417935,
          "standard_error": 0.004703009069497824
        }
      }
    },
    "memchr3/krate/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/uncommon",
        "directory_name": "memchr3/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.58629787151757,
            "upper_bound": 82.65215168736151
          },
          "point_estimate": 82.62004995503099,
          "standard_error": 0.016737803518435457
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.58068941355258,
            "upper_bound": 82.65405069895336
          },
          "point_estimate": 82.62929598099927,
          "standard_error": 0.018527882708683167
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008431076899746188,
            "upper_bound": 0.09729919995518128
          },
          "point_estimate": 0.04291648005307024,
          "standard_error": 0.020948540609177575
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.60657082218914,
            "upper_bound": 82.64793000866389
          },
          "point_estimate": 82.62862372384032,
          "standard_error": 0.01079339322276162
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02636437918547454,
            "upper_bound": 0.07479026171519199
          },
          "point_estimate": 0.055865715574108146,
          "standard_error": 0.012595244532338202
        }
      }
    },
    "memchr3/naive/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/naive/empty/never",
        "directory_name": "memchr3/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6523082462278189,
            "upper_bound": 0.6536765310779703
          },
          "point_estimate": 0.6528960211520103,
          "standard_error": 0.0003573926198481918
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6521729259458366,
            "upper_bound": 0.6532790758376743
          },
          "point_estimate": 0.6526024671462858,
          "standard_error": 0.0002756905547746307
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0001362632752241759,
            "upper_bound": 0.0013962539491623328
          },
          "point_estimate": 0.0007595628404404272,
          "standard_error": 0.0003352097142262177
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6525279695778086,
            "upper_bound": 0.6540745413458094
          },
          "point_estimate": 0.6530442568452166,
          "standard_error": 0.00042776901416165535
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003704665612122103,
            "upper_bound": 0.0017200902532378938
          },
          "point_estimate": 0.001189519632370185,
          "standard_error": 0.000405956917391151
        }
      }
    },
    "memchr3/naive/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/common",
        "directory_name": "memchr3/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1154329.5445758929,
            "upper_bound": 1155886.234876209
          },
          "point_estimate": 1155114.63077753,
          "standard_error": 397.9061507341005
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1154294.8984375,
            "upper_bound": 1155965.734375
          },
          "point_estimate": 1155189.619642857,
          "standard_error": 441.0729355768529
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258.2764442428015,
            "upper_bound": 2287.055630646623
          },
          "point_estimate": 1115.2186498884455,
          "standard_error": 481.3390952121924
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1154795.1389237945,
            "upper_bound": 1155677.5105299207
          },
          "point_estimate": 1155273.2342532468,
          "standard_error": 222.62071828331528
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 598.9782752363122,
            "upper_bound": 1789.3909310208508
          },
          "point_estimate": 1330.0157530242632,
          "standard_error": 310.8592598363273
        }
      }
    },
    "memchr3/naive/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/never",
        "directory_name": "memchr3/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290944.80746383325,
            "upper_bound": 291308.41315238096
          },
          "point_estimate": 291116.2410123809,
          "standard_error": 92.47244032997494
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290977.2994285714,
            "upper_bound": 291202.06666666665
          },
          "point_estimate": 291101.96239999996,
          "standard_error": 62.09733564287531
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.19375069678672,
            "upper_bound": 454.1342095375133
          },
          "point_estimate": 148.19673976896905,
          "standard_error": 105.1883239040196
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290979.67148606735,
            "upper_bound": 291503.30490842304
          },
          "point_estimate": 291226.6206961039,
          "standard_error": 139.43927549989138
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.11347017968058,
            "upper_bound": 441.24571637767343
          },
          "point_estimate": 307.61437845578376,
          "standard_error": 92.42585076659812
        }
      }
    },
    "memchr3/naive/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/rare",
        "directory_name": "memchr3/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295814.6532691557,
            "upper_bound": 296140.464333785
          },
          "point_estimate": 295976.0142021551,
          "standard_error": 83.41932175157879
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295747.1609756098,
            "upper_bound": 296301.65203252033
          },
          "point_estimate": 295941.25920280034,
          "standard_error": 138.4383737972378
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.419307847973386,
            "upper_bound": 479.5077870968007
          },
          "point_estimate": 347.43623790357503,
          "standard_error": 110.27506141014636
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295786.3044832647,
            "upper_bound": 296096.28248928976
          },
          "point_estimate": 295925.1445465104,
          "standard_error": 78.17710464750319
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171.512460449654,
            "upper_bound": 329.2179472198724
          },
          "point_estimate": 277.60280974731404,
          "standard_error": 40.050248747625965
        }
      }
    },
    "memchr3/naive/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/uncommon",
        "directory_name": "memchr3/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483013.3507360197,
            "upper_bound": 486500.6916045321
          },
          "point_estimate": 484964.8065752924,
          "standard_error": 901.9960723167084
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483673.28289473685,
            "upper_bound": 487027.8114035088
          },
          "point_estimate": 485949.6812865497,
          "standard_error": 881.1671645093943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243.2920546281552,
            "upper_bound": 4184.021353076205
          },
          "point_estimate": 1613.1478867117626,
          "standard_error": 990.5355706214216
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 484452.9407894737,
            "upper_bound": 486547.4386689865
          },
          "point_estimate": 485667.5365003418,
          "standard_error": 534.9937254260061
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 909.7433613097812,
            "upper_bound": 4168.767554913146
          },
          "point_estimate": 3007.9257264774265,
          "standard_error": 892.5606639151973
        }
      }
    },
    "memchr3/naive/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/common",
        "directory_name": "memchr3/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 373.1523985249138,
            "upper_bound": 376.39234952068256
          },
          "point_estimate": 374.58108581135446,
          "standard_error": 0.8386084753079012
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 372.35598194130927,
            "upper_bound": 375.9866768417812
          },
          "point_estimate": 373.90796010329024,
          "standard_error": 0.7131973022804028
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2456091809463341,
            "upper_bound": 3.64724177164287
          },
          "point_estimate": 1.6780194807776252,
          "standard_error": 0.9680819489836576
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 373.9163296741381,
            "upper_bound": 378.586428928007
          },
          "point_estimate": 376.4572516197121,
          "standard_error": 1.2171021521190135
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8980542260538228,
            "upper_bound": 3.901870916630872
          },
          "point_estimate": 2.800245431074009,
          "standard_error": 0.8500286192470368
        }
      }
    },
    "memchr3/naive/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/never",
        "directory_name": "memchr3/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.0462451130454,
            "upper_bound": 335.4488322432636
          },
          "point_estimate": 335.2477220370039,
          "standard_error": 0.1031677204939939
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.9386614749909,
            "upper_bound": 335.54354553166377
          },
          "point_estimate": 335.2056151569358,
          "standard_error": 0.15125233751020314
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08281969249563927,
            "upper_bound": 0.6038496459479937
          },
          "point_estimate": 0.424239115743988,
          "standard_error": 0.1427749335536887
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.21481266544544,
            "upper_bound": 335.5953446053529
          },
          "point_estimate": 335.4341289054323,
          "standard_error": 0.0962526879751775
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.20553912597883156,
            "upper_bound": 0.421091084583153
          },
          "point_estimate": 0.3439173287748171,
          "standard_error": 0.055249791171777966
        }
      }
    },
    "memchr3/naive/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/rare",
        "directory_name": "memchr3/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 365.4123632812313,
            "upper_bound": 365.8042321717574
          },
          "point_estimate": 365.6093629704032,
          "standard_error": 0.10004599532286064
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 365.429893527006,
            "upper_bound": 365.81300827339817
          },
          "point_estimate": 365.6402921879512,
          "standard_error": 0.08663272541260282
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02080614121857007,
            "upper_bound": 0.5783666693258133
          },
          "point_estimate": 0.23867250034605164,
          "standard_error": 0.14377118499323124
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 365.3568298315651,
            "upper_bound": 365.8163231270629
          },
          "point_estimate": 365.57235223828894,
          "standard_error": 0.11499964411544558
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13474484206297635,
            "upper_bound": 0.4554769277735607
          },
          "point_estimate": 0.33412791992144797,
          "standard_error": 0.07978361546572366
        }
      }
    },
    "memchr3/naive/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/uncommon",
        "directory_name": "memchr3/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 407.7580368508055,
            "upper_bound": 408.5625514258231
          },
          "point_estimate": 408.16197902962296,
          "standard_error": 0.20517653211681233
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 407.72220979396434,
            "upper_bound": 408.6269673932226
          },
          "point_estimate": 408.1965261776396,
          "standard_error": 0.2156178080481809
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07889776106544008,
            "upper_bound": 1.157645873275586
          },
          "point_estimate": 0.5261470394946177,
          "standard_error": 0.2789972247347206
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 408.0022518995585,
            "upper_bound": 408.72122916890817
          },
          "point_estimate": 408.31939408659616,
          "standard_error": 0.18169812954585016
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3401718256812433,
            "upper_bound": 0.9016192885750672
          },
          "point_estimate": 0.6843277391396647,
          "standard_error": 0.1436903530698573
        }
      }
    },
    "memchr3/naive/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/never",
        "directory_name": "memchr3/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.13893145199649,
            "upper_bound": 44.23947333928502
          },
          "point_estimate": 44.18799714660172,
          "standard_error": 0.025576399563192877
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.14044202942257,
            "upper_bound": 44.24959365559725
          },
          "point_estimate": 44.17155352374198,
          "standard_error": 0.02439891982057022
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005557834663794818,
            "upper_bound": 0.1418728116051274
          },
          "point_estimate": 0.05984686026724503,
          "standard_error": 0.03710574200540319
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.122100601072106,
            "upper_bound": 44.18787464563734
          },
          "point_estimate": 44.15791931920085,
          "standard_error": 0.016675406605108422
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03884667122944637,
            "upper_bound": 0.11109687029397552
          },
          "point_estimate": 0.08489784250433295,
          "standard_error": 0.018243453823440677
        }
      }
    },
    "memchr3/naive/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/rare",
        "directory_name": "memchr3/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.170802835683496,
            "upper_bound": 44.36300309526529
          },
          "point_estimate": 44.25635171670283,
          "standard_error": 0.04928793110306923
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.159351598445426,
            "upper_bound": 44.301663300367096
          },
          "point_estimate": 44.23610006902737,
          "standard_error": 0.03932534198516428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030514730287971203,
            "upper_bound": 0.21206897329314617
          },
          "point_estimate": 0.1019648734817318,
          "standard_error": 0.04568839590997487
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.106207348242464,
            "upper_bound": 44.261038584753834
          },
          "point_estimate": 44.18313676936314,
          "standard_error": 0.04068880070177646
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05476052873805922,
            "upper_bound": 0.23640449173917208
          },
          "point_estimate": 0.16448233713410934,
          "standard_error": 0.0528321955608013
        }
      }
    },
    "memchr3/naive/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memchr3/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/uncommon",
        "directory_name": "memchr3/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.2836194215253,
            "upper_bound": 42.357793708233615
          },
          "point_estimate": 42.32268588866439,
          "standard_error": 0.01907595985734069
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.2683168936584,
            "upper_bound": 42.369292938703495
          },
          "point_estimate": 42.34896543592312,
          "standard_error": 0.02699968649264393
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008694851149065933,
            "upper_bound": 0.09896308854287808
          },
          "point_estimate": 0.047294587227128315,
          "standard_error": 0.025410617793325655
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.30014261178081,
            "upper_bound": 42.366801925062504
          },
          "point_estimate": 42.33925393433391,
          "standard_error": 0.017145305956337186
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03019788377723425,
            "upper_bound": 0.07899163338261908
          },
          "point_estimate": 0.06369757695298311,
          "standard_error": 0.0121350374580891
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89515.00953072465,
            "upper_bound": 89641.9859148487
          },
          "point_estimate": 89574.95418728987,
          "standard_error": 32.599834272625884
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89483.50963718821,
            "upper_bound": 89632.07881773399
          },
          "point_estimate": 89585.19996921183,
          "standard_error": 40.6873745308639
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.0342958334712,
            "upper_bound": 189.67178008912077
          },
          "point_estimate": 92.46692824344046,
          "standard_error": 42.20731295071842
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89517.394201242,
            "upper_bound": 89615.93651166333
          },
          "point_estimate": 89564.41798989188,
          "standard_error": 24.96189934588107
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.79062917310029,
            "upper_bound": 145.9097678626467
          },
          "point_estimate": 108.2837378985994,
          "standard_error": 25.368392524108117
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2113807.7143518524,
            "upper_bound": 2115680.1096296296
          },
          "point_estimate": 2114822.2249074075,
          "standard_error": 481.5432219919016
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2113905.5069444445,
            "upper_bound": 2115982.888888889
          },
          "point_estimate": 2115123.263888889,
          "standard_error": 521.5671000406347
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 339.7075828579248,
            "upper_bound": 2467.341503418282
          },
          "point_estimate": 1241.2107335196345,
          "standard_error": 556.4187217634772
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2114355.242680776,
            "upper_bound": 2115433.9809741247
          },
          "point_estimate": 2114872.8412698414,
          "standard_error": 271.16097926977136
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 643.0575178150141,
            "upper_bound": 2142.223194689154
          },
          "point_estimate": 1607.221430635253,
          "standard_error": 403.4769887165428
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2114158.7227592594,
            "upper_bound": 2117360.699981316
          },
          "point_estimate": 2115662.931779101,
          "standard_error": 818.9087630221421
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2113213.092592593,
            "upper_bound": 2117355.9146825396
          },
          "point_estimate": 2115346.8877314813,
          "standard_error": 1094.2738951265842
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 471.8168499571016,
            "upper_bound": 4471.780632415784
          },
          "point_estimate": 2966.806097328548,
          "standard_error": 1021.9747123113906
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2114574.145135566,
            "upper_bound": 2119247.009005265
          },
          "point_estimate": 2117097.0665223664,
          "standard_error": 1242.2604665884946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1371.8661431199232,
            "upper_bound": 3558.3204824752193
          },
          "point_estimate": 2723.172134279586,
          "standard_error": 591.8229204774157
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 803227.7250163043,
            "upper_bound": 804622.921085145
          },
          "point_estimate": 803890.7732556935,
          "standard_error": 358.65374273183994
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 803019.8876811594,
            "upper_bound": 805031.9086956521
          },
          "point_estimate": 803502.4239130435,
          "standard_error": 476.73926009996023
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200.83186436930052,
            "upper_bound": 1818.6393153214055
          },
          "point_estimate": 852.2223691091726,
          "standard_error": 473.2749712590629
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 803021.3483860968,
            "upper_bound": 804805.6846222907
          },
          "point_estimate": 803942.6945228685,
          "standard_error": 464.7592241703111
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 474.3993491099435,
            "upper_bound": 1455.9564887766296
          },
          "point_estimate": 1196.9107540240514,
          "standard_error": 223.77276738559348
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 900978.9053430362,
            "upper_bound": 901758.9522069785
          },
          "point_estimate": 901446.8994066976,
          "standard_error": 209.76682318600191
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 901376.3922764228,
            "upper_bound": 901784.8902439025
          },
          "point_estimate": 901680.1856368564,
          "standard_error": 132.34013571076514
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.73870816661236,
            "upper_bound": 552.8711731113035
          },
          "point_estimate": 312.565222011876,
          "standard_error": 142.8138794238685
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 901487.559832317,
            "upper_bound": 901850.5745713472
          },
          "point_estimate": 901693.2553690212,
          "standard_error": 91.45137957372891
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160.34811740017707,
            "upper_bound": 1054.686502288957
          },
          "point_estimate": 697.3482437098126,
          "standard_error": 294.0318871644857
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11629.60916557419,
            "upper_bound": 11648.11591413471
          },
          "point_estimate": 11637.69505535473,
          "standard_error": 4.80532863332189
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11627.655499120425,
            "upper_bound": 11642.424519538758
          },
          "point_estimate": 11633.943637892377,
          "standard_error": 4.198266877926766
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6118811662661328,
            "upper_bound": 20.275702285773217
          },
          "point_estimate": 12.035125195527886,
          "standard_error": 4.875283563507978
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11628.411844156515,
            "upper_bound": 11639.255593494754
          },
          "point_estimate": 11634.221448954631,
          "standard_error": 2.8201084631973656
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.469734767944504,
            "upper_bound": 23.161570283349413
          },
          "point_estimate": 16.01384591004844,
          "standard_error": 5.33681656376213
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10958.16828564246,
            "upper_bound": 10980.75868627451
          },
          "point_estimate": 10969.033638751229,
          "standard_error": 5.789767581911002
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10954.470782159016,
            "upper_bound": 10982.475691302165
          },
          "point_estimate": 10966.689998324116,
          "standard_error": 8.669156585602838
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.23095463843449937,
            "upper_bound": 34.562673964066235
          },
          "point_estimate": 18.124905184503906,
          "standard_error": 7.8040460131966
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10956.080363819186,
            "upper_bound": 10972.42790784874
          },
          "point_estimate": 10965.167506219272,
          "standard_error": 4.123625014227093
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.4062493172547,
            "upper_bound": 24.93732541077123
          },
          "point_estimate": 19.21014697790664,
          "standard_error": 3.873270373748959
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1324642.5306845242,
            "upper_bound": 1326954.266232993
          },
          "point_estimate": 1325745.299683957,
          "standard_error": 589.5460320980181
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1324305.115079365,
            "upper_bound": 1326658.3989158163
          },
          "point_estimate": 1325889.7574404762,
          "standard_error": 684.0916157035205
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.01263076605062,
            "upper_bound": 3381.3357549692837
          },
          "point_estimate": 1400.3269270140745,
          "standard_error": 762.7398832199219
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1324838.9143046108,
            "upper_bound": 1326467.9100033457
          },
          "point_estimate": 1325790.508812616,
          "standard_error": 428.8511795560878
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 943.076801707489,
            "upper_bound": 2666.112857053039
          },
          "point_estimate": 1961.190127432399,
          "standard_error": 468.75177618459264
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62390.66166339388,
            "upper_bound": 62600.591908034694
          },
          "point_estimate": 62488.87420900834,
          "standard_error": 53.8050165306189
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62356.91712485682,
            "upper_bound": 62629.25515463918
          },
          "point_estimate": 62423.3836984536,
          "standard_error": 65.82530497676764
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.354823790699243,
            "upper_bound": 294.93310964791067
          },
          "point_estimate": 157.31583009368066,
          "standard_error": 68.86549200135049
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62338.76576533133,
            "upper_bound": 62461.704033709706
          },
          "point_estimate": 62388.65679028875,
          "standard_error": 31.350045449472663
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.63336432278079,
            "upper_bound": 227.23926390310191
          },
          "point_estimate": 179.32055310634,
          "standard_error": 38.04888753347107
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60418.51497041667,
            "upper_bound": 60631.10521666666
          },
          "point_estimate": 60530.07610833333,
          "standard_error": 54.138003678198366
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60446.096666666665,
            "upper_bound": 60669.38
          },
          "point_estimate": 60521.31625,
          "standard_error": 51.59059462802897
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.835540041329345,
            "upper_bound": 285.9972414225396
          },
          "point_estimate": 117.12354467065047,
          "standard_error": 67.51777799922762
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60524.020918472954,
            "upper_bound": 60715.22244825125
          },
          "point_estimate": 60631.39501731602,
          "standard_error": 48.620817870659835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.71015957106961,
            "upper_bound": 250.66146652770243
          },
          "point_estimate": 180.37188392554205,
          "standard_error": 46.681364894630995
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 811495.9262592595,
            "upper_bound": 812842.8156259259
          },
          "point_estimate": 812153.2847142856,
          "standard_error": 344.97916408168305
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 811265.2888888889,
            "upper_bound": 812881.1285714286
          },
          "point_estimate": 812230.2325925926,
          "standard_error": 396.969095526504
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.23351461162214,
            "upper_bound": 1983.881144779056
          },
          "point_estimate": 1038.0701016816408,
          "standard_error": 452.67263325882664
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 811663.7974554339,
            "upper_bound": 812443.14728908
          },
          "point_estimate": 812118.621010101,
          "standard_error": 197.81590087473023
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 582.3659023586144,
            "upper_bound": 1522.2195430756587
          },
          "point_estimate": 1152.439645435697,
          "standard_error": 242.6350773967951
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66368.70986699603,
            "upper_bound": 66537.29471001838
          },
          "point_estimate": 66452.21808845413,
          "standard_error": 43.22147457839078
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66316.49931066176,
            "upper_bound": 66595.1068802521
          },
          "point_estimate": 66428.91697303922,
          "standard_error": 63.17279735351292
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.7765797376532,
            "upper_bound": 248.4788499268529
          },
          "point_estimate": 177.30406953145211,
          "standard_error": 60.189212028294
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66371.31531368173,
            "upper_bound": 66522.07157863992
          },
          "point_estimate": 66447.92223548511,
          "standard_error": 38.09090383873435
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.48733899243729,
            "upper_bound": 174.7083318096055
          },
          "point_estimate": 144.2369122953707,
          "standard_error": 22.37674599531921
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24117.014043608586,
            "upper_bound": 24192.171002089664
          },
          "point_estimate": 24155.658478449004,
          "standard_error": 19.117263342575228
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24117.876894946807,
            "upper_bound": 24193.683890577508
          },
          "point_estimate": 24164.68691637116,
          "standard_error": 22.320654836380204
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.656379784168442,
            "upper_bound": 110.25309358783016
          },
          "point_estimate": 55.0138238504377,
          "standard_error": 23.12977659272768
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24118.987954800945,
            "upper_bound": 24174.170715589855
          },
          "point_estimate": 24147.07318665377,
          "standard_error": 13.984915299633226
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.60058293135682,
            "upper_bound": 85.35573158799856
          },
          "point_estimate": 63.738613842613795,
          "standard_error": 14.304197486217175
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11917.33536042585,
            "upper_bound": 11946.306236156706
          },
          "point_estimate": 11933.076150064097,
          "standard_error": 7.473233567942004
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11918.946552856203,
            "upper_bound": 11950.11129349967
          },
          "point_estimate": 11937.466800722255,
          "standard_error": 7.556655144756734
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.439473246856636,
            "upper_bound": 37.89909720080195
          },
          "point_estimate": 20.59291694398999,
          "standard_error": 8.528976162425446
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11932.48906028614,
            "upper_bound": 11956.330478579186
          },
          "point_estimate": 11946.484227984753,
          "standard_error": 6.19504339614094
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.521931823606108,
            "upper_bound": 34.28237000910823
          },
          "point_estimate": 24.959301098794263,
          "standard_error": 6.643529870550195
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9407.145688532475,
            "upper_bound": 9418.637980453208
          },
          "point_estimate": 9413.236124774508,
          "standard_error": 2.938899218824493
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9407.358822003114,
            "upper_bound": 9420.517384535548
          },
          "point_estimate": 9414.004366520869,
          "standard_error": 2.8674942167972572
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1698460093296923,
            "upper_bound": 16.79362802178019
          },
          "point_estimate": 7.37433840047516,
          "standard_error": 4.413029157876736
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9413.39931985558,
            "upper_bound": 9420.788130010696
          },
          "point_estimate": 9417.226031985658,
          "standard_error": 1.886392017356986
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.548502019002583,
            "upper_bound": 12.937003377203634
          },
          "point_estimate": 9.767326183231864,
          "standard_error": 2.245742276259054
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9486.81624369984,
            "upper_bound": 9499.382584854964
          },
          "point_estimate": 9493.261883496374,
          "standard_error": 3.225451180975005
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9484.274108877868,
            "upper_bound": 9502.858820897192
          },
          "point_estimate": 9494.788599582354,
          "standard_error": 5.505536127559887
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.203340544280622,
            "upper_bound": 17.769685665990767
          },
          "point_estimate": 12.1575699525241,
          "standard_error": 4.235257918507861
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9481.35544719847,
            "upper_bound": 9498.853540761726
          },
          "point_estimate": 9489.368715909515,
          "standard_error": 4.552302556775374
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.52872272383397,
            "upper_bound": 12.949447005974797
          },
          "point_estimate": 10.743801324531011,
          "standard_error": 1.6388954476522637
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57395.64528744726,
            "upper_bound": 57477.199136013165
          },
          "point_estimate": 57435.899677077054,
          "standard_error": 20.917902411312607
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57378.44894514768,
            "upper_bound": 57502.692721518986
          },
          "point_estimate": 57429.091475474685,
          "standard_error": 25.014951789875628
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.252596669147728,
            "upper_bound": 123.05884746716649
          },
          "point_estimate": 71.35809974579863,
          "standard_error": 35.3536849225947
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57403.163425878665,
            "upper_bound": 57507.881892627134
          },
          "point_estimate": 57465.556279796154,
          "standard_error": 26.66323174687233
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.513375178909655,
            "upper_bound": 87.94690781749583
          },
          "point_estimate": 69.6573198933936,
          "standard_error": 12.774757496342929
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74353.9391419158,
            "upper_bound": 74615.51161440581
          },
          "point_estimate": 74474.62569002176,
          "standard_error": 67.0921485169062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74324.11315610088,
            "upper_bound": 74599.69000511247
          },
          "point_estimate": 74438.6479493297,
          "standard_error": 81.10946243470057
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.816303637500855,
            "upper_bound": 346.05520612623775
          },
          "point_estimate": 168.93554628505714,
          "standard_error": 82.77427572139312
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74373.07503917807,
            "upper_bound": 74565.5576114928
          },
          "point_estimate": 74487.23223647517,
          "standard_error": 48.297129161111016
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.6787602356434,
            "upper_bound": 304.56452091843664
          },
          "point_estimate": 223.70719994249183,
          "standard_error": 56.90035538396037
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112402.99232419282,
            "upper_bound": 112708.04593395253
          },
          "point_estimate": 112558.61064204627,
          "standard_error": 78.12905235519757
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112303.5993660622,
            "upper_bound": 112815.9334365325
          },
          "point_estimate": 112596.78637770898,
          "standard_error": 125.8964972951518
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.53654295999919,
            "upper_bound": 451.67010390075706
          },
          "point_estimate": 326.8163284857776,
          "standard_error": 100.67255897271004
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112455.02760411466,
            "upper_bound": 112785.65932393404
          },
          "point_estimate": 112642.26490289895,
          "standard_error": 85.19331756188237
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159.6952730517337,
            "upper_bound": 311.4419691479995
          },
          "point_estimate": 260.77756705869234,
          "standard_error": 38.72315174053674
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61786.15637724733,
            "upper_bound": 61938.10220481049
          },
          "point_estimate": 61849.39147797214,
          "standard_error": 39.90500448095752
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61771.06802721089,
            "upper_bound": 61897.782069970846
          },
          "point_estimate": 61785.74149659864,
          "standard_error": 37.766276311739304
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5978278110240725,
            "upper_bound": 153.5794761424672
          },
          "point_estimate": 30.140274107758025,
          "standard_error": 42.09572984228575
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61781.65316499195,
            "upper_bound": 61875.47560541311
          },
          "point_estimate": 61823.15304355508,
          "standard_error": 24.12283961170733
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.883515640278596,
            "upper_bound": 195.5164343934157
          },
          "point_estimate": 132.8473003777338,
          "standard_error": 49.407376516816605
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881916.1803968254,
            "upper_bound": 883228.3607426304
          },
          "point_estimate": 882570.5001077097,
          "standard_error": 337.22645396899406
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881477.3333333334,
            "upper_bound": 883384.9771825396
          },
          "point_estimate": 882675.0,
          "standard_error": 399.4101688072766
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.1514816085114,
            "upper_bound": 2088.6803712517813
          },
          "point_estimate": 1192.1238431212903,
          "standard_error": 604.8848119488817
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 882308.9849346818,
            "upper_bound": 883784.1745198079
          },
          "point_estimate": 883082.7074211503,
          "standard_error": 393.2582359600901
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 639.2284350945794,
            "upper_bound": 1406.6684976182696
          },
          "point_estimate": 1123.2668159431612,
          "standard_error": 199.53505772004416
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 718371.3886836874,
            "upper_bound": 719524.5699396788
          },
          "point_estimate": 718916.0287527234,
          "standard_error": 296.44580417854405
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 718214.8349673203,
            "upper_bound": 719638.5291666667
          },
          "point_estimate": 718599.1323529412,
          "standard_error": 329.95966022816356
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.04377453465732,
            "upper_bound": 1689.7864157356546
          },
          "point_estimate": 767.0264879512074,
          "standard_error": 410.0653027713893
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 718371.8545349174,
            "upper_bound": 719553.5186331306
          },
          "point_estimate": 719038.1171377642,
          "standard_error": 294.9315619953003
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 428.4833121726853,
            "upper_bound": 1260.713025657544
          },
          "point_estimate": 988.1974853310304,
          "standard_error": 210.1989621533996
        }
      }
    },
    "memmem/bstr/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/bstr_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72666.21747233333,
            "upper_bound": 72801.296956875
          },
          "point_estimate": 72731.82750388888,
          "standard_error": 34.64598886372046
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72624.90675,
            "upper_bound": 72790.594
          },
          "point_estimate": 72753.28583333333,
          "standard_error": 42.525173247036264
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.174652521619338,
            "upper_bound": 210.87439495622323
          },
          "point_estimate": 87.31774192313436,
          "standard_error": 49.91585541548641
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72666.15559690929,
            "upper_bound": 72774.38578920836
          },
          "point_estimate": 72719.9687012987,
          "standard_error": 27.736246995565537
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.55570809594902,
            "upper_bound": 153.80934652091616
          },
          "point_estimate": 115.71569213792522,
          "standard_error": 24.96580668114665
        }
      }
    },
    "memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/bstr_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72113.42592985285,
            "upper_bound": 72257.5527402211
          },
          "point_estimate": 72178.66391605254,
          "standard_error": 37.1005860652812
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72095.70634920635,
            "upper_bound": 72246.42552437642
          },
          "point_estimate": 72136.73991402116,
          "standard_error": 40.392152781825914
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.007412180308123,
            "upper_bound": 182.6202743372039
          },
          "point_estimate": 81.97560739880494,
          "standard_error": 42.926106126535664
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72105.91026272962,
            "upper_bound": 72201.40400126901
          },
          "point_estimate": 72141.31196145125,
          "standard_error": 24.934890538264412
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.43124380774318,
            "upper_bound": 170.0733855235193
          },
          "point_estimate": 124.01065013675064,
          "standard_error": 33.975051906941644
        }
      }
    },
    "memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/bstr_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 440081.5612774909,
            "upper_bound": 441043.36037292023
          },
          "point_estimate": 440564.94621390325,
          "standard_error": 245.2101850442626
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 440046.1325301205,
            "upper_bound": 441160.1852409639
          },
          "point_estimate": 440608.9062918339,
          "standard_error": 251.37616290544025
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107.60817784862958,
            "upper_bound": 1442.8428177579003
          },
          "point_estimate": 825.8472598864845,
          "standard_error": 322.0511796716708
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 440584.2962556232,
            "upper_bound": 441341.4135072461
          },
          "point_estimate": 440995.39718353935,
          "standard_error": 188.3792535509992
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 386.9195535103749,
            "upper_bound": 1087.852503684154
          },
          "point_estimate": 814.5409848597716,
          "standard_error": 179.09679322483316
        }
      }
    },
    "memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/bstr_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1100.0485703669435,
            "upper_bound": 1103.2800469692208
          },
          "point_estimate": 1101.411332608208,
          "standard_error": 0.8490434536947634
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1099.788608998664,
            "upper_bound": 1102.0484092697393
          },
          "point_estimate": 1100.5269539354445,
          "standard_error": 0.5465772712367117
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04650203107235316,
            "upper_bound": 3.0487299610060945
          },
          "point_estimate": 1.3092872289217146,
          "standard_error": 0.7927368252633319
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1100.0711392039532,
            "upper_bound": 1101.9083543048555
          },
          "point_estimate": 1100.8822938705214,
          "standard_error": 0.4727022834295523
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7608916093839293,
            "upper_bound": 4.152170188864429
          },
          "point_estimate": 2.8289976280864275,
          "standard_error": 1.036864870888649
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.22103502111912,
            "upper_bound": 57.29201513411947
          },
          "point_estimate": 57.25587211023262,
          "standard_error": 0.01817188107259949
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.20756994794206,
            "upper_bound": 57.28362062186434
          },
          "point_estimate": 57.27167740254021,
          "standard_error": 0.023969152093344576
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00029321822922977604,
            "upper_bound": 0.1127984878532054
          },
          "point_estimate": 0.04069131604071496,
          "standard_error": 0.02994116669772084
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.20253538325256,
            "upper_bound": 57.25893149558232
          },
          "point_estimate": 57.22344783819738,
          "standard_error": 0.01435801952731684
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.031142275835801928,
            "upper_bound": 0.08035785546025821
          },
          "point_estimate": 0.06029499082406691,
          "standard_error": 0.012957535912151652
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.970432748320455,
            "upper_bound": 52.1016055839781
          },
          "point_estimate": 52.03224692955224,
          "standard_error": 0.03376281327518726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.93682948327571,
            "upper_bound": 52.10801958607111
          },
          "point_estimate": 52.017497264608465,
          "standard_error": 0.05570536616604133
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013499801009215311,
            "upper_bound": 0.21793245587988985
          },
          "point_estimate": 0.12449423430074692,
          "standard_error": 0.05099077941035184
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.96834384942155,
            "upper_bound": 52.08073845487894
          },
          "point_estimate": 52.02593523575031,
          "standard_error": 0.02948408832567346
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0630065235723721,
            "upper_bound": 0.14767002346055064
          },
          "point_estimate": 0.11290885663926828,
          "standard_error": 0.02391817781957552
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.558567562316675,
            "upper_bound": 36.64431043301135
          },
          "point_estimate": 36.60185907471684,
          "standard_error": 0.02184473097885463
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.56395008284755,
            "upper_bound": 36.64771161229754
          },
          "point_estimate": 36.60396367023237,
          "standard_error": 0.018150660674396454
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030771784472295827,
            "upper_bound": 0.12261043430744248
          },
          "point_estimate": 0.04514027210070126,
          "standard_error": 0.03446395027614164
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.577427021139435,
            "upper_bound": 36.62987051859744
          },
          "point_estimate": 36.60620776438439,
          "standard_error": 0.013085206605312438
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03242002612432571,
            "upper_bound": 0.0975735109292491
          },
          "point_estimate": 0.07289574853208546,
          "standard_error": 0.01644865962937519
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.897774353248714,
            "upper_bound": 47.97103630121423
          },
          "point_estimate": 47.9354655669133,
          "standard_error": 0.018746158381770645
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.8820626095783,
            "upper_bound": 47.99038148783551
          },
          "point_estimate": 47.94683590608893,
          "standard_error": 0.029975064618601957
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01221294635510709,
            "upper_bound": 0.1043581923892176
          },
          "point_estimate": 0.07035699976595569,
          "standard_error": 0.024248990956587276
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.91315016505676,
            "upper_bound": 47.983501796389056
          },
          "point_estimate": 47.95361508184271,
          "standard_error": 0.01813912074881477
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03536978175329731,
            "upper_bound": 0.0751499126119156
          },
          "point_estimate": 0.0623245473597251,
          "standard_error": 0.010044301670818954
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.1966056797026,
            "upper_bound": 53.282532043617174
          },
          "point_estimate": 53.238193595393874,
          "standard_error": 0.02192761960649202
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.177987222864616,
            "upper_bound": 53.31372497238104
          },
          "point_estimate": 53.19569344739878,
          "standard_error": 0.044007822515132605
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005313353034432926,
            "upper_bound": 0.113271729279653
          },
          "point_estimate": 0.04282174281160111,
          "standard_error": 0.03131886016951233
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.17956481844616,
            "upper_bound": 53.21838027132082
          },
          "point_estimate": 53.19292641008934,
          "standard_error": 0.010010324802717798
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04402702878298853,
            "upper_bound": 0.0819709399830222
          },
          "point_estimate": 0.07304374063990766,
          "standard_error": 0.009668501810956808
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.59988747088678,
            "upper_bound": 74.71561570408235
          },
          "point_estimate": 74.65676177740303,
          "standard_error": 0.029505675728817966
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.59616539966535,
            "upper_bound": 74.7090139055407
          },
          "point_estimate": 74.66833133511636,
          "standard_error": 0.028312915248423495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035921376594870635,
            "upper_bound": 0.1655034865633948
          },
          "point_estimate": 0.08659932275780734,
          "standard_error": 0.04014789242507665
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.59963048065646,
            "upper_bound": 74.6636846301597
          },
          "point_estimate": 74.63232503690679,
          "standard_error": 0.016228546665624694
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.039894542999205,
            "upper_bound": 0.13390269814209146
          },
          "point_estimate": 0.09804096268140144,
          "standard_error": 0.023859050842090397
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.09784340962173,
            "upper_bound": 88.2832636068145
          },
          "point_estimate": 88.19337566691995,
          "standard_error": 0.04744711505934301
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.10561251716126,
            "upper_bound": 88.32182256116197
          },
          "point_estimate": 88.18122165732265,
          "standard_error": 0.056545092553895504
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029572011509286136,
            "upper_bound": 0.27826259318826324
          },
          "point_estimate": 0.14122103253655058,
          "standard_error": 0.06287112862671052
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.16452462262653,
            "upper_bound": 88.28609047737186
          },
          "point_estimate": 88.2120418030447,
          "standard_error": 0.03089605502776075
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08455651316090387,
            "upper_bound": 0.20767198468386777
          },
          "point_estimate": 0.15806515724686088,
          "standard_error": 0.032897342616581914
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.20805717479405,
            "upper_bound": 66.3000467910571
          },
          "point_estimate": 66.2555779106645,
          "standard_error": 0.023587604394101146
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.17701646592045,
            "upper_bound": 66.32332610925785
          },
          "point_estimate": 66.29156514915609,
          "standard_error": 0.04615437928743969
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012996298747770396,
            "upper_bound": 0.12365543897952697
          },
          "point_estimate": 0.06660373045572493,
          "standard_error": 0.030217690476610187
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.21069987705765,
            "upper_bound": 66.31079465655763
          },
          "point_estimate": 66.27360263739956,
          "standard_error": 0.025890165640641047
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.046571280714588735,
            "upper_bound": 0.0907739033669788
          },
          "point_estimate": 0.07859329093402273,
          "standard_error": 0.01080042495473261
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.90058077727448,
            "upper_bound": 113.03851173861374
          },
          "point_estimate": 112.96472785227756,
          "standard_error": 0.035341263457866715
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.887264799408,
            "upper_bound": 113.03288860540796
          },
          "point_estimate": 112.9411248318982,
          "standard_error": 0.032892560175564266
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017303709522538225,
            "upper_bound": 0.18577555756061592
          },
          "point_estimate": 0.07633990057801288,
          "standard_error": 0.04281506083614428
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.92093278362964,
            "upper_bound": 113.07967868335864
          },
          "point_estimate": 112.99038189222696,
          "standard_error": 0.04011415922846516
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05008084108711983,
            "upper_bound": 0.15982625890139782
          },
          "point_estimate": 0.1176914356924211,
          "standard_error": 0.02989919351121861
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.8317345215694,
            "upper_bound": 61.906529245732166
          },
          "point_estimate": 61.869247402201154,
          "standard_error": 0.01923224455848034
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.8099906454681,
            "upper_bound": 61.92655112902129
          },
          "point_estimate": 61.86857614092362,
          "standard_error": 0.02991649046106548
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02050129753441506,
            "upper_bound": 0.11358153335612375
          },
          "point_estimate": 0.07103006218264794,
          "standard_error": 0.02276470518857388
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.81717430277192,
            "upper_bound": 61.890211222199554
          },
          "point_estimate": 61.847444602258925,
          "standard_error": 0.01865415067732323
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0395552185350036,
            "upper_bound": 0.0777290963910302
          },
          "point_estimate": 0.06422476975854884,
          "standard_error": 0.00975924937582277
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.4135531332618,
            "upper_bound": 50.493480309839114
          },
          "point_estimate": 50.45249747568276,
          "standard_error": 0.020367588545014647
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.408700822307935,
            "upper_bound": 50.47985976849441
          },
          "point_estimate": 50.45021172180552,
          "standard_error": 0.014773006103304035
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0050007908748091095,
            "upper_bound": 0.1139974490464185
          },
          "point_estimate": 0.03156975510482433,
          "standard_error": 0.0287433376385252
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.425573009871854,
            "upper_bound": 50.468055095182784
          },
          "point_estimate": 50.44774353940817,
          "standard_error": 0.010727714352495416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022327369191477937,
            "upper_bound": 0.09438459449926433
          },
          "point_estimate": 0.06799041538166357,
          "standard_error": 0.017418316261310816
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.32132394347738,
            "upper_bound": 108.43451439927942
          },
          "point_estimate": 108.37364773698248,
          "standard_error": 0.029160088964408668
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.30087320409208,
            "upper_bound": 108.45604569126812
          },
          "point_estimate": 108.33458999713984,
          "standard_error": 0.035864816640548455
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002081753845146222,
            "upper_bound": 0.1535598281263243
          },
          "point_estimate": 0.07943900736087,
          "standard_error": 0.037645375260103434
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.32629799088312,
            "upper_bound": 108.48175557514251
          },
          "point_estimate": 108.4025211235714,
          "standard_error": 0.040395097473926195
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03759248292367585,
            "upper_bound": 0.12272890737989954
          },
          "point_estimate": 0.09735292001400486,
          "standard_error": 0.02191303043891142
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2019124.690215774,
            "upper_bound": 2020518.536945602
          },
          "point_estimate": 2019828.4980621696,
          "standard_error": 356.9393656872687
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2018603.388888889,
            "upper_bound": 2020664.433333333
          },
          "point_estimate": 2020256.259259259,
          "standard_error": 639.8044004726286
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.3934990937703,
            "upper_bound": 2041.355819314302
          },
          "point_estimate": 1411.3751159152027,
          "standard_error": 530.7004191706735
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2018787.891774572,
            "upper_bound": 2020583.753424553
          },
          "point_estimate": 2019654.910678211,
          "standard_error": 475.30427309261023
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 742.5457145685092,
            "upper_bound": 1432.643115796443
          },
          "point_estimate": 1190.5055577094015,
          "standard_error": 174.4539161813806
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1668438.4023819442,
            "upper_bound": 1669870.1586144029
          },
          "point_estimate": 1669207.6511020923,
          "standard_error": 369.0171178061371
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1668208.588068182,
            "upper_bound": 1670230.0833333333
          },
          "point_estimate": 1669538.1284271283,
          "standard_error": 408.31569363767915
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193.15469702545928,
            "upper_bound": 1977.8252694320024
          },
          "point_estimate": 887.3051967471263,
          "standard_error": 444.9323836355949
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1669143.1655953475,
            "upper_bound": 1669825.8953208555
          },
          "point_estimate": 1669536.4236127508,
          "standard_error": 172.61818398816246
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464.54832051556673,
            "upper_bound": 1577.709180967334
          },
          "point_estimate": 1232.866056062088,
          "standard_error": 289.6254264810184
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2179952.011420752,
            "upper_bound": 2183920.582026144
          },
          "point_estimate": 2181572.8669421105,
          "standard_error": 1058.344511422409
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2179523.4172268906,
            "upper_bound": 2182054.120915033
          },
          "point_estimate": 2180775.5392156863,
          "standard_error": 794.8846568068396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 183.62436732847448,
            "upper_bound": 3187.5700785075064
          },
          "point_estimate": 1526.761830247358,
          "standard_error": 752.7314452144063
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2179480.1399676846,
            "upper_bound": 2181882.2352012456
          },
          "point_estimate": 2180544.024446142,
          "standard_error": 622.5176523676116
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 948.3456467840272,
            "upper_bound": 5283.6858661310725
          },
          "point_estimate": 3528.1210906393862,
          "standard_error": 1413.5959872117226
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 381774.730380117,
            "upper_bound": 383440.72734853806
          },
          "point_estimate": 382591.60062573093,
          "standard_error": 426.62901464699354
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 381517.5223684211,
            "upper_bound": 383762.2701754386
          },
          "point_estimate": 382442.1716374269,
          "standard_error": 628.04475637802
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.85306301083074,
            "upper_bound": 2427.546771639264
          },
          "point_estimate": 1532.849275067141,
          "standard_error": 530.8960282032987
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 381895.7849555395,
            "upper_bound": 383146.6174082838
          },
          "point_estimate": 382413.0671770335,
          "standard_error": 315.16247333274316
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 822.9285874491467,
            "upper_bound": 1750.5930712596826
          },
          "point_estimate": 1423.1090958593454,
          "standard_error": 233.90606858495624
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 491755.5060617761,
            "upper_bound": 492879.4002927927
          },
          "point_estimate": 492242.6004761905,
          "standard_error": 291.9053682742707
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 491529.1824324324,
            "upper_bound": 492563.9996782497
          },
          "point_estimate": 492082.8327702703,
          "standard_error": 302.55071181729676
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.82280875059396,
            "upper_bound": 1254.87132974858
          },
          "point_estimate": 750.3132930982407,
          "standard_error": 278.60036718205833
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 491725.7478030422,
            "upper_bound": 492289.89533921675
          },
          "point_estimate": 491997.33524043526,
          "standard_error": 142.94669720816958
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 361.493458603623,
            "upper_bound": 1408.415600492288
          },
          "point_estimate": 973.1900520442168,
          "standard_error": 325.0383636776082
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 826102.4435227273,
            "upper_bound": 826957.3126383928
          },
          "point_estimate": 826513.9952164502,
          "standard_error": 219.1344663969404
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 825885.0386363637,
            "upper_bound": 827093.1448863636
          },
          "point_estimate": 826343.7992424243,
          "standard_error": 331.1764477904778
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182.7933449366671,
            "upper_bound": 1172.6944598622829
          },
          "point_estimate": 811.1211793497205,
          "standard_error": 266.2501449614457
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 825978.1769509107,
            "upper_bound": 827219.8869400527
          },
          "point_estimate": 826546.9329397875,
          "standard_error": 318.6480218666057
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 408.000713687925,
            "upper_bound": 903.8981010215654
          },
          "point_estimate": 728.82297121552,
          "standard_error": 128.10179208895372
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 940322.6882905982,
            "upper_bound": 941723.8602359584
          },
          "point_estimate": 941009.4298087098,
          "standard_error": 358.70314211269954
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 939809.9948717948,
            "upper_bound": 941898.3603988604
          },
          "point_estimate": 940970.0961538462,
          "standard_error": 435.0315814213709
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.08123394050004,
            "upper_bound": 2174.748331903404
          },
          "point_estimate": 1422.134876290516,
          "standard_error": 621.4802515367471
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 940400.7508632723,
            "upper_bound": 941310.6945206488
          },
          "point_estimate": 940910.0256410256,
          "standard_error": 227.62786104714752
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 661.7000163258309,
            "upper_bound": 1486.1426205094888
          },
          "point_estimate": 1196.4692440728793,
          "standard_error": 211.92423556748764
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 763364.1303458579,
            "upper_bound": 764832.8712911087
          },
          "point_estimate": 763986.1021502976,
          "standard_error": 384.6879376635739
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 763272.2270833333,
            "upper_bound": 764206.7825520834
          },
          "point_estimate": 763792.8874007936,
          "standard_error": 232.52591185074303
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.93618888043277,
            "upper_bound": 1309.3818471704958
          },
          "point_estimate": 533.2825693864368,
          "standard_error": 321.88127491006617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 763590.6915744158,
            "upper_bound": 764003.33546619
          },
          "point_estimate": 763795.4847402597,
          "standard_error": 106.46004085803538
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 347.31430834294946,
            "upper_bound": 1899.1968459486009
          },
          "point_estimate": 1285.2293699796603,
          "standard_error": 481.9727544828193
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269372.7172952381,
            "upper_bound": 269908.97174358467
          },
          "point_estimate": 269648.8550520282,
          "standard_error": 137.60449928305474
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269243.22469135805,
            "upper_bound": 269984.85462962964
          },
          "point_estimate": 269738.2066798942,
          "standard_error": 176.0854708097548
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.03045159294231,
            "upper_bound": 770.7524759460013
          },
          "point_estimate": 442.9969172278327,
          "standard_error": 182.61095261011044
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269332.7956925038,
            "upper_bound": 269850.7786210826
          },
          "point_estimate": 269611.0726503127,
          "standard_error": 145.2658201832226
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236.2377676346212,
            "upper_bound": 577.7930978345578
          },
          "point_estimate": 459.7360023719192,
          "standard_error": 85.30385085448827
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252138.27785706075,
            "upper_bound": 252629.2403437329
          },
          "point_estimate": 252363.72259660647,
          "standard_error": 126.01622452143762
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252044.4896551724,
            "upper_bound": 252659.06720306515
          },
          "point_estimate": 252235.69011494255,
          "standard_error": 145.81038203228928
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.0517338294902,
            "upper_bound": 663.9297403508455
          },
          "point_estimate": 331.1521668105608,
          "standard_error": 152.1264623836838
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252155.5011013896,
            "upper_bound": 252461.60194626616
          },
          "point_estimate": 252270.76791759965,
          "standard_error": 77.68792322004134
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.60811097585662,
            "upper_bound": 528.2018252072971
          },
          "point_estimate": 420.1686076493258,
          "standard_error": 95.2606623666509
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128090.05057708747,
            "upper_bound": 128231.67190170188
          },
          "point_estimate": 128164.61009249944,
          "standard_error": 36.30707150741544
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128067.67253521128,
            "upper_bound": 128263.37018779342
          },
          "point_estimate": 128193.56821484462,
          "standard_error": 52.660096442113
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.003969191944854,
            "upper_bound": 207.1052330484903
          },
          "point_estimate": 117.23307949147448,
          "standard_error": 48.36157399844651
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128069.39895998508,
            "upper_bound": 128245.80102089045
          },
          "point_estimate": 128178.16059081764,
          "standard_error": 47.53180361792188
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.25972649854393,
            "upper_bound": 150.32834370632116
          },
          "point_estimate": 121.30682453114596,
          "standard_error": 22.874197945643775
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159165.07103216377,
            "upper_bound": 159855.36525415536
          },
          "point_estimate": 159511.18340973963,
          "standard_error": 176.8618733646288
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158882.15497076022,
            "upper_bound": 160069.71984649124
          },
          "point_estimate": 159519.6019736842,
          "standard_error": 313.07782810450396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147.2547989208111,
            "upper_bound": 980.2964031225767
          },
          "point_estimate": 880.3418267502074,
          "standard_error": 221.80963230566596
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159156.29855161157,
            "upper_bound": 160042.32418221346
          },
          "point_estimate": 159624.0280359991,
          "standard_error": 239.4506063288431
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.7087537863377,
            "upper_bound": 694.1869175055402
          },
          "point_estimate": 590.2889814204437,
          "standard_error": 79.91690060709553
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59817.23497048407,
            "upper_bound": 59930.9483350551
          },
          "point_estimate": 59876.174890594266,
          "standard_error": 29.068483226969317
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59819.77190082645,
            "upper_bound": 59949.62939787485
          },
          "point_estimate": 59877.84033057851,
          "standard_error": 40.35006733206624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.22821889829948,
            "upper_bound": 174.1549799825116
          },
          "point_estimate": 95.48832165185073,
          "standard_error": 37.446563026911896
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59759.42380668436,
            "upper_bound": 59966.73408178402
          },
          "point_estimate": 59862.35208758184,
          "standard_error": 55.65813559957055
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.432379105013936,
            "upper_bound": 126.86206844543686
          },
          "point_estimate": 96.64176316346172,
          "standard_error": 19.991946275076536
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/bstr_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 216015.42346646945,
            "upper_bound": 216198.41561127547
          },
          "point_estimate": 216099.12331525312,
          "standard_error": 46.910682868833334
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 215958.6130177515,
            "upper_bound": 216187.33201840895
          },
          "point_estimate": 216084.7186390533,
          "standard_error": 58.12126439702391
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.143280624646096,
            "upper_bound": 254.50347153120663
          },
          "point_estimate": 162.80838732654652,
          "standard_error": 55.77579693114866
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 216052.31563085096,
            "upper_bound": 216226.4484965584
          },
          "point_estimate": 216131.87729193884,
          "standard_error": 43.66174434424593
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.43893973367099,
            "upper_bound": 213.0130938455204
          },
          "point_estimate": 156.08529163346313,
          "standard_error": 40.25719736186814
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/bstr_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 591079.0149020738,
            "upper_bound": 594585.3584120583
          },
          "point_estimate": 592672.3374443165,
          "standard_error": 901.5038531170892
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 590919.0967741936,
            "upper_bound": 594077.9845430108
          },
          "point_estimate": 591749.8183467742,
          "standard_error": 927.5720716816052
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 313.6906545922108,
            "upper_bound": 4185.296628357448
          },
          "point_estimate": 2516.9077476546568,
          "standard_error": 1049.3205754009778
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 591177.7310904991,
            "upper_bound": 593448.3727446692
          },
          "point_estimate": 592017.4880184332,
          "standard_error": 573.9735327667719
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1213.1758079505812,
            "upper_bound": 4207.669218900571
          },
          "point_estimate": 3006.69157928755,
          "standard_error": 870.161398976338
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/bstr_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1631.3312465851368,
            "upper_bound": 1637.5205796065525
          },
          "point_estimate": 1634.1917951324465,
          "standard_error": 1.5852429943031658
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1630.8206453000507,
            "upper_bound": 1636.6136915194822
          },
          "point_estimate": 1632.9989595973216,
          "standard_error": 1.6438319991839183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3100856375370286,
            "upper_bound": 7.813718028252599
          },
          "point_estimate": 4.213497781471615,
          "standard_error": 1.8047846907518592
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1630.2794660914117,
            "upper_bound": 1634.9824143021413
          },
          "point_estimate": 1632.4146265132108,
          "standard_error": 1.1958244301447587
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.2206945245072065,
            "upper_bound": 7.314437042786824
          },
          "point_estimate": 5.2804077050799805,
          "standard_error": 1.4219132342045064
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88228.58263374842,
            "upper_bound": 88549.82512913641
          },
          "point_estimate": 88364.7006603828,
          "standard_error": 84.17362732236181
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88204.10098870056,
            "upper_bound": 88430.62863196126
          },
          "point_estimate": 88304.09588377723,
          "standard_error": 62.18427096499932
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.687995678791154,
            "upper_bound": 295.18397950520284
          },
          "point_estimate": 157.56169677898703,
          "standard_error": 64.09400107508927
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88232.59478450363,
            "upper_bound": 88389.0306605913
          },
          "point_estimate": 88299.61349643093,
          "standard_error": 40.03212415017125
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.75789527086299,
            "upper_bound": 413.3616326706912
          },
          "point_estimate": 280.75149704911945,
          "standard_error": 103.51623313716152
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2115342.4758344353,
            "upper_bound": 2122939.4556172835
          },
          "point_estimate": 2118421.849303351,
          "standard_error": 2020.32476401819
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2114332.626984127,
            "upper_bound": 2119634.3441358022
          },
          "point_estimate": 2116044.827777778,
          "standard_error": 1763.7268506724831
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397.6250762741538,
            "upper_bound": 6864.930705345421
          },
          "point_estimate": 3041.5867926678143,
          "standard_error": 1684.884537476571
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2115270.047386587,
            "upper_bound": 2118596.5238791425
          },
          "point_estimate": 2116975.257864358,
          "standard_error": 841.1774109478135
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1834.766282093069,
            "upper_bound": 10072.482599841947
          },
          "point_estimate": 6722.747323743994,
          "standard_error": 2687.6050627434993
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2116574.8185333996,
            "upper_bound": 2117947.8145023156
          },
          "point_estimate": 2117288.466521164,
          "standard_error": 351.3108295449436
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2116292.076388889,
            "upper_bound": 2118325.8333333335
          },
          "point_estimate": 2117475.328703704,
          "standard_error": 512.3837207109464
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 303.5692084995092,
            "upper_bound": 2010.7820976348096
          },
          "point_estimate": 1285.290627181645,
          "standard_error": 448.2221458864112
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2116073.492979243,
            "upper_bound": 2118341.691296874
          },
          "point_estimate": 2117399.1379509377,
          "standard_error": 580.2390873564246
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 621.5321196849499,
            "upper_bound": 1435.047982168962
          },
          "point_estimate": 1168.2702210545542,
          "standard_error": 206.1959177761827
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 802716.8190640096,
            "upper_bound": 803527.0457155799
          },
          "point_estimate": 803115.6517943408,
          "standard_error": 207.56410816569175
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 802573.3804347826,
            "upper_bound": 803663.0416666667
          },
          "point_estimate": 803128.4800724639,
          "standard_error": 285.17782531553917
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119.20426092723962,
            "upper_bound": 1210.0421556911915
          },
          "point_estimate": 708.0086341695934,
          "standard_error": 264.1537551675647
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 802488.0499382092,
            "upper_bound": 803762.101483187
          },
          "point_estimate": 803037.4966120835,
          "standard_error": 324.5347743918203
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 381.9198454114238,
            "upper_bound": 885.7784066737437
          },
          "point_estimate": 692.6251169206434,
          "standard_error": 128.88630158991552
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 900596.5147560977,
            "upper_bound": 902093.778408682
          },
          "point_estimate": 901347.6059988386,
          "standard_error": 384.5670371828775
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 900340.1707317072,
            "upper_bound": 902467.3140243902
          },
          "point_estimate": 901291.8471544716,
          "standard_error": 502.2072715679524
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389.04645894671626,
            "upper_bound": 2347.933137828072
          },
          "point_estimate": 1109.3580132318143,
          "standard_error": 501.61321841901145
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 900815.7715856704,
            "upper_bound": 901790.6577834424
          },
          "point_estimate": 901292.9737092176,
          "standard_error": 246.4546225861326
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 733.0681965409607,
            "upper_bound": 1608.034622355612
          },
          "point_estimate": 1284.753335708917,
          "standard_error": 225.43171791196525
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11564.601071806965,
            "upper_bound": 11591.58030318575
          },
          "point_estimate": 11578.012771888094,
          "standard_error": 6.919359968409358
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11551.207229492498,
            "upper_bound": 11598.757520746887
          },
          "point_estimate": 11581.881414689506,
          "standard_error": 11.413945930299526
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3529178262846642,
            "upper_bound": 41.47471815398447
          },
          "point_estimate": 32.22873267391504,
          "standard_error": 10.626797809513771
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11566.831587265282,
            "upper_bound": 11599.184540691982
          },
          "point_estimate": 11585.87676638714,
          "standard_error": 8.365252283121812
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.611626992372493,
            "upper_bound": 27.797819304769025
          },
          "point_estimate": 23.172433970383494,
          "standard_error": 3.399368745695677
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11828.27533514296,
            "upper_bound": 11885.241765653276
          },
          "point_estimate": 11850.168716987231,
          "standard_error": 15.859985227593755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11821.42754252624,
            "upper_bound": 11851.418631921824
          },
          "point_estimate": 11835.00829533116,
          "standard_error": 8.511056947126908
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.746283974154396,
            "upper_bound": 33.04161780997331
          },
          "point_estimate": 21.012414410069344,
          "standard_error": 8.181028910008864
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11824.840875574228,
            "upper_bound": 11843.30419515338
          },
          "point_estimate": 11833.246424975676,
          "standard_error": 4.6918356356482125
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.200201719787016,
            "upper_bound": 81.01105181674068
          },
          "point_estimate": 52.9508914949277,
          "standard_error": 24.6716500060112
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1324896.936845238,
            "upper_bound": 1326867.5792857143
          },
          "point_estimate": 1325848.977797619,
          "standard_error": 506.01083100567814
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1324566.7366071427,
            "upper_bound": 1327459.0357142857
          },
          "point_estimate": 1325231.1785714286,
          "standard_error": 776.8532378498536
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.3977645102796,
            "upper_bound": 2668.5122776247654
          },
          "point_estimate": 1443.6883431193592,
          "standard_error": 671.4851312143089
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1324727.483796921,
            "upper_bound": 1326426.9827122153
          },
          "point_estimate": 1325345.4623376622,
          "standard_error": 436.873209173839
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 862.0089718776474,
            "upper_bound": 2017.757698771328
          },
          "point_estimate": 1682.614510951142,
          "standard_error": 280.10587140552894
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60139.23958905943,
            "upper_bound": 60272.0769512987
          },
          "point_estimate": 60210.290009248325,
          "standard_error": 34.05207073794188
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60142.86725698544,
            "upper_bound": 60296.26170798898
          },
          "point_estimate": 60221.64834710744,
          "standard_error": 38.76320240354248
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.862105539023766,
            "upper_bound": 178.74662302495682
          },
          "point_estimate": 100.3107129133457,
          "standard_error": 42.79451121292296
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60062.44323653604,
            "upper_bound": 60227.59847917044
          },
          "point_estimate": 60147.54652355909,
          "standard_error": 43.67394536069398
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.83097830068371,
            "upper_bound": 149.40981752141695
          },
          "point_estimate": 113.28650841310562,
          "standard_error": 26.512332009226107
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59994.84617456559,
            "upper_bound": 60112.038196458
          },
          "point_estimate": 60053.78284792501,
          "standard_error": 30.085691577686823
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59962.191103789126,
            "upper_bound": 60139.65019220208
          },
          "point_estimate": 60074.42984231584,
          "standard_error": 53.057920616576816
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.593053893607344,
            "upper_bound": 156.37910481020722
          },
          "point_estimate": 117.95625639020491,
          "standard_error": 36.849269313619395
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60070.678322074935,
            "upper_bound": 60149.98298564216
          },
          "point_estimate": 60118.08620210103,
          "standard_error": 20.094894850104147
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.824315808398794,
            "upper_bound": 120.03656818099972
          },
          "point_estimate": 100.35643701886556,
          "standard_error": 14.394323885159585
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 811104.0907037037,
            "upper_bound": 812515.3309523809
          },
          "point_estimate": 811777.709451499,
          "standard_error": 361.5125109066503
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 810730.5055555556,
            "upper_bound": 812641.52
          },
          "point_estimate": 811646.0833333333,
          "standard_error": 502.7946482760418
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228.6981843841828,
            "upper_bound": 2083.9994475201147
          },
          "point_estimate": 1318.1013882656584,
          "standard_error": 441.7416876978007
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 810611.9886561955,
            "upper_bound": 812579.1822467095
          },
          "point_estimate": 811531.8158152958,
          "standard_error": 502.3289302107131
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 646.6161555997031,
            "upper_bound": 1534.047741101776
          },
          "point_estimate": 1200.617643954616,
          "standard_error": 232.3637438633731
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66480.7907534595,
            "upper_bound": 66672.19549145298
          },
          "point_estimate": 66575.64207010581,
          "standard_error": 49.020711226946446
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66454.685999186,
            "upper_bound": 66713.14926739926
          },
          "point_estimate": 66581.75274725274,
          "standard_error": 62.50456877476097
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.618083793716707,
            "upper_bound": 281.55505782191875
          },
          "point_estimate": 173.98849242390776,
          "standard_error": 62.26082489228093
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66406.69559245103,
            "upper_bound": 66566.23910805554
          },
          "point_estimate": 66489.36898339755,
          "standard_error": 41.295473068797854
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.25750373946138,
            "upper_bound": 211.09584367056192
          },
          "point_estimate": 163.21772043059363,
          "standard_error": 31.811549095024628
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23988.8630849306,
            "upper_bound": 24052.42968775704
          },
          "point_estimate": 24021.221885694347,
          "standard_error": 16.27942060174202
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23972.349636483807,
            "upper_bound": 24068.113879709188
          },
          "point_estimate": 24027.830886942793,
          "standard_error": 28.688294199980803
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.085862211848521,
            "upper_bound": 88.8471919471077
          },
          "point_estimate": 60.777307492807864,
          "standard_error": 20.857192609292465
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24006.53101629246,
            "upper_bound": 24059.55995845529
          },
          "point_estimate": 24037.00860421799,
          "standard_error": 13.61193828089925
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.615953232342875,
            "upper_bound": 65.38664663909601
          },
          "point_estimate": 54.08189767722335,
          "standard_error": 8.17647174181055
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11781.525410469085,
            "upper_bound": 11802.42191761722
          },
          "point_estimate": 11792.16204623151,
          "standard_error": 5.340071260538383
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11779.820176505567,
            "upper_bound": 11807.45629257217
          },
          "point_estimate": 11792.083288283418,
          "standard_error": 7.92166303269631
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.338506462873197,
            "upper_bound": 31.340920672909206
          },
          "point_estimate": 20.02043290627172,
          "standard_error": 6.4752232217427945
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11779.666819931925,
            "upper_bound": 11799.546064379992
          },
          "point_estimate": 11789.701457932271,
          "standard_error": 4.9728745908291545
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.280382601308176,
            "upper_bound": 22.79928851554579
          },
          "point_estimate": 17.833341961673735,
          "standard_error": 3.2658159050536373
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10292.98111011371,
            "upper_bound": 10311.221878556653
          },
          "point_estimate": 10302.01927450967,
          "standard_error": 4.643262355728865
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10293.095917115075,
            "upper_bound": 10320.812800906258
          },
          "point_estimate": 10298.079848327512,
          "standard_error": 6.890050008639808
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6276434361808124,
            "upper_bound": 28.94803224355098
          },
          "point_estimate": 7.704785121166816,
          "standard_error": 7.698212897454729
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10292.473321649752,
            "upper_bound": 10303.267889983505
          },
          "point_estimate": 10297.351535012707,
          "standard_error": 2.7193885701663367
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.582025770578737,
            "upper_bound": 19.449898070327993
          },
          "point_estimate": 15.499491999105391,
          "standard_error": 2.754175842515614
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9355.319938262495,
            "upper_bound": 9377.702811687544
          },
          "point_estimate": 9366.660034896448,
          "standard_error": 5.707054916269403
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9355.838507231403,
            "upper_bound": 9381.726239669422
          },
          "point_estimate": 9363.486091382576,
          "standard_error": 7.838850855664036
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.864482580207585,
            "upper_bound": 36.586115466170845
          },
          "point_estimate": 15.331212744344713,
          "standard_error": 8.371998223319224
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9355.407503471586,
            "upper_bound": 9366.797161172366
          },
          "point_estimate": 9360.387803209189,
          "standard_error": 2.883798505801209
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.238413841326372,
            "upper_bound": 25.01301472864788
          },
          "point_estimate": 19.013097819554822,
          "standard_error": 3.884436318885687
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57366.89171270344,
            "upper_bound": 57532.97016789733
          },
          "point_estimate": 57452.63586824392,
          "standard_error": 42.371981959008224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57356.81256153305,
            "upper_bound": 57529.908887130805
          },
          "point_estimate": 57472.75323048523,
          "standard_error": 40.607158471293594
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.86553374984577,
            "upper_bound": 245.83058268732313
          },
          "point_estimate": 88.43950134127658,
          "standard_error": 55.66609060151362
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57292.30800062186,
            "upper_bound": 57481.145536838005
          },
          "point_estimate": 57378.77229574223,
          "standard_error": 47.58922807771419
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.57451131985962,
            "upper_bound": 189.33637859036787
          },
          "point_estimate": 141.4985780442649,
          "standard_error": 32.65222528168271
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74580.00624121778,
            "upper_bound": 74707.9785878769
          },
          "point_estimate": 74643.43966603238,
          "standard_error": 32.77279423278348
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74523.91495901639,
            "upper_bound": 74713.5737704918
          },
          "point_estimate": 74673.79768613388,
          "standard_error": 51.89185489272267
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.276008798445474,
            "upper_bound": 217.6710950290002
          },
          "point_estimate": 106.43503181941738,
          "standard_error": 53.28746672438209
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74578.29767488665,
            "upper_bound": 74770.26143592042
          },
          "point_estimate": 74691.26334362359,
          "standard_error": 49.44581574634604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.18726047851756,
            "upper_bound": 137.22851731011633
          },
          "point_estimate": 109.152103159817,
          "standard_error": 18.92217604016452
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112967.4049259546,
            "upper_bound": 113090.6364783282
          },
          "point_estimate": 113030.39743550052,
          "standard_error": 31.46999662478994
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112975.83281733746,
            "upper_bound": 113133.34210526316
          },
          "point_estimate": 113015.23911248708,
          "standard_error": 39.00322401248568
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.24090399276957,
            "upper_bound": 208.79222864614343
          },
          "point_estimate": 66.91391379655401,
          "standard_error": 47.214160035649954
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112941.00042735857,
            "upper_bound": 113096.64664837226
          },
          "point_estimate": 113023.80073177596,
          "standard_error": 40.894117393865194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.080168866156654,
            "upper_bound": 138.55987118471168
          },
          "point_estimate": 105.11205055878108,
          "standard_error": 21.88890565531088
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61601.7032766966,
            "upper_bound": 61688.835028719404
          },
          "point_estimate": 61648.51858763788,
          "standard_error": 22.40177644899669
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61600.596368038736,
            "upper_bound": 61700.43881355932
          },
          "point_estimate": 61672.396963276835,
          "standard_error": 29.08380838802331
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.8259455591943663,
            "upper_bound": 119.10076693921664
          },
          "point_estimate": 52.92993190776353,
          "standard_error": 30.090094170620524
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61640.01588032736,
            "upper_bound": 61700.418731103986
          },
          "point_estimate": 61674.5724235087,
          "standard_error": 15.50701278178678
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.8520874480571,
            "upper_bound": 98.39252694242408
          },
          "point_estimate": 74.58319548787478,
          "standard_error": 17.53456499227616
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881397.4583843539,
            "upper_bound": 882230.0840265733
          },
          "point_estimate": 881771.2745153062,
          "standard_error": 214.7677892845567
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881226.4571428571,
            "upper_bound": 882146.1076388889
          },
          "point_estimate": 881526.9333333333,
          "standard_error": 229.42229106198047
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.675164796837386,
            "upper_bound": 1009.391715412944
          },
          "point_estimate": 467.6301859836439,
          "standard_error": 253.83730799365495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881340.0443554227,
            "upper_bound": 882706.5759858411
          },
          "point_estimate": 881929.4119975263,
          "standard_error": 367.4086574969086
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201.05825140896212,
            "upper_bound": 955.91198921911
          },
          "point_estimate": 717.7677182721308,
          "standard_error": 191.55147759746217
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 718411.1851540615,
            "upper_bound": 719254.9350326797
          },
          "point_estimate": 718824.0396358544,
          "standard_error": 214.86907416085913
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 718405.5882352941,
            "upper_bound": 719227.8104575163
          },
          "point_estimate": 718733.1928104575,
          "standard_error": 218.40927919997603
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164.3256500237956,
            "upper_bound": 1236.306894521697
          },
          "point_estimate": 515.0283751702259,
          "standard_error": 258.8293122581271
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 718434.5435012329,
            "upper_bound": 719010.6164784379
          },
          "point_estimate": 718747.959816654,
          "standard_error": 145.87114692526916
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307.71519243331744,
            "upper_bound": 969.0643393705795
          },
          "point_estimate": 715.8341413347392,
          "standard_error": 173.18641109072888
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/bstr_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73410.13670141668,
            "upper_bound": 73538.15238142858
          },
          "point_estimate": 73470.67636904761,
          "standard_error": 32.85683941993061
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73389.29433333334,
            "upper_bound": 73556.82117857144
          },
          "point_estimate": 73434.6698,
          "standard_error": 40.955611956225056
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.1252568227056,
            "upper_bound": 182.7731456351281
          },
          "point_estimate": 73.18918310063322,
          "standard_error": 42.71684504489628
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73402.74819067975,
            "upper_bound": 73471.79681428571
          },
          "point_estimate": 73430.82182857142,
          "standard_error": 17.56561076032264
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.77896157984158,
            "upper_bound": 137.0519579212177
          },
          "point_estimate": 109.53942294511062,
          "standard_error": 22.025104732042756
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/bstr_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72846.03624548168,
            "upper_bound": 73012.63255068363
          },
          "point_estimate": 72924.8468323118,
          "standard_error": 42.70724546101146
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72835.83168316832,
            "upper_bound": 72999.17425742574
          },
          "point_estimate": 72912.8096369637,
          "standard_error": 51.4772759032916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.817810681286677,
            "upper_bound": 227.2244998933366
          },
          "point_estimate": 108.69292708021008,
          "standard_error": 56.16430019605386
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72779.51663154871,
            "upper_bound": 72943.14467846001
          },
          "point_estimate": 72862.64864600745,
          "standard_error": 42.92298180954536
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.2233165311473,
            "upper_bound": 193.95985077300435
          },
          "point_estimate": 142.50558213610302,
          "standard_error": 34.471068421312964
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/bstr_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 428784.8173314659,
            "upper_bound": 429147.0229673202
          },
          "point_estimate": 428977.59310177405,
          "standard_error": 92.95888200096724
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 428783.9130252101,
            "upper_bound": 429250.96352941176
          },
          "point_estimate": 429017.3922875817,
          "standard_error": 91.26965840494972
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.330192879134556,
            "upper_bound": 513.7988969566882
          },
          "point_estimate": 250.0032752086049,
          "standard_error": 145.00932640290958
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 428876.1756337553,
            "upper_bound": 429235.45934320934
          },
          "point_estimate": 429091.1475630253,
          "standard_error": 92.5730891486349
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.0613394572173,
            "upper_bound": 408.7686273410738
          },
          "point_estimate": 309.62797765931293,
          "standard_error": 72.20523575676695
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/bstr_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1071.4812846627833,
            "upper_bound": 1072.983240160915
          },
          "point_estimate": 1072.2108145429818,
          "standard_error": 0.38509780585041314
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1071.0747362480986,
            "upper_bound": 1073.27479758575
          },
          "point_estimate": 1072.013726525205,
          "standard_error": 0.6389849746618128
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08150334906101993,
            "upper_bound": 2.146194024608936
          },
          "point_estimate": 1.2856105434938363,
          "standard_error": 0.5374276854857593
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1071.1564726330769,
            "upper_bound": 1072.848223056628
          },
          "point_estimate": 1072.02179646351,
          "standard_error": 0.4342565167611384
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7618478013637596,
            "upper_bound": 1.560429103138123
          },
          "point_estimate": 1.28596119221212,
          "standard_error": 0.2027483870291961
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.96469829163821,
            "upper_bound": 22.996613463031117
          },
          "point_estimate": 22.979514789557477,
          "standard_error": 0.008212344756665645
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.958672872493395,
            "upper_bound": 22.994647567749745
          },
          "point_estimate": 22.977887361871232,
          "standard_error": 0.008470127467876577
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032304099947144064,
            "upper_bound": 0.04346902371301264
          },
          "point_estimate": 0.021851476166292627,
          "standard_error": 0.010717405065769415
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.96832046448119,
            "upper_bound": 22.984028322644427
          },
          "point_estimate": 22.977760337930963,
          "standard_error": 0.004016278498324946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012273512462623272,
            "upper_bound": 0.036422168827605954
          },
          "point_estimate": 0.02738842945275557,
          "standard_error": 0.006550142250947426
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.166276994847715,
            "upper_bound": 13.244831567961644
          },
          "point_estimate": 13.205167966946217,
          "standard_error": 0.020127693080008637
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.151939277846742,
            "upper_bound": 13.263632131389526
          },
          "point_estimate": 13.2019001254401,
          "standard_error": 0.026704526294892512
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018260666814916865,
            "upper_bound": 0.11443887603595508
          },
          "point_estimate": 0.07559375624999638,
          "standard_error": 0.02604621950178245
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.14397325229558,
            "upper_bound": 13.252001634984522
          },
          "point_estimate": 13.191113904606606,
          "standard_error": 0.028271048288609536
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0383873815923629,
            "upper_bound": 0.08470999886533984
          },
          "point_estimate": 0.06714554761296225,
          "standard_error": 0.01192178992944194
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.191664687045764,
            "upper_bound": 13.249978137810109
          },
          "point_estimate": 13.220018782790811,
          "standard_error": 0.014889474691263891
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.177056922161857,
            "upper_bound": 13.249642393015623
          },
          "point_estimate": 13.22296652820418,
          "standard_error": 0.019081988471614655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009427748473999176,
            "upper_bound": 0.08921654116627516
          },
          "point_estimate": 0.04022771064752714,
          "standard_error": 0.019522564982001632
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.208739546340574,
            "upper_bound": 13.273175572152557
          },
          "point_estimate": 13.240183078981946,
          "standard_error": 0.01652215964507689
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02630733674209468,
            "upper_bound": 0.06510810074777852
          },
          "point_estimate": 0.04946927795402415,
          "standard_error": 0.01023701429781159
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.69324578767047,
            "upper_bound": 35.745899769274196
          },
          "point_estimate": 35.721515228708746,
          "standard_error": 0.01352002703592112
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.691480510322656,
            "upper_bound": 35.75465638261539
          },
          "point_estimate": 35.73423080513256,
          "standard_error": 0.015518762813607154
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009885966932929103,
            "upper_bound": 0.07197921929776717
          },
          "point_estimate": 0.03492898106382931,
          "standard_error": 0.0160790052409971
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.69188495223035,
            "upper_bound": 35.744592030683606
          },
          "point_estimate": 35.716797883094095,
          "standard_error": 0.013292770672234794
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017927296303601268,
            "upper_bound": 0.058378853479495965
          },
          "point_estimate": 0.04511534845170016,
          "standard_error": 0.010453809880819709
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.104552681515013,
            "upper_bound": 21.136443698205657
          },
          "point_estimate": 21.119710747484326,
          "standard_error": 0.008175925572225887
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.10002340267978,
            "upper_bound": 21.14493409141531
          },
          "point_estimate": 21.10844801810739,
          "standard_error": 0.011567678838428855
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002543181409452228,
            "upper_bound": 0.046103509389415806
          },
          "point_estimate": 0.02717519945327556,
          "standard_error": 0.01153162516760892
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.10890502089959,
            "upper_bound": 21.1371657487368
          },
          "point_estimate": 21.12308899435644,
          "standard_error": 0.007174585373282144
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01355925278266866,
            "upper_bound": 0.03391547301152714
          },
          "point_estimate": 0.027272992368037704,
          "standard_error": 0.005168951455986569
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.138432153401336,
            "upper_bound": 22.20050589008054
          },
          "point_estimate": 22.16909956143358,
          "standard_error": 0.015910534282711677
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.113163574416348,
            "upper_bound": 22.226398664178788
          },
          "point_estimate": 22.16390985010804,
          "standard_error": 0.026996450937927952
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008906293502615084,
            "upper_bound": 0.0905819877402326
          },
          "point_estimate": 0.07849997528502009,
          "standard_error": 0.021526477245404217
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.12807060730618,
            "upper_bound": 22.211388760064796
          },
          "point_estimate": 22.17317162016032,
          "standard_error": 0.021575996812104807
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0330889080147073,
            "upper_bound": 0.06209306790663887
          },
          "point_estimate": 0.052986630750091405,
          "standard_error": 0.007295332017458322
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.507841746315846,
            "upper_bound": 11.549665668710936
          },
          "point_estimate": 11.528340520591737,
          "standard_error": 0.010730462970893024
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.49675752910116,
            "upper_bound": 11.557949458282422
          },
          "point_estimate": 11.524116165296952,
          "standard_error": 0.020161406604392116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006425415125631014,
            "upper_bound": 0.0600187962840428
          },
          "point_estimate": 0.042910601468049134,
          "standard_error": 0.01502603139536499
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.492803761389212,
            "upper_bound": 11.552921043160426
          },
          "point_estimate": 11.519836290669476,
          "standard_error": 0.01562498938628035
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022935799204164147,
            "upper_bound": 0.043299753972097045
          },
          "point_estimate": 0.03576615750680937,
          "standard_error": 0.00533160412173448
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.18190538648472,
            "upper_bound": 19.21845307325073
          },
          "point_estimate": 19.20004646904636,
          "standard_error": 0.009259441699539432
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.185195256564032,
            "upper_bound": 19.213980282799035
          },
          "point_estimate": 19.200610795455734,
          "standard_error": 0.006977249742500485
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004227563720293432,
            "upper_bound": 0.050616988263911616
          },
          "point_estimate": 0.017643629088546952,
          "standard_error": 0.010997519969314262
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.191462795801247,
            "upper_bound": 19.20714732215544
          },
          "point_estimate": 19.19998527620194,
          "standard_error": 0.004027174938730352
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00997866901132874,
            "upper_bound": 0.04285948504674874
          },
          "point_estimate": 0.03084998483272703,
          "standard_error": 0.008419415242051819
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.658172136629688,
            "upper_bound": 26.70703063914151
          },
          "point_estimate": 26.68175704038947,
          "standard_error": 0.012539201529354949
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.651135205796216,
            "upper_bound": 26.714624848405364
          },
          "point_estimate": 26.671876511955407,
          "standard_error": 0.01833364177534583
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005694182772507329,
            "upper_bound": 0.07152306710509838
          },
          "point_estimate": 0.038256183178523896,
          "standard_error": 0.016533819999409165
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.655341033950492,
            "upper_bound": 26.70481873003372
          },
          "point_estimate": 26.680184305930855,
          "standard_error": 0.013103674611983966
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02262100885194777,
            "upper_bound": 0.054426886577822395
          },
          "point_estimate": 0.0420258542250493,
          "standard_error": 0.00847839849531061
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.806089411502606,
            "upper_bound": 12.879009348888632
          },
          "point_estimate": 12.84631284650958,
          "standard_error": 0.018829151811549025
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.809774852642676,
            "upper_bound": 12.885373350576415
          },
          "point_estimate": 12.863080229584188,
          "standard_error": 0.01692831870213526
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009493222734131484,
            "upper_bound": 0.09006578848437802
          },
          "point_estimate": 0.03386878739996891,
          "standard_error": 0.020468359628453392
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.85438899820568,
            "upper_bound": 12.888763660814986
          },
          "point_estimate": 12.87008433440603,
          "standard_error": 0.00877021237192483
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01875577895698804,
            "upper_bound": 0.08386570082016691
          },
          "point_estimate": 0.06259551594077128,
          "standard_error": 0.01721460610158576
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.604458435572507,
            "upper_bound": 17.625734111587636
          },
          "point_estimate": 17.615246150561312,
          "standard_error": 0.005458909216579431
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.597391615967354,
            "upper_bound": 17.630618022479364
          },
          "point_estimate": 17.620051879078687,
          "standard_error": 0.009223587294468108
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0034333894271429627,
            "upper_bound": 0.029243368752239436
          },
          "point_estimate": 0.024014086520444785,
          "standard_error": 0.006917388116393496
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.598801255813406,
            "upper_bound": 17.627655791770028
          },
          "point_estimate": 17.61082547329721,
          "standard_error": 0.007363366021793936
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011454905068140687,
            "upper_bound": 0.02153629288639968
          },
          "point_estimate": 0.018257788798745685,
          "standard_error": 0.002573846343091381
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.127949305535836,
            "upper_bound": 27.163127363019004
          },
          "point_estimate": 27.145473633826903,
          "standard_error": 0.00896780686313181
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.120732367126653,
            "upper_bound": 27.16894611282907
          },
          "point_estimate": 27.145781520662048,
          "standard_error": 0.010745897341445564
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006975893719463831,
            "upper_bound": 0.056541855258619225
          },
          "point_estimate": 0.0226677075370128,
          "standard_error": 0.01263258708262132
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.14772586007116,
            "upper_bound": 27.174522947136683
          },
          "point_estimate": 27.16135356967533,
          "standard_error": 0.006886333985196297
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016619154839374293,
            "upper_bound": 0.03774190965425446
          },
          "point_estimate": 0.029897311265776823,
          "standard_error": 0.0054700507608835105
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2017083.2625708536,
            "upper_bound": 2020517.2553132833
          },
          "point_estimate": 2018819.8973454472,
          "standard_error": 876.2433302427538
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2017033.567251462,
            "upper_bound": 2021595.210526316
          },
          "point_estimate": 2017908.6938596491,
          "standard_error": 1495.1325832840744
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 309.78536292119946,
            "upper_bound": 5823.545403190413
          },
          "point_estimate": 3263.6651578479036,
          "standard_error": 1552.2645499908365
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2017232.371287129,
            "upper_bound": 2018968.1441713169
          },
          "point_estimate": 2017910.5591250856,
          "standard_error": 441.7550068041036
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1800.229264327657,
            "upper_bound": 3701.1851801560138
          },
          "point_estimate": 2912.7930567592484,
          "standard_error": 518.2519456789621
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1669027.216178842,
            "upper_bound": 1671464.5202348037
          },
          "point_estimate": 1670250.506722583,
          "standard_error": 624.5721361516287
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1668452.2792207792,
            "upper_bound": 1672201.0619318185
          },
          "point_estimate": 1670333.7840909092,
          "standard_error": 963.843530132362
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198.645932836987,
            "upper_bound": 3437.1875744323697
          },
          "point_estimate": 2135.7515832948893,
          "standard_error": 833.3757405530507
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1669754.0963875367,
            "upper_bound": 1671778.6243116627
          },
          "point_estimate": 1670913.3722550175,
          "standard_error": 510.5929884054005
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1233.258344149683,
            "upper_bound": 2595.4266660629924
          },
          "point_estimate": 2084.0113841158272,
          "standard_error": 347.8198608193692
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2194370.884656863,
            "upper_bound": 2202029.431096814
          },
          "point_estimate": 2198511.9633870213,
          "standard_error": 1963.442565618857
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2195374.330882353,
            "upper_bound": 2202789.926470588
          },
          "point_estimate": 2199947.7663398692,
          "standard_error": 2168.942221878899
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 784.934939005992,
            "upper_bound": 9629.44592257225
          },
          "point_estimate": 5470.947976988696,
          "standard_error": 2296.5974470997567
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2197602.5550402733,
            "upper_bound": 2201713.9932497656
          },
          "point_estimate": 2199972.7714285715,
          "standard_error": 1063.4207427155588
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2789.029270092296,
            "upper_bound": 9160.734893579136
          },
          "point_estimate": 6565.565028441877,
          "standard_error": 1819.613396263916
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356844.6007625078,
            "upper_bound": 357524.4808244632
          },
          "point_estimate": 357158.460157174,
          "standard_error": 175.15852204253952
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356742.57666122005,
            "upper_bound": 357423.3382352941
          },
          "point_estimate": 357092.67401960783,
          "standard_error": 160.16731165920197
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.70903262143857,
            "upper_bound": 881.8913708138907
          },
          "point_estimate": 460.29641829866193,
          "standard_error": 218.27798173993895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356726.5773480663,
            "upper_bound": 357416.083357608
          },
          "point_estimate": 357061.6749936338,
          "standard_error": 176.15151265249517
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252.19262866568496,
            "upper_bound": 801.5830886394737
          },
          "point_estimate": 586.2014353375912,
          "standard_error": 154.07586816353376
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 469691.1754199134,
            "upper_bound": 470439.0715151516
          },
          "point_estimate": 470063.5239177489,
          "standard_error": 191.41540146148353
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 469587.8659090909,
            "upper_bound": 470508.9246753247
          },
          "point_estimate": 470034.72294372297,
          "standard_error": 214.46784573613095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.71934322255984,
            "upper_bound": 1041.4526724195923
          },
          "point_estimate": 462.94184178109623,
          "standard_error": 249.5730683693592
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 469886.8947689441,
            "upper_bound": 470688.4375631924
          },
          "point_estimate": 470209.8715466352,
          "standard_error": 210.8937725939263
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323.5559959740229,
            "upper_bound": 836.5158700562968
          },
          "point_estimate": 638.7639842310907,
          "standard_error": 130.54677835572895
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 826413.9427017722,
            "upper_bound": 827737.3548232324
          },
          "point_estimate": 827199.7308703103,
          "standard_error": 351.94608283868627
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 827094.8409090909,
            "upper_bound": 827809.7316558441
          },
          "point_estimate": 827395.0662878788,
          "standard_error": 174.78206616928264
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.458188669700483,
            "upper_bound": 1089.361156322725
          },
          "point_estimate": 449.45074361149375,
          "standard_error": 277.853183940407
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 827285.3793706293,
            "upper_bound": 827935.9415902308
          },
          "point_estimate": 827640.5472255017,
          "standard_error": 172.60104549772544
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.8419507809571,
            "upper_bound": 1769.5325640399824
          },
          "point_estimate": 1174.9083377497338,
          "standard_error": 485.9320637028166
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 938044.495414072,
            "upper_bound": 939748.8895657052
          },
          "point_estimate": 938841.8545695972,
          "standard_error": 436.1845779121803
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 938024.8034188034,
            "upper_bound": 939327.7884615384
          },
          "point_estimate": 938823.84004884,
          "standard_error": 291.32598027070543
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.86044325782437,
            "upper_bound": 2129.1171541237304
          },
          "point_estimate": 799.0349008143581,
          "standard_error": 534.7488686825674
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 938465.818692156,
            "upper_bound": 939102.2652043126
          },
          "point_estimate": 938747.302031302,
          "standard_error": 164.27458876557895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 444.85781112875287,
            "upper_bound": 2067.385997743389
          },
          "point_estimate": 1453.0516350152704,
          "standard_error": 425.714562365873
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 762470.0421916337,
            "upper_bound": 763816.1431301669
          },
          "point_estimate": 763102.3745246363,
          "standard_error": 345.51634279883194
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 762134.2692708333,
            "upper_bound": 763891.1333333333
          },
          "point_estimate": 762926.8949032738,
          "standard_error": 456.95801078855544
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218.2420622713389,
            "upper_bound": 1908.864625833056
          },
          "point_estimate": 1284.653315109469,
          "standard_error": 438.0324892052953
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 762409.7694869194,
            "upper_bound": 763169.2108360389
          },
          "point_estimate": 762782.3373376623,
          "standard_error": 194.9954162534372
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 555.3877649410715,
            "upper_bound": 1459.8885652726526
          },
          "point_estimate": 1149.9912955898983,
          "standard_error": 228.8447891209512
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258113.43648858776,
            "upper_bound": 258337.6036042511
          },
          "point_estimate": 258219.4435508837,
          "standard_error": 57.573342293599815
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258058.97030141845,
            "upper_bound": 258389.56737588652
          },
          "point_estimate": 258160.77945232467,
          "standard_error": 89.7362102984845
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.466110670676873,
            "upper_bound": 320.25678249399226
          },
          "point_estimate": 185.6371576085336,
          "standard_error": 75.27809582206739
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258064.98532881745,
            "upper_bound": 258236.3246260796
          },
          "point_estimate": 258124.50918301556,
          "standard_error": 43.92570227139094
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.78401121225968,
            "upper_bound": 241.65918020452264
          },
          "point_estimate": 192.15129984914276,
          "standard_error": 36.58374031315946
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 245834.83043020224,
            "upper_bound": 246158.0680520222
          },
          "point_estimate": 246006.90048352905,
          "standard_error": 83.11713053829749
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 245774.48202054793,
            "upper_bound": 246223.66324200912
          },
          "point_estimate": 246081.3887475538,
          "standard_error": 112.19898767034364
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.24725139098757,
            "upper_bound": 434.9554099149586
          },
          "point_estimate": 224.51227553465168,
          "standard_error": 105.594357971809
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 245809.912092244,
            "upper_bound": 246157.23849361035
          },
          "point_estimate": 245963.11866215977,
          "standard_error": 91.15891501461452
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122.85364717753583,
            "upper_bound": 354.8983847801862
          },
          "point_estimate": 277.403850305911,
          "standard_error": 58.17863845083675
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125840.47630334488,
            "upper_bound": 125990.36952820345
          },
          "point_estimate": 125915.6089086615,
          "standard_error": 38.24044929866455
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125839.93858131488,
            "upper_bound": 126032.73760092272
          },
          "point_estimate": 125892.70943867744,
          "standard_error": 46.68492464019416
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.129786529349214,
            "upper_bound": 222.50927096316067
          },
          "point_estimate": 116.96508217950868,
          "standard_error": 54.700496340560726
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125869.96551286268,
            "upper_bound": 125990.61252968929
          },
          "point_estimate": 125915.32488203836,
          "standard_error": 31.327353384901887
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.7547877031555,
            "upper_bound": 165.4530053896147
          },
          "point_estimate": 126.98182182842862,
          "standard_error": 25.347460813486354
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149532.46255370672,
            "upper_bound": 149817.31604991065
          },
          "point_estimate": 149681.2956206546,
          "standard_error": 72.83013983723575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149536.7704889807,
            "upper_bound": 149831.58842975207
          },
          "point_estimate": 149722.01900088548,
          "standard_error": 74.57992594350074
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.17854680734261,
            "upper_bound": 407.1839064590496
          },
          "point_estimate": 190.59383158531125,
          "standard_error": 85.92014638245637
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149651.69684927037,
            "upper_bound": 149909.60177104615
          },
          "point_estimate": 149783.61413545132,
          "standard_error": 67.21105783786167
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.65443584171965,
            "upper_bound": 330.81487206103805
          },
          "point_estimate": 243.41477523771945,
          "standard_error": 58.235554733151936
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59388.06374926849,
            "upper_bound": 59465.74573001631
          },
          "point_estimate": 59423.58741675082,
          "standard_error": 19.89363464773249
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59377.97868924623,
            "upper_bound": 59441.601767264816
          },
          "point_estimate": 59426.62438825449,
          "standard_error": 15.815647496330111
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.874099459578288,
            "upper_bound": 94.66262434441492
          },
          "point_estimate": 42.53303066957513,
          "standard_error": 22.56847857094371
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59381.164510092414,
            "upper_bound": 59447.1951967153
          },
          "point_estimate": 59413.984207961694,
          "standard_error": 17.054376431127707
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.005369267334167,
            "upper_bound": 94.0404833330035
          },
          "point_estimate": 66.2958173961175,
          "standard_error": 19.82670289296836
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 216143.8691717923,
            "upper_bound": 216549.5246438019
          },
          "point_estimate": 216335.95680154004,
          "standard_error": 103.84629500268676
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 216119.5992063492,
            "upper_bound": 216526.06805555557
          },
          "point_estimate": 216288.2384259259,
          "standard_error": 84.30419498092617
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.15988529552446,
            "upper_bound": 562.9879653620933
          },
          "point_estimate": 211.5248033280265,
          "standard_error": 132.5640874323745
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 216212.9185421102,
            "upper_bound": 216446.33786630037
          },
          "point_estimate": 216317.36883116883,
          "standard_error": 59.28424160921331
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120.7789824530518,
            "upper_bound": 472.0146521254792
          },
          "point_estimate": 344.77396164750905,
          "standard_error": 89.25539621376822
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 935094.9830671296,
            "upper_bound": 938288.7053164936
          },
          "point_estimate": 936612.558474766,
          "standard_error": 818.1839102533359
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 934213.9230769232,
            "upper_bound": 938734.4583333333
          },
          "point_estimate": 936602.355448718,
          "standard_error": 1029.4204211834535
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 342.77331237608064,
            "upper_bound": 4818.330377192569
          },
          "point_estimate": 2996.336922765869,
          "standard_error": 1224.0985651551412
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 934718.6455773024,
            "upper_bound": 938822.922883406
          },
          "point_estimate": 936327.5313353314,
          "standard_error": 1050.3219215865795
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1329.3598397673918,
            "upper_bound": 3431.847426780445
          },
          "point_estimate": 2725.2167621453445,
          "standard_error": 547.0677658009126
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1907.9285450490947,
            "upper_bound": 1914.4942426215116
          },
          "point_estimate": 1911.2261090167024,
          "standard_error": 1.671799782297976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1907.7265786133587,
            "upper_bound": 1915.1627393355143
          },
          "point_estimate": 1911.2607951240152,
          "standard_error": 1.2780563888502228
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5486201235298449,
            "upper_bound": 10.182045770895613
          },
          "point_estimate": 1.5641442354565374,
          "standard_error": 3.0477876818831144
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1910.6944611393328,
            "upper_bound": 1917.7493128943795
          },
          "point_estimate": 1914.1153815986752,
          "standard_error": 1.9348348127175068
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.474124653793977,
            "upper_bound": 7.414571576547714
          },
          "point_estimate": 5.58749717580124,
          "standard_error": 1.2479800830597054
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40946.27834918171,
            "upper_bound": 41141.27266219231
          },
          "point_estimate": 41021.38570559677,
          "standard_error": 54.141751078087246
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40952.482944569856,
            "upper_bound": 41008.83630549285
          },
          "point_estimate": 40972.22548371494,
          "standard_error": 19.1629550964691
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.791365926394165,
            "upper_bound": 98.01372858347251
          },
          "point_estimate": 31.725984666246337,
          "standard_error": 26.02718594533475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40951.27268623025,
            "upper_bound": 41162.4352896915
          },
          "point_estimate": 41013.950854563045,
          "standard_error": 58.26683752123312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.30581423277121,
            "upper_bound": 275.33980756915327
          },
          "point_estimate": 179.87516818715332,
          "standard_error": 85.0124819966551
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48607.081978086,
            "upper_bound": 48670.61457101376
          },
          "point_estimate": 48643.03679033228,
          "standard_error": 16.53665946280452
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48622.5467222974,
            "upper_bound": 48676.09336941813
          },
          "point_estimate": 48665.428281461434,
          "standard_error": 17.428851534646483
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.857668957220489,
            "upper_bound": 71.50493739765984
          },
          "point_estimate": 34.444995413514235,
          "standard_error": 17.534465109628247
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48631.14851345749,
            "upper_bound": 48679.7905101612
          },
          "point_estimate": 48658.081556332705,
          "standard_error": 12.602608660927146
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.751829092479284,
            "upper_bound": 79.17488776379952
          },
          "point_estimate": 55.07386557840793,
          "standard_error": 18.15241632310085
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48226.12223899639,
            "upper_bound": 48551.073321302094
          },
          "point_estimate": 48372.69048678303,
          "standard_error": 83.89667565543154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48204.43081009296,
            "upper_bound": 48517.53142983621
          },
          "point_estimate": 48231.41441377349,
          "standard_error": 90.13361163791951
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.00716306935857,
            "upper_bound": 411.2282427789206
          },
          "point_estimate": 95.71820318777938,
          "standard_error": 106.355073925828
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48206.2025751162,
            "upper_bound": 48553.45077476212
          },
          "point_estimate": 48368.75206705645,
          "standard_error": 93.37613439025452
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.42178903888623,
            "upper_bound": 370.8826394550976
          },
          "point_estimate": 279.222383998845,
          "standard_error": 75.90581932774535
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14107.048670179358,
            "upper_bound": 14123.21328023828
          },
          "point_estimate": 14114.857883320385,
          "standard_error": 4.119533432731758
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14103.74592074592,
            "upper_bound": 14122.959207459207
          },
          "point_estimate": 14116.212587412589,
          "standard_error": 5.043492991775753
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9010057876573994,
            "upper_bound": 22.58843394240856
          },
          "point_estimate": 11.223394118927068,
          "standard_error": 5.217618271267957
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14102.99448839143,
            "upper_bound": 14124.326370122875
          },
          "point_estimate": 14113.030332293967,
          "standard_error": 5.440954762927841
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.754771905550863,
            "upper_bound": 18.495617967238996
          },
          "point_estimate": 13.723637217559084,
          "standard_error": 3.1208621194798467
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25618.18475065485,
            "upper_bound": 25672.703366635324
          },
          "point_estimate": 25644.420731020666,
          "standard_error": 14.017560401740988
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25607.871297602254,
            "upper_bound": 25682.87535260931
          },
          "point_estimate": 25637.39393511989,
          "standard_error": 17.31825289108285
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.199796525793245,
            "upper_bound": 76.81191074256778
          },
          "point_estimate": 55.6005049896211,
          "standard_error": 19.616247304134315
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25623.20379653111,
            "upper_bound": 25676.01263522669
          },
          "point_estimate": 25652.17899547561,
          "standard_error": 13.398667105421875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.46733383283672,
            "upper_bound": 59.6845020543679
          },
          "point_estimate": 46.75141175283655,
          "standard_error": 9.047068028099034
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17364.42672105207,
            "upper_bound": 17403.807647920992
          },
          "point_estimate": 17382.962431918513,
          "standard_error": 10.091143004747638
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17356.761428913356,
            "upper_bound": 17402.632519546834
          },
          "point_estimate": 17382.074557204403,
          "standard_error": 12.955823805439245
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.079799539824924,
            "upper_bound": 55.55413921317648
          },
          "point_estimate": 32.24063511454698,
          "standard_error": 12.063832911745926
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17358.683941754553,
            "upper_bound": 17382.72814053616
          },
          "point_estimate": 17369.561630805765,
          "standard_error": 6.214097682301242
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.371968773642553,
            "upper_bound": 45.34385816166755
          },
          "point_estimate": 33.453634558968666,
          "standard_error": 8.019727719427973
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17363.513572706313,
            "upper_bound": 17394.899921032884
          },
          "point_estimate": 17378.326186553415,
          "standard_error": 8.052194048642594
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17360.977363896847,
            "upper_bound": 17395.68848495702
          },
          "point_estimate": 17371.424737344794,
          "standard_error": 8.160230644071685
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3599588474759025,
            "upper_bound": 45.201362616808886
          },
          "point_estimate": 22.132276951868302,
          "standard_error": 11.153240745699495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17366.185798433384,
            "upper_bound": 17382.27200780198
          },
          "point_estimate": 17373.778636549698,
          "standard_error": 4.0262967843627395
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.790805682947004,
            "upper_bound": 34.71269669202329
          },
          "point_estimate": 26.839539774179123,
          "standard_error": 5.884596038828958
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/krate_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17340.018109436383,
            "upper_bound": 17363.09662238572
          },
          "point_estimate": 17351.51812163635,
          "standard_error": 5.903920282148087
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17335.2453743443,
            "upper_bound": 17367.827849308538
          },
          "point_estimate": 17349.845353055385,
          "standard_error": 8.506817740189
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.383359743482311,
            "upper_bound": 33.20313395988449
          },
          "point_estimate": 22.181409852981897,
          "standard_error": 7.306177035443091
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17340.4025334979,
            "upper_bound": 17364.724009110276
          },
          "point_estimate": 17355.472021254853,
          "standard_error": 6.194028023559686
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.637359418494604,
            "upper_bound": 24.47448609577602
          },
          "point_estimate": 19.722374698493365,
          "standard_error": 3.2864231496856653
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39390.486605788574,
            "upper_bound": 39519.56513535057
          },
          "point_estimate": 39444.78234161378,
          "standard_error": 33.75625684892319
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39369.01011195377,
            "upper_bound": 39463.17544691224
          },
          "point_estimate": 39430.86908631275,
          "standard_error": 28.22450462594634
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.730955749653084,
            "upper_bound": 124.83192608010214
          },
          "point_estimate": 54.957394907305144,
          "standard_error": 27.87739651264323
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39374.38020457322,
            "upper_bound": 39450.93462539844
          },
          "point_estimate": 39420.08512332738,
          "standard_error": 19.57787056804853
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.783616844941214,
            "upper_bound": 166.78049903267987
          },
          "point_estimate": 112.95743822707186,
          "standard_error": 41.86551833120067
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18768.721984004,
            "upper_bound": 18785.124709022035
          },
          "point_estimate": 18776.154074962287,
          "standard_error": 4.222050627325962
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18764.90857438017,
            "upper_bound": 18784.713842975205
          },
          "point_estimate": 18771.39705291552,
          "standard_error": 6.695057024789441
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24992618357812768,
            "upper_bound": 25.81538062984065
          },
          "point_estimate": 10.04788985691038,
          "standard_error": 6.885947750629287
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18766.742103271776,
            "upper_bound": 18783.032114519938
          },
          "point_estimate": 18774.103735107867,
          "standard_error": 4.154654334723165
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.765396705095979,
            "upper_bound": 19.36582652018984
          },
          "point_estimate": 14.135281541237587,
          "standard_error": 3.6198912078371097
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18810.11818592863,
            "upper_bound": 18845.754043735094
          },
          "point_estimate": 18828.844800168597,
          "standard_error": 9.124789734645445
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18813.355354058724,
            "upper_bound": 18855.927910909617
          },
          "point_estimate": 18826.04273316062,
          "standard_error": 12.80397697498771
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.380352743858839,
            "upper_bound": 57.17222162057045
          },
          "point_estimate": 28.773364808689458,
          "standard_error": 12.85457766078963
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18819.85476164569,
            "upper_bound": 18852.829262779363
          },
          "point_estimate": 18837.418698607093,
          "standard_error": 8.39339653808352
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.344147350048505,
            "upper_bound": 40.512196966502934
          },
          "point_estimate": 30.443272120891976,
          "standard_error": 6.884802677298091
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17495.699381204457,
            "upper_bound": 17524.26500644507
          },
          "point_estimate": 17507.877730494827,
          "standard_error": 7.452883398358017
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17492.34897337183,
            "upper_bound": 17512.692974013473
          },
          "point_estimate": 17505.715618650414,
          "standard_error": 5.1487919711806684
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7631483325123631,
            "upper_bound": 27.539964384344728
          },
          "point_estimate": 11.714775345438582,
          "standard_error": 6.183968883202524
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17488.162470157367,
            "upper_bound": 17505.810227993774
          },
          "point_estimate": 17496.46855867905,
          "standard_error": 4.71436008461517
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.379092601346122,
            "upper_bound": 36.67535998336565
          },
          "point_estimate": 24.91549534291014,
          "standard_error": 9.096871834797824
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17640.177809570112,
            "upper_bound": 17658.343597066927
          },
          "point_estimate": 17649.18085648309,
          "standard_error": 4.658409056761369
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17636.236315021877,
            "upper_bound": 17663.849295089938
          },
          "point_estimate": 17647.15560687085,
          "standard_error": 6.490417810069531
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3842164557362835,
            "upper_bound": 27.13849879870131
          },
          "point_estimate": 20.46950176104732,
          "standard_error": 6.81498111474205
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17639.2403198498,
            "upper_bound": 17664.13973209032
          },
          "point_estimate": 17652.3362177929,
          "standard_error": 6.465894725180899
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.230810780048332,
            "upper_bound": 18.8875391969336
          },
          "point_estimate": 15.524888252796323,
          "standard_error": 2.4551248094059255
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17386.61207407281,
            "upper_bound": 17405.15045875219
          },
          "point_estimate": 17395.12223044747,
          "standard_error": 4.7723104165951735
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17382.428075634274,
            "upper_bound": 17406.149832455718
          },
          "point_estimate": 17390.746757391324,
          "standard_error": 5.5118264169961195
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.0634860500079544,
            "upper_bound": 25.103026357828696
          },
          "point_estimate": 13.977959074484314,
          "standard_error": 5.7625878564039
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17383.99451787735,
            "upper_bound": 17397.202391556748
          },
          "point_estimate": 17390.009827606573,
          "standard_error": 3.4276810369345276
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.8595768086895434,
            "upper_bound": 20.25511794668813
          },
          "point_estimate": 15.942273234330084,
          "standard_error": 3.6461882551497866
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17630.55021013051,
            "upper_bound": 17652.714685257135
          },
          "point_estimate": 17641.61775262687,
          "standard_error": 5.687343876952903
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17622.519572608064,
            "upper_bound": 17654.921090335112
          },
          "point_estimate": 17645.46343964168,
          "standard_error": 8.765194897393313
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.694059848669786,
            "upper_bound": 34.01657666012516
          },
          "point_estimate": 19.646924851537907,
          "standard_error": 7.948462405969713
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17629.305130489927,
            "upper_bound": 17648.162816728982
          },
          "point_estimate": 17640.065153302257,
          "standard_error": 4.829355067746683
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.458437324656762,
            "upper_bound": 23.63704057814981
          },
          "point_estimate": 18.99411634323532,
          "standard_error": 3.1520777051842113
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15413.428037153157,
            "upper_bound": 15426.286897626447
          },
          "point_estimate": 15420.12185262598,
          "standard_error": 3.3113232050130823
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15410.030079970918,
            "upper_bound": 15428.916878710772
          },
          "point_estimate": 15421.146366977664,
          "standard_error": 4.1847156042151274
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.0496759503219093,
            "upper_bound": 19.69509921211745
          },
          "point_estimate": 9.96606259236444,
          "standard_error": 4.639967631986543
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15411.126740889986,
            "upper_bound": 15427.08600681257
          },
          "point_estimate": 15420.634739984358,
          "standard_error": 4.193467573297299
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.2599727689432605,
            "upper_bound": 13.869722328721997
          },
          "point_estimate": 11.024552047982368,
          "standard_error": 2.1723222812642975
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16230.87117949753,
            "upper_bound": 16251.023314687322
          },
          "point_estimate": 16241.15407823534,
          "standard_error": 5.169641766974946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16222.38197233378,
            "upper_bound": 16255.518118771382
          },
          "point_estimate": 16245.003639248353,
          "standard_error": 7.890887465863729
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.0740154175899717,
            "upper_bound": 29.424176254724753
          },
          "point_estimate": 18.180415256214623,
          "standard_error": 7.054432922329879
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16225.43137200816,
            "upper_bound": 16252.626760763093
          },
          "point_estimate": 16238.321778890451,
          "standard_error": 7.064374875087851
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.115912108372749,
            "upper_bound": 20.93178046604119
          },
          "point_estimate": 17.253225586108368,
          "standard_error": 2.7331729861262706
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17955.858055004956,
            "upper_bound": 17988.0175033036
          },
          "point_estimate": 17972.437150093603,
          "standard_error": 8.222286673895232
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17953.469276511398,
            "upper_bound": 17996.8760736703
          },
          "point_estimate": 17973.62378592666,
          "standard_error": 11.95979814626842
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.47847313654604,
            "upper_bound": 49.87176936298725
          },
          "point_estimate": 26.493999081172547,
          "standard_error": 10.358937954707216
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17957.401690982708,
            "upper_bound": 17984.441375141458
          },
          "point_estimate": 17969.457296024095,
          "standard_error": 7.022812133056324
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.44012335925076,
            "upper_bound": 35.288371545093554
          },
          "point_estimate": 27.393709604157166,
          "standard_error": 5.239236659619786
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18191.682219438877,
            "upper_bound": 18218.409747155027
          },
          "point_estimate": 18204.7740143382,
          "standard_error": 6.852442127389369
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18180.62307949232,
            "upper_bound": 18221.94607965932
          },
          "point_estimate": 18204.70344259949,
          "standard_error": 9.574049999005508
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.65148410078668,
            "upper_bound": 40.86491693983368
          },
          "point_estimate": 28.192178257005153,
          "standard_error": 9.572270315696088
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18181.22606824135,
            "upper_bound": 18209.337490297832
          },
          "point_estimate": 18190.634842412095,
          "standard_error": 7.060260969918606
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.182691446748397,
            "upper_bound": 28.25813170944295
          },
          "point_estimate": 22.818026911177142,
          "standard_error": 3.885346769624479
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68606.55926332652,
            "upper_bound": 68697.42518047708
          },
          "point_estimate": 68649.83452971332,
          "standard_error": 23.260575701126488
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68588.83792372882,
            "upper_bound": 68701.64783427495
          },
          "point_estimate": 68646.50725047081,
          "standard_error": 22.57473682988547
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.55251932301518,
            "upper_bound": 140.15380290254205
          },
          "point_estimate": 48.01686514250342,
          "standard_error": 35.391736240918966
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68597.63939926252,
            "upper_bound": 68674.63162841537
          },
          "point_estimate": 68631.66330129381,
          "standard_error": 19.564767581470388
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.461447635037565,
            "upper_bound": 101.82144227697016
          },
          "point_estimate": 77.67939608650285,
          "standard_error": 16.989844932411536
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1216321.7091224203,
            "upper_bound": 1218547.9478375
          },
          "point_estimate": 1217436.714710317,
          "standard_error": 570.0800853642094
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1215938.2666666666,
            "upper_bound": 1219563.1
          },
          "point_estimate": 1217205.2652777778,
          "standard_error": 764.999281515781
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182.3880367620078,
            "upper_bound": 3704.065999239775
          },
          "point_estimate": 1920.0266817461109,
          "standard_error": 922.9137660601384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1216661.585694355,
            "upper_bound": 1218778.5192139738
          },
          "point_estimate": 1217532.382164502,
          "standard_error": 534.9547366734527
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1077.3537868831588,
            "upper_bound": 2386.6678978727173
          },
          "point_estimate": 1903.9530631328337,
          "standard_error": 333.54649264700765
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205449.72054613935,
            "upper_bound": 205727.1244670882
          },
          "point_estimate": 205582.3885234508,
          "standard_error": 71.09091424543811
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205347.87288135593,
            "upper_bound": 205726.29802259887
          },
          "point_estimate": 205590.54943502825,
          "standard_error": 98.85998627328954
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.51282933079081,
            "upper_bound": 447.7665980769207
          },
          "point_estimate": 204.54679368499964,
          "standard_error": 95.8577340471012
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205445.76364628933,
            "upper_bound": 205645.7774505707
          },
          "point_estimate": 205541.40093917385,
          "standard_error": 50.37722983348344
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.9167673242484,
            "upper_bound": 312.75524988752016
          },
          "point_estimate": 236.67092659370664,
          "standard_error": 51.00852542256619
        }
      }
    },
    "memmem/krate/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5474.149909882006,
            "upper_bound": 5496.641536194833
          },
          "point_estimate": 5484.47044631038,
          "standard_error": 5.799369540891931
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5469.538669381189,
            "upper_bound": 5496.695449823221
          },
          "point_estimate": 5476.609830090987,
          "standard_error": 8.539002070880224
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3999678682146346,
            "upper_bound": 34.23454609155652
          },
          "point_estimate": 15.32350041902614,
          "standard_error": 7.828734152356792
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5468.858926865396,
            "upper_bound": 5490.1849158229525
          },
          "point_estimate": 5478.584765769595,
          "standard_error": 5.553576385379232
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.03296342583184,
            "upper_bound": 25.89800808007808
          },
          "point_estimate": 19.2574021855709,
          "standard_error": 4.7050760798364655
        }
      }
    },
    "memmem/krate/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5527.308378881777,
            "upper_bound": 5539.911335781289
          },
          "point_estimate": 5533.693053953014,
          "standard_error": 3.219301641571551
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5527.486978055471,
            "upper_bound": 5542.2005676622975
          },
          "point_estimate": 5534.17739446815,
          "standard_error": 3.04174213820966
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9148037358165215,
            "upper_bound": 18.399479138446715
          },
          "point_estimate": 6.55874257328822,
          "standard_error": 4.957466806815307
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5521.902859851119,
            "upper_bound": 5540.2985054851615
          },
          "point_estimate": 5530.041095326496,
          "standard_error": 4.663042093740925
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.167805281666949,
            "upper_bound": 14.137707245612717
          },
          "point_estimate": 10.72261992707902,
          "standard_error": 2.300998913796792
        }
      }
    },
    "memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14827.997101487004,
            "upper_bound": 14865.715956700014
          },
          "point_estimate": 14848.053217299575,
          "standard_error": 9.61266785366688
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14835.088158432012,
            "upper_bound": 14874.524499795834
          },
          "point_estimate": 14847.228537158024,
          "standard_error": 9.689241148410357
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.2627112681276453,
            "upper_bound": 51.27057276167268
          },
          "point_estimate": 18.26630214508366,
          "standard_error": 12.009633579152176
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14840.221021620986,
            "upper_bound": 14866.4674459371
          },
          "point_estimate": 14852.171540994734,
          "standard_error": 7.316335644318587
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.530107390576775,
            "upper_bound": 44.27808932071897
          },
          "point_estimate": 31.98012105052152,
          "standard_error": 8.468125099814241
        }
      }
    },
    "memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.10723973487921,
            "upper_bound": 58.25365690775451
          },
          "point_estimate": 58.17496838145062,
          "standard_error": 0.037592075609395315
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.09270467784968,
            "upper_bound": 58.27208332934674
          },
          "point_estimate": 58.122262772510155,
          "standard_error": 0.0467818311060304
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011970234673518754,
            "upper_bound": 0.20343426034511028
          },
          "point_estimate": 0.06568459880856202,
          "standard_error": 0.05155922700855874
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.09823406442733,
            "upper_bound": 58.18509320625965
          },
          "point_estimate": 58.135073057358426,
          "standard_error": 0.02229286050332247
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05134978324553921,
            "upper_bound": 0.16395777892873942
          },
          "point_estimate": 0.126184023337884,
          "standard_error": 0.02905615095287427
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.729225464497876,
            "upper_bound": 23.753068005316116
          },
          "point_estimate": 23.740757109535434,
          "standard_error": 0.00609168461993225
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.721483299024538,
            "upper_bound": 23.760343074943343
          },
          "point_estimate": 23.734630315100937,
          "standard_error": 0.00953898949978767
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0021281901468961453,
            "upper_bound": 0.03460642762896723
          },
          "point_estimate": 0.022298040241940927,
          "standard_error": 0.008167820637692788
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.723424425236875,
            "upper_bound": 23.741733872535864
          },
          "point_estimate": 23.730205150149956,
          "standard_error": 0.004699491987699234
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011026297712157,
            "upper_bound": 0.02436996002186485
          },
          "point_estimate": 0.02028477783300975,
          "standard_error": 0.0033518187771033555
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.955383864906253,
            "upper_bound": 23.99091866215392
          },
          "point_estimate": 23.97304615727571,
          "standard_error": 0.009073542053194416
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.94960846506376,
            "upper_bound": 23.999481109572734
          },
          "point_estimate": 23.972291339175055,
          "standard_error": 0.013526737268013915
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00553953640451455,
            "upper_bound": 0.051310266652179456
          },
          "point_estimate": 0.031234837210913403,
          "standard_error": 0.011979880959642556
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.959798361662266,
            "upper_bound": 23.98778264847731
          },
          "point_estimate": 23.976307868792553,
          "standard_error": 0.006989270299275702
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017636145427726343,
            "upper_bound": 0.03783142614058709
          },
          "point_estimate": 0.03037116320769877,
          "standard_error": 0.005105287732721729
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.11187721994414,
            "upper_bound": 28.33462348682345
          },
          "point_estimate": 27.690518018607015,
          "standard_error": 0.31201075708279075
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.035962698427895,
            "upper_bound": 29.04146410585374
          },
          "point_estimate": 27.277386017655058,
          "standard_error": 0.47240176189627014
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10968495188530296,
            "upper_bound": 1.5491224551201754
          },
          "point_estimate": 0.3844357507146791,
          "standard_error": 0.44966176790028006
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.157025473252386,
            "upper_bound": 28.91836768898857
          },
          "point_estimate": 28.36596047469754,
          "standard_error": 0.41413701926409824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4381831339274375,
            "upper_bound": 1.2460668992480344
          },
          "point_estimate": 1.0416394679694048,
          "standard_error": 0.1834860098284874
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.092207940214855,
            "upper_bound": 29.802245889634143
          },
          "point_estimate": 29.470437846939166,
          "standard_error": 0.18374729945817483
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.027021708669253,
            "upper_bound": 29.942286863107448
          },
          "point_estimate": 29.61340889060374,
          "standard_error": 0.22862154640119323
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1404471955976644,
            "upper_bound": 1.0428404281605357
          },
          "point_estimate": 0.5456504424704371,
          "standard_error": 0.2270590663965142
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.49743613914668,
            "upper_bound": 29.977979606386256
          },
          "point_estimate": 29.75624843138098,
          "standard_error": 0.12527660519612685
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.26470942147551796,
            "upper_bound": 0.7792145906509881
          },
          "point_estimate": 0.614050169086903,
          "standard_error": 0.13193138283943942
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.82103225125377,
            "upper_bound": 19.857464698781214
          },
          "point_estimate": 19.83872594209833,
          "standard_error": 0.009309997847638037
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.815512276224048,
            "upper_bound": 19.85658965529852
          },
          "point_estimate": 19.834100231176475,
          "standard_error": 0.008449922780349247
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009571480192840994,
            "upper_bound": 0.05154794933040665
          },
          "point_estimate": 0.020660635330752953,
          "standard_error": 0.014703061932510903
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.83028979103652,
            "upper_bound": 19.863709167900595
          },
          "point_estimate": 19.842416116435125,
          "standard_error": 0.008959003904785556
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014441296311705362,
            "upper_bound": 0.04134579208847926
          },
          "point_estimate": 0.031025363565374192,
          "standard_error": 0.007016468926843453
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.665180334119366,
            "upper_bound": 25.69447585575421
          },
          "point_estimate": 25.680166111673525,
          "standard_error": 0.007451092782318534
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.667412207904345,
            "upper_bound": 25.697128239805487
          },
          "point_estimate": 25.67682690724658,
          "standard_error": 0.008135139279019508
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030200738098134874,
            "upper_bound": 0.04375923657179086
          },
          "point_estimate": 0.022563014253404564,
          "standard_error": 0.009666483595792317
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.674584369252095,
            "upper_bound": 25.697572355663713
          },
          "point_estimate": 25.68651971806234,
          "standard_error": 0.00601473388876806
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011706551950465896,
            "upper_bound": 0.03348096027951134
          },
          "point_estimate": 0.02480228556444262,
          "standard_error": 0.005614565191840667
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.01115321648165,
            "upper_bound": 33.06298107763727
          },
          "point_estimate": 33.034767206882265,
          "standard_error": 0.01336624173587962
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.00566586278866,
            "upper_bound": 33.058141923707645
          },
          "point_estimate": 33.028220546996614,
          "standard_error": 0.012337281868788514
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00939058031017015,
            "upper_bound": 0.06761776162492554
          },
          "point_estimate": 0.027844952072915367,
          "standard_error": 0.015127597648162384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.015994117595234,
            "upper_bound": 33.06016141984123
          },
          "point_estimate": 33.03343811242372,
          "standard_error": 0.01120379242353747
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017721449112955655,
            "upper_bound": 0.06154661535490226
          },
          "point_estimate": 0.04455856709009336,
          "standard_error": 0.012257692525322664
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.91428524987512,
            "upper_bound": 26.95472889721949
          },
          "point_estimate": 26.934957415628723,
          "standard_error": 0.010413349639047949
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.908309278839106,
            "upper_bound": 26.97282076649541
          },
          "point_estimate": 26.932614463107143,
          "standard_error": 0.013655944266341462
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00469266497445378,
            "upper_bound": 0.06568810564760229
          },
          "point_estimate": 0.039466638139793536,
          "standard_error": 0.018267851367566573
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.92368133419836,
            "upper_bound": 26.956791912163663
          },
          "point_estimate": 26.93755416240284,
          "standard_error": 0.008647472868568673
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020014894755623125,
            "upper_bound": 0.04334293093621825
          },
          "point_estimate": 0.034847158287031714,
          "standard_error": 0.0061417057577918465
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.987115180471356,
            "upper_bound": 35.027816918586325
          },
          "point_estimate": 35.00743845492518,
          "standard_error": 0.010408883247629655
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.979815931455434,
            "upper_bound": 35.03799851830586
          },
          "point_estimate": 35.00408647112201,
          "standard_error": 0.018033121223805695
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035829068089928257,
            "upper_bound": 0.05460046061658123
          },
          "point_estimate": 0.038657815057602074,
          "standard_error": 0.013762226932502343
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.98636001746817,
            "upper_bound": 35.01816932440613
          },
          "point_estimate": 35.002834912065055,
          "standard_error": 0.008156140931541494
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021881144454518348,
            "upper_bound": 0.04242284178564694
          },
          "point_estimate": 0.0347824733037039,
          "standard_error": 0.005258583579043043
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.16936454446071,
            "upper_bound": 26.192804963224336
          },
          "point_estimate": 26.18093762507832,
          "standard_error": 0.00599714765761405
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.166680605772772,
            "upper_bound": 26.193518297644175
          },
          "point_estimate": 26.183302308162222,
          "standard_error": 0.007665012055920158
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004054254343794719,
            "upper_bound": 0.03431683299278079
          },
          "point_estimate": 0.02197845692731269,
          "standard_error": 0.007816073758969625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.16450099607319,
            "upper_bound": 26.19218286967051
          },
          "point_estimate": 26.175982244974588,
          "standard_error": 0.0069675843956279754
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010379846775937953,
            "upper_bound": 0.0263086991204544
          },
          "point_estimate": 0.02000439763612918,
          "standard_error": 0.00411820047651085
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.57334382934083,
            "upper_bound": 18.60732329198322
          },
          "point_estimate": 18.590968772139103,
          "standard_error": 0.008641822642928542
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.577419984536988,
            "upper_bound": 18.60879841764223
          },
          "point_estimate": 18.59003175494962,
          "standard_error": 0.007653526383703848
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00020937880990617915,
            "upper_bound": 0.04628612979295689
          },
          "point_estimate": 0.01827720229266773,
          "standard_error": 0.010661252422686757
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.577242346000933,
            "upper_bound": 18.607199058090934
          },
          "point_estimate": 18.590047552947677,
          "standard_error": 0.007726327143642381
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010994056005213509,
            "upper_bound": 0.03992427845014507
          },
          "point_estimate": 0.02869528900329202,
          "standard_error": 0.007445439203676275
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.83482605481245,
            "upper_bound": 28.86940907390626
          },
          "point_estimate": 28.852901003440017,
          "standard_error": 0.008788140116605746
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.842828488353625,
            "upper_bound": 28.868939374688313
          },
          "point_estimate": 28.85313180332342,
          "standard_error": 0.004949996667036447
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023641302757679613,
            "upper_bound": 0.04528761132853757
          },
          "point_estimate": 0.006403125558005877,
          "standard_error": 0.01173323864175295
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.85044470244239,
            "upper_bound": 28.867893918486537
          },
          "point_estimate": 28.856607018429845,
          "standard_error": 0.004502175104386886
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008644620111696177,
            "upper_bound": 0.04148765278765198
          },
          "point_estimate": 0.02914841085993179,
          "standard_error": 0.008322302534969307
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99052.2376988452,
            "upper_bound": 99262.94583450112
          },
          "point_estimate": 99158.18800019464,
          "standard_error": 54.2044347506867
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98984.12443233424,
            "upper_bound": 99319.67629427792
          },
          "point_estimate": 99188.37130206304,
          "standard_error": 94.6688843201807
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.14850596088961,
            "upper_bound": 295.5274587860366
          },
          "point_estimate": 244.31645119384336,
          "standard_error": 63.58986094695105
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99154.57094934914,
            "upper_bound": 99337.61534180696
          },
          "point_estimate": 99269.16224211756,
          "standard_error": 46.71094110408592
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.71011220583958,
            "upper_bound": 213.87391591437705
          },
          "point_estimate": 180.9819889474901,
          "standard_error": 25.101197024709503
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52317.39376149879,
            "upper_bound": 52420.63770677233
          },
          "point_estimate": 52361.333266547736,
          "standard_error": 26.976638382954285
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52302.45770893372,
            "upper_bound": 52388.30662824208
          },
          "point_estimate": 52343.681195965415,
          "standard_error": 21.95720548986727
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.246612997301275,
            "upper_bound": 101.48587900998815
          },
          "point_estimate": 59.95571628427921,
          "standard_error": 22.156430225984767
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52313.57609246099,
            "upper_bound": 52371.022742678535
          },
          "point_estimate": 52338.59940491785,
          "standard_error": 14.551817026608129
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.589606082993555,
            "upper_bound": 131.6569498724546
          },
          "point_estimate": 90.05144743780458,
          "standard_error": 31.90689947748164
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143289.82577302837,
            "upper_bound": 143578.40299962505
          },
          "point_estimate": 143433.99457880267,
          "standard_error": 73.896216695022
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143234.88976377953,
            "upper_bound": 143624.02296587927
          },
          "point_estimate": 143449.5832895888,
          "standard_error": 95.67508709409606
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.62694089680735,
            "upper_bound": 419.1660901960979
          },
          "point_estimate": 261.48710795609736,
          "standard_error": 91.82733016407926
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143302.21164541933,
            "upper_bound": 143526.40008394295
          },
          "point_estimate": 143402.14087329994,
          "standard_error": 56.10305171984947
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139.6226338635848,
            "upper_bound": 311.923373226244
          },
          "point_estimate": 246.87800463623128,
          "standard_error": 44.22325032679205
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397473.45436853,
            "upper_bound": 397897.0982117624
          },
          "point_estimate": 397663.1906806418,
          "standard_error": 109.28463525493564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397406.2391304347,
            "upper_bound": 397828.6838768116
          },
          "point_estimate": 397557.7888198758,
          "standard_error": 117.17076520155688
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.90609872341156,
            "upper_bound": 507.63237000950375
          },
          "point_estimate": 249.60370562305897,
          "standard_error": 114.37718500749484
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397434.8532757594,
            "upper_bound": 397774.6468379447
          },
          "point_estimate": 397582.3416996048,
          "standard_error": 92.11872208547597
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.24889473463116,
            "upper_bound": 504.01957335427613
          },
          "point_estimate": 363.4347514967516,
          "standard_error": 103.04124574374104
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 571371.6853278925,
            "upper_bound": 572179.3863144531
          },
          "point_estimate": 571739.1824386161,
          "standard_error": 208.2380277207323
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 571191.2239583333,
            "upper_bound": 572143.71875
          },
          "point_estimate": 571583.3766276042,
          "standard_error": 243.5095925065753
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.88950060586515,
            "upper_bound": 1094.7193886898283
          },
          "point_estimate": 585.5894144085656,
          "standard_error": 233.04468735503693
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 571381.3281726371,
            "upper_bound": 571815.045979064
          },
          "point_estimate": 571621.9487012987,
          "standard_error": 110.04874602470515
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307.73758588366667,
            "upper_bound": 949.0449830547255
          },
          "point_estimate": 693.9727158915939,
          "standard_error": 182.0650283519128
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41285.42700288026,
            "upper_bound": 41370.88913160247
          },
          "point_estimate": 41322.88240623363,
          "standard_error": 22.11674598569672
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41282.29243187605,
            "upper_bound": 41364.0888225256
          },
          "point_estimate": 41298.82544874225,
          "standard_error": 17.577165372552983
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.94169134313547,
            "upper_bound": 100.2253746313324
          },
          "point_estimate": 26.672935876484576,
          "standard_error": 22.671595745689153
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41281.33899079768,
            "upper_bound": 41320.48445502818
          },
          "point_estimate": 41300.86547286616,
          "standard_error": 9.906820173388844
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.801174688677563,
            "upper_bound": 100.5465556033388
          },
          "point_estimate": 73.6674155154784,
          "standard_error": 22.22653772617615
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98055.56105249649,
            "upper_bound": 98148.46695784666
          },
          "point_estimate": 98100.78024622428,
          "standard_error": 23.68215845715833
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98038.82383519446,
            "upper_bound": 98190.14948337823
          },
          "point_estimate": 98049.98936807428,
          "standard_error": 48.645681667377694
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.2075661782300107,
            "upper_bound": 119.62276186996232
          },
          "point_estimate": 31.821173334422628,
          "standard_error": 34.401397317446126
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98049.81732202315,
            "upper_bound": 98157.43985508486
          },
          "point_estimate": 98092.9710155074,
          "standard_error": 27.20226845925658
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.821159678750384,
            "upper_bound": 87.85676371563636
          },
          "point_estimate": 78.8751859563083,
          "standard_error": 10.5964319777802
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73555.89662843004,
            "upper_bound": 73691.86364934772
          },
          "point_estimate": 73620.83879779576,
          "standard_error": 34.71327835457334
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73546.6789248763,
            "upper_bound": 73680.27125506072
          },
          "point_estimate": 73612.50394736842,
          "standard_error": 37.06132152153208
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.165993444836527,
            "upper_bound": 188.05647707630115
          },
          "point_estimate": 93.4834305670676,
          "standard_error": 40.21337190095699
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73556.22762033288,
            "upper_bound": 73655.30698119834
          },
          "point_estimate": 73610.07244860403,
          "standard_error": 25.38449701136483
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.20597284627736,
            "upper_bound": 159.17112822002915
          },
          "point_estimate": 116.23235891815546,
          "standard_error": 28.419684991872252
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307399.89646591974,
            "upper_bound": 309111.5037563026
          },
          "point_estimate": 308081.64300653595,
          "standard_error": 468.0776963485281
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307335.19327731093,
            "upper_bound": 308341.3352941177
          },
          "point_estimate": 307525.82371615316,
          "standard_error": 231.8016124355745
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.427324282246865,
            "upper_bound": 1141.6152691440268
          },
          "point_estimate": 275.79681863304404,
          "standard_error": 286.0255537501419
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307280.66011441266,
            "upper_bound": 307641.34810865234
          },
          "point_estimate": 307464.76846011134,
          "standard_error": 93.39526234826724
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160.4671662688553,
            "upper_bound": 2332.3163291370215
          },
          "point_estimate": 1556.8941436237014,
          "standard_error": 666.3629099186628
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36095.90908511432,
            "upper_bound": 36174.20702451954
          },
          "point_estimate": 36131.35825215375,
          "standard_error": 20.123656334703817
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36079.90295725646,
            "upper_bound": 36170.05894632207
          },
          "point_estimate": 36115.76267395626,
          "standard_error": 25.16793237433457
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.4151432757716975,
            "upper_bound": 98.17877241205107
          },
          "point_estimate": 47.11172654928918,
          "standard_error": 26.88051230385244
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36085.19584871722,
            "upper_bound": 36140.82520674726
          },
          "point_estimate": 36105.86965479848,
          "standard_error": 14.151410746549844
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.968099046764507,
            "upper_bound": 90.84593336199872
          },
          "point_estimate": 67.1577677382137,
          "standard_error": 17.523548057957733
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64094.41368438734,
            "upper_bound": 64156.0203707378
          },
          "point_estimate": 64122.75591542791,
          "standard_error": 15.845230981402404
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64082.98977072311,
            "upper_bound": 64150.20304232804
          },
          "point_estimate": 64116.4341563786,
          "standard_error": 19.52607092109197
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.54224642694946,
            "upper_bound": 80.70095348790883
          },
          "point_estimate": 45.0902269596262,
          "standard_error": 18.114406329795813
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64090.808922094366,
            "upper_bound": 64197.06114411227
          },
          "point_estimate": 64150.84694564695,
          "standard_error": 27.44279938384587
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.17693428338359,
            "upper_bound": 72.39982115813727
          },
          "point_estimate": 53.00368885301802,
          "standard_error": 13.837706327813
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170090.0602699896,
            "upper_bound": 170386.928619641
          },
          "point_estimate": 170244.7656644044,
          "standard_error": 75.86707137970821
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170090.52712876428,
            "upper_bound": 170476.18691588784
          },
          "point_estimate": 170258.00186915888,
          "standard_error": 100.57595174133462
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.2479114446848,
            "upper_bound": 443.2965261252464
          },
          "point_estimate": 270.14426408247334,
          "standard_error": 96.55645529428872
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170133.601595443,
            "upper_bound": 170445.60497329774
          },
          "point_estimate": 170267.1520937007,
          "standard_error": 79.85396332642094
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.81233045792632,
            "upper_bound": 330.428310555186
          },
          "point_estimate": 252.72681328732384,
          "standard_error": 52.589554122450274
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33892.395045887795,
            "upper_bound": 33938.06630218847
          },
          "point_estimate": 33915.1983816119,
          "standard_error": 11.68071036928134
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33882.96985460851,
            "upper_bound": 33947.47931580039
          },
          "point_estimate": 33911.39623015873,
          "standard_error": 16.49602122617073
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.832836652670345,
            "upper_bound": 66.71125391368572
          },
          "point_estimate": 46.26527097367871,
          "standard_error": 13.80471088367284
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33896.14442479982,
            "upper_bound": 33946.53012045076
          },
          "point_estimate": 33925.64080420047,
          "standard_error": 12.78954145172511
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.819070323922016,
            "upper_bound": 48.44397408031313
          },
          "point_estimate": 38.98508497047811,
          "standard_error": 6.550903022262377
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11276.27761390262,
            "upper_bound": 11299.767259012404
          },
          "point_estimate": 11286.75343766859,
          "standard_error": 6.052076881428229
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11273.820214708896,
            "upper_bound": 11297.861615216714
          },
          "point_estimate": 11282.337659806672,
          "standard_error": 5.63997362741212
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.230618628332528,
            "upper_bound": 27.266776938223497
          },
          "point_estimate": 15.922900161652748,
          "standard_error": 6.779724530175278
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11278.507081103977,
            "upper_bound": 11304.315425085682
          },
          "point_estimate": 11292.291791090109,
          "standard_error": 6.440031170549423
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.700557085919483,
            "upper_bound": 28.259762390010778
          },
          "point_estimate": 20.11686345635563,
          "standard_error": 5.937059192274396
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 502963.034234888,
            "upper_bound": 504699.2982164601
          },
          "point_estimate": 503882.2705908894,
          "standard_error": 443.48008015217897
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 503292.6126331811,
            "upper_bound": 505001.459760274
          },
          "point_estimate": 503745.4378669276,
          "standard_error": 499.1277759484253
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.16429745795839,
            "upper_bound": 2418.192365972596
          },
          "point_estimate": 1060.895059247538,
          "standard_error": 600.3456774841526
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 503407.6530984997,
            "upper_bound": 504675.9200475227
          },
          "point_estimate": 503953.01444582816,
          "standard_error": 325.35643711056076
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 652.564533776532,
            "upper_bound": 2049.1691358806415
          },
          "point_estimate": 1481.227121464175,
          "standard_error": 383.0801969923759
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1032.830840592704,
            "upper_bound": 1034.334942102285
          },
          "point_estimate": 1033.5430970666805,
          "standard_error": 0.38558503276520506
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1032.489372308446,
            "upper_bound": 1034.524995523893
          },
          "point_estimate": 1033.1621059847907,
          "standard_error": 0.634955497330364
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.16972485789765523,
            "upper_bound": 2.3487524148359022
          },
          "point_estimate": 1.3812291591778816,
          "standard_error": 0.5488862092970823
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1032.648652331378,
            "upper_bound": 1034.0336669530416
          },
          "point_estimate": 1033.3288140745692,
          "standard_error": 0.35365376008255445
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7132814802562694,
            "upper_bound": 1.6252867259491424
          },
          "point_estimate": 1.282765484550492,
          "standard_error": 0.24534121988525295
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40909.096934580106,
            "upper_bound": 41118.49069371475
          },
          "point_estimate": 40986.48819072843,
          "standard_error": 60.18308218856105
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40921.30101526044,
            "upper_bound": 40947.91611804767
          },
          "point_estimate": 40931.15172152857,
          "standard_error": 15.33098114304592
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9689396370430714,
            "upper_bound": 69.88153757887953
          },
          "point_estimate": 15.247363929074927,
          "standard_error": 22.9554073250889
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40920.062948802195,
            "upper_bound": 40938.832011335355
          },
          "point_estimate": 40929.687860607046,
          "standard_error": 4.755870069016974
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.911868136170996,
            "upper_bound": 307.83592897257876
          },
          "point_estimate": 200.2911691625997,
          "standard_error": 102.65750154913064
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48575.615394600216,
            "upper_bound": 48654.163429387336
          },
          "point_estimate": 48612.95412714307,
          "standard_error": 20.121328413991208
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48553.39839786382,
            "upper_bound": 48666.83934134401
          },
          "point_estimate": 48594.79048254816,
          "standard_error": 37.494925235818855
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9854710083683984,
            "upper_bound": 119.68001818678316
          },
          "point_estimate": 65.25866464583645,
          "standard_error": 33.84827085957652
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48557.74298991248,
            "upper_bound": 48633.74814964591
          },
          "point_estimate": 48582.08275276126,
          "standard_error": 19.726144180347937
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.238466775358674,
            "upper_bound": 83.2320957225985
          },
          "point_estimate": 66.94123101575485,
          "standard_error": 11.80699493983548
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48798.23290643176,
            "upper_bound": 48866.26658932566
          },
          "point_estimate": 48833.4001993182,
          "standard_error": 17.384779082128127
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48796.94765100671,
            "upper_bound": 48881.99552572707
          },
          "point_estimate": 48838.83793544263,
          "standard_error": 20.684887388374225
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.736225599677024,
            "upper_bound": 96.7700631410463
          },
          "point_estimate": 63.0459884109126,
          "standard_error": 20.71467892942016
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48807.70686867344,
            "upper_bound": 48866.45519136626
          },
          "point_estimate": 48841.79067026933,
          "standard_error": 15.089744988798929
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.457986691088557,
            "upper_bound": 76.64193367815226
          },
          "point_estimate": 57.87646426242786,
          "standard_error": 12.397665156016274
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14249.51100139324,
            "upper_bound": 14262.423385859827
          },
          "point_estimate": 14255.10026782604,
          "standard_error": 3.3525910282976414
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14246.66304858934,
            "upper_bound": 14257.76038401254
          },
          "point_estimate": 14254.508788625168,
          "standard_error": 3.089034275825227
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.911520124883976,
            "upper_bound": 13.98839820711703
          },
          "point_estimate": 8.006606290064525,
          "standard_error": 3.147618350757187
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14248.761407833315,
            "upper_bound": 14255.050200663947
          },
          "point_estimate": 14251.277351097178,
          "standard_error": 1.6122925543489834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.033073488806873,
            "upper_bound": 16.27751934334623
          },
          "point_estimate": 11.188525593056497,
          "standard_error": 3.8047150347442584
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25761.814666756534,
            "upper_bound": 25857.286004953996
          },
          "point_estimate": 25798.11411985082,
          "standard_error": 26.807854039158908
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25756.498084116873,
            "upper_bound": 25798.11924982307
          },
          "point_estimate": 25769.559943382876,
          "standard_error": 12.245543249717974
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4900029637032412,
            "upper_bound": 48.84915841088358
          },
          "point_estimate": 23.227242199097542,
          "standard_error": 13.088613303969309
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25750.838145910493,
            "upper_bound": 25784.664289794713
          },
          "point_estimate": 25763.60032352644,
          "standard_error": 8.768384076133646
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.087607342668472,
            "upper_bound": 136.8104716774474
          },
          "point_estimate": 89.28157453833228,
          "standard_error": 42.585627912182815
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17333.10055456018,
            "upper_bound": 17363.05317413835
          },
          "point_estimate": 17347.052935959415,
          "standard_error": 7.723342108121446
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17327.76634209451,
            "upper_bound": 17364.138961709563
          },
          "point_estimate": 17340.221337782015,
          "standard_error": 9.398935369257964
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.050875636969467,
            "upper_bound": 42.13019591552878
          },
          "point_estimate": 19.991214096944315,
          "standard_error": 9.422988267479507
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17331.94263176085,
            "upper_bound": 17350.3730262767
          },
          "point_estimate": 17339.360273853887,
          "standard_error": 4.779754382291884
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.141685272986,
            "upper_bound": 33.04252989050983
          },
          "point_estimate": 25.667962391349867,
          "standard_error": 5.649901412484881
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17327.51278333863,
            "upper_bound": 17359.112543791707
          },
          "point_estimate": 17343.119987421884,
          "standard_error": 8.077789241467809
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17320.352716873214,
            "upper_bound": 17366.522044804577
          },
          "point_estimate": 17339.36716714331,
          "standard_error": 12.670647460721446
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.595542889222272,
            "upper_bound": 44.017805894727886
          },
          "point_estimate": 35.82783279338466,
          "standard_error": 10.194980438479966
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17329.96078894722,
            "upper_bound": 17368.58911532045
          },
          "point_estimate": 17350.911381278398,
          "standard_error": 9.935398839707736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.36227131410661,
            "upper_bound": 32.99746940213059
          },
          "point_estimate": 26.85672215397062,
          "standard_error": 4.266676316467386
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17326.26063049727,
            "upper_bound": 17370.788140862605
          },
          "point_estimate": 17344.38194936455,
          "standard_error": 11.94953447746093
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17324.853759338737,
            "upper_bound": 17353.014830710537
          },
          "point_estimate": 17326.886653552694,
          "standard_error": 7.065029751662943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5655855566392539,
            "upper_bound": 36.37477577767901
          },
          "point_estimate": 4.971144435665229,
          "standard_error": 8.84613139238829
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17325.343220263152,
            "upper_bound": 17342.270579808173
          },
          "point_estimate": 17332.53136515368,
          "standard_error": 4.37710578913076
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.8815119531158553,
            "upper_bound": 58.75765337532943
          },
          "point_estimate": 39.91097841552283,
          "standard_error": 16.14904261189673
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37262.90804786605,
            "upper_bound": 37343.56985763889
          },
          "point_estimate": 37302.72959586682,
          "standard_error": 20.717434893092683
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37236.37283950617,
            "upper_bound": 37368.31532921811
          },
          "point_estimate": 37293.37635030864,
          "standard_error": 33.8140881672372
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.08265993677692,
            "upper_bound": 119.1823719195878
          },
          "point_estimate": 90.56108067617036,
          "standard_error": 25.606726362115506
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37235.40174168663,
            "upper_bound": 37297.78639491298
          },
          "point_estimate": 37257.67058949281,
          "standard_error": 15.985270915962268
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.593358290502735,
            "upper_bound": 82.40367929026903
          },
          "point_estimate": 69.15314323007922,
          "standard_error": 10.071156420056608
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17887.777333005248,
            "upper_bound": 17920.3391574686
          },
          "point_estimate": 17902.85730285668,
          "standard_error": 8.34821606295977
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17882.857283464567,
            "upper_bound": 17913.720819565522
          },
          "point_estimate": 17905.06627296588,
          "standard_error": 7.935870171291434
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8073877631882225,
            "upper_bound": 42.48830918662788
          },
          "point_estimate": 20.57425762808914,
          "standard_error": 10.608613183205108
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17891.12947820625,
            "upper_bound": 17910.42313749809
          },
          "point_estimate": 17900.937508947743,
          "standard_error": 4.920122995422754
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.7596224013845,
            "upper_bound": 38.57467961723466
          },
          "point_estimate": 27.719714426935703,
          "standard_error": 7.599806932099016
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18491.369476647105,
            "upper_bound": 18569.79369180972
          },
          "point_estimate": 18522.621722149484,
          "standard_error": 21.21781069728441
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18490.14685772476,
            "upper_bound": 18530.676425661914
          },
          "point_estimate": 18496.083299389,
          "standard_error": 11.23341807438852
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.761044689613948,
            "upper_bound": 56.07863764493127
          },
          "point_estimate": 11.970950525762584,
          "standard_error": 15.552767451118632
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18482.745637623048,
            "upper_bound": 18503.109590978744
          },
          "point_estimate": 18492.359418890683,
          "standard_error": 5.161836811989554
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.97126866653858,
            "upper_bound": 105.96801159488784
          },
          "point_estimate": 70.52892836875985,
          "standard_error": 29.654506030850285
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17702.471852882736,
            "upper_bound": 17723.16182149236
          },
          "point_estimate": 17713.630144813244,
          "standard_error": 5.3109124163643795
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17704.48660496834,
            "upper_bound": 17725.41217730151
          },
          "point_estimate": 17715.90715097534,
          "standard_error": 5.239463650604649
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.0111518461984796,
            "upper_bound": 26.531695031483583
          },
          "point_estimate": 14.895724543145125,
          "standard_error": 5.84841894633067
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17708.079792468394,
            "upper_bound": 17724.086236353523
          },
          "point_estimate": 17717.087256533043,
          "standard_error": 4.285732734629789
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.543120551998852,
            "upper_bound": 24.491428045979266
          },
          "point_estimate": 17.77672539647913,
          "standard_error": 4.760711911171433
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17572.94877970059,
            "upper_bound": 17599.029384355385
          },
          "point_estimate": 17587.094311888282,
          "standard_error": 6.672233655031021
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17578.725494929986,
            "upper_bound": 17603.724867213907
          },
          "point_estimate": 17587.544507292696,
          "standard_error": 4.778715751915265
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1510808221517823,
            "upper_bound": 33.76083531242274
          },
          "point_estimate": 9.661007575145405,
          "standard_error": 9.18476064367572
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17583.681517867288,
            "upper_bound": 17595.201716719672
          },
          "point_estimate": 17589.922012704825,
          "standard_error": 2.9026803849389693
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.31980389703804,
            "upper_bound": 30.979791039066285
          },
          "point_estimate": 22.197758664770856,
          "standard_error": 6.37703195684385
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17593.24176911907,
            "upper_bound": 17622.97691029364
          },
          "point_estimate": 17607.237011132624,
          "standard_error": 7.68617321748599
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17592.054114230395,
            "upper_bound": 17628.110277508873
          },
          "point_estimate": 17602.04836640852,
          "standard_error": 7.656890002268494
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.326340011940901,
            "upper_bound": 41.80519295093518
          },
          "point_estimate": 13.73899896228159,
          "standard_error": 10.143528475289754
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17584.482440258667,
            "upper_bound": 17605.05197215963
          },
          "point_estimate": 17596.55170289536,
          "standard_error": 5.483744524329261
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.995855514437588,
            "upper_bound": 33.32990347343304
          },
          "point_estimate": 25.68875989638623,
          "standard_error": 5.975621790566719
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17609.085817550593,
            "upper_bound": 17631.37347231826
          },
          "point_estimate": 17620.120985206442,
          "standard_error": 5.6928605634001785
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17604.825477976767,
            "upper_bound": 17635.774878993223
          },
          "point_estimate": 17619.316922509563,
          "standard_error": 8.777394472021554
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.474173886357169,
            "upper_bound": 31.249437783386227
          },
          "point_estimate": 22.25120310770546,
          "standard_error": 6.6609131299292335
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17604.610371168557,
            "upper_bound": 17629.754321805372
          },
          "point_estimate": 17617.078038998756,
          "standard_error": 6.611063097402526
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.365941846404962,
            "upper_bound": 23.352319775659467
          },
          "point_estimate": 18.971654806220236,
          "standard_error": 3.053297104479757
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15213.666264091624,
            "upper_bound": 15234.533303155991
          },
          "point_estimate": 15223.77368283215,
          "standard_error": 5.364854871267999
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15208.504745951985,
            "upper_bound": 15237.930873115578
          },
          "point_estimate": 15219.320089202629,
          "standard_error": 7.638703452255895
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.7743537406141465,
            "upper_bound": 28.811052377948105
          },
          "point_estimate": 16.295612276997954,
          "standard_error": 6.620249568380096
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15212.186666487729,
            "upper_bound": 15226.237306085266
          },
          "point_estimate": 15219.746718440689,
          "standard_error": 3.6060531349952543
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.52455718174309,
            "upper_bound": 22.088303289124777
          },
          "point_estimate": 17.876429483810707,
          "standard_error": 3.138948447242999
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16314.52386730059,
            "upper_bound": 16340.45809856592
          },
          "point_estimate": 16327.446598871204,
          "standard_error": 6.6063288639003135
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16316.185384845763,
            "upper_bound": 16339.057271210371
          },
          "point_estimate": 16327.390897973444,
          "standard_error": 5.68834188361138
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8784298932747856,
            "upper_bound": 37.25631867819167
          },
          "point_estimate": 14.716880556332988,
          "standard_error": 8.126917962875666
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16318.056512262488,
            "upper_bound": 16333.934267627386
          },
          "point_estimate": 16326.04113254221,
          "standard_error": 3.963894416497424
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.708092393586387,
            "upper_bound": 30.16515304158712
          },
          "point_estimate": 22.051874597344643,
          "standard_error": 5.503959836474864
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17954.211758630514,
            "upper_bound": 17982.006822479856
          },
          "point_estimate": 17966.944611142433,
          "standard_error": 7.094927591833551
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17954.287288274954,
            "upper_bound": 17976.506448657412
          },
          "point_estimate": 17961.32590856767,
          "standard_error": 6.6077898690273855
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9749905105661856,
            "upper_bound": 33.832332675132854
          },
          "point_estimate": 18.848247019840414,
          "standard_error": 8.199653959999697
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17957.46681673799,
            "upper_bound": 17975.61440620657
          },
          "point_estimate": 17964.40577143626,
          "standard_error": 4.604917407987462
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.67866527453776,
            "upper_bound": 33.41332342103526
          },
          "point_estimate": 23.58114944031768,
          "standard_error": 6.910809242130821
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18382.071005612503,
            "upper_bound": 18400.97394038295
          },
          "point_estimate": 18391.26833162893,
          "standard_error": 4.838341104077026
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18378.08198585144,
            "upper_bound": 18400.591712986356
          },
          "point_estimate": 18392.478495632713,
          "standard_error": 5.823919306814658
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.02467083304307,
            "upper_bound": 28.63725381195088
          },
          "point_estimate": 13.965391014066892,
          "standard_error": 6.124306772032847
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18377.59716496902,
            "upper_bound": 18403.44900069147
          },
          "point_estimate": 18388.38359528294,
          "standard_error": 6.558381028846615
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.347968217539353,
            "upper_bound": 21.39766393990684
          },
          "point_estimate": 16.131470345073854,
          "standard_error": 3.4367604642515266
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68280.81698031351,
            "upper_bound": 68363.61288509033
          },
          "point_estimate": 68319.10222647391,
          "standard_error": 21.26477565343236
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68257.51785714286,
            "upper_bound": 68359.16953842941
          },
          "point_estimate": 68305.21103540101,
          "standard_error": 25.738067575694025
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.98873970099856,
            "upper_bound": 115.14302427159944
          },
          "point_estimate": 73.13356330688792,
          "standard_error": 24.65903116976116
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68267.50684933533,
            "upper_bound": 68336.84393453063
          },
          "point_estimate": 68309.2320476516,
          "standard_error": 17.799193022195272
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.974966872602835,
            "upper_bound": 94.4730099042824
          },
          "point_estimate": 70.86894459794455,
          "standard_error": 16.692926355967327
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1216134.899179894,
            "upper_bound": 1220111.1519865408
          },
          "point_estimate": 1218007.919191799,
          "standard_error": 1017.4688668809082
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1215245.2053571427,
            "upper_bound": 1219720.0761574074
          },
          "point_estimate": 1218091.63,
          "standard_error": 1087.1017934732583
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406.7869950003337,
            "upper_bound": 5454.6483401327805
          },
          "point_estimate": 3040.31411002345,
          "standard_error": 1277.0743521081995
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1216864.1337257782,
            "upper_bound": 1219441.8391866195
          },
          "point_estimate": 1218234.8601731602,
          "standard_error": 644.643651515139
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1593.2212470771306,
            "upper_bound": 4580.363918190977
          },
          "point_estimate": 3400.0144618526406,
          "standard_error": 820.60955484974
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204971.79168808853,
            "upper_bound": 205338.41608215636
          },
          "point_estimate": 205148.4646408394,
          "standard_error": 94.3762064240929
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204970.7099811676,
            "upper_bound": 205397.9927360775
          },
          "point_estimate": 205087.47410546136,
          "standard_error": 97.39853233116818
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.916586897471554,
            "upper_bound": 554.1002410666719
          },
          "point_estimate": 194.2966810138191,
          "standard_error": 133.90544757551072
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205014.14933614145,
            "upper_bound": 205339.69173847703
          },
          "point_estimate": 205161.81661163695,
          "standard_error": 82.3977720097592
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.36125119218684,
            "upper_bound": 421.5992206057013
          },
          "point_estimate": 314.034577364794,
          "standard_error": 74.98825823326281
        }
      }
    },
    "memmem/krate/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5430.978980462152,
            "upper_bound": 5447.1572989069655
          },
          "point_estimate": 5438.874781814317,
          "standard_error": 4.144203640771159
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5428.255111583762,
            "upper_bound": 5454.083046535987
          },
          "point_estimate": 5432.572011072871,
          "standard_error": 7.91258287025116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.130431023873845,
            "upper_bound": 20.703686444745877
          },
          "point_estimate": 14.376331199204603,
          "standard_error": 5.774512935288655
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5428.6755503854265,
            "upper_bound": 5450.890294788283
          },
          "point_estimate": 5440.872037015804,
          "standard_error": 5.768449137052523
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.80056120350186,
            "upper_bound": 15.974662158912633
          },
          "point_estimate": 13.782018030698776,
          "standard_error": 1.8184012114235384
        }
      }
    },
    "memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5458.502455091826,
            "upper_bound": 5473.966637034804
          },
          "point_estimate": 5466.473213341034,
          "standard_error": 3.9299884226883086
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5460.829576615941,
            "upper_bound": 5473.087690221486
          },
          "point_estimate": 5466.219950655417,
          "standard_error": 3.1304041114401273
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3433479383321494,
            "upper_bound": 21.35490136111873
          },
          "point_estimate": 8.600489338671576,
          "standard_error": 4.707751680508408
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5461.753060860909,
            "upper_bound": 5471.864240237658
          },
          "point_estimate": 5466.807276405981,
          "standard_error": 2.5277094531291744
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.57313869151727,
            "upper_bound": 18.258850362369277
          },
          "point_estimate": 13.130406381339595,
          "standard_error": 3.5332378111639113
        }
      }
    },
    "memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14478.763421745856,
            "upper_bound": 14498.30188561571
          },
          "point_estimate": 14488.34941106498,
          "standard_error": 5.013386214960616
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14473.10079238702,
            "upper_bound": 14507.44291401274
          },
          "point_estimate": 14487.775004976114,
          "standard_error": 8.612904516336325
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.3337645427241656,
            "upper_bound": 27.504044398250535
          },
          "point_estimate": 22.121949261913404,
          "standard_error": 6.760162801841
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14475.44180837281,
            "upper_bound": 14492.626058937887
          },
          "point_estimate": 14481.81904313839,
          "standard_error": 4.347058303534306
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.294406541544593,
            "upper_bound": 19.759618633408227
          },
          "point_estimate": 16.736256231305873,
          "standard_error": 2.364808609425977
        }
      }
    },
    "memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.536752317704796,
            "upper_bound": 36.09242687532653
          },
          "point_estimate": 35.811742947177635,
          "standard_error": 0.14282798552800532
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.33069179422851,
            "upper_bound": 36.323267655611254
          },
          "point_estimate": 35.7091692053626,
          "standard_error": 0.2807806284574602
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08210445623086783,
            "upper_bound": 0.7586187208355569
          },
          "point_estimate": 0.6347328492664883,
          "standard_error": 0.1907521029212012
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.37408076276196,
            "upper_bound": 35.7198749809066
          },
          "point_estimate": 35.50820721930636,
          "standard_error": 0.08939942419441317
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3183302653050166,
            "upper_bound": 0.5430255735100745
          },
          "point_estimate": 0.4766587125874027,
          "standard_error": 0.05823519174574011
        }
      }
    },
    "memmem/krate/prebuilt/sliceslice-i386/words": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/krate_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22975452.607916664,
            "upper_bound": 23007328.553571425
          },
          "point_estimate": 22991034.31172619,
          "standard_error": 8161.438149803643
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22967920.5,
            "upper_bound": 23013410.335714288
          },
          "point_estimate": 22987741.083333336,
          "standard_error": 11488.561075428572
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7570.665109343827,
            "upper_bound": 46651.25602677483
          },
          "point_estimate": 27528.907045014756,
          "standard_error": 9922.061390268396
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22975387.53935447,
            "upper_bound": 23008897.74875622
          },
          "point_estimate": 22991174.86623377,
          "standard_error": 8507.448688216733
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15579.612075043951,
            "upper_bound": 33762.00870112786
          },
          "point_estimate": 27260.858911232608,
          "standard_error": 4608.3809389012895
        }
      }
    },
    "memmem/krate/prebuilt/sliceslice-words/words": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/krate_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81082482.46666667,
            "upper_bound": 81154667.865
          },
          "point_estimate": 81116759.7,
          "standard_error": 18512.433594914477
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81076054.66666666,
            "upper_bound": 81154571.16666666
          },
          "point_estimate": 81118688.16666667,
          "standard_error": 17274.800271166096
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3246.1526423692703,
            "upper_bound": 102683.3915770054
          },
          "point_estimate": 50078.76771093151,
          "standard_error": 28164.191948825806
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27521.825068772963,
            "upper_bound": 82238.10234081639
          },
          "point_estimate": 61761.23044064126,
          "standard_error": 14323.240371477195
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.041615812521632,
            "upper_bound": 9.056279542362802
          },
          "point_estimate": 9.049143719792411,
          "standard_error": 0.003754878271622678
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.0400874252098,
            "upper_bound": 9.06025601236681
          },
          "point_estimate": 9.049136255804562,
          "standard_error": 0.005187595464781649
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00271407353149893,
            "upper_bound": 0.02236088825154035
          },
          "point_estimate": 0.01428591158907704,
          "standard_error": 0.004920323363054291
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.047834782886232,
            "upper_bound": 9.058581590713548
          },
          "point_estimate": 9.05295569406764,
          "standard_error": 0.002747225725339847
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007112085573800405,
            "upper_bound": 0.01556181070820765
          },
          "point_estimate": 0.012514747432272651,
          "standard_error": 0.002174991607674352
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.044566370672168,
            "upper_bound": 9.05516904263923
          },
          "point_estimate": 9.049729843017328,
          "standard_error": 0.002719356294896029
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.0431543787071,
            "upper_bound": 9.057346491800748
          },
          "point_estimate": 9.049230974720356,
          "standard_error": 0.002980404496395876
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019203128090422368,
            "upper_bound": 0.017919242497973626
          },
          "point_estimate": 0.00639833355880728,
          "standard_error": 0.004097396966930788
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.044391165542182,
            "upper_bound": 9.054494155846506
          },
          "point_estimate": 9.048852090773552,
          "standard_error": 0.0025646023807760357
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004547939112661869,
            "upper_bound": 0.011493011771803152
          },
          "point_estimate": 0.009054683366271812,
          "standard_error": 0.001777710203641281
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04814948550683,
            "upper_bound": 9.063475783199566
          },
          "point_estimate": 9.055362277664427,
          "standard_error": 0.003933481312704878
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.045786201248408,
            "upper_bound": 9.066429865539508
          },
          "point_estimate": 9.050899522312392,
          "standard_error": 0.005287010719473479
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0021255868428200174,
            "upper_bound": 0.02156018210306279
          },
          "point_estimate": 0.009510024265449347,
          "standard_error": 0.0051408658674997046
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.0471844906014,
            "upper_bound": 9.06594181157317
          },
          "point_estimate": 9.055039451223331,
          "standard_error": 0.004797143674845585
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005545160987250342,
            "upper_bound": 0.015976442759136313
          },
          "point_estimate": 0.013145170229605738,
          "standard_error": 0.0024612720822269787
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.804426233258134,
            "upper_bound": 8.825123727967247
          },
          "point_estimate": 8.81231288983565,
          "standard_error": 0.005793731198196107
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.80264185129428,
            "upper_bound": 8.812250453805989
          },
          "point_estimate": 8.805909339192002,
          "standard_error": 0.0025123115443933942
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000024420573863988447,
            "upper_bound": 0.010420741692409467
          },
          "point_estimate": 0.005273323653973341,
          "standard_error": 0.0031562458369708596
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.803979170784219,
            "upper_bound": 8.811592259278994
          },
          "point_estimate": 8.807965727426383,
          "standard_error": 0.0019244176336090577
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0020217187537849035,
            "upper_bound": 0.02955212660940923
          },
          "point_estimate": 0.019330112034456372,
          "standard_error": 0.009120693989383836
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.538058740611133,
            "upper_bound": 9.557125711648272
          },
          "point_estimate": 9.546113103692816,
          "standard_error": 0.004982070165541532
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.535164584928912,
            "upper_bound": 9.54832026928641
          },
          "point_estimate": 9.54518988925014,
          "standard_error": 0.0033738445513076796
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001007016194777934,
            "upper_bound": 0.01758930775746741
          },
          "point_estimate": 0.005382221795733386,
          "standard_error": 0.0043814554788058424
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.538488238191771,
            "upper_bound": 9.54716243091297
          },
          "point_estimate": 9.543630552799812,
          "standard_error": 0.002251585648233003
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0044300429294716745,
            "upper_bound": 0.024533097451678525
          },
          "point_estimate": 0.01653864517959449,
          "standard_error": 0.006211490069607976
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.465053684901996,
            "upper_bound": 10.479379112648964
          },
          "point_estimate": 10.47196032683586,
          "standard_error": 0.0036604035256623166
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.462174593090218,
            "upper_bound": 10.48145454161232
          },
          "point_estimate": 10.471338080353762,
          "standard_error": 0.004325098211180006
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002754010797152016,
            "upper_bound": 0.02102835239549133
          },
          "point_estimate": 0.010952729692933328,
          "standard_error": 0.004913996609873207
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.462479254309155,
            "upper_bound": 10.479729533731504
          },
          "point_estimate": 10.470361972332377,
          "standard_error": 0.004543819073759972
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006316289806992523,
            "upper_bound": 0.015523734483133834
          },
          "point_estimate": 0.012202152153878562,
          "standard_error": 0.0023666745782475007
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.127113972163238,
            "upper_bound": 8.146140716538582
          },
          "point_estimate": 8.135620242498113,
          "standard_error": 0.004903184289114367
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.123097531754386,
            "upper_bound": 8.144047336372271
          },
          "point_estimate": 8.13128536347045,
          "standard_error": 0.006203286882129766
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022451004709319825,
            "upper_bound": 0.025197671756107355
          },
          "point_estimate": 0.013986961307000294,
          "standard_error": 0.005598503316591339
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.12392897518008,
            "upper_bound": 8.139000916556947
          },
          "point_estimate": 8.130610818864014,
          "standard_error": 0.003959082606848445
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007068401109736307,
            "upper_bound": 0.022939489608326348
          },
          "point_estimate": 0.016344845212591608,
          "standard_error": 0.004717265829047391
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.886860098647862,
            "upper_bound": 9.906741823304769
          },
          "point_estimate": 9.896002582675363,
          "standard_error": 0.005137351531212569
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.884582281584338,
            "upper_bound": 9.90845756021232
          },
          "point_estimate": 9.890437772330491,
          "standard_error": 0.004992809620903682
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015241191923324072,
            "upper_bound": 0.027803634668917224
          },
          "point_estimate": 0.009661955909003296,
          "standard_error": 0.0059772624674369265
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.889634577961653,
            "upper_bound": 9.896893981774276
          },
          "point_estimate": 9.892443606810822,
          "standard_error": 0.0018545078502577369
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005585718915530729,
            "upper_bound": 0.021979054803024433
          },
          "point_estimate": 0.017123599118890883,
          "standard_error": 0.004190690645900703
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.822991892907009,
            "upper_bound": 12.837040792955758
          },
          "point_estimate": 12.830144463158408,
          "standard_error": 0.003581806276996917
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.82232388448233,
            "upper_bound": 12.838072587281513
          },
          "point_estimate": 12.830058656409248,
          "standard_error": 0.003677020392781409
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0020375279129022724,
            "upper_bound": 0.020395050313977436
          },
          "point_estimate": 0.008831991184807734,
          "standard_error": 0.004725259011181203
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.82370971230013,
            "upper_bound": 12.835995975548698
          },
          "point_estimate": 12.82997600578011,
          "standard_error": 0.0031448737592778622
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005442165669893447,
            "upper_bound": 0.015886856627552418
          },
          "point_estimate": 0.011939458415337044,
          "standard_error": 0.002641950490916042
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04381338073436,
            "upper_bound": 9.05908198585935
          },
          "point_estimate": 9.051533261474386,
          "standard_error": 0.003925793456861618
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.039797265881608,
            "upper_bound": 9.063379055710747
          },
          "point_estimate": 9.051740869963382,
          "standard_error": 0.005953381234385602
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002740226843039677,
            "upper_bound": 0.023951702985127422
          },
          "point_estimate": 0.016837465145652954,
          "standard_error": 0.005262956088124495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.048103436557938,
            "upper_bound": 9.062713553255938
          },
          "point_estimate": 9.055957596669982,
          "standard_error": 0.003859911866315535
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007797555477554736,
            "upper_bound": 0.015728428439705226
          },
          "point_estimate": 0.013110862490998096,
          "standard_error": 0.0019965857527590673
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.778600443857282,
            "upper_bound": 9.789218856501012
          },
          "point_estimate": 9.783770548841474,
          "standard_error": 0.00272405273121672
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.77665285956772,
            "upper_bound": 9.791603392140066
          },
          "point_estimate": 9.780629084263676,
          "standard_error": 0.005129304452771748
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008003276419752867,
            "upper_bound": 0.014634557845734354
          },
          "point_estimate": 0.009677759501231082,
          "standard_error": 0.0038892768652691746
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.776087937709578,
            "upper_bound": 9.789507610271656
          },
          "point_estimate": 9.782777852967277,
          "standard_error": 0.00348974398784491
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005507568751431074,
            "upper_bound": 0.011023084814054233
          },
          "point_estimate": 0.009083750834701502,
          "standard_error": 0.0014144150665559051
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.49112457064507,
            "upper_bound": 19.523095611733545
          },
          "point_estimate": 19.508770336223435,
          "standard_error": 0.008264630941102887
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.49586791820247,
            "upper_bound": 19.527905822389968
          },
          "point_estimate": 19.514521037400243,
          "standard_error": 0.006977672437164313
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002548556721835905,
            "upper_bound": 0.038670238232455335
          },
          "point_estimate": 0.012618140634419349,
          "standard_error": 0.009570529599033453
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.50794464175488,
            "upper_bound": 19.524686063121592
          },
          "point_estimate": 19.51422593293126,
          "standard_error": 0.004282254114338514
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009897048212873865,
            "upper_bound": 0.03850821903932871
          },
          "point_estimate": 0.027698379282299,
          "standard_error": 0.008115263812677463
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97827.11878189804,
            "upper_bound": 98071.33589107305
          },
          "point_estimate": 97938.7432715694,
          "standard_error": 62.999253871351215
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97793.19444444444,
            "upper_bound": 98064.98037634409
          },
          "point_estimate": 97856.24855990784,
          "standard_error": 74.21708791869484
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.332608584028144,
            "upper_bound": 323.8659932824904
          },
          "point_estimate": 139.16616952527127,
          "standard_error": 76.55034422435841
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97775.32414491152,
            "upper_bound": 98042.70277644038
          },
          "point_estimate": 97887.58084066473,
          "standard_error": 67.92080730506494
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.10119091763215,
            "upper_bound": 275.28622162738577
          },
          "point_estimate": 209.06368939562344,
          "standard_error": 51.13659399344565
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52853.993863589094,
            "upper_bound": 53064.76892597213
          },
          "point_estimate": 52944.33941602551,
          "standard_error": 55.112056330424046
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52841.06744298884,
            "upper_bound": 53013.99654294032
          },
          "point_estimate": 52873.76111457683,
          "standard_error": 40.841783806489346
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8792313462666854,
            "upper_bound": 220.0453284733243
          },
          "point_estimate": 80.22549158881115,
          "standard_error": 55.14239240723835
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52858.80703487757,
            "upper_bound": 53165.8993211479
          },
          "point_estimate": 52983.332070549535,
          "standard_error": 82.25877841740954
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.648612986431,
            "upper_bound": 267.13561144921727
          },
          "point_estimate": 185.43451427993247,
          "standard_error": 64.00823246617814
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142624.55405104268,
            "upper_bound": 142781.54184687207
          },
          "point_estimate": 142697.04351244945,
          "standard_error": 40.32804017844688
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142611.34727668847,
            "upper_bound": 142762.9381886088
          },
          "point_estimate": 142670.37549019608,
          "standard_error": 35.49175175587503
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.224182135920254,
            "upper_bound": 203.3135937597428
          },
          "point_estimate": 93.14963257827924,
          "standard_error": 44.84112762082026
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142560.05416802556,
            "upper_bound": 142713.08340459876
          },
          "point_estimate": 142630.18748153807,
          "standard_error": 40.549905622577995
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.85992013603929,
            "upper_bound": 186.47286713489777
          },
          "point_estimate": 134.18733036823343,
          "standard_error": 36.811062118429554
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392033.9931387609,
            "upper_bound": 392794.8235808158
          },
          "point_estimate": 392383.16516683734,
          "standard_error": 195.7066251731244
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391875.84408602153,
            "upper_bound": 392778.2172043011
          },
          "point_estimate": 392239.51161674346,
          "standard_error": 197.2362299076999
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.83919498994342,
            "upper_bound": 1058.1023079085564
          },
          "point_estimate": 424.829900127998,
          "standard_error": 237.2335605266123
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392029.8986909771,
            "upper_bound": 392433.4471461334
          },
          "point_estimate": 392231.8285434995,
          "standard_error": 102.67689772660388
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 259.46715246821225,
            "upper_bound": 865.7479089278116
          },
          "point_estimate": 651.7760124259815,
          "standard_error": 161.47573992134406
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 574142.5294455467,
            "upper_bound": 574596.0558766534
          },
          "point_estimate": 574388.6401631392,
          "standard_error": 116.59051868593276
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 574100.5564373898,
            "upper_bound": 574666.8544973545
          },
          "point_estimate": 574487.411111111,
          "standard_error": 128.01293211313447
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.68592441859442,
            "upper_bound": 632.5014665485864
          },
          "point_estimate": 241.35168488178047,
          "standard_error": 138.40271333820544
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 574101.9804319536,
            "upper_bound": 574617.5619197038
          },
          "point_estimate": 574363.7253349825,
          "standard_error": 132.69083862672304
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.96836033409215,
            "upper_bound": 505.5295144890681
          },
          "point_estimate": 388.09613876123063,
          "standard_error": 94.7547829920386
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41028.74068633774,
            "upper_bound": 41129.9559839971
          },
          "point_estimate": 41077.12062614211,
          "standard_error": 25.912473903074737
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41004.95963130173,
            "upper_bound": 41161.95297215952
          },
          "point_estimate": 41058.00661077072,
          "standard_error": 32.98452705512543
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.519206603015377,
            "upper_bound": 138.19527429597872
          },
          "point_estimate": 79.75686241021663,
          "standard_error": 38.56721198010397
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41040.46755442312,
            "upper_bound": 41134.00153826549
          },
          "point_estimate": 41092.91982938055,
          "standard_error": 23.732849799493017
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.40554112739569,
            "upper_bound": 108.39216515029162
          },
          "point_estimate": 86.21611858587308,
          "standard_error": 16.79125715740111
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96705.93716138268,
            "upper_bound": 96890.6165260068
          },
          "point_estimate": 96803.59262483686,
          "standard_error": 47.32326994560537
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96697.77740937224,
            "upper_bound": 96904.9193339228
          },
          "point_estimate": 96843.26991758242,
          "standard_error": 46.557809357959606
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.609831248708588,
            "upper_bound": 269.28492161182106
          },
          "point_estimate": 119.96966962077732,
          "standard_error": 66.28203376658453
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96763.3823148172,
            "upper_bound": 96897.14779442616
          },
          "point_estimate": 96825.1148644459,
          "standard_error": 34.24597308522638
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.47955419222083,
            "upper_bound": 204.7643229010124
          },
          "point_estimate": 158.50541779354282,
          "standard_error": 34.849376841145514
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72211.62228862329,
            "upper_bound": 72393.60452559209
          },
          "point_estimate": 72301.46044931386,
          "standard_error": 46.680150375978066
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72169.2900287738,
            "upper_bound": 72441.66254980079
          },
          "point_estimate": 72277.82810424967,
          "standard_error": 81.46190828875756
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.98997398385894,
            "upper_bound": 248.4732382513995
          },
          "point_estimate": 205.55082047424696,
          "standard_error": 56.62892577442865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72159.5120002595,
            "upper_bound": 72421.38472561368
          },
          "point_estimate": 72263.2682257981,
          "standard_error": 67.88076324732738
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.40496048671656,
            "upper_bound": 188.01689886188575
          },
          "point_estimate": 155.4664348366012,
          "standard_error": 23.04283068761727
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307338.360064585,
            "upper_bound": 307805.9935762207
          },
          "point_estimate": 307554.05107311,
          "standard_error": 120.1952864929134
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307264.10122410546,
            "upper_bound": 307790.69632768363
          },
          "point_estimate": 307435.89300847455,
          "standard_error": 159.05784272187742
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.15641295137325,
            "upper_bound": 644.5870379206627
          },
          "point_estimate": 299.95007772567203,
          "standard_error": 151.7533071418889
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307270.3849817072,
            "upper_bound": 307678.1664696865
          },
          "point_estimate": 307461.62969403475,
          "standard_error": 103.14547192140702
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184.53342120196825,
            "upper_bound": 536.9420294853664
          },
          "point_estimate": 400.6765182946808,
          "standard_error": 97.49708575077808
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35701.233866231254,
            "upper_bound": 35755.42833308775
          },
          "point_estimate": 35728.785170463394,
          "standard_error": 13.83982587391103
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35699.55930746562,
            "upper_bound": 35760.8586935167
          },
          "point_estimate": 35733.19002401222,
          "standard_error": 14.784024287739127
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5118423096969846,
            "upper_bound": 76.30734633682486
          },
          "point_estimate": 40.949261184424834,
          "standard_error": 18.799480029401035
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35717.538524892545,
            "upper_bound": 35765.23943961475
          },
          "point_estimate": 35743.47993519251,
          "standard_error": 12.36476649493013
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.33800263857684,
            "upper_bound": 62.5143739908752
          },
          "point_estimate": 46.20401404444934,
          "standard_error": 10.488408402757264
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63740.422228627125,
            "upper_bound": 63809.366284663054
          },
          "point_estimate": 63774.113784182686,
          "standard_error": 17.709053834867213
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63737.97184210526,
            "upper_bound": 63829.57614035088
          },
          "point_estimate": 63761.48912559176,
          "standard_error": 22.012428713761807
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.758568283310876,
            "upper_bound": 107.09832575360052
          },
          "point_estimate": 45.25259267029177,
          "standard_error": 24.887188394276116
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63733.83162844415,
            "upper_bound": 63788.876202159794
          },
          "point_estimate": 63755.97745272271,
          "standard_error": 13.8876678721559
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.838599296529484,
            "upper_bound": 76.08931023787078
          },
          "point_estimate": 58.99593896770363,
          "standard_error": 11.553103162765549
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168339.01305813954,
            "upper_bound": 168903.96405317
          },
          "point_estimate": 168577.2150791805,
          "standard_error": 146.890987865334
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168342.53215946845,
            "upper_bound": 168645.3550387597
          },
          "point_estimate": 168485.69565891474,
          "standard_error": 81.55725158414931
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.60612343316876,
            "upper_bound": 490.13745385645416
          },
          "point_estimate": 188.66213961571052,
          "standard_error": 108.25253496015038
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168347.38903433218,
            "upper_bound": 168608.29854542288
          },
          "point_estimate": 168443.42327997583,
          "standard_error": 67.59728910008343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.29249550343752,
            "upper_bound": 727.2504903948418
          },
          "point_estimate": 491.0110519068948,
          "standard_error": 186.82019697259645
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34183.000764577635,
            "upper_bound": 34229.71843890457
          },
          "point_estimate": 34205.65766526303,
          "standard_error": 11.941904296958969
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34167.966839134526,
            "upper_bound": 34233.02391031671
          },
          "point_estimate": 34212.20526810912,
          "standard_error": 16.496088758623866
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6438636877817223,
            "upper_bound": 71.20489727771724
          },
          "point_estimate": 48.90187947989072,
          "standard_error": 18.014605204275774
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34179.49624421334,
            "upper_bound": 34215.33027279347
          },
          "point_estimate": 34199.03472651525,
          "standard_error": 9.1201510805416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.248794974040013,
            "upper_bound": 50.937358743227
          },
          "point_estimate": 39.76118807940946,
          "standard_error": 7.578266928465452
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11169.450859316095,
            "upper_bound": 11217.857839360991
          },
          "point_estimate": 11190.858866097744,
          "standard_error": 12.504117369455754
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11160.39152786346,
            "upper_bound": 11210.284906436356
          },
          "point_estimate": 11179.679727020355,
          "standard_error": 11.662563436253471
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.959921814815346,
            "upper_bound": 55.10294841716151
          },
          "point_estimate": 28.44210887197852,
          "standard_error": 13.051116312636404
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11161.058856632892,
            "upper_bound": 11183.453884683731
          },
          "point_estimate": 11172.208293742038,
          "standard_error": 5.855382898017572
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.3093400330787,
            "upper_bound": 57.72541923615976
          },
          "point_estimate": 41.44734190290991,
          "standard_error": 12.456086311462666
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500438.0637867063,
            "upper_bound": 500935.0231668297
          },
          "point_estimate": 500685.330470211,
          "standard_error": 127.52659416733242
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500326.4834474886,
            "upper_bound": 501104.5095890411
          },
          "point_estimate": 500657.25587084144,
          "standard_error": 176.50766056241383
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.5032685683566,
            "upper_bound": 755.1150203839976
          },
          "point_estimate": 470.438110963132,
          "standard_error": 177.30479600781527
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500661.5181671587,
            "upper_bound": 501153.30009287206
          },
          "point_estimate": 500966.2447607187,
          "standard_error": 126.54950018268656
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 251.098832564354,
            "upper_bound": 525.3407129123445
          },
          "point_estimate": 424.8431028629415,
          "standard_error": 70.5476036660547
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1039.1200967568095,
            "upper_bound": 1040.5978787100316
          },
          "point_estimate": 1039.8754779868052,
          "standard_error": 0.3787691207269156
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1038.436692672611,
            "upper_bound": 1040.9886790450062
          },
          "point_estimate": 1040.3408171867854,
          "standard_error": 0.7167987095846513
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.26175460997922795,
            "upper_bound": 1.958925254574012
          },
          "point_estimate": 1.345539342967402,
          "standard_error": 0.4944248773000719
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1039.4535739754313,
            "upper_bound": 1040.6759896650044
          },
          "point_estimate": 1040.2342512425846,
          "standard_error": 0.3140576311095537
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.808360568879249,
            "upper_bound": 1.4534942923988097
          },
          "point_estimate": 1.264825295344112,
          "standard_error": 0.16756623137528265
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40945.82354888065,
            "upper_bound": 41001.981132983485
          },
          "point_estimate": 40971.724134634314,
          "standard_error": 14.36675637752044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40937.44846191013,
            "upper_bound": 41005.30622886133
          },
          "point_estimate": 40960.674370537396,
          "standard_error": 16.511687522716407
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.178435949217728,
            "upper_bound": 76.39726418295047
          },
          "point_estimate": 40.684861058990386,
          "standard_error": 17.246346706949133
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40957.33290816712,
            "upper_bound": 41003.12120893886
          },
          "point_estimate": 40983.23504004451,
          "standard_error": 11.669961852691983
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.145276866787334,
            "upper_bound": 64.5389730703159
          },
          "point_estimate": 47.90382513959669,
          "standard_error": 11.875278541591458
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48572.496131350264,
            "upper_bound": 48654.4749279189
          },
          "point_estimate": 48613.10614936126,
          "standard_error": 21.069546107005117
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48538.47281639928,
            "upper_bound": 48683.74104278075
          },
          "point_estimate": 48598.83096590909,
          "standard_error": 39.88281499176853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.548690140041056,
            "upper_bound": 104.22548979401338
          },
          "point_estimate": 97.0356810145651,
          "standard_error": 28.559235705887193
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48576.44995943036,
            "upper_bound": 48671.08208388859
          },
          "point_estimate": 48637.39573928745,
          "standard_error": 23.83073436597493
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.859320365767566,
            "upper_bound": 80.78481321235708
          },
          "point_estimate": 70.13213872041027,
          "standard_error": 8.948421111014891
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48200.15158415807,
            "upper_bound": 48272.750153354835
          },
          "point_estimate": 48236.071011773005,
          "standard_error": 18.542881504839695
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48191.454404603806,
            "upper_bound": 48297.78618857902
          },
          "point_estimate": 48214.91985391766,
          "standard_error": 33.03906175053249
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3181455742084416,
            "upper_bound": 108.47230391752666
          },
          "point_estimate": 77.77583113912215,
          "standard_error": 28.05621815669681
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48198.76549961769,
            "upper_bound": 48286.85435590919
          },
          "point_estimate": 48246.97019023473,
          "standard_error": 22.305222428665452
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.086107535726214,
            "upper_bound": 75.49217084292272
          },
          "point_estimate": 61.910714785096125,
          "standard_error": 9.465638999127112
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14307.595980701064,
            "upper_bound": 14328.055995333902
          },
          "point_estimate": 14317.689494379738,
          "standard_error": 5.228050591985418
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14301.009807010634,
            "upper_bound": 14331.379783707496
          },
          "point_estimate": 14321.336276311758,
          "standard_error": 8.717107400288752
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.050530944299391,
            "upper_bound": 32.18655379512348
          },
          "point_estimate": 25.680151710292996,
          "standard_error": 8.54321073503781
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14302.122139419898,
            "upper_bound": 14322.528840909248
          },
          "point_estimate": 14311.945321555168,
          "standard_error": 5.271864325960853
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.882820350003811,
            "upper_bound": 21.336409898044323
          },
          "point_estimate": 17.399489011585178,
          "standard_error": 2.7678543560386384
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25648.932320169253,
            "upper_bound": 25679.79825087313
          },
          "point_estimate": 25663.821629390823,
          "standard_error": 7.8877748479925325
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25641.537094499297,
            "upper_bound": 25678.833921015514
          },
          "point_estimate": 25669.24047954866,
          "standard_error": 10.75896979081387
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6544454394761905,
            "upper_bound": 47.83276533292105
          },
          "point_estimate": 23.559886746749036,
          "standard_error": 11.579815485519225
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25652.65455411051,
            "upper_bound": 25682.731984138492
          },
          "point_estimate": 25667.078436795928,
          "standard_error": 7.954817013065178
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.132022904681852,
            "upper_bound": 34.927238801179314
          },
          "point_estimate": 26.28099852430084,
          "standard_error": 5.662902531876265
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17372.144492187268,
            "upper_bound": 17436.390011324318
          },
          "point_estimate": 17396.142371638743,
          "standard_error": 18.217914341871776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17368.276250796684,
            "upper_bound": 17389.707479741417
          },
          "point_estimate": 17380.434386950285,
          "standard_error": 7.596286421221696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7767822233053172,
            "upper_bound": 27.706953196756388
          },
          "point_estimate": 17.63726122965409,
          "standard_error": 7.554703292210162
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17371.68630502793,
            "upper_bound": 17388.10259925513
          },
          "point_estimate": 17379.4280934171,
          "standard_error": 4.276748223975337
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.1915665303713,
            "upper_bound": 93.70210977299055
          },
          "point_estimate": 61.0086461455895,
          "standard_error": 29.915194976724937
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17573.14236204177,
            "upper_bound": 17593.392520654877
          },
          "point_estimate": 17583.57955305333,
          "standard_error": 5.184554949511663
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17567.048194713087,
            "upper_bound": 17597.016150870408
          },
          "point_estimate": 17587.136208206684,
          "standard_error": 6.952213673729077
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.0901240505557297,
            "upper_bound": 30.00411696731936
          },
          "point_estimate": 16.05637711823529,
          "standard_error": 6.820118852517118
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17570.513228187967,
            "upper_bound": 17601.358767398637
          },
          "point_estimate": 17588.790827451077,
          "standard_error": 7.961444126381415
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.2989137575254,
            "upper_bound": 21.80903174957571
          },
          "point_estimate": 17.30170613216727,
          "standard_error": 3.1569957009119856
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17585.87602801618,
            "upper_bound": 17611.083864111337
          },
          "point_estimate": 17598.10206726037,
          "standard_error": 6.375983715808389
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17590.73261870155,
            "upper_bound": 17609.139950166114
          },
          "point_estimate": 17594.542332848836,
          "standard_error": 4.616129712461406
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5509467925414774,
            "upper_bound": 32.33086152630191
          },
          "point_estimate": 6.609026990223306,
          "standard_error": 7.940080713455285
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17591.61863951125,
            "upper_bound": 17605.37130398671
          },
          "point_estimate": 17598.167060555723,
          "standard_error": 3.5985362170571804
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.397380062220243,
            "upper_bound": 29.682634862960892
          },
          "point_estimate": 21.196397555924456,
          "standard_error": 6.126472070194166
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95172.8912380563,
            "upper_bound": 95343.577093202
          },
          "point_estimate": 95254.0107110654,
          "standard_error": 43.67445777011861
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95140.74461896453,
            "upper_bound": 95366.35688575168
          },
          "point_estimate": 95211.5992582897,
          "standard_error": 47.61247533761252
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.297644668538616,
            "upper_bound": 259.11318836447754
          },
          "point_estimate": 122.70431100873792,
          "standard_error": 68.10832442843325
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95135.32280645128,
            "upper_bound": 95321.54084015435
          },
          "point_estimate": 95215.5839328211,
          "standard_error": 47.08802399440989
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.20962414282786,
            "upper_bound": 183.5547964787432
          },
          "point_estimate": 145.21535248377393,
          "standard_error": 29.535503548010293
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101649.59191317506,
            "upper_bound": 102044.79817504654
          },
          "point_estimate": 101807.7101217079,
          "standard_error": 106.91085048862602
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101617.95100093108,
            "upper_bound": 101890.32588454377
          },
          "point_estimate": 101689.7503990423,
          "standard_error": 73.30082434687763
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.528332748893146,
            "upper_bound": 305.0757285503102
          },
          "point_estimate": 117.56983773115432,
          "standard_error": 74.6947058010811
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101635.48574928942,
            "upper_bound": 101769.2906034526
          },
          "point_estimate": 101696.9548211565,
          "standard_error": 33.834547577700924
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.97684964849157,
            "upper_bound": 536.9639653032976
          },
          "point_estimate": 355.9567760047104,
          "standard_error": 148.9206311985969
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18540.593699429104,
            "upper_bound": 18568.96124261531
          },
          "point_estimate": 18552.816227277897,
          "standard_error": 7.367979077532779
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18537.386937627813,
            "upper_bound": 18560.420309304704
          },
          "point_estimate": 18546.51183253806,
          "standard_error": 5.84552475393988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.46482245624389,
            "upper_bound": 28.91251862780204
          },
          "point_estimate": 15.95372613376941,
          "standard_error": 6.835593094181022
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18540.95455009691,
            "upper_bound": 18554.00013618382
          },
          "point_estimate": 18547.064468966615,
          "standard_error": 3.2698887998263455
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.150707729084342,
            "upper_bound": 35.66335680322222
          },
          "point_estimate": 24.556302941883075,
          "standard_error": 8.383713193617101
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17728.314706168036,
            "upper_bound": 17751.877754725865
          },
          "point_estimate": 17740.34053641183,
          "standard_error": 6.0407292830219035
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17723.289465332033,
            "upper_bound": 17758.34228515625
          },
          "point_estimate": 17742.251098632812,
          "standard_error": 9.50343537622769
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.283711865981416,
            "upper_bound": 33.618944630194434
          },
          "point_estimate": 24.932727389386855,
          "standard_error": 7.019372857040622
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17733.793140734626,
            "upper_bound": 17757.21783598699
          },
          "point_estimate": 17745.81794718953,
          "standard_error": 6.098261176214704
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.209865982489466,
            "upper_bound": 24.33514475609334
          },
          "point_estimate": 20.098350038830876,
          "standard_error": 3.091686540460779
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17385.49258933705,
            "upper_bound": 17415.343021539236
          },
          "point_estimate": 17399.50043546216,
          "standard_error": 7.605599386699794
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17384.687110685194,
            "upper_bound": 17411.113300590958
          },
          "point_estimate": 17396.56508544961,
          "standard_error": 6.08201368002187
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.9692760387795,
            "upper_bound": 38.6903892508198
          },
          "point_estimate": 14.91423112956569,
          "standard_error": 8.49503598140284
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17392.082883146286,
            "upper_bound": 17405.039837159326
          },
          "point_estimate": 17396.951826707074,
          "standard_error": 3.3073313310699453
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.757295577961553,
            "upper_bound": 35.55259797835183
          },
          "point_estimate": 25.190282979605495,
          "standard_error": 7.039553082702446
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17643.45217244568,
            "upper_bound": 17667.117389821717
          },
          "point_estimate": 17654.83197347434,
          "standard_error": 6.057565119577436
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17640.657443365697,
            "upper_bound": 17668.448915857603
          },
          "point_estimate": 17654.70821197411,
          "standard_error": 7.071354346359511
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.445540259979811,
            "upper_bound": 34.613872343006065
          },
          "point_estimate": 18.54044217083989,
          "standard_error": 8.220397338104462
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17635.927744416927,
            "upper_bound": 17656.779686110367
          },
          "point_estimate": 17643.73239566259,
          "standard_error": 5.25713526840225
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.445371891327865,
            "upper_bound": 26.53652123114167
          },
          "point_estimate": 20.204975326200913,
          "standard_error": 4.355102817548861
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17407.933428093944,
            "upper_bound": 17428.940932140194
          },
          "point_estimate": 17418.22848076367,
          "standard_error": 5.396410230517318
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17402.535885495137,
            "upper_bound": 17436.68079258549
          },
          "point_estimate": 17411.57778709918,
          "standard_error": 10.143134341872557
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.7424909290841377,
            "upper_bound": 29.140623308152357
          },
          "point_estimate": 19.23926257390539,
          "standard_error": 6.8799071433939005
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17406.400888613065,
            "upper_bound": 17427.959007252368
          },
          "point_estimate": 17417.141963118378,
          "standard_error": 5.698474184356244
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.959354061298052,
            "upper_bound": 20.847597536414657
          },
          "point_estimate": 18.03879019318962,
          "standard_error": 2.464988550515385
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15412.35871701547,
            "upper_bound": 15432.556546355272
          },
          "point_estimate": 15421.811061166716,
          "standard_error": 5.1696735309229895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15409.25219896876,
            "upper_bound": 15432.068365180468
          },
          "point_estimate": 15419.592392073602,
          "standard_error": 5.546134399423807
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.3509592398729473,
            "upper_bound": 26.984833448736527
          },
          "point_estimate": 16.476470318949122,
          "standard_error": 6.260199336656803
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15409.5469676904,
            "upper_bound": 15426.25634010238
          },
          "point_estimate": 15416.602767253977,
          "standard_error": 4.255326945335369
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.956475526049406,
            "upper_bound": 22.97785669488935
          },
          "point_estimate": 17.145319437281568,
          "standard_error": 4.094573855076995
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16369.25781167142,
            "upper_bound": 16395.328716134052
          },
          "point_estimate": 16382.668229008095,
          "standard_error": 6.679307787331746
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16362.457406573616,
            "upper_bound": 16399.70456250938
          },
          "point_estimate": 16390.16260371776,
          "standard_error": 10.390476169642833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8599809665315192,
            "upper_bound": 35.92855909311797
          },
          "point_estimate": 23.74821762205848,
          "standard_error": 8.790866681558063
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16365.457501212444,
            "upper_bound": 16401.328863227718
          },
          "point_estimate": 16381.44465053182,
          "standard_error": 9.41678806836375
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.758884732169408,
            "upper_bound": 27.17656388793648
          },
          "point_estimate": 22.22306080863496,
          "standard_error": 3.64125898269431
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17732.596492978053,
            "upper_bound": 17769.863454591297
          },
          "point_estimate": 17748.64857648577,
          "standard_error": 9.668022049378235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17726.90909016927,
            "upper_bound": 17760.9862874349
          },
          "point_estimate": 17741.395403180803,
          "standard_error": 9.324602657798478
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.435013919835191,
            "upper_bound": 38.530397460480465
          },
          "point_estimate": 20.75990032967928,
          "standard_error": 9.741240439066573
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17729.985519040823,
            "upper_bound": 17748.867295631488
          },
          "point_estimate": 17739.75842887581,
          "standard_error": 4.90791753902628
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.647625613602262,
            "upper_bound": 46.250040950514965
          },
          "point_estimate": 32.11697104435525,
          "standard_error": 10.668497878138512
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18182.293358429215,
            "upper_bound": 18210.531095071343
          },
          "point_estimate": 18195.02082962116,
          "standard_error": 7.272158648122008
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18174.864182091045,
            "upper_bound": 18205.91095547774
          },
          "point_estimate": 18191.901050525263,
          "standard_error": 8.065905632056175
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.321036016333219,
            "upper_bound": 35.6217079218667
          },
          "point_estimate": 20.92502673372631,
          "standard_error": 7.961893736971641
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18180.994817408704,
            "upper_bound": 18199.155309675203
          },
          "point_estimate": 18189.88711238736,
          "standard_error": 4.6184975134660515
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.211445841787388,
            "upper_bound": 33.49378754187047
          },
          "point_estimate": 24.244746975078137,
          "standard_error": 6.716273421079854
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58191.3479208,
            "upper_bound": 58389.58643644443
          },
          "point_estimate": 58290.39224203173,
          "standard_error": 50.87923311774228
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58163.69872,
            "upper_bound": 58473.3564
          },
          "point_estimate": 58275.17933333333,
          "standard_error": 81.33232726687152
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.783327675668303,
            "upper_bound": 296.4049120310985
          },
          "point_estimate": 182.9818166394252,
          "standard_error": 66.87616274371945
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58189.439389403,
            "upper_bound": 58449.41427757042
          },
          "point_estimate": 58348.17871792208,
          "standard_error": 66.15378967559636
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.12368553042742,
            "upper_bound": 206.27440642746953
          },
          "point_estimate": 168.53293548278333,
          "standard_error": 26.618366991664505
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881269.8715970569,
            "upper_bound": 883115.3028306877
          },
          "point_estimate": 882180.5777314814,
          "standard_error": 473.2495479727229
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881102.623015873,
            "upper_bound": 883567.1428571428
          },
          "point_estimate": 881828.3947089948,
          "standard_error": 636.9966768369111
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.03612755263805,
            "upper_bound": 2797.424933669165
          },
          "point_estimate": 1397.0392468642583,
          "standard_error": 658.9536361556388
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881334.2172138074,
            "upper_bound": 882557.6228030972
          },
          "point_estimate": 881906.9175015461,
          "standard_error": 306.5819820261141
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 870.4016839446682,
            "upper_bound": 1990.356076619535
          },
          "point_estimate": 1576.205717010413,
          "standard_error": 283.9635397862197
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205509.64488915232,
            "upper_bound": 205901.8145965496
          },
          "point_estimate": 205694.31184669532,
          "standard_error": 100.05587449392718
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205495.2254237288,
            "upper_bound": 205852.53080441215
          },
          "point_estimate": 205684.05626177025,
          "standard_error": 89.38088965798222
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.484541437299725,
            "upper_bound": 525.3137397416617
          },
          "point_estimate": 219.598150013798,
          "standard_error": 117.72112817897256
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205524.4573662482,
            "upper_bound": 205814.35613252973
          },
          "point_estimate": 205651.98908210432,
          "standard_error": 75.15238882851357
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.0471002749224,
            "upper_bound": 463.0109721903672
          },
          "point_estimate": 332.90697368076604,
          "standard_error": 88.93556649941969
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5514.444380512381,
            "upper_bound": 5532.853746629715
          },
          "point_estimate": 5523.202592362566,
          "standard_error": 4.695291456039401
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5516.494499470739,
            "upper_bound": 5531.737810805556
          },
          "point_estimate": 5520.783872675034,
          "standard_error": 3.015028641334688
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1813320320536242,
            "upper_bound": 26.190477691687015
          },
          "point_estimate": 5.156106845932349,
          "standard_error": 6.411132809255338
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5516.958451042057,
            "upper_bound": 5521.9163031269645
          },
          "point_estimate": 5519.714925540209,
          "standard_error": 1.251891745280567
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.344816408890406,
            "upper_bound": 21.829885545417245
          },
          "point_estimate": 15.706876851535757,
          "standard_error": 4.285542181725171
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5530.368593008787,
            "upper_bound": 5542.969375965913
          },
          "point_estimate": 5536.679733172257,
          "standard_error": 3.2335065454530243
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5525.729346335517,
            "upper_bound": 5546.91356849002
          },
          "point_estimate": 5538.778759713546,
          "standard_error": 5.754002288291121
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4051869449572516,
            "upper_bound": 17.415813800208714
          },
          "point_estimate": 14.801734012700372,
          "standard_error": 4.270750470436314
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5528.931463276351,
            "upper_bound": 5546.341989789974
          },
          "point_estimate": 5536.380369683646,
          "standard_error": 4.473828675384477
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.958967415479692,
            "upper_bound": 12.61487929282031
          },
          "point_estimate": 10.742224452511133,
          "standard_error": 1.4605222947587686
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14621.461712434791,
            "upper_bound": 14655.505693411578
          },
          "point_estimate": 14639.315382216464,
          "standard_error": 8.655701630126028
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14627.88508064516,
            "upper_bound": 14654.9466890681
          },
          "point_estimate": 14643.021144153226,
          "standard_error": 6.9000924704072935
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.195412641521691,
            "upper_bound": 44.28377994229927
          },
          "point_estimate": 18.143190140597017,
          "standard_error": 10.068723288783753
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14636.584287450289,
            "upper_bound": 14653.114686880526
          },
          "point_estimate": 14643.581870548804,
          "standard_error": 4.2306548264532795
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.171317151548209,
            "upper_bound": 40.62641353891533
          },
          "point_estimate": 28.78252070857712,
          "standard_error": 7.862107420452539
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.53818452126371,
            "upper_bound": 59.63199617597212
          },
          "point_estimate": 59.58423118578643,
          "standard_error": 0.023968212230725188
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.52038673741917,
            "upper_bound": 59.6303750874463
          },
          "point_estimate": 59.59243119982507,
          "standard_error": 0.029220267953760013
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01820455647386124,
            "upper_bound": 0.1508705646927644
          },
          "point_estimate": 0.06430427713485078,
          "standard_error": 0.033022623179039125
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.557530308478825,
            "upper_bound": 59.61312650587491
          },
          "point_estimate": 59.589563099191345,
          "standard_error": 0.01407255853838557
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04319021448437792,
            "upper_bound": 0.1052494703669412
          },
          "point_estimate": 0.08007526161608355,
          "standard_error": 0.01649475584292092
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.23310103528954,
            "upper_bound": 35.279764703317376
          },
          "point_estimate": 35.255444550288516,
          "standard_error": 0.01194720428983369
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.222585733696654,
            "upper_bound": 35.295612772780345
          },
          "point_estimate": 35.24265291847979,
          "standard_error": 0.019795982482120143
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005511344672558985,
            "upper_bound": 0.06499925342397801
          },
          "point_estimate": 0.03642032046214982,
          "standard_error": 0.016042106115990978
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.23004607180633,
            "upper_bound": 35.27959959220812
          },
          "point_estimate": 35.24897060097693,
          "standard_error": 0.01261872177843893
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022061979069019647,
            "upper_bound": 0.04850361269589117
          },
          "point_estimate": 0.039868933427584984,
          "standard_error": 0.006713170961565273
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.876488338698785,
            "upper_bound": 39.01593420553239
          },
          "point_estimate": 38.931649061208894,
          "standard_error": 0.03780983016575192
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.86721193807387,
            "upper_bound": 38.9285620676025
          },
          "point_estimate": 38.90813215554226,
          "standard_error": 0.01747008031258581
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009645446069669304,
            "upper_bound": 0.08603132491174267
          },
          "point_estimate": 0.0379954783341507,
          "standard_error": 0.021327263609905694
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.88337997112536,
            "upper_bound": 38.92203690993136
          },
          "point_estimate": 38.90342030039486,
          "standard_error": 0.009780510939310576
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02294746785915088,
            "upper_bound": 0.1918894121408804
          },
          "point_estimate": 0.12611043233028243,
          "standard_error": 0.05589685669290025
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.29813427240165,
            "upper_bound": 31.322896645827463
          },
          "point_estimate": 31.31121951392863,
          "standard_error": 0.006373053223155457
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.2943502930265,
            "upper_bound": 31.326716477779627
          },
          "point_estimate": 31.31726306577068,
          "standard_error": 0.00697990470877651
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003215874044406013,
            "upper_bound": 0.036593836322251314
          },
          "point_estimate": 0.016916529362457182,
          "standard_error": 0.008445448252820993
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.308932696008313,
            "upper_bound": 31.32654555045615
          },
          "point_estimate": 31.317280423081364,
          "standard_error": 0.004439827800395115
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008960700169764017,
            "upper_bound": 0.02691588869674449
          },
          "point_estimate": 0.021147430148777503,
          "standard_error": 0.0045435162831782665
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.943601133590366,
            "upper_bound": 24.993251989003028
          },
          "point_estimate": 24.96681386201497,
          "standard_error": 0.012781157752556664
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.935347015194115,
            "upper_bound": 24.999838790124937
          },
          "point_estimate": 24.94925336755126,
          "standard_error": 0.017695083582664396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0060759595530902885,
            "upper_bound": 0.07045149382779371
          },
          "point_estimate": 0.03900592378163468,
          "standard_error": 0.0175656897240915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.940837146633474,
            "upper_bound": 24.971641247383317
          },
          "point_estimate": 24.951818214249847,
          "standard_error": 0.007871436535580282
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020621199115378207,
            "upper_bound": 0.05457961192552942
          },
          "point_estimate": 0.04257690523907625,
          "standard_error": 0.008877289397588072
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.611983906452124,
            "upper_bound": 34.6622508851304
          },
          "point_estimate": 34.63864435335471,
          "standard_error": 0.012940276625772611
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.602000885063184,
            "upper_bound": 34.66430969104862
          },
          "point_estimate": 34.64807367402071,
          "standard_error": 0.012631042270624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0070122903493864104,
            "upper_bound": 0.07190955744806998
          },
          "point_estimate": 0.02207279758410658,
          "standard_error": 0.016930055198669268
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.63301849412966,
            "upper_bound": 34.664984305040825
          },
          "point_estimate": 34.649716986569494,
          "standard_error": 0.008103138290948782
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015970671109685055,
            "upper_bound": 0.05532816809598459
          },
          "point_estimate": 0.04314623730115368,
          "standard_error": 0.009788397919366204
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.62811478198384,
            "upper_bound": 48.806283239489744
          },
          "point_estimate": 48.72270224592232,
          "standard_error": 0.045773152434726426
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.61043641710767,
            "upper_bound": 48.8145284785126
          },
          "point_estimate": 48.75167145461332,
          "standard_error": 0.04280949978685752
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02807668340700489,
            "upper_bound": 0.2582286610838465
          },
          "point_estimate": 0.08611048297931101,
          "standard_error": 0.060262288963371186
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.59389955104831,
            "upper_bound": 48.79770041547308
          },
          "point_estimate": 48.70493374870749,
          "standard_error": 0.053230398693134416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05774354665976332,
            "upper_bound": 0.19812705958056193
          },
          "point_estimate": 0.152564580546996,
          "standard_error": 0.0353746909261995
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.2289726484786,
            "upper_bound": 57.32018889984647
          },
          "point_estimate": 57.27694272877507,
          "standard_error": 0.02341444355106768
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.21343149351367,
            "upper_bound": 57.33497499638233
          },
          "point_estimate": 57.30580271349997,
          "standard_error": 0.03254837529957938
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013245227630685722,
            "upper_bound": 0.12510427071306118
          },
          "point_estimate": 0.055853425502923465,
          "standard_error": 0.030872381137942044
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.24037833360011,
            "upper_bound": 57.30912651560459
          },
          "point_estimate": 57.28144208763968,
          "standard_error": 0.01746010384462989
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03737481234881267,
            "upper_bound": 0.09833103316909242
          },
          "point_estimate": 0.07810961594003511,
          "standard_error": 0.015462247373901556
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.438231835885446,
            "upper_bound": 42.52307661958362
          },
          "point_estimate": 42.47747217613926,
          "standard_error": 0.021841980118792535
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.424617567176846,
            "upper_bound": 42.511326338031054
          },
          "point_estimate": 42.466051592531485,
          "standard_error": 0.02536822056737102
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01870336423204133,
            "upper_bound": 0.11221723722537244
          },
          "point_estimate": 0.06421646943809026,
          "standard_error": 0.023113179196851727
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.41687711553936,
            "upper_bound": 42.48500127726569
          },
          "point_estimate": 42.44408104642151,
          "standard_error": 0.017471109200367988
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03327733135838129,
            "upper_bound": 0.09948913455581145
          },
          "point_estimate": 0.07277961409602038,
          "standard_error": 0.018979568559863283
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.03507552593203,
            "upper_bound": 66.12687865483711
          },
          "point_estimate": 66.07260698361112,
          "standard_error": 0.0244754779981019
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.0264186080479,
            "upper_bound": 66.08353264441686
          },
          "point_estimate": 66.04748482598569,
          "standard_error": 0.016832206756416196
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006274903793263258,
            "upper_bound": 0.07152013693958213
          },
          "point_estimate": 0.04155107422763179,
          "standard_error": 0.01683716332489919
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.04419802736629,
            "upper_bound": 66.20737542694317
          },
          "point_estimate": 66.1164449367911,
          "standard_error": 0.04860547194445465
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02071553076649808,
            "upper_bound": 0.12232286230716598
          },
          "point_estimate": 0.08169860103801929,
          "standard_error": 0.033185286296555525
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.88220804206581,
            "upper_bound": 46.986931521205776
          },
          "point_estimate": 46.931738557035224,
          "standard_error": 0.02690989874965203
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.864593337053655,
            "upper_bound": 47.00065459692408
          },
          "point_estimate": 46.91338787648175,
          "standard_error": 0.03071827821787279
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021399944745778197,
            "upper_bound": 0.154999621509987
          },
          "point_estimate": 0.07045202104759357,
          "standard_error": 0.03435646017917611
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.867473312142856,
            "upper_bound": 46.939988347561936
          },
          "point_estimate": 46.90003366855503,
          "standard_error": 0.01908706608877046
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04104716424630123,
            "upper_bound": 0.11389056598658234
          },
          "point_estimate": 0.0896182444128981,
          "standard_error": 0.018538613665718427
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.70825730723447,
            "upper_bound": 37.78513187660312
          },
          "point_estimate": 37.74392670962995,
          "standard_error": 0.019766141453011473
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.69542518868694,
            "upper_bound": 37.79427379636865
          },
          "point_estimate": 37.722590976367805,
          "standard_error": 0.02280986385336459
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005110534063408536,
            "upper_bound": 0.10726300477250432
          },
          "point_estimate": 0.05964906400485603,
          "standard_error": 0.024923715662824756
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.70650329096717,
            "upper_bound": 37.79186372813344
          },
          "point_estimate": 37.74284097760437,
          "standard_error": 0.021931051491404407
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028203678290090715,
            "upper_bound": 0.0846303023879025
          },
          "point_estimate": 0.06566640970670044,
          "standard_error": 0.01463962194044294
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.4413031846342,
            "upper_bound": 65.60037876556241
          },
          "point_estimate": 65.5223615635881,
          "standard_error": 0.0408348452637996
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.41604025716913,
            "upper_bound": 65.63135931209163
          },
          "point_estimate": 65.52352383220025,
          "standard_error": 0.054534314080456885
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02473651490818845,
            "upper_bound": 0.2470033297829448
          },
          "point_estimate": 0.15131759629755284,
          "standard_error": 0.05376436946753166
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.52495798318604,
            "upper_bound": 65.63367010136979
          },
          "point_estimate": 65.58838055806173,
          "standard_error": 0.02708732402325113
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0767962669314362,
            "upper_bound": 0.1680453842585378
          },
          "point_estimate": 0.13590929956944212,
          "standard_error": 0.02315107244436599
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154063.58394304788,
            "upper_bound": 154211.2995281951
          },
          "point_estimate": 154138.21042526737,
          "standard_error": 37.71498556822347
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154061.641416309,
            "upper_bound": 154183.21649976156
          },
          "point_estimate": 154166.29521765787,
          "standard_error": 30.848861104366396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.523701225393477,
            "upper_bound": 211.349504616906
          },
          "point_estimate": 36.59047190404097,
          "standard_error": 58.93443728274239
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154121.35210467913,
            "upper_bound": 154172.8842632332
          },
          "point_estimate": 154156.14948999498,
          "standard_error": 13.434371689112028
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.84426257655191,
            "upper_bound": 172.12013856786405
          },
          "point_estimate": 125.53495977389976,
          "standard_error": 31.043219519334222
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53736.039275398,
            "upper_bound": 53888.07176480915
          },
          "point_estimate": 53809.014634123465,
          "standard_error": 38.90579239787126
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53691.180206794685,
            "upper_bound": 53904.5141802068
          },
          "point_estimate": 53820.18159011512,
          "standard_error": 58.24628261797005
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.652218016207796,
            "upper_bound": 223.55740665883823
          },
          "point_estimate": 182.4053730418144,
          "standard_error": 61.357658961349806
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53721.93287409712,
            "upper_bound": 53884.66422350125
          },
          "point_estimate": 53788.42214506321,
          "standard_error": 41.865294462439
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.19838770276385,
            "upper_bound": 162.5857354685333
          },
          "point_estimate": 128.7122778859699,
          "standard_error": 23.952635283948116
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 242075.12825982744,
            "upper_bound": 242389.23724700385
          },
          "point_estimate": 242234.92167705335,
          "standard_error": 80.40222618323804
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241982.33976510068,
            "upper_bound": 242467.077852349
          },
          "point_estimate": 242283.23434004473,
          "standard_error": 140.6240450004911
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.94360545405407,
            "upper_bound": 441.2170612943417
          },
          "point_estimate": 345.2025437036491,
          "standard_error": 94.93927550927212
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 242123.22716256525,
            "upper_bound": 242490.7535198859
          },
          "point_estimate": 242336.2104070426,
          "standard_error": 94.97973724697935
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170.5117195824142,
            "upper_bound": 316.4974963274233
          },
          "point_estimate": 267.6305187163189,
          "standard_error": 37.44402528439228
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 810857.1117772489,
            "upper_bound": 812193.8360361111
          },
          "point_estimate": 811509.6853492064,
          "standard_error": 340.7881937463874
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 810571.424074074,
            "upper_bound": 812184.9674074075
          },
          "point_estimate": 811509.8118518519,
          "standard_error": 345.0956154218689
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.64540358383107,
            "upper_bound": 2036.532270510978
          },
          "point_estimate": 792.986834255003,
          "standard_error": 508.46535530740994
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 811468.2420775534,
            "upper_bound": 812789.1665800866
          },
          "point_estimate": 812157.887041847,
          "standard_error": 337.7194597631977
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 567.1657638086539,
            "upper_bound": 1483.181566784383
          },
          "point_estimate": 1134.706640438248,
          "standard_error": 236.32736384304332
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1846630.052361111,
            "upper_bound": 1849907.3808055553
          },
          "point_estimate": 1848190.122972222,
          "standard_error": 842.2366560274418
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1845940.75,
            "upper_bound": 1850346.1125
          },
          "point_estimate": 1847505.74375,
          "standard_error": 943.3602712603688
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 363.6632410436523,
            "upper_bound": 4724.329526126466
          },
          "point_estimate": 2440.0372970972226,
          "standard_error": 1071.3755874151602
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1846289.0895835888,
            "upper_bound": 1848285.774825452
          },
          "point_estimate": 1846995.2712987016,
          "standard_error": 509.9092236395189
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1203.9438970593214,
            "upper_bound": 3564.788430760835
          },
          "point_estimate": 2811.7179215236383,
          "standard_error": 595.4858560837
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62475.48307471265,
            "upper_bound": 62662.38109434866
          },
          "point_estimate": 62557.421737205805,
          "standard_error": 48.25863859375432
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62470.87298850575,
            "upper_bound": 62605.46724137931
          },
          "point_estimate": 62525.50959051724,
          "standard_error": 34.61286909385829
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.984827847055428,
            "upper_bound": 197.75367205239456
          },
          "point_estimate": 84.78055625346664,
          "standard_error": 43.162882005715765
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62478.63373040752,
            "upper_bound": 62573.30735116352
          },
          "point_estimate": 62523.474388714734,
          "standard_error": 23.94078536759875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.51707354243256,
            "upper_bound": 232.4934647055519
          },
          "point_estimate": 161.40650194467287,
          "standard_error": 53.93735922650464
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 212436.66159570497,
            "upper_bound": 212935.13447720587
          },
          "point_estimate": 212682.71297152192,
          "standard_error": 127.82624147831908
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 212249.46974789916,
            "upper_bound": 213155.68588235293
          },
          "point_estimate": 212551.18602941177,
          "standard_error": 239.92707719429148
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.83237820513311,
            "upper_bound": 625.1037575786814
          },
          "point_estimate": 537.3926586260413,
          "standard_error": 178.6709377302683
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 212259.0830332845,
            "upper_bound": 212775.3643598616
          },
          "point_estimate": 212439.4799541635,
          "standard_error": 132.26601254118077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.3479066880713,
            "upper_bound": 488.53034089057206
          },
          "point_estimate": 425.5112691972749,
          "standard_error": 54.29261542010578
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142311.26080576258,
            "upper_bound": 142519.96359392576
          },
          "point_estimate": 142423.1056388264,
          "standard_error": 53.50029389467381
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142332.4943405512,
            "upper_bound": 142566.68188976377
          },
          "point_estimate": 142440.62689820022,
          "standard_error": 61.1040764886743
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.10228633531969,
            "upper_bound": 285.28922697250994
          },
          "point_estimate": 172.28424579960537,
          "standard_error": 60.12786418195226
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142337.25251310444,
            "upper_bound": 142527.19912384602
          },
          "point_estimate": 142451.8414868596,
          "standard_error": 48.868751634667625
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.00354179036431,
            "upper_bound": 239.97704126949603
          },
          "point_estimate": 177.74347377968658,
          "standard_error": 43.2534708346654
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 920209.4805505952,
            "upper_bound": 921813.2957517552
          },
          "point_estimate": 920908.0829884006,
          "standard_error": 416.1803967217597
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 919940.4310897436,
            "upper_bound": 921224.587912088
          },
          "point_estimate": 920803.7047008548,
          "standard_error": 337.3928890708678
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.09822119332335,
            "upper_bound": 1790.469144494988
          },
          "point_estimate": 742.7893752744178,
          "standard_error": 401.086658420604
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 920167.3401797912,
            "upper_bound": 921035.664505354
          },
          "point_estimate": 920669.41005661,
          "standard_error": 224.75497769800631
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 448.9450407824376,
            "upper_bound": 2013.60508296339
          },
          "point_estimate": 1384.649471094204,
          "standard_error": 469.3209957820542
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62826.53086969056,
            "upper_bound": 62868.098332482994
          },
          "point_estimate": 62846.44048338036,
          "standard_error": 10.639445356619072
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62825.52661633759,
            "upper_bound": 62871.00165505226
          },
          "point_estimate": 62841.967595818816,
          "standard_error": 9.862811473384852
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.5015942570601934,
            "upper_bound": 61.09138428126508
          },
          "point_estimate": 20.593890488043822,
          "standard_error": 14.359963896506924
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62818.00649932447,
            "upper_bound": 62866.88060404913
          },
          "point_estimate": 62841.901592832255,
          "standard_error": 12.437742571413423
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.88581266693057,
            "upper_bound": 46.85434437205863
          },
          "point_estimate": 35.58324558052111,
          "standard_error": 8.142952075222102
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123937.17078465065,
            "upper_bound": 124061.84392525777
          },
          "point_estimate": 123998.56613783888,
          "standard_error": 31.891051431835297
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123891.0545532646,
            "upper_bound": 124086.54381443298
          },
          "point_estimate": 124015.47938144328,
          "standard_error": 54.49467228146644
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.732608963550977,
            "upper_bound": 192.79630425187867
          },
          "point_estimate": 164.62400352064023,
          "standard_error": 50.81588026401325
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123922.09981035994,
            "upper_bound": 124048.49387506762
          },
          "point_estimate": 123977.50984960057,
          "standard_error": 32.44915327354346
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.37115921773902,
            "upper_bound": 128.4785855943756
          },
          "point_estimate": 106.076509129362,
          "standard_error": 16.19990153130041
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 403467.0155699349,
            "upper_bound": 404082.652991952
          },
          "point_estimate": 403787.8574777065,
          "standard_error": 157.20604196151223
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 403524.1702247191,
            "upper_bound": 404193.34332084894
          },
          "point_estimate": 403744.54654895666,
          "standard_error": 184.8645230922092
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.32680664872544,
            "upper_bound": 894.4694109458436
          },
          "point_estimate": 468.54083153195273,
          "standard_error": 195.54612007441827
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 403689.5244507119,
            "upper_bound": 404149.49483404367
          },
          "point_estimate": 403909.73220487376,
          "standard_error": 117.94892035978933
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 259.12630459946195,
            "upper_bound": 705.3700518318958
          },
          "point_estimate": 524.2953198700845,
          "standard_error": 121.05929237179843
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57903.583562844506,
            "upper_bound": 58024.7677789421
          },
          "point_estimate": 57958.09820190885,
          "standard_error": 31.19896511485933
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57892.8994462308,
            "upper_bound": 58010.70659163987
          },
          "point_estimate": 57937.33802250803,
          "standard_error": 28.68847057296553
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.49505726811393,
            "upper_bound": 142.7339158341302
          },
          "point_estimate": 71.07326844880107,
          "standard_error": 33.44713090886509
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57887.91667601783,
            "upper_bound": 57962.74452802021
          },
          "point_estimate": 57919.96030400468,
          "standard_error": 18.845440745045053
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.79737528354367,
            "upper_bound": 145.30863639317275
          },
          "point_estimate": 103.72249392720371,
          "standard_error": 30.30840635782687
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23023.936881293463,
            "upper_bound": 23060.904548102033
          },
          "point_estimate": 23041.817620243954,
          "standard_error": 9.506334572162228
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23016.15276886893,
            "upper_bound": 23075.013790891597
          },
          "point_estimate": 23032.92447336001,
          "standard_error": 15.356653047652744
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.658967728042812,
            "upper_bound": 52.73103339781401
          },
          "point_estimate": 30.475005712969548,
          "standard_error": 12.046867657490717
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23014.041389321075,
            "upper_bound": 23060.813738351797
          },
          "point_estimate": 23034.1878593504,
          "standard_error": 12.113610912976348
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.913510115402662,
            "upper_bound": 37.59215581492989
          },
          "point_estimate": 31.728622505736755,
          "standard_error": 4.859604177789103
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-repeated-rare-huge_common-m"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2008580.613254386,
            "upper_bound": 2011934.789005848
          },
          "point_estimate": 2010266.4625730992,
          "standard_error": 857.8314370557289
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2007627.3815789472,
            "upper_bound": 2012624.2631578948
          },
          "point_estimate": 2010588.5763157897,
          "standard_error": 1102.479816650767
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 552.3738328250085,
            "upper_bound": 4715.821049610908
          },
          "point_estimate": 2800.7353923823553,
          "standard_error": 1140.0233165198167
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2010415.833521524,
            "upper_bound": 2013246.92640112
          },
          "point_estimate": 2011862.865208476,
          "standard_error": 721.5872963085328
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1550.8523148798836,
            "upper_bound": 3625.108316078315
          },
          "point_estimate": 2850.785840491404,
          "standard_error": 527.7637382115557
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-repeated-rare-small_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4026.32329947335,
            "upper_bound": 4033.1635232919825
          },
          "point_estimate": 4029.297942444694,
          "standard_error": 1.7795194789516922
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4024.9269158256625,
            "upper_bound": 4030.9099346736584
          },
          "point_estimate": 4028.8580736386825,
          "standard_error": 1.558753128977362
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5300901508421461,
            "upper_bound": 7.379755936411261
          },
          "point_estimate": 4.087905916112995,
          "standard_error": 1.6776795068477357
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4025.694082427358,
            "upper_bound": 4030.8319451251577
          },
          "point_estimate": 4027.794234843564,
          "standard_error": 1.326304315510675
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.1265514517856507,
            "upper_bound": 8.634710071338576
          },
          "point_estimate": 5.949267158172032,
          "standard_error": 2.015011389537992
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41398.86080783279,
            "upper_bound": 41723.88039086175
          },
          "point_estimate": 41549.25486408731,
          "standard_error": 83.6004042467082
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41343.526875,
            "upper_bound": 41760.17357954546
          },
          "point_estimate": 41453.136728896105,
          "standard_error": 99.206443949775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.02519728186772,
            "upper_bound": 441.6213942903369
          },
          "point_estimate": 207.23563411628623,
          "standard_error": 101.00285973004992
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41359.53664274322,
            "upper_bound": 41474.80317045454
          },
          "point_estimate": 41410.426531877216,
          "standard_error": 29.133885847219776
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.58130968318932,
            "upper_bound": 354.3618111997168
          },
          "point_estimate": 278.15505973462,
          "standard_error": 63.21163008889499
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49247.89752355144,
            "upper_bound": 49298.962811115416
          },
          "point_estimate": 49273.2421411365,
          "standard_error": 13.087752648211696
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49234.268800813006,
            "upper_bound": 49307.00494687486
          },
          "point_estimate": 49269.939600271005,
          "standard_error": 22.834371434481824
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.419202707729375,
            "upper_bound": 73.09278138526645
          },
          "point_estimate": 44.61705155206469,
          "standard_error": 17.939329036012143
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49252.86026859098,
            "upper_bound": 49297.56520859575
          },
          "point_estimate": 49275.69018759019,
          "standard_error": 11.649005786269926
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.58429782327467,
            "upper_bound": 53.29416735567552
          },
          "point_estimate": 43.838840361858864,
          "standard_error": 6.6716561132474075
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48838.45168272744,
            "upper_bound": 48900.245694630874
          },
          "point_estimate": 48869.535649142425,
          "standard_error": 15.82316083772216
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48820.63718120805,
            "upper_bound": 48920.86845637584
          },
          "point_estimate": 48870.19174496645,
          "standard_error": 29.008028139283574
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.220628793107895,
            "upper_bound": 85.60154135275776
          },
          "point_estimate": 71.39686266981295,
          "standard_error": 19.54009576861524
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48816.12849685735,
            "upper_bound": 48885.77463674779
          },
          "point_estimate": 48842.98654580319,
          "standard_error": 17.881510549203803
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.26346719832856,
            "upper_bound": 61.82535235093371
          },
          "point_estimate": 52.743099646954526,
          "standard_error": 7.054475859614817
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14251.244581502371,
            "upper_bound": 14279.875259273773
          },
          "point_estimate": 14264.558277230431,
          "standard_error": 7.362800629798225
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14250.719731800767,
            "upper_bound": 14282.343789184952
          },
          "point_estimate": 14256.40865007837,
          "standard_error": 7.1283148664623255
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5017825607305904,
            "upper_bound": 39.6213929290237
          },
          "point_estimate": 12.282806302312808,
          "standard_error": 9.627681910686576
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14250.400333874131,
            "upper_bound": 14273.10265649228
          },
          "point_estimate": 14259.029928143957,
          "standard_error": 5.827086366640595
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.378782383449057,
            "upper_bound": 31.89640586090753
          },
          "point_estimate": 24.478443667377466,
          "standard_error": 5.866356437546034
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25587.89541497974,
            "upper_bound": 25637.57890000112
          },
          "point_estimate": 25613.497706393857,
          "standard_error": 12.724960558781978
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25586.002656665103,
            "upper_bound": 25650.21659634318
          },
          "point_estimate": 25613.487552742616,
          "standard_error": 15.413665727356042
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.691903309985164,
            "upper_bound": 71.99483279356868
          },
          "point_estimate": 47.601792638257514,
          "standard_error": 17.00025978005296
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25599.14794559084,
            "upper_bound": 25647.87901441631
          },
          "point_estimate": 25624.95368330685,
          "standard_error": 12.42075750346749
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.44391942968508,
            "upper_bound": 54.40174283422342
          },
          "point_estimate": 42.449714727175184,
          "standard_error": 8.289492585346249
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17328.163059393362,
            "upper_bound": 17358.613617595678
          },
          "point_estimate": 17342.10158435234,
          "standard_error": 7.834429691960787
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17317.75619342544,
            "upper_bound": 17353.081884230585
          },
          "point_estimate": 17341.982181991425,
          "standard_error": 10.13077024423696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.446984520636766,
            "upper_bound": 44.79743981926705
          },
          "point_estimate": 21.271593508330184,
          "standard_error": 10.036781268075227
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17327.594377142857,
            "upper_bound": 17347.09184278381
          },
          "point_estimate": 17337.240934767946,
          "standard_error": 4.910629321196439
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.495696061862793,
            "upper_bound": 36.028601457598505
          },
          "point_estimate": 26.11036405570929,
          "standard_error": 6.969362105755201
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17570.709408586037,
            "upper_bound": 17601.24107027749
          },
          "point_estimate": 17583.34482262962,
          "standard_error": 8.110973884596644
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17566.780629539953,
            "upper_bound": 17589.60239709443
          },
          "point_estimate": 17577.138821630346,
          "standard_error": 5.385592604329958
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8610350339546216,
            "upper_bound": 26.12868858696998
          },
          "point_estimate": 12.316109950834674,
          "standard_error": 6.664161114308206
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17566.964117132648,
            "upper_bound": 17582.497825718692
          },
          "point_estimate": 17573.45689883966,
          "standard_error": 4.004174452050898
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.429448839737168,
            "upper_bound": 39.94685628347806
          },
          "point_estimate": 27.005138151120807,
          "standard_error": 10.359267752737503
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17342.752353878557,
            "upper_bound": 17361.965960777303
          },
          "point_estimate": 17352.41139696922,
          "standard_error": 4.932538256398272
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17335.92083929423,
            "upper_bound": 17368.16080114449
          },
          "point_estimate": 17354.87450988184,
          "standard_error": 10.987894200463526
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.708136164239994,
            "upper_bound": 24.72997259385873
          },
          "point_estimate": 23.02795403005739,
          "standard_error": 7.030658573172743
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17341.076801505864,
            "upper_bound": 17367.237479742063
          },
          "point_estimate": 17352.985692609727,
          "standard_error": 6.904274761678551
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.297000298532666,
            "upper_bound": 18.07940566114536
          },
          "point_estimate": 16.379611590604412,
          "standard_error": 1.750600873908935
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93230.59883509614,
            "upper_bound": 93777.33464294871
          },
          "point_estimate": 93435.6564563492,
          "standard_error": 154.4584542111422
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93220.46573260074,
            "upper_bound": 93396.07735042735
          },
          "point_estimate": 93294.4246153846,
          "standard_error": 59.94269636076563
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.903381388731585,
            "upper_bound": 235.5734936061909
          },
          "point_estimate": 124.8330170145197,
          "standard_error": 65.08816276019208
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93223.85492662645,
            "upper_bound": 93357.37401440016
          },
          "point_estimate": 93281.44124542124,
          "standard_error": 34.38361386579604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.2222056716479,
            "upper_bound": 791.1085636089701
          },
          "point_estimate": 514.7401393736261,
          "standard_error": 250.06695951583123
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100720.16855428043,
            "upper_bound": 100852.2857248384
          },
          "point_estimate": 100783.7365598206,
          "standard_error": 33.859559598500574
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100702.20775623267,
            "upper_bound": 100869.95969529086
          },
          "point_estimate": 100770.82299168975,
          "standard_error": 36.73783273585568
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.853850752032177,
            "upper_bound": 194.23329868463657
          },
          "point_estimate": 87.47955883473949,
          "standard_error": 45.15272787294406
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100724.33367802124,
            "upper_bound": 100896.17829181557
          },
          "point_estimate": 100815.32373997194,
          "standard_error": 44.57685216010431
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.17879363122769,
            "upper_bound": 146.07535953720586
          },
          "point_estimate": 113.42782219274076,
          "standard_error": 23.61956091337063
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18712.0954472404,
            "upper_bound": 18733.35058369891
          },
          "point_estimate": 18721.8293965313,
          "standard_error": 5.454457552196221
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18708.31613688536,
            "upper_bound": 18731.050351689824
          },
          "point_estimate": 18719.483594956255,
          "standard_error": 6.0279905149211315
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.434449518717821,
            "upper_bound": 26.892536025392957
          },
          "point_estimate": 13.436502073174337,
          "standard_error": 5.772297171187256
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18710.597136960063,
            "upper_bound": 18724.60409617357
          },
          "point_estimate": 18718.143992086145,
          "standard_error": 3.639536619581589
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.955523740389654,
            "upper_bound": 25.097607461591966
          },
          "point_estimate": 18.228491890976706,
          "standard_error": 4.905698303866274
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17704.44691541776,
            "upper_bound": 17727.130763448724
          },
          "point_estimate": 17715.231197306002,
          "standard_error": 5.79835224409657
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17702.56341854148,
            "upper_bound": 17734.15709410044
          },
          "point_estimate": 17710.176787068638,
          "standard_error": 6.871965499639032
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8434460850212764,
            "upper_bound": 32.80530544717122
          },
          "point_estimate": 13.360090960193489,
          "standard_error": 9.026901602222487
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17700.336200169655,
            "upper_bound": 17716.83526616728
          },
          "point_estimate": 17707.213578425475,
          "standard_error": 4.209410029390558
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.122179511904786,
            "upper_bound": 24.93975887290171
          },
          "point_estimate": 19.32554288770948,
          "standard_error": 3.9986413998296424
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17579.076682769726,
            "upper_bound": 17591.324553140097
          },
          "point_estimate": 17585.569009661835,
          "standard_error": 3.127790715465243
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17579.53570853462,
            "upper_bound": 17592.566344605475
          },
          "point_estimate": 17588.038128019325,
          "standard_error": 3.445442008366924
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0531473436281278,
            "upper_bound": 17.117082246837917
          },
          "point_estimate": 9.019627327791795,
          "standard_error": 3.822528777132803
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17574.90827076781,
            "upper_bound": 17592.521492472457
          },
          "point_estimate": 17585.129673128802,
          "standard_error": 4.590020469862662
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.777119021945711,
            "upper_bound": 14.070543805098245
          },
          "point_estimate": 10.44563338936308,
          "standard_error": 2.4967436782754606
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17578.08652795519,
            "upper_bound": 17592.81080715321
          },
          "point_estimate": 17585.4324135193,
          "standard_error": 3.7707572479200566
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17573.25229579507,
            "upper_bound": 17594.178129531174
          },
          "point_estimate": 17587.342677622037,
          "standard_error": 4.944303527748796
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.412359986172378,
            "upper_bound": 22.446651979449825
          },
          "point_estimate": 13.093797715826447,
          "standard_error": 5.269273085636488
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17575.866573906063,
            "upper_bound": 17594.53286017538
          },
          "point_estimate": 17586.922072900517,
          "standard_error": 4.765709684654616
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.241521246651611,
            "upper_bound": 15.749884394604672
          },
          "point_estimate": 12.597682720652788,
          "standard_error": 2.1862777536866624
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17370.287869994652,
            "upper_bound": 17393.9331657111
          },
          "point_estimate": 17383.321730882988,
          "standard_error": 6.088333948818768
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17374.555502468545,
            "upper_bound": 17397.44083452779
          },
          "point_estimate": 17386.798381509794,
          "standard_error": 5.813095742612047
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.134933888770833,
            "upper_bound": 27.502241317757072
          },
          "point_estimate": 16.964896354332044,
          "standard_error": 6.127619721733948
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17372.44044989145,
            "upper_bound": 17393.854999006147
          },
          "point_estimate": 17384.529831659023,
          "standard_error": 5.550135777906383
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.087545735158834,
            "upper_bound": 28.500020779970495
          },
          "point_estimate": 20.314466585777,
          "standard_error": 5.985245821764072
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15329.353453869677,
            "upper_bound": 15353.25327398556
          },
          "point_estimate": 15340.831584183315,
          "standard_error": 6.102120587670623
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15327.249542317984,
            "upper_bound": 15356.509807471784
          },
          "point_estimate": 15337.795025348543,
          "standard_error": 6.967462975190128
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.1980848785941127,
            "upper_bound": 35.133187861684945
          },
          "point_estimate": 16.55318903649948,
          "standard_error": 7.9447655158925485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15333.104642719976,
            "upper_bound": 15366.75800206564
          },
          "point_estimate": 15349.80738070548,
          "standard_error": 9.296267746953312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.632251120491617,
            "upper_bound": 27.109863510357005
          },
          "point_estimate": 20.39070553947123,
          "standard_error": 4.598545083628595
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16156.2468741119,
            "upper_bound": 16172.346650301586
          },
          "point_estimate": 16164.5202777954,
          "standard_error": 4.105581801532892
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16154.95293072824,
            "upper_bound": 16175.30348579041
          },
          "point_estimate": 16165.72246891652,
          "standard_error": 5.700203845542336
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.1469280924341105,
            "upper_bound": 22.611753569476097
          },
          "point_estimate": 13.248817082726086,
          "standard_error": 5.011980128497613
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16158.442321592536,
            "upper_bound": 16174.578110986056
          },
          "point_estimate": 16168.012947798205,
          "standard_error": 4.177221546266105
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.621485704599973,
            "upper_bound": 17.505534702464704
          },
          "point_estimate": 13.699377211427484,
          "standard_error": 2.5545404439326083
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17925.044352218774,
            "upper_bound": 17945.431131356356
          },
          "point_estimate": 17935.70183513139,
          "standard_error": 5.2429925603574
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17924.345514560708,
            "upper_bound": 17948.435587364263
          },
          "point_estimate": 17939.807666995723,
          "standard_error": 7.331793639307689
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.521967706476272,
            "upper_bound": 30.63142287177805
          },
          "point_estimate": 16.77878794866257,
          "standard_error": 6.607906338641856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17929.15363409124,
            "upper_bound": 17947.026912186026
          },
          "point_estimate": 17939.518753605724,
          "standard_error": 4.527212816285242
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.127914056947944,
            "upper_bound": 23.03378181749436
          },
          "point_estimate": 17.51222918704314,
          "standard_error": 3.7520827438465423
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18123.261895123866,
            "upper_bound": 18147.233412976548
          },
          "point_estimate": 18134.367353644608,
          "standard_error": 6.1577259977575585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18119.590871231052,
            "upper_bound": 18150.060069965017
          },
          "point_estimate": 18129.061362176057,
          "standard_error": 6.950246707197413
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.741801818152202,
            "upper_bound": 33.35974663913202
          },
          "point_estimate": 18.64266221900261,
          "standard_error": 8.330602916075785
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18123.506785324153,
            "upper_bound": 18144.53864467766
          },
          "point_estimate": 18131.59825282164,
          "standard_error": 5.306137092117483
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.970395501185598,
            "upper_bound": 26.90321137363929
          },
          "point_estimate": 20.57269439731673,
          "standard_error": 4.775922349308269
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-freq_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58447.43107593795,
            "upper_bound": 58773.53805499189
          },
          "point_estimate": 58597.15556727994,
          "standard_error": 83.53790990603412
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58414.961183261184,
            "upper_bound": 58677.815543831166
          },
          "point_estimate": 58609.78287337662,
          "standard_error": 79.03932390968708
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.128121286826815,
            "upper_bound": 389.16206187887286
          },
          "point_estimate": 163.01301034457262,
          "standard_error": 100.16579793652726
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58431.5996587418,
            "upper_bound": 58646.73870732712
          },
          "point_estimate": 58527.05785123967,
          "standard_error": 54.55582558628519
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.47830641324931,
            "upper_bound": 395.6832303708222
          },
          "point_estimate": 277.99941019756864,
          "standard_error": 82.19480111799213
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-repeated_"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 880916.1164846229,
            "upper_bound": 881655.7796924604
          },
          "point_estimate": 881272.6130753967,
          "standard_error": 188.12549859305375
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 880712.8630952381,
            "upper_bound": 881642.7896825396
          },
          "point_estimate": 881313.9583333334,
          "standard_error": 253.69463612659536
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.92148104449348,
            "upper_bound": 1149.910717085119
          },
          "point_estimate": 587.5178929028051,
          "standard_error": 243.65510204708409
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 880915.4814098052,
            "upper_bound": 881659.433451119
          },
          "point_estimate": 881245.1242424243,
          "standard_error": 191.12176602940076
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 343.203595683221,
            "upper_bound": 823.7011412582272
          },
          "point_estimate": 626.9038439329979,
          "standard_error": 129.0641759653937
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector_rare-alph"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205179.19057752664,
            "upper_bound": 205392.78173493405
          },
          "point_estimate": 205285.6936660389,
          "standard_error": 54.73081174650611
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205124.89985875704,
            "upper_bound": 205469.31073446327
          },
          "point_estimate": 205255.26553672316,
          "standard_error": 101.03799278026108
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.84711688250312,
            "upper_bound": 290.8396265314881
          },
          "point_estimate": 252.05423418804952,
          "standard_error": 69.68896608468097
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205125.995144774,
            "upper_bound": 205403.2170581792
          },
          "point_estimate": 205252.29674957812,
          "standard_error": 71.76251346851946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119.26699526022033,
            "upper_bound": 214.22670298656635
          },
          "point_estimate": 182.22577244858735,
          "standard_error": 24.404387762898537
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5443.335560573093,
            "upper_bound": 5466.611764015218
          },
          "point_estimate": 5454.522041668153,
          "standard_error": 5.953383986190125
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5435.535880149812,
            "upper_bound": 5467.468117977528
          },
          "point_estimate": 5453.811109446526,
          "standard_error": 6.731921396820035
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.095998740541927,
            "upper_bound": 34.46483311846377
          },
          "point_estimate": 19.689038706629685,
          "standard_error": 8.776677547006425
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5442.359207425501,
            "upper_bound": 5471.0555401769725
          },
          "point_estimate": 5454.8925908847705,
          "standard_error": 7.359689123619596
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.404694110125902,
            "upper_bound": 26.036524713023372
          },
          "point_estimate": 19.939305655656863,
          "standard_error": 4.179743617434352
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5463.2686034157905,
            "upper_bound": 5474.884309725595
          },
          "point_estimate": 5469.223790903403,
          "standard_error": 2.9698280624676117
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5459.934544047798,
            "upper_bound": 5476.751580495796
          },
          "point_estimate": 5471.950171756682,
          "standard_error": 4.4560478572240045
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7461156816024683,
            "upper_bound": 16.558255685546932
          },
          "point_estimate": 12.169941113863604,
          "standard_error": 3.86048391092326
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5464.973367207911,
            "upper_bound": 5478.224640439315
          },
          "point_estimate": 5473.179153177198,
          "standard_error": 3.315646241510431
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.742529113023434,
            "upper_bound": 12.175294831437537
          },
          "point_estimate": 9.939110404450153,
          "standard_error": 1.6311033833534
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-repeated-rare-huge_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14465.851907666953,
            "upper_bound": 14487.930795382756
          },
          "point_estimate": 14477.651552077406,
          "standard_error": 5.670773302009071
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14464.812267596282,
            "upper_bound": 14493.733266932271
          },
          "point_estimate": 14482.837732403716,
          "standard_error": 6.265494505708009
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7870744224586286,
            "upper_bound": 29.767392499413265
          },
          "point_estimate": 16.774964034855596,
          "standard_error": 6.988754870264241
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14473.826660363084,
            "upper_bound": 14490.76955822032
          },
          "point_estimate": 14483.482566357943,
          "standard_error": 4.368438018288284
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.979610031330927,
            "upper_bound": 24.76167735572675
          },
          "point_estimate": 18.872086729831032,
          "standard_error": 4.433026148625569
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-repeated-rare-small_never-tric"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.61857491924192,
            "upper_bound": 36.14793445442651
          },
          "point_estimate": 35.8902217149503,
          "standard_error": 0.1358605628002046
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.47868207145328,
            "upper_bound": 36.35802868036162
          },
          "point_estimate": 35.953000159186644,
          "standard_error": 0.2168493664278444
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08310923301774126,
            "upper_bound": 0.7850825105665169
          },
          "point_estimate": 0.5890231051574918,
          "standard_error": 0.1702915722414801
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.53440066143077,
            "upper_bound": 36.08660518855741
          },
          "point_estimate": 35.82104819041961,
          "standard_error": 0.142364138678573
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2727259754289929,
            "upper_bound": 0.5466211102022401
          },
          "point_estimate": 0.4530844042134386,
          "standard_error": 0.06942757438948818
        }
      }
    },
    "memmem/krate_nopre/prebuilt/sliceslice-i386/words": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/krate_nopre_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23147167.99375,
            "upper_bound": 23183192.51055556
          },
          "point_estimate": 23165178.50259921,
          "standard_error": 9210.80176381265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23136486.025,
            "upper_bound": 23195683.1
          },
          "point_estimate": 23163861.78174603,
          "standard_error": 19402.359954833348
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4636.534897686156,
            "upper_bound": 47178.28560575635
          },
          "point_estimate": 41692.111198980376,
          "standard_error": 12343.891315529683
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23141664.708001737,
            "upper_bound": 23185076.408831857
          },
          "point_estimate": 23160122.44025974,
          "standard_error": 11219.43352960287
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20774.844495943365,
            "upper_bound": 34829.94019338966
          },
          "point_estimate": 30700.15305150831,
          "standard_error": 3554.068337437832
        }
      }
    },
    "memmem/krate_nopre/prebuilt/sliceslice-words/words": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/krate_nopre_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81083778.26666668,
            "upper_bound": 81205505.85083334
          },
          "point_estimate": 81144426.46666667,
          "standard_error": 31165.415152374375
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81066058.66666666,
            "upper_bound": 81227451.66666667
          },
          "point_estimate": 81126452.0,
          "standard_error": 38462.83606258376
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3766.2981331274777,
            "upper_bound": 186283.7446928098
          },
          "point_estimate": 109886.35644913088,
          "standard_error": 45704.691393962814
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57451.881552748,
            "upper_bound": 132793.2826282278
          },
          "point_estimate": 103918.60770056998,
          "standard_error": 19398.048976563987
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.049099162750748,
            "upper_bound": 9.061979828241922
          },
          "point_estimate": 9.055172710487389,
          "standard_error": 0.003271791338322634
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.050967072932664,
            "upper_bound": 9.0599041369896
          },
          "point_estimate": 9.053791898982292,
          "standard_error": 0.002171697424409443
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004038975045379987,
            "upper_bound": 0.01630463017949653
          },
          "point_estimate": 0.004828334866726418,
          "standard_error": 0.0038639527339441393
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04339536319135,
            "upper_bound": 9.056100589288798
          },
          "point_estimate": 9.050013334679608,
          "standard_error": 0.00337275912042598
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032210196589660986,
            "upper_bound": 0.01562387824738836
          },
          "point_estimate": 0.010892021758521012,
          "standard_error": 0.003246211291723245
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.046391961796788,
            "upper_bound": 9.057326290568
          },
          "point_estimate": 9.05141346182434,
          "standard_error": 0.002808173982133218
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.045371543480684,
            "upper_bound": 9.05883584568776
          },
          "point_estimate": 9.047998611370982,
          "standard_error": 0.0032936495204661936
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008437359685180894,
            "upper_bound": 0.015218584734015134
          },
          "point_estimate": 0.005156194082139822,
          "standard_error": 0.0037341960294230096
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.044373493370252,
            "upper_bound": 9.052061266699988
          },
          "point_estimate": 9.047902440772784,
          "standard_error": 0.0019509627888475449
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003828543711857904,
            "upper_bound": 0.012339397076218906
          },
          "point_estimate": 0.00935645616893069,
          "standard_error": 0.0022309732684740154
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.047550601707425,
            "upper_bound": 9.06284167330184
          },
          "point_estimate": 9.053872615149157,
          "standard_error": 0.0040381599584317545
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04601128120473,
            "upper_bound": 9.055838224123889
          },
          "point_estimate": 9.051198595902122,
          "standard_error": 0.0038126342758943426
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000605837168027275,
            "upper_bound": 0.01398848837584497
          },
          "point_estimate": 0.00706862749993739,
          "standard_error": 0.003697532153808995
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.047950914313107,
            "upper_bound": 9.054892564415615
          },
          "point_estimate": 9.05242207610423,
          "standard_error": 0.0017760744712506044
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004235769153246756,
            "upper_bound": 0.01994077847286519
          },
          "point_estimate": 0.013399343763606112,
          "standard_error": 0.005156141908878038
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.799591155845857,
            "upper_bound": 8.815994849300306
          },
          "point_estimate": 8.808145979997082,
          "standard_error": 0.004177519311823702
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.800981821264138,
            "upper_bound": 8.82046947072634
          },
          "point_estimate": 8.807697875640356,
          "standard_error": 0.0050553710357735355
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003723404118015832,
            "upper_bound": 0.024668322673208523
          },
          "point_estimate": 0.01228991055673262,
          "standard_error": 0.005236100447760658
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.801719676124026,
            "upper_bound": 8.813913607759018
          },
          "point_estimate": 8.806227293194075,
          "standard_error": 0.003093282016701084
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007142010915319238,
            "upper_bound": 0.018450908827871815
          },
          "point_estimate": 0.013859925273807729,
          "standard_error": 0.003083984307969049
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.533853407216212,
            "upper_bound": 9.549685268191883
          },
          "point_estimate": 9.54160080396461,
          "standard_error": 0.004021127416178553
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.529498252983172,
            "upper_bound": 9.547398914526518
          },
          "point_estimate": 9.543878201287452,
          "standard_error": 0.004511079348877909
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012119987044866066,
            "upper_bound": 0.02794067143269621
          },
          "point_estimate": 0.005559042759195485,
          "standard_error": 0.006852752148823055
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.534782831943136,
            "upper_bound": 9.545271882299645
          },
          "point_estimate": 9.541247279014534,
          "standard_error": 0.0027339947907143343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006926585296133575,
            "upper_bound": 0.017782117292937237
          },
          "point_estimate": 0.013402234323253643,
          "standard_error": 0.002925419042261491
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.470954849346716,
            "upper_bound": 10.48387682743841
          },
          "point_estimate": 10.477451539317892,
          "standard_error": 0.0033145676139613015
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.469443995285706,
            "upper_bound": 10.48699823965003
          },
          "point_estimate": 10.476132055359033,
          "standard_error": 0.005744719886678747
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013783104911186843,
            "upper_bound": 0.019087303097499283
          },
          "point_estimate": 0.01305266957837355,
          "standard_error": 0.004603017984197984
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.467455738615698,
            "upper_bound": 10.485807853517684
          },
          "point_estimate": 10.476780534330365,
          "standard_error": 0.004687710593124078
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006825921135974735,
            "upper_bound": 0.01377685664436686
          },
          "point_estimate": 0.011055615923943446,
          "standard_error": 0.0018130952143754029
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.122569562300857,
            "upper_bound": 8.137694484212831
          },
          "point_estimate": 8.129949442115265,
          "standard_error": 0.003851428281763712
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.121411931913467,
            "upper_bound": 8.13764726927162
          },
          "point_estimate": 8.12948445040714,
          "standard_error": 0.004517342149943564
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019281673043578464,
            "upper_bound": 0.02202550653501325
          },
          "point_estimate": 0.01112081203354582,
          "standard_error": 0.0047632579471866094
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.122955863289183,
            "upper_bound": 8.135467543727804
          },
          "point_estimate": 8.12910854686149,
          "standard_error": 0.003235413150167643
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0060674052019545534,
            "upper_bound": 0.01727582267775547
          },
          "point_estimate": 0.012821330036805752,
          "standard_error": 0.0028881887608055573
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.89580603386018,
            "upper_bound": 9.92832941359519
          },
          "point_estimate": 9.911725448020002,
          "standard_error": 0.008330067639301967
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.894303725421526,
            "upper_bound": 9.9331390270921
          },
          "point_estimate": 9.906732708535426,
          "standard_error": 0.009743743628772722
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004512330188782486,
            "upper_bound": 0.04796312409844359
          },
          "point_estimate": 0.01989134258700061,
          "standard_error": 0.012140748316303084
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.89776053991234,
            "upper_bound": 9.917283889627988
          },
          "point_estimate": 9.90578363737246,
          "standard_error": 0.004908753598555013
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013652745282501945,
            "upper_bound": 0.035824460628008044
          },
          "point_estimate": 0.02784689111863572,
          "standard_error": 0.005472658807102554
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.83206146785437,
            "upper_bound": 12.850605151272855
          },
          "point_estimate": 12.841093435268451,
          "standard_error": 0.004757702787727062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.829117510030208,
            "upper_bound": 12.855240097152354
          },
          "point_estimate": 12.837569730138494,
          "standard_error": 0.0060936435689120565
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022222038195031653,
            "upper_bound": 0.02776339360713898
          },
          "point_estimate": 0.014290506156293792,
          "standard_error": 0.007126610408105129
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.833251028457962,
            "upper_bound": 12.854327408901217
          },
          "point_estimate": 12.842665280310325,
          "standard_error": 0.005525777377084754
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008587482350948058,
            "upper_bound": 0.019717042497763085
          },
          "point_estimate": 0.015880258674335068,
          "standard_error": 0.002785448773505055
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.046957195935526,
            "upper_bound": 9.055923758379514
          },
          "point_estimate": 9.05131215772541,
          "standard_error": 0.002301421723082325
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.044179657263424,
            "upper_bound": 9.05873572606944
          },
          "point_estimate": 9.049896021306676,
          "standard_error": 0.003888279817741592
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001219377817626063,
            "upper_bound": 0.012367220753034971
          },
          "point_estimate": 0.008836530112256434,
          "standard_error": 0.003055811423798409
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04514976677282,
            "upper_bound": 9.052282714973304
          },
          "point_estimate": 9.047706318024634,
          "standard_error": 0.0018266292671457584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00438437153175263,
            "upper_bound": 0.009086783671760357
          },
          "point_estimate": 0.0076520254500644334,
          "standard_error": 0.0011639584190623572
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.784839565762969,
            "upper_bound": 9.811868937703458
          },
          "point_estimate": 9.796730598887368,
          "standard_error": 0.00701463158197266
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.782945306817703,
            "upper_bound": 9.807245356980552
          },
          "point_estimate": 9.789346980032072,
          "standard_error": 0.006024516967165486
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00187936936695898,
            "upper_bound": 0.028624465571681423
          },
          "point_estimate": 0.014122117868766791,
          "standard_error": 0.007249604188940842
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.784142472505838,
            "upper_bound": 9.794951907429576
          },
          "point_estimate": 9.789551814702328,
          "standard_error": 0.0028130458624023007
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008021858360541385,
            "upper_bound": 0.03330469417695259
          },
          "point_estimate": 0.023295726500593923,
          "standard_error": 0.007600907979153582
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.494457632894196,
            "upper_bound": 19.53370262625837
          },
          "point_estimate": 19.51442983317673,
          "standard_error": 0.01004685050341108
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.49895236415131,
            "upper_bound": 19.54669025250394
          },
          "point_estimate": 19.50885693230761,
          "standard_error": 0.010810040111839827
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003640461907470643,
            "upper_bound": 0.06343805639881116
          },
          "point_estimate": 0.01950212906826924,
          "standard_error": 0.016968044073722034
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.50474431250514,
            "upper_bound": 19.54475980199532
          },
          "point_estimate": 19.522190673991812,
          "standard_error": 0.010247429759915713
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017259382788316686,
            "upper_bound": 0.04443985369641172
          },
          "point_estimate": 0.0335708571278701,
          "standard_error": 0.0072309304326734335
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97989.16466981132,
            "upper_bound": 98135.22930560904
          },
          "point_estimate": 98062.64364426905,
          "standard_error": 37.4559958998343
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97966.65566037736,
            "upper_bound": 98194.1334231806
          },
          "point_estimate": 98046.66994609164,
          "standard_error": 47.34900946525632
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.760866937170888,
            "upper_bound": 239.1554976409172
          },
          "point_estimate": 125.67365908960704,
          "standard_error": 66.31037106510915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98008.75452405756,
            "upper_bound": 98126.11543458088
          },
          "point_estimate": 98060.13140336752,
          "standard_error": 29.44637467030656
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.54110300265145,
            "upper_bound": 155.8475646028109
          },
          "point_estimate": 124.74263385598849,
          "standard_error": 21.93515734043828
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52852.2729405281,
            "upper_bound": 52925.30890503875
          },
          "point_estimate": 52885.48983688631,
          "standard_error": 18.754429012284685
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52845.02180232558,
            "upper_bound": 52921.072537144704
          },
          "point_estimate": 52864.936845930235,
          "standard_error": 17.654129844564284
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.291457386645912,
            "upper_bound": 98.22874899736752
          },
          "point_estimate": 38.22195169061571,
          "standard_error": 24.816047917313735
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52847.52129066651,
            "upper_bound": 52930.54345399441
          },
          "point_estimate": 52886.12927740864,
          "standard_error": 21.696711630615887
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.3833336968594,
            "upper_bound": 81.41979463329317
          },
          "point_estimate": 62.33807605715206,
          "standard_error": 15.72499319730263
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142621.23245617413,
            "upper_bound": 142862.2691635543
          },
          "point_estimate": 142734.6418040772,
          "standard_error": 61.925850348247906
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142577.15991285403,
            "upper_bound": 142916.51225490196
          },
          "point_estimate": 142645.90240196078,
          "standard_error": 96.87222699339117
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.55247993206843,
            "upper_bound": 342.88903567719984
          },
          "point_estimate": 153.4172892076599,
          "standard_error": 92.45177141146031
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142609.70001827064,
            "upper_bound": 142803.25595946243
          },
          "point_estimate": 142682.23531448943,
          "standard_error": 49.16476469609341
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.24377486733336,
            "upper_bound": 251.6271927109968
          },
          "point_estimate": 206.3940046555294,
          "standard_error": 38.34111525471781
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391679.1156571088,
            "upper_bound": 392279.97912664287
          },
          "point_estimate": 391956.6879190135,
          "standard_error": 154.07040617046508
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391687.78375149344,
            "upper_bound": 392166.322734255
          },
          "point_estimate": 391818.94623655913,
          "standard_error": 129.91231002377722
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.33933872476934,
            "upper_bound": 777.7757671952776
          },
          "point_estimate": 285.66034847689315,
          "standard_error": 186.48953963445592
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391532.5185277088,
            "upper_bound": 392085.48225091986
          },
          "point_estimate": 391788.36413908674,
          "standard_error": 138.10298538271658
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.68063881139483,
            "upper_bound": 723.0837799671748
          },
          "point_estimate": 515.4001162123521,
          "standard_error": 146.5193725934443
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 573910.1016729653,
            "upper_bound": 574662.0973015872
          },
          "point_estimate": 574280.212254976,
          "standard_error": 192.53496498781792
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 573757.3952380952,
            "upper_bound": 574852.9126984127
          },
          "point_estimate": 574179.9007936508,
          "standard_error": 323.25949368731324
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.1424586762864,
            "upper_bound": 1050.502486905391
          },
          "point_estimate": 773.1859719874567,
          "standard_error": 238.61564796593913
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 573838.815163143,
            "upper_bound": 574854.6780186277
          },
          "point_estimate": 574252.9597196454,
          "standard_error": 275.0124035331244
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395.69909521673304,
            "upper_bound": 787.09586709892
          },
          "point_estimate": 642.1770763695546,
          "standard_error": 100.23466179881382
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40908.523353744065,
            "upper_bound": 41013.65940175769
          },
          "point_estimate": 40959.037291722714,
          "standard_error": 26.952261143198655
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40872.65480225989,
            "upper_bound": 41038.84717514124
          },
          "point_estimate": 40958.67997489014,
          "standard_error": 38.74765995551837
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.40433378138604,
            "upper_bound": 168.75924547850505
          },
          "point_estimate": 102.84490748862976,
          "standard_error": 42.73483110153667
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40899.54157370076,
            "upper_bound": 40978.00432495616
          },
          "point_estimate": 40944.53622422775,
          "standard_error": 20.02660607186641
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.97776243788278,
            "upper_bound": 110.59005809726456
          },
          "point_estimate": 89.68618498088965,
          "standard_error": 16.14389989272432
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96925.3623035238,
            "upper_bound": 97184.3660668783
          },
          "point_estimate": 97036.20180243386,
          "standard_error": 67.56141790517468
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96896.89288888888,
            "upper_bound": 97112.97053333334
          },
          "point_estimate": 96955.24973809524,
          "standard_error": 54.920561441800075
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.958527336864147,
            "upper_bound": 262.96157495816504
          },
          "point_estimate": 95.74646711127718,
          "standard_error": 63.777727929977445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96929.26428971814,
            "upper_bound": 97036.7629162373
          },
          "point_estimate": 96972.766808658,
          "standard_error": 27.378019537052793
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.14330791467609,
            "upper_bound": 323.6861323328147
          },
          "point_estimate": 225.93858747381904,
          "standard_error": 76.75552768103573
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72513.69877485714,
            "upper_bound": 72686.81478666665
          },
          "point_estimate": 72587.31631841269,
          "standard_error": 45.28255931415159
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72489.1008611111,
            "upper_bound": 72634.23533333333
          },
          "point_estimate": 72547.13933333333,
          "standard_error": 39.753091446113395
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.36405026275683,
            "upper_bound": 165.2621114559964
          },
          "point_estimate": 87.84780789038174,
          "standard_error": 43.1368681054979
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72483.19852066298,
            "upper_bound": 72545.60685128205
          },
          "point_estimate": 72503.5757090909,
          "standard_error": 16.059998543404138
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.42076812950496,
            "upper_bound": 218.47273072184672
          },
          "point_estimate": 150.90896902915924,
          "standard_error": 52.32432936943907
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307094.30339405104,
            "upper_bound": 307678.6732506053
          },
          "point_estimate": 307361.46755380684,
          "standard_error": 150.46589901591884
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307042.56688861985,
            "upper_bound": 307709.4427966102
          },
          "point_estimate": 307226.3118644068,
          "standard_error": 165.66620719246137
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.31273012375842,
            "upper_bound": 789.5033325936754
          },
          "point_estimate": 280.6875860337896,
          "standard_error": 196.58922520277295
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307100.05434879573,
            "upper_bound": 307734.27741184033
          },
          "point_estimate": 307360.07933083863,
          "standard_error": 161.7746534435685
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202.319291037023,
            "upper_bound": 677.4850808983513
          },
          "point_estimate": 502.3139508458953,
          "standard_error": 129.20581138473813
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35845.201203716766,
            "upper_bound": 35900.54861807417
          },
          "point_estimate": 35872.689388975065,
          "standard_error": 14.149174584245165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35840.28593561478,
            "upper_bound": 35922.94274432379
          },
          "point_estimate": 35863.98185493349,
          "standard_error": 20.91757669234827
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.285236109636353,
            "upper_bound": 87.35010980164017
          },
          "point_estimate": 42.14890490323806,
          "standard_error": 19.715519682049077
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35819.08059441446,
            "upper_bound": 35879.1721973677
          },
          "point_estimate": 35843.88736811066,
          "standard_error": 15.631105436382338
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.630334110784105,
            "upper_bound": 58.83514762943539
          },
          "point_estimate": 47.13302236339571,
          "standard_error": 8.009853573218294
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64123.4652779323,
            "upper_bound": 64276.11540384885
          },
          "point_estimate": 64193.68830650372,
          "standard_error": 39.28739603746047
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64090.65172074729,
            "upper_bound": 64283.0
          },
          "point_estimate": 64143.80337126001,
          "standard_error": 49.559236209110935
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.192473919336702,
            "upper_bound": 209.8390656551464
          },
          "point_estimate": 83.89887261079271,
          "standard_error": 50.11363703355337
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64094.64179585343,
            "upper_bound": 64166.52955602955
          },
          "point_estimate": 64119.79568785197,
          "standard_error": 18.58346633317278
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.13103025143117,
            "upper_bound": 163.6623701627876
          },
          "point_estimate": 130.87925128198358,
          "standard_error": 29.14662480701846
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168739.66123787378,
            "upper_bound": 169165.42823527133
          },
          "point_estimate": 168921.89817386487,
          "standard_error": 111.16709943945708
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168682.70166112957,
            "upper_bound": 169031.7550387597
          },
          "point_estimate": 168880.3623255814,
          "standard_error": 96.89519884262626
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.538278201285344,
            "upper_bound": 426.5237354509585
          },
          "point_estimate": 237.2547847413805,
          "standard_error": 92.47056841802134
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168723.85521712285,
            "upper_bound": 168944.37405828133
          },
          "point_estimate": 168835.91733011176,
          "standard_error": 56.050838754035546
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123.9877938246413,
            "upper_bound": 543.0155177435444
          },
          "point_estimate": 370.88055468792464,
          "standard_error": 131.15334799766296
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34138.78447171127,
            "upper_bound": 34192.277401390376
          },
          "point_estimate": 34163.95612498508,
          "standard_error": 13.726954967962936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34138.79915413534,
            "upper_bound": 34185.03926109918
          },
          "point_estimate": 34155.58004385965,
          "standard_error": 11.611514546818682
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.524774608475995,
            "upper_bound": 75.64537722845188
          },
          "point_estimate": 30.021627624901623,
          "standard_error": 18.502590435827475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34137.96490867974,
            "upper_bound": 34182.588136813996
          },
          "point_estimate": 34159.71429792012,
          "standard_error": 11.445338897272274
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.87316306265773,
            "upper_bound": 62.10545934128469
          },
          "point_estimate": 45.7777366778554,
          "standard_error": 11.492462592171387
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11541.82950266339,
            "upper_bound": 11568.160262298366
          },
          "point_estimate": 11555.18271796989,
          "standard_error": 6.7429769486759925
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11539.651666401527,
            "upper_bound": 11577.425135221129
          },
          "point_estimate": 11550.165137695762,
          "standard_error": 9.112326709759904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.46914668803367854,
            "upper_bound": 43.649701890245
          },
          "point_estimate": 24.19961778306667,
          "standard_error": 10.834995569962857
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11543.92072004332,
            "upper_bound": 11571.272299693488
          },
          "point_estimate": 11557.260486506812,
          "standard_error": 6.908608589162613
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.06104597987658,
            "upper_bound": 28.501260993357345
          },
          "point_estimate": 22.50381959435747,
          "standard_error": 4.073166232783753
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-repeated-rare-huge_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500801.8125463416,
            "upper_bound": 501609.964340074
          },
          "point_estimate": 501219.9244998912,
          "standard_error": 207.13717338907043
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500781.5827625571,
            "upper_bound": 501640.0391389433
          },
          "point_estimate": 501390.8386605784,
          "standard_error": 240.28136416880025
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131.22895657431883,
            "upper_bound": 1192.9725456013196
          },
          "point_estimate": 548.4570573862624,
          "standard_error": 263.88594101923456
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 501105.6646744592,
            "upper_bound": 501621.91131553205
          },
          "point_estimate": 501415.25404732255,
          "standard_error": 132.16037426559694
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.13704539728934,
            "upper_bound": 912.4405892391334
          },
          "point_estimate": 691.7235602361718,
          "standard_error": 151.47500870831638
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-repeated-rare-small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1039.1042880795887,
            "upper_bound": 1040.156693444454
          },
          "point_estimate": 1039.6005815218948,
          "standard_error": 0.2704879305915099
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1038.9888592948994,
            "upper_bound": 1040.3776026004523
          },
          "point_estimate": 1039.3258656241946,
          "standard_error": 0.35571713377545383
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08345648962590539,
            "upper_bound": 1.4822019046934471
          },
          "point_estimate": 0.5535494231134793,
          "standard_error": 0.36888536745202616
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1039.3484474978225,
            "upper_bound": 1040.7933507886469
          },
          "point_estimate": 1040.086182185658,
          "standard_error": 0.3914320274671549
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4323911506313423,
            "upper_bound": 1.1705963660648495
          },
          "point_estimate": 0.9002931829963893,
          "standard_error": 0.1920319475898429
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247027.26682879817,
            "upper_bound": 247266.3121957672
          },
          "point_estimate": 247144.74920256995,
          "standard_error": 60.998181341004866
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246998.11972789117,
            "upper_bound": 247291.38265306124
          },
          "point_estimate": 247119.84958427816,
          "standard_error": 57.64531443556647
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.11494532168879,
            "upper_bound": 371.69134340116057
          },
          "point_estimate": 128.07834336107445,
          "standard_error": 101.5536929906332
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247101.46565240005,
            "upper_bound": 247241.1780303449
          },
          "point_estimate": 247156.20243837795,
          "standard_error": 35.78378417887807
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.49626020631229,
            "upper_bound": 264.87142890430056
          },
          "point_estimate": 203.17937141167815,
          "standard_error": 41.831907774340515
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195236.8754863764,
            "upper_bound": 195887.4976046176
          },
          "point_estimate": 195501.8552026568,
          "standard_error": 173.1936404517308
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195208.95387700535,
            "upper_bound": 195583.1475935829
          },
          "point_estimate": 195313.98395721923,
          "standard_error": 118.92410650279471
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.27037083631442,
            "upper_bound": 498.702458953754
          },
          "point_estimate": 246.22655659117623,
          "standard_error": 126.20489451516582
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195247.38257810305,
            "upper_bound": 195557.92434982615
          },
          "point_estimate": 195432.5256059449,
          "standard_error": 79.59609177991375
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.12006226229445,
            "upper_bound": 866.4989825072806
          },
          "point_estimate": 576.8140153634982,
          "standard_error": 234.23929957458515
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174334.41517424243,
            "upper_bound": 174492.5283826612
          },
          "point_estimate": 174411.71847649425,
          "standard_error": 40.37026763398094
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174340.13397129186,
            "upper_bound": 174510.07456140348
          },
          "point_estimate": 174406.5757044125,
          "standard_error": 33.67859303927646
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.027229988712656,
            "upper_bound": 252.70743147847585
          },
          "point_estimate": 62.05106516631473,
          "standard_error": 66.42039092680905
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174358.6351337163,
            "upper_bound": 174504.34353723997
          },
          "point_estimate": 174424.57927049027,
          "standard_error": 36.89218939522296
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.59160373186682,
            "upper_bound": 177.41623409368128
          },
          "point_estimate": 134.43276323584465,
          "standard_error": 30.102486415078545
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/libc_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37428.41493411471,
            "upper_bound": 37472.70925284323
          },
          "point_estimate": 37450.527357470135,
          "standard_error": 11.38323012347266
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37416.15567010309,
            "upper_bound": 37483.38479381443
          },
          "point_estimate": 37452.42477090492,
          "standard_error": 20.245846051674395
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2543509055589617,
            "upper_bound": 63.812529423808094
          },
          "point_estimate": 52.68711110928906,
          "standard_error": 15.302337288236686
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37422.78876911174,
            "upper_bound": 37471.05410089648
          },
          "point_estimate": 37451.34257062525,
          "standard_error": 12.546030333836224
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.12969521696555,
            "upper_bound": 45.939979791753544
          },
          "point_estimate": 38.06911601436991,
          "standard_error": 5.600357882634648
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/libc_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71574.79902085094,
            "upper_bound": 71692.02716688777
          },
          "point_estimate": 71633.02620307755,
          "standard_error": 29.883834596876028
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71558.80029585799,
            "upper_bound": 71687.06911806142
          },
          "point_estimate": 71636.11526134123,
          "standard_error": 33.442964098210716
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.919409098063982,
            "upper_bound": 162.06055649168874
          },
          "point_estimate": 64.4608826737935,
          "standard_error": 42.74601590364293
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71607.68439727415,
            "upper_bound": 71679.09491627074
          },
          "point_estimate": 71656.28903404289,
          "standard_error": 18.505176027546792
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.40256143793443,
            "upper_bound": 133.51398544058634
          },
          "point_estimate": 99.43209321597377,
          "standard_error": 22.09499445666167
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68729.8653598485,
            "upper_bound": 68787.87650317913
          },
          "point_estimate": 68761.49302940116,
          "standard_error": 14.978093479083483
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68735.2124368687,
            "upper_bound": 68794.88005050505
          },
          "point_estimate": 68776.78787878787,
          "standard_error": 13.245890289670731
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.302154486839391,
            "upper_bound": 73.6965374226793
          },
          "point_estimate": 34.833477790658584,
          "standard_error": 19.081970372597574
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68761.22728673635,
            "upper_bound": 68788.96710885337
          },
          "point_estimate": 68775.96327233373,
          "standard_error": 6.9333890068913195
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.927362166258792,
            "upper_bound": 68.00655118617064
          },
          "point_estimate": 49.80346586743018,
          "standard_error": 13.550377432465949
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/libc_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82396.84169607765,
            "upper_bound": 82488.91014541266
          },
          "point_estimate": 82445.55652107405,
          "standard_error": 23.57490602212401
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82404.85317460318,
            "upper_bound": 82503.42630385488
          },
          "point_estimate": 82446.53161645611,
          "standard_error": 28.491113288619392
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.443363288907715,
            "upper_bound": 124.5566881514963
          },
          "point_estimate": 76.11060880749875,
          "standard_error": 27.986050448468564
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82417.04091385659,
            "upper_bound": 82496.36948056296
          },
          "point_estimate": 82458.79128309332,
          "standard_error": 21.305842843313847
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.23317414778647,
            "upper_bound": 107.07750601043412
          },
          "point_estimate": 78.73298687685717,
          "standard_error": 19.301717750105865
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/libc_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321448.64703670645,
            "upper_bound": 321797.8607329522
          },
          "point_estimate": 321617.9950929407,
          "standard_error": 89.16516644351827
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321420.9566885965,
            "upper_bound": 321911.5745614035
          },
          "point_estimate": 321481.908625731,
          "standard_error": 137.4123072085522
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.030832627902116,
            "upper_bound": 479.4704471894519
          },
          "point_estimate": 288.35187678860893,
          "standard_error": 146.40826715358946
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321430.6864812741,
            "upper_bound": 321898.72344854905
          },
          "point_estimate": 321694.7254499886,
          "standard_error": 123.761417704655
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164.98627617657172,
            "upper_bound": 366.1679700955522
          },
          "point_estimate": 297.70032377026456,
          "standard_error": 49.80929884127279
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47747.42618912041,
            "upper_bound": 47847.25895415389
          },
          "point_estimate": 47796.86495140062,
          "standard_error": 25.43409244038284
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47727.82276609723,
            "upper_bound": 47847.55650459921
          },
          "point_estimate": 47793.02106668335,
          "standard_error": 26.03026860841479
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.683635962879233,
            "upper_bound": 146.6755214104484
          },
          "point_estimate": 61.283688780590666,
          "standard_error": 38.24175049217437
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47782.0723636273,
            "upper_bound": 47855.909707786355
          },
          "point_estimate": 47815.97468129768,
          "standard_error": 18.474127219533532
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.34642883498629,
            "upper_bound": 110.4121434703504
          },
          "point_estimate": 84.57951532284733,
          "standard_error": 17.390997256572014
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33954.43522411734,
            "upper_bound": 34055.50090083074
          },
          "point_estimate": 34002.49673883697,
          "standard_error": 25.784307332678388
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33944.708566978195,
            "upper_bound": 34022.54261682243
          },
          "point_estimate": 34009.13869418484,
          "standard_error": 19.49131152044072
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6843790588835084,
            "upper_bound": 122.28937214667646
          },
          "point_estimate": 19.910690274240604,
          "standard_error": 36.53983214221923
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33950.07750676467,
            "upper_bound": 34011.799485072355
          },
          "point_estimate": 33982.02175506736,
          "standard_error": 16.50725350638968
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.829633093977563,
            "upper_bound": 122.79179972326138
          },
          "point_estimate": 85.91255055098655,
          "standard_error": 25.028713477100037
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36972.72091281977,
            "upper_bound": 37024.80857352192
          },
          "point_estimate": 36998.22150065531,
          "standard_error": 13.38177013255564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36959.95786612301,
            "upper_bound": 37042.6493374108
          },
          "point_estimate": 36987.36879884472,
          "standard_error": 22.872894750524033
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.99581059233054,
            "upper_bound": 71.0443973115856
          },
          "point_estimate": 53.58102703040037,
          "standard_error": 16.067341158678314
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36962.13013008022,
            "upper_bound": 37007.26898648731
          },
          "point_estimate": 36977.3684419556,
          "standard_error": 11.604475712964785
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.450322769590315,
            "upper_bound": 53.24007743827391
          },
          "point_estimate": 44.71582197429549,
          "standard_error": 6.602501962836119
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83453.3794754607,
            "upper_bound": 83569.43037748586
          },
          "point_estimate": 83508.42285477104,
          "standard_error": 29.80585225841193
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83428.46075533662,
            "upper_bound": 83573.50191570881
          },
          "point_estimate": 83503.72107279694,
          "standard_error": 35.03845869417013
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.802181906170155,
            "upper_bound": 164.33077516683574
          },
          "point_estimate": 95.52090566048906,
          "standard_error": 35.26241587040229
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83468.02988505748,
            "upper_bound": 83556.74735149907
          },
          "point_estimate": 83520.21692192864,
          "standard_error": 22.73828935288593
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.30205483710406,
            "upper_bound": 132.6333585596133
          },
          "point_estimate": 99.33630738513524,
          "standard_error": 22.565344686402025
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65297.27197985238,
            "upper_bound": 65383.23868878771
          },
          "point_estimate": 65339.03191188624,
          "standard_error": 22.072057189614767
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65284.83204667864,
            "upper_bound": 65400.54059445442
          },
          "point_estimate": 65311.728231597845,
          "standard_error": 34.63118295818093
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.373165471336362,
            "upper_bound": 120.74044212632336
          },
          "point_estimate": 83.44515834799407,
          "standard_error": 29.016545522542195
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65314.9246758773,
            "upper_bound": 65412.26660379755
          },
          "point_estimate": 65364.37732285668,
          "standard_error": 25.136006857087533
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.067683894315,
            "upper_bound": 91.5723734745128
          },
          "point_estimate": 73.63927199311506,
          "standard_error": 12.904131473311285
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148726.47678571427,
            "upper_bound": 148926.72850647476
          },
          "point_estimate": 148817.348760447,
          "standard_error": 51.24533754206512
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148700.62285714285,
            "upper_bound": 148887.56559766765
          },
          "point_estimate": 148797.86037414966,
          "standard_error": 53.453307442229566
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.54705467056077,
            "upper_bound": 248.1691065226989
          },
          "point_estimate": 133.62816203580763,
          "standard_error": 52.497045872803376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148679.51827198168,
            "upper_bound": 148836.24612668596
          },
          "point_estimate": 148754.5436522661,
          "standard_error": 39.913411948265775
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.92161392255328,
            "upper_bound": 237.83790844400855
          },
          "point_estimate": 170.58553206984183,
          "standard_error": 47.67369473736473
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198366.2407785367,
            "upper_bound": 198641.99420650097
          },
          "point_estimate": 198506.51962377483,
          "standard_error": 70.7839602670987
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198257.4184881603,
            "upper_bound": 198696.712295082
          },
          "point_estimate": 198594.20206435944,
          "standard_error": 125.0893030545908
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.15010292511239,
            "upper_bound": 370.2594831579281
          },
          "point_estimate": 291.2902194551573,
          "standard_error": 93.24844473287712
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198359.2602339193,
            "upper_bound": 198701.96030243483
          },
          "point_estimate": 198561.96790859415,
          "standard_error": 87.83558563344802
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148.6597753447804,
            "upper_bound": 277.6830937140862
          },
          "point_estimate": 236.1469566981405,
          "standard_error": 32.87003289139457
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119531.112123538,
            "upper_bound": 119700.87309797607
          },
          "point_estimate": 119608.7036686769,
          "standard_error": 43.7078207150954
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119509.8550575658,
            "upper_bound": 119725.72236842103
          },
          "point_estimate": 119539.6739766082,
          "standard_error": 48.72154922095259
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.911858048634836,
            "upper_bound": 215.0757548987712
          },
          "point_estimate": 84.50643737032253,
          "standard_error": 50.73203574313272
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119537.51035982862,
            "upper_bound": 119627.70909118936
          },
          "point_estimate": 119580.33884996582,
          "standard_error": 22.1157567854277
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.49787766264578,
            "upper_bound": 183.5878673423723
          },
          "point_estimate": 145.86364665859676,
          "standard_error": 36.052450985608274
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52482.1431368747,
            "upper_bound": 52596.8641377594
          },
          "point_estimate": 52541.57939090452,
          "standard_error": 29.42170221209765
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52493.0873015873,
            "upper_bound": 52619.30074555075
          },
          "point_estimate": 52531.658170995666,
          "standard_error": 36.157633252607745
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.667208664000032,
            "upper_bound": 167.74178176946617
          },
          "point_estimate": 80.59150311467094,
          "standard_error": 39.3431658281776
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52520.87522876852,
            "upper_bound": 52613.23200477683
          },
          "point_estimate": 52564.94439759375,
          "standard_error": 23.3997114389315
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.4457778865268,
            "upper_bound": 133.57318483160952
          },
          "point_estimate": 98.37126366149457,
          "standard_error": 23.125369411524517
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77189.02034383109,
            "upper_bound": 77266.69052429817
          },
          "point_estimate": 77225.13671477439,
          "standard_error": 19.965395956723498
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77172.51871019107,
            "upper_bound": 77272.36258551545
          },
          "point_estimate": 77203.90430694571,
          "standard_error": 24.9535064452644
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.567939833988679,
            "upper_bound": 111.3115161289247
          },
          "point_estimate": 63.19759449904233,
          "standard_error": 25.256216138432663
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77190.56707016556,
            "upper_bound": 77278.4079731206
          },
          "point_estimate": 77240.39568202499,
          "standard_error": 22.589532940867
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.369587586841224,
            "upper_bound": 88.68425925171067
          },
          "point_estimate": 66.65807821155575,
          "standard_error": 15.715159890346666
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40829.57482786358,
            "upper_bound": 40871.607743994
          },
          "point_estimate": 40848.97939081939,
          "standard_error": 10.854476593625742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40822.417229729734,
            "upper_bound": 40868.863682432435
          },
          "point_estimate": 40842.16084834835,
          "standard_error": 9.927033279857984
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.9245563137357555,
            "upper_bound": 56.86853352867105
          },
          "point_estimate": 22.481285603128143,
          "standard_error": 14.530708102458824
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40829.17095754291,
            "upper_bound": 40858.85341544076
          },
          "point_estimate": 40844.98584006084,
          "standard_error": 7.556216521597798
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.61544207723268,
            "upper_bound": 48.27861240836017
          },
          "point_estimate": 36.20873987252976,
          "standard_error": 9.066468872129214
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93500.84721508142,
            "upper_bound": 93638.5371905986
          },
          "point_estimate": 93576.99672083484,
          "standard_error": 35.53403947638473
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93521.559125964,
            "upper_bound": 93666.54201248624
          },
          "point_estimate": 93600.18744644387,
          "standard_error": 41.4706357312583
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.42641818186058,
            "upper_bound": 170.6514811734156
          },
          "point_estimate": 90.04770081135601,
          "standard_error": 39.887732750753585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93561.798303458,
            "upper_bound": 93662.32780756244
          },
          "point_estimate": 93616.32540313156,
          "standard_error": 26.28434146689566
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.855655994507984,
            "upper_bound": 165.9847607275617
          },
          "point_estimate": 118.6990270993372,
          "standard_error": 34.1586687996691
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1319876.1635121526,
            "upper_bound": 1321502.4140816326
          },
          "point_estimate": 1320716.965738379,
          "standard_error": 417.30849796994846
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1319529.0198412698,
            "upper_bound": 1321879.363095238
          },
          "point_estimate": 1320967.259375,
          "standard_error": 526.7368990311462
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 312.3193919551961,
            "upper_bound": 2296.7890486582623
          },
          "point_estimate": 1353.4756962389536,
          "standard_error": 538.3224526781651
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1320098.2208806232,
            "upper_bound": 1321539.6889483328
          },
          "point_estimate": 1320795.6028756958,
          "standard_error": 365.24190512347184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 713.3103026370932,
            "upper_bound": 1801.5536182130645
          },
          "point_estimate": 1391.3110641293524,
          "standard_error": 281.04244804466265
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157082.99013957306,
            "upper_bound": 157239.66953835267
          },
          "point_estimate": 157148.50700619182,
          "standard_error": 41.15293766111011
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157073.95359195402,
            "upper_bound": 157171.5615421456
          },
          "point_estimate": 157129.55464901478,
          "standard_error": 22.84361570151443
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.693103042079226,
            "upper_bound": 141.71454570243026
          },
          "point_estimate": 54.678671981866,
          "standard_error": 37.75779638177207
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157059.73180211158,
            "upper_bound": 157137.64868466527
          },
          "point_estimate": 157105.60570980742,
          "standard_error": 20.524651921349243
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.541553870612034,
            "upper_bound": 203.12618091826832
          },
          "point_estimate": 137.07848946913282,
          "standard_error": 51.86207570692564
        }
      }
    },
    "memmem/libc/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/libc_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6759.660998043963,
            "upper_bound": 6777.12896362631
          },
          "point_estimate": 6768.505739693481,
          "standard_error": 4.460938264484807
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6757.786605812221,
            "upper_bound": 6780.300679955291
          },
          "point_estimate": 6768.424971021693,
          "standard_error": 4.8844077396623735
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0376788166497346,
            "upper_bound": 28.782767238631283
          },
          "point_estimate": 12.546913113219851,
          "standard_error": 6.954906972024109
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6768.367350441925,
            "upper_bound": 6783.787248834319
          },
          "point_estimate": 6777.141941181025,
          "standard_error": 3.958814189356295
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.104950103924633,
            "upper_bound": 18.749153655822195
          },
          "point_estimate": 14.849118265092748,
          "standard_error": 2.758709592756995
        }
      }
    },
    "memmem/libc/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/libc_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7488.237853657293,
            "upper_bound": 7595.442071346103
          },
          "point_estimate": 7545.567696278184,
          "standard_error": 27.89437763310036
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7427.65260798696,
            "upper_bound": 7617.065369464819
          },
          "point_estimate": 7603.038325183374,
          "standard_error": 51.53027019525278
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6953733859020392,
            "upper_bound": 146.62882524930407
          },
          "point_estimate": 25.16595644808136,
          "standard_error": 42.68550481249337
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7575.255967359044,
            "upper_bound": 7612.739178630978
          },
          "point_estimate": 7601.297461869834,
          "standard_error": 9.838043033758842
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.59206190904326,
            "upper_bound": 104.31185315387796
          },
          "point_estimate": 92.61829424121689,
          "standard_error": 15.292976092546049
        }
      }
    },
    "memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/libc_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47642.05866674858,
            "upper_bound": 47665.57935597787
          },
          "point_estimate": 47654.06879678171,
          "standard_error": 5.995699650707975
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47640.13630406291,
            "upper_bound": 47665.938646788985
          },
          "point_estimate": 47655.25666229795,
          "standard_error": 5.346873530909759
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6354586865696974,
            "upper_bound": 36.13929734005867
          },
          "point_estimate": 11.977289787358904,
          "standard_error": 9.30625930912513
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47646.16764758594,
            "upper_bound": 47674.50455124867
          },
          "point_estimate": 47660.2813194669,
          "standard_error": 7.405726486732231
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.077158904051885,
            "upper_bound": 26.233712115426453
          },
          "point_estimate": 20.00518104883721,
          "standard_error": 4.321354219865873
        }
      }
    },
    "memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/libc_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.74901515995928,
            "upper_bound": 115.9748905756878
          },
          "point_estimate": 115.8596666555204,
          "standard_error": 0.0578839717176991
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.68456987689234,
            "upper_bound": 116.0258352918884
          },
          "point_estimate": 115.837839336915,
          "standard_error": 0.11325958816649724
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027876979699811476,
            "upper_bound": 0.31436643341547416
          },
          "point_estimate": 0.2251117339583459,
          "standard_error": 0.08509136926088834
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.74444217410262,
            "upper_bound": 115.99514381937352
          },
          "point_estimate": 115.87829861845664,
          "standard_error": 0.06407856621582175
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1257058375963592,
            "upper_bound": 0.22792983497074465
          },
          "point_estimate": 0.19290705842273465,
          "standard_error": 0.026622479979464212
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.095833990829924,
            "upper_bound": 19.11865144407392
          },
          "point_estimate": 19.107074654701957,
          "standard_error": 0.0058442076798917155
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.092966557785733,
            "upper_bound": 19.129608791574213
          },
          "point_estimate": 19.10259822566871,
          "standard_error": 0.008627313626452169
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0033813052694345127,
            "upper_bound": 0.034992523489363776
          },
          "point_estimate": 0.018879934952304056,
          "standard_error": 0.008512349177804563
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.095076229320757,
            "upper_bound": 19.117118437848667
          },
          "point_estimate": 19.104668358669866,
          "standard_error": 0.005555363350233611
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011348833038325682,
            "upper_bound": 0.023820788942796697
          },
          "point_estimate": 0.01946562601058509,
          "standard_error": 0.003149304677305579
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.313988214643455,
            "upper_bound": 19.36597970471407
          },
          "point_estimate": 19.33769841655734,
          "standard_error": 0.013372385756671237
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.305513634521205,
            "upper_bound": 19.370692910415045
          },
          "point_estimate": 19.32841854384546,
          "standard_error": 0.012827738467146093
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004314360841814214,
            "upper_bound": 0.0644703592747684
          },
          "point_estimate": 0.02232884240812029,
          "standard_error": 0.015489614286543154
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.30720569029227,
            "upper_bound": 19.334216726055583
          },
          "point_estimate": 19.322322387883155,
          "standard_error": 0.007169002640779692
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014573228687071152,
            "upper_bound": 0.057313895541854454
          },
          "point_estimate": 0.044455460709935286,
          "standard_error": 0.01122501881096494
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.634608243265497,
            "upper_bound": 18.682656273284305
          },
          "point_estimate": 18.659500823898885,
          "standard_error": 0.01229438460592946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.63138789831311,
            "upper_bound": 18.69258125654096
          },
          "point_estimate": 18.665786118708116,
          "standard_error": 0.018403972611250215
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004427598840448683,
            "upper_bound": 0.0722772061063194
          },
          "point_estimate": 0.04817783568318531,
          "standard_error": 0.01604066144261617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.609786732541675,
            "upper_bound": 18.66584841435021
          },
          "point_estimate": 18.634565779654817,
          "standard_error": 0.014249046195341666
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023027696132132028,
            "upper_bound": 0.0531185945701287
          },
          "point_estimate": 0.04100375101534039,
          "standard_error": 0.008081412348298112
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.33550660919788,
            "upper_bound": 19.02663926479781
          },
          "point_estimate": 18.69415745658864,
          "standard_error": 0.1778233729526734
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.307272519948384,
            "upper_bound": 19.229668478474792
          },
          "point_estimate": 18.621851819373244,
          "standard_error": 0.22767291766405612
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04376845413046117,
            "upper_bound": 1.0137201420910356
          },
          "point_estimate": 0.752174592180607,
          "standard_error": 0.2686081408271521
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.0179417418802,
            "upper_bound": 18.8861505590644
          },
          "point_estimate": 18.456361504494783,
          "standard_error": 0.2255678301714459
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3171643232420788,
            "upper_bound": 0.7650782734577076
          },
          "point_estimate": 0.5929544088742453,
          "standard_error": 0.11869054377085324
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.94202037560394,
            "upper_bound": 22.97762963814558
          },
          "point_estimate": 22.95798913675919,
          "standard_error": 0.009188114424191872
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.934850243974953,
            "upper_bound": 22.973641303546785
          },
          "point_estimate": 22.947581860857703,
          "standard_error": 0.01027707835177316
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0058304938810204475,
            "upper_bound": 0.04383192479630744
          },
          "point_estimate": 0.023597313940485736,
          "standard_error": 0.010198615723283632
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.939316265242702,
            "upper_bound": 22.96341892540581
          },
          "point_estimate": 22.950110368847067,
          "standard_error": 0.006032559036431207
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011732892522902282,
            "upper_bound": 0.042401344237819834
          },
          "point_estimate": 0.03063302574285488,
          "standard_error": 0.008662277395996175
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.255289551099008,
            "upper_bound": 26.30401167952611
          },
          "point_estimate": 26.27941006148439,
          "standard_error": 0.01244229246412648
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.24426555832668,
            "upper_bound": 26.304550837302397
          },
          "point_estimate": 26.28419095743321,
          "standard_error": 0.01544850226418393
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010079306982448771,
            "upper_bound": 0.07410041125723471
          },
          "point_estimate": 0.03592130148833814,
          "standard_error": 0.015862454226269588
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.261022629065412,
            "upper_bound": 26.29361255843508
          },
          "point_estimate": 26.27976068729229,
          "standard_error": 0.008270658871145024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02201322122617788,
            "upper_bound": 0.05455821196339816
          },
          "point_estimate": 0.04170442918381962,
          "standard_error": 0.008422936657056119
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.2038337350772,
            "upper_bound": 24.26168530371371
          },
          "point_estimate": 24.23099597146381,
          "standard_error": 0.014819838800042713
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.19480036937955,
            "upper_bound": 24.258844204898963
          },
          "point_estimate": 24.22459024919784,
          "standard_error": 0.013168931124548098
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003694780045924485,
            "upper_bound": 0.08104344758272339
          },
          "point_estimate": 0.03800797907484331,
          "standard_error": 0.02047600308746996
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.20563670533183,
            "upper_bound": 24.24555768887041
          },
          "point_estimate": 24.223835713314084,
          "standard_error": 0.010065331263366034
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02121861659834498,
            "upper_bound": 0.06635098112543189
          },
          "point_estimate": 0.04946988365798451,
          "standard_error": 0.011885337898244216
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.0021856857039,
            "upper_bound": 29.038993037853505
          },
          "point_estimate": 29.0210750731556,
          "standard_error": 0.009398649242467146
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.99845826989299,
            "upper_bound": 29.046077370739383
          },
          "point_estimate": 29.020394140393996,
          "standard_error": 0.011482232171404406
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002061213954106075,
            "upper_bound": 0.05268012431933339
          },
          "point_estimate": 0.0311082391208822,
          "standard_error": 0.012290505008416308
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.007863361066356,
            "upper_bound": 29.04212643843236
          },
          "point_estimate": 29.0281348309117,
          "standard_error": 0.008841790749144566
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01685951825346448,
            "upper_bound": 0.040007233790869394
          },
          "point_estimate": 0.03127402583633063,
          "standard_error": 0.005954412274082202
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.569213620615166,
            "upper_bound": 37.66928783514368
          },
          "point_estimate": 37.61397091018429,
          "standard_error": 0.025671984608847453
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.54158546170379,
            "upper_bound": 37.63770026397566
          },
          "point_estimate": 37.61360000725921,
          "standard_error": 0.023110750303925513
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009011784913578652,
            "upper_bound": 0.12600572063430115
          },
          "point_estimate": 0.039524385622888056,
          "standard_error": 0.030290871377643382
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.57741869041065,
            "upper_bound": 37.629990819592905
          },
          "point_estimate": 37.60673218701495,
          "standard_error": 0.01334829386037898
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03378516080365734,
            "upper_bound": 0.12236070190233964
          },
          "point_estimate": 0.08571183378920444,
          "standard_error": 0.026468949523622193
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.76341674705012,
            "upper_bound": 20.806263433158996
          },
          "point_estimate": 20.783884976378708,
          "standard_error": 0.0109997310134848
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.75616291877665,
            "upper_bound": 20.814795725688786
          },
          "point_estimate": 20.778214327720807,
          "standard_error": 0.011269921322896288
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004757613390773165,
            "upper_bound": 0.06918538103932755
          },
          "point_estimate": 0.019928604395432137,
          "standard_error": 0.016943987375304215
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.766668475357864,
            "upper_bound": 20.78836377932647
          },
          "point_estimate": 20.779180602077524,
          "standard_error": 0.005541792470880584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016542422570949723,
            "upper_bound": 0.046640855488921384
          },
          "point_estimate": 0.03654930580712917,
          "standard_error": 0.007635753193140291
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.239309021181892,
            "upper_bound": 23.31164072177119
          },
          "point_estimate": 23.268696269528952,
          "standard_error": 0.019319731149513585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.231655034401054,
            "upper_bound": 23.274756331167865
          },
          "point_estimate": 23.256325902829023,
          "standard_error": 0.01438473577141598
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035682190006483686,
            "upper_bound": 0.05865371217488751
          },
          "point_estimate": 0.03077993918453006,
          "standard_error": 0.013624527022786574
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.22696637016661,
            "upper_bound": 23.26193180666092
          },
          "point_estimate": 23.244499902920808,
          "standard_error": 0.009090126438250974
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01685918469214001,
            "upper_bound": 0.09658073820649676
          },
          "point_estimate": 0.06420577031270706,
          "standard_error": 0.026312752367579347
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.958565358328137,
            "upper_bound": 32.04299829470356
          },
          "point_estimate": 32.00134357183272,
          "standard_error": 0.02163088689701625
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.936413236474632,
            "upper_bound": 32.070330690942775
          },
          "point_estimate": 32.00054348640216,
          "standard_error": 0.03079597213799461
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01654145607042319,
            "upper_bound": 0.1209449868740944
          },
          "point_estimate": 0.0987509814194754,
          "standard_error": 0.02912548483075586
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.96898461383415,
            "upper_bound": 32.070656188852574
          },
          "point_estimate": 32.02366595988461,
          "standard_error": 0.02637180332567693
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.042133093000874734,
            "upper_bound": 0.08984381520839274
          },
          "point_estimate": 0.07205277218021952,
          "standard_error": 0.01221449162801901
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 890730.4420383275,
            "upper_bound": 891699.3870847852
          },
          "point_estimate": 891217.1100058071,
          "standard_error": 248.6119179163352
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 890513.7652439025,
            "upper_bound": 892020.9022648083
          },
          "point_estimate": 891097.4573170731,
          "standard_error": 452.65012106491895
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.04911398949764,
            "upper_bound": 1371.032517610525
          },
          "point_estimate": 1287.937050915017,
          "standard_error": 363.3747523992112
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 890412.4347154471,
            "upper_bound": 891889.0326762446
          },
          "point_estimate": 891154.1643332278,
          "standard_error": 385.4495366160202
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 539.3607939102334,
            "upper_bound": 974.8409633565172
          },
          "point_estimate": 827.0936666384619,
          "standard_error": 112.9617773679326
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166731.3078953692,
            "upper_bound": 167013.0466389981
          },
          "point_estimate": 166873.2363561235,
          "standard_error": 71.8620685097742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166687.5122324159,
            "upper_bound": 167042.69941386342
          },
          "point_estimate": 166893.4482798165,
          "standard_error": 95.90962257984128
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.343695271449164,
            "upper_bound": 405.64445348645063
          },
          "point_estimate": 209.1636692223969,
          "standard_error": 97.11297985679076
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166900.8126591526,
            "upper_bound": 167152.72954774252
          },
          "point_estimate": 167037.73579173122,
          "standard_error": 67.79220147796029
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.43133861297267,
            "upper_bound": 304.7655881839625
          },
          "point_estimate": 239.6949008372409,
          "standard_error": 44.26077831847113
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 698062.05606469,
            "upper_bound": 699218.5563026168
          },
          "point_estimate": 698647.8637219227,
          "standard_error": 297.06000062276416
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697957.7877358492,
            "upper_bound": 699625.013477089
          },
          "point_estimate": 698656.462264151,
          "standard_error": 380.1269558845171
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189.1363975855637,
            "upper_bound": 1689.913956082921
          },
          "point_estimate": 1213.9572542970309,
          "standard_error": 429.54089895843896
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 698553.4036666019,
            "upper_bound": 699379.1709237003
          },
          "point_estimate": 698942.5602548395,
          "standard_error": 211.13664758803847
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 561.0111948427225,
            "upper_bound": 1228.0990008000829
          },
          "point_estimate": 987.4862539427696,
          "standard_error": 172.7816142257923
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 379418.38963913696,
            "upper_bound": 380024.4951772797
          },
          "point_estimate": 379725.33085110784,
          "standard_error": 154.8741975349881
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 379431.23484002973,
            "upper_bound": 380142.515625
          },
          "point_estimate": 379613.8295717592,
          "standard_error": 187.89543539781897
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.93907153888688,
            "upper_bound": 967.547059385106
          },
          "point_estimate": 437.9097297255605,
          "standard_error": 230.4354611910588
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 379437.76961647096,
            "upper_bound": 379983.8632679181
          },
          "point_estimate": 379718.935551948,
          "standard_error": 143.4307055962669
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.76841714863,
            "upper_bound": 674.4353692056308
          },
          "point_estimate": 516.1816018879408,
          "standard_error": 103.86280898648174
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 580649.2272222223,
            "upper_bound": 581255.2177513227
          },
          "point_estimate": 580926.4321472662,
          "standard_error": 155.84211848111838
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 580539.5277777778,
            "upper_bound": 581233.126984127
          },
          "point_estimate": 580847.6292989417,
          "standard_error": 167.53216481159436
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.9137848219126,
            "upper_bound": 809.3237066502875
          },
          "point_estimate": 514.1650825384295,
          "standard_error": 177.7837443100143
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 580497.5830801522,
            "upper_bound": 581221.1671880431
          },
          "point_estimate": 580813.5590599877,
          "standard_error": 186.66302905031137
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233.8165975970159,
            "upper_bound": 711.2069850944689
          },
          "point_estimate": 519.8039297799006,
          "standard_error": 134.98654620607687
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227920.7290625,
            "upper_bound": 228401.1358484375
          },
          "point_estimate": 228144.87702430552,
          "standard_error": 123.08753098725714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227814.01770833333,
            "upper_bound": 228418.79796875
          },
          "point_estimate": 228088.60052083337,
          "standard_error": 141.97495359550174
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.44040842633694,
            "upper_bound": 676.5790926758488
          },
          "point_estimate": 288.6735402916763,
          "standard_error": 148.16314494960898
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227955.48308189656,
            "upper_bound": 228406.3427039749
          },
          "point_estimate": 228200.79728896104,
          "standard_error": 119.06723060317056
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195.1172812469312,
            "upper_bound": 543.8268944677749
          },
          "point_estimate": 410.8172454938603,
          "standard_error": 94.6521361192661
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389186.79833742825,
            "upper_bound": 390230.6655015198
          },
          "point_estimate": 389695.0031138129,
          "standard_error": 266.2536708791255
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389055.05053191487,
            "upper_bound": 390284.66873522464
          },
          "point_estimate": 389605.14406028367,
          "standard_error": 304.25121507663994
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.17361687412084,
            "upper_bound": 1478.9447338498387
          },
          "point_estimate": 829.6056391013108,
          "standard_error": 370.9918987252621
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389351.9735334872,
            "upper_bound": 390752.7167966169
          },
          "point_estimate": 390195.84780326055,
          "standard_error": 365.54792569535863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 432.0260879931095,
            "upper_bound": 1133.6247642838905
          },
          "point_estimate": 883.7674682640617,
          "standard_error": 175.1505953083297
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 382979.7970643275,
            "upper_bound": 383521.0212105263
          },
          "point_estimate": 383237.9390467837,
          "standard_error": 138.35309509062645
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 382977.6795321638,
            "upper_bound": 383508.0221052632
          },
          "point_estimate": 383170.37342105265,
          "standard_error": 132.59675683283797
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.12458714058207,
            "upper_bound": 787.0696687635585
          },
          "point_estimate": 324.5602519747709,
          "standard_error": 186.59460865196849
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 382999.53976881463,
            "upper_bound": 383457.64796769654
          },
          "point_estimate": 383206.8907997266,
          "standard_error": 118.37475819576034
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201.31026237177568,
            "upper_bound": 611.7216676707966
          },
          "point_estimate": 460.909489064913,
          "standard_error": 104.9700738517009
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332194.3758743236,
            "upper_bound": 332625.89879675326
          },
          "point_estimate": 332392.41030627704,
          "standard_error": 110.7437654664202
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332207.01121212123,
            "upper_bound": 332543.75075757573
          },
          "point_estimate": 332288.28352272726,
          "standard_error": 85.76871636072437
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.072394578274395,
            "upper_bound": 588.6664880264344
          },
          "point_estimate": 175.6195827912321,
          "standard_error": 142.84272498341778
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332141.51860090264,
            "upper_bound": 332567.8890265925
          },
          "point_estimate": 332394.5216292798,
          "standard_error": 108.70622868504984
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 133.9842411532334,
            "upper_bound": 515.9278515644373
          },
          "point_estimate": 370.1700351060716,
          "standard_error": 105.1764847787376
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 288819.6583588436,
            "upper_bound": 289346.93529541447
          },
          "point_estimate": 289048.4964364449,
          "standard_error": 136.40007926970304
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 288840.47156084655,
            "upper_bound": 289158.0586419753
          },
          "point_estimate": 288950.5335600907,
          "standard_error": 64.38312546946955
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.981437582820298,
            "upper_bound": 560.1570011332159
          },
          "point_estimate": 82.70167900793687,
          "standard_error": 146.56345873596896
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 288780.47878205933,
            "upper_bound": 289149.0480103215
          },
          "point_estimate": 288975.0628117914,
          "standard_error": 93.37630254343736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.18363057279274,
            "upper_bound": 662.271797985298
          },
          "point_estimate": 455.95459701025186,
          "standard_error": 161.59357715453692
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318199.2998509317,
            "upper_bound": 318578.34556521743
          },
          "point_estimate": 318381.5250103519,
          "standard_error": 97.44286649177812
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318140.1634057971,
            "upper_bound": 318583.6010869565
          },
          "point_estimate": 318331.119689441,
          "standard_error": 111.99229224811384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.71064873958059,
            "upper_bound": 529.3290158199583
          },
          "point_estimate": 260.55469780901,
          "standard_error": 124.54553342004128
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318065.4071034686,
            "upper_bound": 318385.56794685
          },
          "point_estimate": 318227.54525127047,
          "standard_error": 83.93259140069617
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161.9277706329078,
            "upper_bound": 425.8840966902252
          },
          "point_estimate": 326.96627633486537,
          "standard_error": 68.82761359270283
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166664.97047460044,
            "upper_bound": 166836.513021309
          },
          "point_estimate": 166747.2877181634,
          "standard_error": 43.63110027943633
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166668.0614916286,
            "upper_bound": 166841.46118721462
          },
          "point_estimate": 166703.95062785386,
          "standard_error": 49.03017946118192
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.713428108904452,
            "upper_bound": 246.4703983640181
          },
          "point_estimate": 112.63246558028337,
          "standard_error": 61.332686864148094
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166652.79945942818,
            "upper_bound": 166806.66158942162
          },
          "point_estimate": 166716.22840538458,
          "standard_error": 38.63280484363057
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.98645401664753,
            "upper_bound": 197.0408334254528
          },
          "point_estimate": 145.79548364860793,
          "standard_error": 35.15602920365814
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221672.46664779325,
            "upper_bound": 221932.20934935153
          },
          "point_estimate": 221800.2197355304,
          "standard_error": 66.75512919253335
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221604.2906504065,
            "upper_bound": 222014.2086720867
          },
          "point_estimate": 221773.92983449475,
          "standard_error": 109.95100891514326
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.818312535115226,
            "upper_bound": 376.0021190563289
          },
          "point_estimate": 269.7207302724765,
          "standard_error": 82.6345573430122
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221759.67633582404,
            "upper_bound": 221983.22113206552
          },
          "point_estimate": 221878.60300918593,
          "standard_error": 56.01046017085013
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134.29449502750768,
            "upper_bound": 269.59222046787306
          },
          "point_estimate": 222.57194836100732,
          "standard_error": 34.42973140471805
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/libc_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84260.17412297455,
            "upper_bound": 84413.8183140432
          },
          "point_estimate": 84333.96228780862,
          "standard_error": 39.02090971614704
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84283.72839506173,
            "upper_bound": 84388.57860725309
          },
          "point_estimate": 84317.52291666667,
          "standard_error": 24.267745187942193
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.597363647491823,
            "upper_bound": 195.3828817164431
          },
          "point_estimate": 52.83878771932728,
          "standard_error": 45.47400844520219
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84308.61745033444,
            "upper_bound": 84403.54222524416
          },
          "point_estimate": 84344.32397186148,
          "standard_error": 24.380821267257023
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.361349231487807,
            "upper_bound": 185.21840401379276
          },
          "point_estimate": 130.52000734888708,
          "standard_error": 38.50038353135292
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/libc_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1033595.0968451004,
            "upper_bound": 1035681.0572123016
          },
          "point_estimate": 1034607.0995579804,
          "standard_error": 533.4805502874598
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1033394.9104938272,
            "upper_bound": 1035696.6388888888
          },
          "point_estimate": 1034532.990625,
          "standard_error": 587.2685990109712
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 374.3111916879199,
            "upper_bound": 2928.419113010145
          },
          "point_estimate": 1540.7451194055611,
          "standard_error": 667.4254056687156
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1033586.658394036,
            "upper_bound": 1035001.8937526088
          },
          "point_estimate": 1034231.4671717172,
          "standard_error": 356.2399540857249
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 841.473183961216,
            "upper_bound": 2346.4502784569004
          },
          "point_estimate": 1774.7415531210963,
          "standard_error": 386.39152210282737
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/libc_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2077.174442982671,
            "upper_bound": 2085.0562633254403
          },
          "point_estimate": 2080.825204580406,
          "standard_error": 2.0205597988236965
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2075.572439042946,
            "upper_bound": 2085.574027770985
          },
          "point_estimate": 2078.5799383772683,
          "standard_error": 2.6648396217921135
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.205959643087921,
            "upper_bound": 11.259996156650526
          },
          "point_estimate": 7.2271651909322365,
          "standard_error": 2.5325903578903284
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2075.434505507543,
            "upper_bound": 2082.626009990887
          },
          "point_estimate": 2078.226869319201,
          "standard_error": 1.8403496327555968
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.2312847673910308,
            "upper_bound": 8.847579152700256
          },
          "point_estimate": 6.735954503762396,
          "standard_error": 1.5300809800362458
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86927.874627193,
            "upper_bound": 87152.93834111793
          },
          "point_estimate": 87037.62131569453,
          "standard_error": 57.52613187954925
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86870.87943580543,
            "upper_bound": 87158.46001367054
          },
          "point_estimate": 87075.08708798511,
          "standard_error": 88.53618746354093
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.901969074774044,
            "upper_bound": 364.1741156398881
          },
          "point_estimate": 199.27521021814184,
          "standard_error": 85.91822225565954
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86930.86172248804,
            "upper_bound": 87128.55906651859
          },
          "point_estimate": 87043.55719878207,
          "standard_error": 50.97776412425126
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.33988305658664,
            "upper_bound": 249.12799775072685
          },
          "point_estimate": 191.91056293677897,
          "standard_error": 37.13319932192275
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296824.61315477797,
            "upper_bound": 297478.65088979225
          },
          "point_estimate": 297100.1982378371,
          "standard_error": 171.60937805348306
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296716.4588979223,
            "upper_bound": 297237.14959349594
          },
          "point_estimate": 296988.17847464187,
          "standard_error": 142.1605475549111
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.8607031247969,
            "upper_bound": 621.5998318370176
          },
          "point_estimate": 385.9880057760779,
          "standard_error": 134.5329736131695
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296784.8472758517,
            "upper_bound": 297198.5846665368
          },
          "point_estimate": 297011.4526871503,
          "standard_error": 107.35985170101006
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175.1339278360779,
            "upper_bound": 839.9490667695287
          },
          "point_estimate": 570.7965843733652,
          "standard_error": 209.4332207336521
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278476.5809910714,
            "upper_bound": 278876.2445192308
          },
          "point_estimate": 278657.50761080586,
          "standard_error": 102.89397739501597
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278451.2076923077,
            "upper_bound": 278818.0945054945
          },
          "point_estimate": 278612.7564102564,
          "standard_error": 93.10661427776115
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.92518097163282,
            "upper_bound": 472.349318767914
          },
          "point_estimate": 268.4747154643189,
          "standard_error": 104.33005191350088
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278372.09665641026,
            "upper_bound": 278759.36051007814
          },
          "point_estimate": 278556.0721878122,
          "standard_error": 99.37383364628089
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.43088293883594,
            "upper_bound": 481.7018237553973
          },
          "point_estimate": 341.5604652982163,
          "standard_error": 102.27395490881948
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81030.74596201435,
            "upper_bound": 81152.67604933803
          },
          "point_estimate": 81090.24694939371,
          "standard_error": 31.197729196444715
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80997.20155902005,
            "upper_bound": 81176.06268559763
          },
          "point_estimate": 81084.57906458798,
          "standard_error": 37.922393808972394
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.37158711290757,
            "upper_bound": 190.36752431146877
          },
          "point_estimate": 101.30136735521982,
          "standard_error": 45.54495005746913
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80987.33899716311,
            "upper_bound": 81140.8688390152
          },
          "point_estimate": 81046.60605096462,
          "standard_error": 39.350303564539736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.97421315564383,
            "upper_bound": 130.52205695970693
          },
          "point_estimate": 103.88875848988232,
          "standard_error": 19.3097627582182
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201879.2405501213,
            "upper_bound": 202445.11303174603
          },
          "point_estimate": 202137.44687742504,
          "standard_error": 145.7457393018589
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201742.38055555552,
            "upper_bound": 202451.48117283953
          },
          "point_estimate": 202000.12083333335,
          "standard_error": 170.78570799087478
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.916670523560875,
            "upper_bound": 755.0102451144055
          },
          "point_estimate": 405.58478488278894,
          "standard_error": 180.7122123860826
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201842.64312080535,
            "upper_bound": 202171.26506926637
          },
          "point_estimate": 201991.2624242424,
          "standard_error": 83.29900898853394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182.6498073323341,
            "upper_bound": 635.191534059136
          },
          "point_estimate": 486.40851367096064,
          "standard_error": 119.10502601391184
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12251.061952790356,
            "upper_bound": 12274.932732215108
          },
          "point_estimate": 12262.910435450984,
          "standard_error": 6.094997340828068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12251.056178354687,
            "upper_bound": 12277.823117554508
          },
          "point_estimate": 12259.815436053046,
          "standard_error": 6.780248142317233
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.949444800920639,
            "upper_bound": 36.30395468251567
          },
          "point_estimate": 13.907807894523293,
          "standard_error": 8.698528011187301
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12252.90944318172,
            "upper_bound": 12272.224510416996
          },
          "point_estimate": 12259.131134677864,
          "standard_error": 5.057617122710288
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.829909446184333,
            "upper_bound": 26.695133585902298
          },
          "point_estimate": 20.33168388807377,
          "standard_error": 4.244683290701428
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11763.051371749736,
            "upper_bound": 11789.913576137977
          },
          "point_estimate": 11775.718084352417,
          "standard_error": 6.868028601713341
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11760.92556821691,
            "upper_bound": 11783.257733463037
          },
          "point_estimate": 11775.584662775616,
          "standard_error": 4.4042604017082345
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4470573154261157,
            "upper_bound": 35.50798493218856
          },
          "point_estimate": 5.867423047583221,
          "standard_error": 9.212312869848096
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11773.28886777466,
            "upper_bound": 11787.134996300456
          },
          "point_estimate": 11780.40459009214,
          "standard_error": 3.5075298597421245
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.43931258870042,
            "upper_bound": 32.26977322112966
          },
          "point_estimate": 22.887574007266267,
          "standard_error": 6.47665921924733
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 542650.9393407959,
            "upper_bound": 543476.743659204
          },
          "point_estimate": 543061.5424626865,
          "standard_error": 210.950021577453
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 542527.6847014925,
            "upper_bound": 543651.0597014925
          },
          "point_estimate": 543063.9567164179,
          "standard_error": 283.09108322393007
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.40973522993303,
            "upper_bound": 1171.2871717428131
          },
          "point_estimate": 748.4087217877237,
          "standard_error": 258.26993548389066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 542782.7717502519,
            "upper_bound": 543732.8529715061
          },
          "point_estimate": 543289.1867028493,
          "standard_error": 236.63437677109252
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393.6190718946545,
            "upper_bound": 900.0409962568501
          },
          "point_estimate": 703.9895908387196,
          "standard_error": 130.50902463836556
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53747.90981128748,
            "upper_bound": 53897.87200170488
          },
          "point_estimate": 53815.28549182833,
          "standard_error": 38.561296080090464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53730.61522045855,
            "upper_bound": 53878.01728395062
          },
          "point_estimate": 53762.54396707819,
          "standard_error": 46.1991079828826
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.725701036157043,
            "upper_bound": 187.71449288962893
          },
          "point_estimate": 108.98869402556495,
          "standard_error": 49.298185113946865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53703.73944831583,
            "upper_bound": 53818.18798931633
          },
          "point_estimate": 53754.635186147185,
          "standard_error": 29.78301311800546
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.97357137889398,
            "upper_bound": 179.17264382709246
          },
          "point_estimate": 128.75358885010272,
          "standard_error": 36.72241322049165
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58424.65335470257,
            "upper_bound": 58569.230333936226
          },
          "point_estimate": 58493.36980573418,
          "standard_error": 37.2458312566154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58397.28054662379,
            "upper_bound": 58653.20900321544
          },
          "point_estimate": 58440.295116559486,
          "standard_error": 60.63356205532121
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.83470998816963,
            "upper_bound": 182.211765193539
          },
          "point_estimate": 69.83045876026478,
          "standard_error": 47.33090505434594
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58401.68245996625,
            "upper_bound": 58572.91964704238
          },
          "point_estimate": 58470.71086566167,
          "standard_error": 48.4210788485151
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.92477123467389,
            "upper_bound": 144.88454369045888
          },
          "point_estimate": 124.3057709935139,
          "standard_error": 20.81053089909348
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112914.7937104407,
            "upper_bound": 113002.81161971312
          },
          "point_estimate": 112959.57075014788,
          "standard_error": 22.572734796225312
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112896.57789855072,
            "upper_bound": 113023.67585403728
          },
          "point_estimate": 112967.32598343684,
          "standard_error": 33.9950448835062
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.561376116857677,
            "upper_bound": 124.8778588078273
          },
          "point_estimate": 85.07366995703603,
          "standard_error": 28.16880957243703
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112910.05975813228,
            "upper_bound": 113014.55865463497
          },
          "point_estimate": 112978.36967008148,
          "standard_error": 27.09218325167228
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.994230974967614,
            "upper_bound": 93.05264106210905
          },
          "point_estimate": 75.075149068058,
          "standard_error": 12.733278282620113
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63904.68712716784,
            "upper_bound": 64044.88823391704
          },
          "point_estimate": 63978.97433701464,
          "standard_error": 35.87645332482967
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63885.59787135018,
            "upper_bound": 64064.47896195515
          },
          "point_estimate": 64003.105232216345,
          "standard_error": 35.58920816707776
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.413521196544766,
            "upper_bound": 202.43916896918557
          },
          "point_estimate": 86.94556073154179,
          "standard_error": 48.104795128537376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63887.91897831004,
            "upper_bound": 64029.45929158526
          },
          "point_estimate": 63959.98824526444,
          "standard_error": 35.7632892506384
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.13554468180739,
            "upper_bound": 152.5912731983789
          },
          "point_estimate": 119.69395770876017,
          "standard_error": 26.44969796436897
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22945.707521995464,
            "upper_bound": 23014.649006758624
          },
          "point_estimate": 22979.973670345174,
          "standard_error": 17.648356391026915
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22933.455308641976,
            "upper_bound": 23040.123428571427
          },
          "point_estimate": 22977.184285714284,
          "standard_error": 24.432386380616254
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.985626333952109,
            "upper_bound": 102.33205199065256
          },
          "point_estimate": 73.81141114672823,
          "standard_error": 24.206312588454814
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22948.06553463913,
            "upper_bound": 23016.058810799776
          },
          "point_estimate": 22980.900731395588,
          "standard_error": 18.295078713193227
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.842784425635216,
            "upper_bound": 72.66122823199562
          },
          "point_estimate": 58.93492471755992,
          "standard_error": 9.670588300424088
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11544.26106588905,
            "upper_bound": 11559.380541613638
          },
          "point_estimate": 11551.888211293426,
          "standard_error": 3.876645287236508
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11541.27970887687,
            "upper_bound": 11565.249549262911
          },
          "point_estimate": 11552.906395163856,
          "standard_error": 5.731577383970482
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8813919310272693,
            "upper_bound": 21.626399170619152
          },
          "point_estimate": 17.181890022671922,
          "standard_error": 4.924955228931114
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11542.970479194124,
            "upper_bound": 11554.757207361989
          },
          "point_estimate": 11549.561114164231,
          "standard_error": 2.9812114604859334
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.755556851046173,
            "upper_bound": 15.855154257826074
          },
          "point_estimate": 12.935560846763542,
          "standard_error": 2.074637298432416
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10301.35933127322,
            "upper_bound": 10317.28868321513
          },
          "point_estimate": 10309.319431036813,
          "standard_error": 4.078054916906953
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10299.272333671055,
            "upper_bound": 10318.438947990544
          },
          "point_estimate": 10309.598540189125,
          "standard_error": 6.238199728528879
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9881336051518542,
            "upper_bound": 23.02486872030451
          },
          "point_estimate": 11.600661325962324,
          "standard_error": 6.068797744860684
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10303.265434362324,
            "upper_bound": 10316.267881297446
          },
          "point_estimate": 10310.304592428847,
          "standard_error": 3.328008618522662
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.6783461695953,
            "upper_bound": 17.45991688568316
          },
          "point_estimate": 13.615166472339356,
          "standard_error": 2.537612648706705
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10251.999251040494,
            "upper_bound": 10268.769867714884
          },
          "point_estimate": 10260.399248931795,
          "standard_error": 4.2774464274428405
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10248.964272949586,
            "upper_bound": 10268.231059537246
          },
          "point_estimate": 10262.518932064926,
          "standard_error": 4.598333462480577
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8113899613741533,
            "upper_bound": 23.31910338820287
          },
          "point_estimate": 11.147756734933004,
          "standard_error": 5.736384920794601
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10255.130564386056,
            "upper_bound": 10267.517533996986
          },
          "point_estimate": 10260.82822769781,
          "standard_error": 3.131690780154418
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.485042038582413,
            "upper_bound": 18.91010407625819
          },
          "point_estimate": 14.180355667710742,
          "standard_error": 3.0945113240882756
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55053.17716021075,
            "upper_bound": 55179.71381026261
          },
          "point_estimate": 55116.01147286962,
          "standard_error": 32.556560824613456
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55001.34259818731,
            "upper_bound": 55224.72107250756
          },
          "point_estimate": 55099.94002211912,
          "standard_error": 58.36423850453403
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.59968256479645,
            "upper_bound": 172.2438762975522
          },
          "point_estimate": 148.24147153359456,
          "standard_error": 42.56881478737656
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55057.42030396989,
            "upper_bound": 55178.742302119965
          },
          "point_estimate": 55134.34231569035,
          "standard_error": 30.80762763817946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.2489583870054,
            "upper_bound": 126.24873582904276
          },
          "point_estimate": 108.4589955749569,
          "standard_error": 14.3659075551374
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71646.32081290735,
            "upper_bound": 71929.62279324346
          },
          "point_estimate": 71782.38721871395,
          "standard_error": 72.58595034881651
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71547.42288801572,
            "upper_bound": 71997.29115913557
          },
          "point_estimate": 71735.45411755075,
          "standard_error": 112.34517501384418
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.95274550156167,
            "upper_bound": 395.391248622181
          },
          "point_estimate": 284.4445844736639,
          "standard_error": 94.98933912285617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71609.17181644772,
            "upper_bound": 71891.63095853582
          },
          "point_estimate": 71770.93012017453,
          "standard_error": 73.45980980647768
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131.5575452503133,
            "upper_bound": 298.3278422218909
          },
          "point_estimate": 241.90046122415225,
          "standard_error": 42.14034749708532
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106005.7482795699,
            "upper_bound": 106461.35304496576
          },
          "point_estimate": 106224.98683121536,
          "standard_error": 116.91919284648452
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105911.7741935484,
            "upper_bound": 106473.4802052786
          },
          "point_estimate": 106175.98012381882,
          "standard_error": 160.35503980491748
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.63114428892139,
            "upper_bound": 675.334070561734
          },
          "point_estimate": 425.40468338110503,
          "standard_error": 152.60496487648223
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106002.8282770611,
            "upper_bound": 106584.5801055232
          },
          "point_estimate": 106247.95891381346,
          "standard_error": 152.33234191119485
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208.70188885132575,
            "upper_bound": 508.1283904585357
          },
          "point_estimate": 388.5014215895496,
          "standard_error": 79.9471697923142
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1069510.1084116774,
            "upper_bound": 1070657.9684033613
          },
          "point_estimate": 1070096.310339636,
          "standard_error": 293.5692425148811
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1069187.4871323528,
            "upper_bound": 1071072.8588235294
          },
          "point_estimate": 1070090.588235294,
          "standard_error": 436.2942569883275
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 214.15865913920447,
            "upper_bound": 1749.603147173684
          },
          "point_estimate": 1158.6998458998016,
          "standard_error": 420.76866465871825
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1069433.0754370005,
            "upper_bound": 1070703.9455923778
          },
          "point_estimate": 1070067.6849503438,
          "standard_error": 322.5591940185979
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 571.5645162048816,
            "upper_bound": 1196.157559986679
          },
          "point_estimate": 978.6328851724268,
          "standard_error": 160.78380897593303
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40370203.54730158,
            "upper_bound": 40433582.72892857
          },
          "point_estimate": 40403083.35003968,
          "standard_error": 16247.689041264295
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40369947.25,
            "upper_bound": 40444980.44444445
          },
          "point_estimate": 40405591.04464286,
          "standard_error": 16290.703922754514
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5792.240109667182,
            "upper_bound": 91258.50912650782
          },
          "point_estimate": 45203.32006414977,
          "standard_error": 23271.42805287336
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40396452.886446886,
            "upper_bound": 40447995.8728263
          },
          "point_estimate": 40427785.46753247,
          "standard_error": 13143.752729239372
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26904.263869061928,
            "upper_bound": 71771.40730799678
          },
          "point_estimate": 54154.14175372412,
          "standard_error": 11868.244354229471
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1009460.509436213,
            "upper_bound": 1011543.4195985897
          },
          "point_estimate": 1010393.9379075504,
          "standard_error": 534.3786081600839
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1009351.5873873874,
            "upper_bound": 1011286.4024024024
          },
          "point_estimate": 1009523.9030405404,
          "standard_error": 538.1272741238986
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.47505257496593,
            "upper_bound": 2575.3668689176907
          },
          "point_estimate": 259.6034223505469,
          "standard_error": 662.6002273149188
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1009329.5055770056,
            "upper_bound": 1010620.9777077968
          },
          "point_estimate": 1009858.7127413128,
          "standard_error": 353.71995091670794
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232.46653674716828,
            "upper_bound": 2434.6165688773067
          },
          "point_estimate": 1784.5171190346657,
          "standard_error": 530.7807402497775
        }
      }
    },
    "memmem/regex/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/regex_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38101.69191867575,
            "upper_bound": 38149.970267557655
          },
          "point_estimate": 38125.30761879804,
          "standard_error": 12.344747157230156
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38093.30428895877,
            "upper_bound": 38152.07232704402
          },
          "point_estimate": 38130.15303983229,
          "standard_error": 17.432253244017783
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.533796631312035,
            "upper_bound": 72.70392866207659
          },
          "point_estimate": 48.374227390664565,
          "standard_error": 16.10117553041281
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38103.52538185085,
            "upper_bound": 38161.099690874624
          },
          "point_estimate": 38133.52400827683,
          "standard_error": 14.586250996333876
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.918958265671712,
            "upper_bound": 53.10858726406121
          },
          "point_estimate": 41.1021478886035,
          "standard_error": 7.83596077942877
        }
      }
    },
    "memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/regex_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35379.539337112255,
            "upper_bound": 35435.78609627362
          },
          "point_estimate": 35408.365218930165,
          "standard_error": 14.415510127480234
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35366.05574488802,
            "upper_bound": 35449.29714378448
          },
          "point_estimate": 35418.648949784394,
          "standard_error": 18.518086974095844
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.322204791811815,
            "upper_bound": 87.22159063911062
          },
          "point_estimate": 50.091163132125814,
          "standard_error": 20.150598046177706
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35407.757241966894,
            "upper_bound": 35449.523402976025
          },
          "point_estimate": 35428.53592736377,
          "standard_error": 10.741069480533456
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.514446460000343,
            "upper_bound": 59.57829494508663
          },
          "point_estimate": 48.08800248661333,
          "standard_error": 8.639246895205172
        }
      }
    },
    "memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/regex_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2617138.697168368,
            "upper_bound": 2620748.648086097
          },
          "point_estimate": 2618905.610637755,
          "standard_error": 924.33332818262
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2616833.697916667,
            "upper_bound": 2620818.156462585
          },
          "point_estimate": 2618825.523809524,
          "standard_error": 1046.3177904881584
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 810.0710043684799,
            "upper_bound": 5291.575806055585
          },
          "point_estimate": 2289.880954346516,
          "standard_error": 1096.388241270536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2617200.2169887116,
            "upper_bound": 2619249.1892281105
          },
          "point_estimate": 2618045.763450835,
          "standard_error": 519.4481428112507
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1459.0376876397888,
            "upper_bound": 4139.08760955582
          },
          "point_estimate": 3080.599150016413,
          "standard_error": 691.4289515665615
        }
      }
    },
    "memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/regex_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5185.806358137063,
            "upper_bound": 5189.1266332287705
          },
          "point_estimate": 5187.3924953260885,
          "standard_error": 0.8498467187231828
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5185.11098812095,
            "upper_bound": 5189.370124790017
          },
          "point_estimate": 5186.870842332613,
          "standard_error": 0.9355146458995096
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.17488595585844752,
            "upper_bound": 5.298421642435451
          },
          "point_estimate": 1.9212386807940285,
          "standard_error": 1.2673717260971815
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5185.383428329344,
            "upper_bound": 5190.889959208005
          },
          "point_estimate": 5188.7594135741865,
          "standard_error": 1.4086277989005456
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3813427733841597,
            "upper_bound": 3.629076897420769
          },
          "point_estimate": 2.8260901384385635,
          "standard_error": 0.5759245354253302
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.586755298188187,
            "upper_bound": 14.618405389018768
          },
          "point_estimate": 14.602604002092187,
          "standard_error": 0.008122932318183963
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.58197794824794,
            "upper_bound": 14.627007033876309
          },
          "point_estimate": 14.600290951807995,
          "standard_error": 0.012351606879687222
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007342536225580906,
            "upper_bound": 0.04606627761084315
          },
          "point_estimate": 0.0292364404118896,
          "standard_error": 0.009650843035524229
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.59356017339105,
            "upper_bound": 14.627437366885458
          },
          "point_estimate": 14.611671383562276,
          "standard_error": 0.008502946981184404
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016389339202257106,
            "upper_bound": 0.03343313161336694
          },
          "point_estimate": 0.027087203255943677,
          "standard_error": 0.004378413113358135
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.556457851504485,
            "upper_bound": 10.575921413004492
          },
          "point_estimate": 10.565991785075909,
          "standard_error": 0.004975160355082818
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.553865205379196,
            "upper_bound": 10.57757535635745
          },
          "point_estimate": 10.562895721285022,
          "standard_error": 0.005777759564716027
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010359664176634478,
            "upper_bound": 0.027983924139270137
          },
          "point_estimate": 0.016787631183815648,
          "standard_error": 0.007045628516978417
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.556819630647343,
            "upper_bound": 10.569765782941376
          },
          "point_estimate": 10.562954827168248,
          "standard_error": 0.0032373241341296074
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008254978820325953,
            "upper_bound": 0.021888065341973535
          },
          "point_estimate": 0.016552003864389576,
          "standard_error": 0.0034717676872217766
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.555934084739532,
            "upper_bound": 10.578591562574156
          },
          "point_estimate": 10.56649139175802,
          "standard_error": 0.005820604474726457
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.554209063660554,
            "upper_bound": 10.57967800203799
          },
          "point_estimate": 10.5596425017419,
          "standard_error": 0.0067958368545955015
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016850457726935316,
            "upper_bound": 0.03177137200065642
          },
          "point_estimate": 0.009556603177538632,
          "standard_error": 0.008034155761731589
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.555970977630016,
            "upper_bound": 10.576449935261063
          },
          "point_estimate": 10.565573440442853,
          "standard_error": 0.005239800807114756
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0069949219150096656,
            "upper_bound": 0.024248648153059712
          },
          "point_estimate": 0.019372636441617862,
          "standard_error": 0.004194571576554278
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.24278202521491,
            "upper_bound": 27.28543260588225
          },
          "point_estimate": 27.26247254943943,
          "standard_error": 0.010939000010567926
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.229920918137996,
            "upper_bound": 27.284417472460053
          },
          "point_estimate": 27.25328332900274,
          "standard_error": 0.014357430915889704
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006410158810655741,
            "upper_bound": 0.06152796178774453
          },
          "point_estimate": 0.03642768808471846,
          "standard_error": 0.013157187727984533
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.244693637470014,
            "upper_bound": 27.27356553566373
          },
          "point_estimate": 27.25693418685004,
          "standard_error": 0.007293303207768005
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017801150465077444,
            "upper_bound": 0.04853226757134515
          },
          "point_estimate": 0.03636568178856587,
          "standard_error": 0.008582968331363783
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.241021232856449,
            "upper_bound": 15.284490233898335
          },
          "point_estimate": 15.262614578598225,
          "standard_error": 0.011099615058919083
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.2341120113656,
            "upper_bound": 15.2911002882839
          },
          "point_estimate": 15.261688350046532,
          "standard_error": 0.013685133305982837
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011945482098996516,
            "upper_bound": 0.06274685417045603
          },
          "point_estimate": 0.03233643346447393,
          "standard_error": 0.01358505862804546
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.255959091976916,
            "upper_bound": 15.300673945374408
          },
          "point_estimate": 15.276808102079194,
          "standard_error": 0.012019240523584478
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020127506047566954,
            "upper_bound": 0.047363003681223426
          },
          "point_estimate": 0.037021621912581154,
          "standard_error": 0.006976057024835731
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.767966245166235,
            "upper_bound": 11.77932652377272
          },
          "point_estimate": 11.773537014011506,
          "standard_error": 0.002917062717944591
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.765558594422153,
            "upper_bound": 11.78437494070748
          },
          "point_estimate": 11.7698632836903,
          "standard_error": 0.00541127480823993
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00196305622712639,
            "upper_bound": 0.01530131666564915
          },
          "point_estimate": 0.011520562215143512,
          "standard_error": 0.003883936707914832
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.766468126181625,
            "upper_bound": 11.779790364864056
          },
          "point_estimate": 11.773149081573363,
          "standard_error": 0.0035651256205148927
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0063012663137662715,
            "upper_bound": 0.011246566210854757
          },
          "point_estimate": 0.009763589522816444,
          "standard_error": 0.0012814219275498243
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.174337313526312,
            "upper_bound": 10.201992644878652
          },
          "point_estimate": 10.187565086392414,
          "standard_error": 0.007129376989155401
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.167877425410849,
            "upper_bound": 10.217941948004984
          },
          "point_estimate": 10.178483853267045,
          "standard_error": 0.012111783701602378
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032857456938918616,
            "upper_bound": 0.03665817556406612
          },
          "point_estimate": 0.021657188961202813,
          "standard_error": 0.00927660808564127
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.17006591719065,
            "upper_bound": 10.189970111912803
          },
          "point_estimate": 10.177332490043554,
          "standard_error": 0.005098564817923854
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012106787812968403,
            "upper_bound": 0.02764017504080534
          },
          "point_estimate": 0.023776483301146435,
          "standard_error": 0.003669446820470504
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.318865834214336,
            "upper_bound": 11.344712681909945
          },
          "point_estimate": 11.331392269111872,
          "standard_error": 0.006617889038417652
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.314475981625872,
            "upper_bound": 11.358490071081231
          },
          "point_estimate": 11.32845915293588,
          "standard_error": 0.010325440000309364
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028089314956041027,
            "upper_bound": 0.04170607399432778
          },
          "point_estimate": 0.02213591516396514,
          "standard_error": 0.009749584991091223
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.32256024896394,
            "upper_bound": 11.351072315507365
          },
          "point_estimate": 11.336688596910056,
          "standard_error": 0.007659437186052797
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012554682509635665,
            "upper_bound": 0.02666733214851616
          },
          "point_estimate": 0.022109458966293,
          "standard_error": 0.0035007886486816487
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.271393127553347,
            "upper_bound": 11.313997596355335
          },
          "point_estimate": 11.291901327474395,
          "standard_error": 0.010904399863119816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.261480632639898,
            "upper_bound": 11.317483797643492
          },
          "point_estimate": 11.289025517737064,
          "standard_error": 0.015633924787901377
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010826451475011956,
            "upper_bound": 0.06322355634274572
          },
          "point_estimate": 0.03654623717268054,
          "standard_error": 0.012792201668443371
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.26980413979744,
            "upper_bound": 11.306545183003353
          },
          "point_estimate": 11.286318048171193,
          "standard_error": 0.009334812699522986
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020359115922471017,
            "upper_bound": 0.04638505537767922
          },
          "point_estimate": 0.03627869342466846,
          "standard_error": 0.006855010328803912
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.563438321713484,
            "upper_bound": 10.582232399783775
          },
          "point_estimate": 10.57293077354966,
          "standard_error": 0.004820765563789267
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.559881784002734,
            "upper_bound": 10.588470740522576
          },
          "point_estimate": 10.572881707702924,
          "standard_error": 0.007218628586119219
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00561973648857491,
            "upper_bound": 0.026778758894134037
          },
          "point_estimate": 0.02062255582210864,
          "standard_error": 0.005793874504167657
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.55559922557508,
            "upper_bound": 10.57328175426774
          },
          "point_estimate": 10.562990028944084,
          "standard_error": 0.004531837561392602
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009724638545194245,
            "upper_bound": 0.01982667260165723
          },
          "point_estimate": 0.016137293478626455,
          "standard_error": 0.002574423741741938
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.772067018279206,
            "upper_bound": 11.846225196952872
          },
          "point_estimate": 11.799667886158252,
          "standard_error": 0.02120146186376603
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.769144580636844,
            "upper_bound": 11.794756992268384
          },
          "point_estimate": 11.774963724657946,
          "standard_error": 0.008660210595031186
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0026758018206758088,
            "upper_bound": 0.03204722576258582
          },
          "point_estimate": 0.012562507435341024,
          "standard_error": 0.009356417239696144
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.767971081382528,
            "upper_bound": 11.782526425661937
          },
          "point_estimate": 11.77490734100208,
          "standard_error": 0.003799039748585584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008311926961693052,
            "upper_bound": 0.10829521747603275
          },
          "point_estimate": 0.07050840669285892,
          "standard_error": 0.03470344198992858
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.67566953692409,
            "upper_bound": 14.735251733069166
          },
          "point_estimate": 14.704204923974237,
          "standard_error": 0.015231531434059751
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.660891254902168,
            "upper_bound": 14.732017081461716
          },
          "point_estimate": 14.707768543429884,
          "standard_error": 0.02382106056203459
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004797885168352813,
            "upper_bound": 0.09788087168901809
          },
          "point_estimate": 0.0490783002972105,
          "standard_error": 0.022679694890634938
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.68121602246452,
            "upper_bound": 14.727340625798853
          },
          "point_estimate": 14.709517613935043,
          "standard_error": 0.011916727306158255
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028543579592700135,
            "upper_bound": 0.0662268814014688
          },
          "point_estimate": 0.050649807665046225,
          "standard_error": 0.010414562125877916
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391296.7319098822,
            "upper_bound": 392108.9065611239
          },
          "point_estimate": 391725.561229732,
          "standard_error": 207.6842825397381
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391106.2634408602,
            "upper_bound": 392180.0010752688
          },
          "point_estimate": 391915.91645331966,
          "standard_error": 254.3881740840682
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.51826759741551,
            "upper_bound": 1103.957313913378
          },
          "point_estimate": 423.7520987851743,
          "standard_error": 296.80151411650803
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391713.5677878087,
            "upper_bound": 392291.96036487416
          },
          "point_estimate": 392059.39941348974,
          "standard_error": 146.92795952485255
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 304.2496110732538,
            "upper_bound": 873.347731265883
          },
          "point_estimate": 691.6987545144455,
          "standard_error": 140.908777702903
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263458.21999424894,
            "upper_bound": 263954.35318840574
          },
          "point_estimate": 263700.2533356337,
          "standard_error": 127.00374473004338
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263311.51469404186,
            "upper_bound": 264072.39734299516
          },
          "point_estimate": 263672.8600759144,
          "standard_error": 205.4281906277584
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.78410277158441,
            "upper_bound": 720.866010921901
          },
          "point_estimate": 423.6373644354756,
          "standard_error": 163.53822942738728
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263425.1825960658,
            "upper_bound": 263838.23536789295
          },
          "point_estimate": 263588.73497082625,
          "standard_error": 106.33967702635296
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253.61140394804536,
            "upper_bound": 512.0801034200589
          },
          "point_estimate": 423.1543948063155,
          "standard_error": 65.99869657504118
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 583375.9987385675,
            "upper_bound": 584113.6494480663
          },
          "point_estimate": 583770.707314185,
          "standard_error": 189.42365039478128
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 583293.7823129252,
            "upper_bound": 584303.8587301588
          },
          "point_estimate": 583877.0152116402,
          "standard_error": 248.1837094659427
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.49528462879633,
            "upper_bound": 1034.3142713991956
          },
          "point_estimate": 661.1384602995753,
          "standard_error": 255.97472314553764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 583680.2369755647,
            "upper_bound": 584249.5202496052
          },
          "point_estimate": 584028.5807462379,
          "standard_error": 147.3782073545461
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 264.8156021311405,
            "upper_bound": 827.0747432876708
          },
          "point_estimate": 633.9737260085843,
          "standard_error": 142.29031522952098
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649552.4129195906,
            "upper_bound": 650144.1469172932
          },
          "point_estimate": 649809.8706954888,
          "standard_error": 153.2561091210905
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649484.754385965,
            "upper_bound": 650024.7280701754
          },
          "point_estimate": 649697.5800438595,
          "standard_error": 131.00874637653632
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.89204626965543,
            "upper_bound": 625.3220866176007
          },
          "point_estimate": 338.96212841189146,
          "standard_error": 148.56171248390743
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649423.5097860257,
            "upper_bound": 650292.113283208
          },
          "point_estimate": 649748.2044657097,
          "standard_error": 226.03580389159677
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171.51795914178123,
            "upper_bound": 734.3303350417598
          },
          "point_estimate": 512.9969239118832,
          "standard_error": 168.5766124813801
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1247298.2607243385,
            "upper_bound": 1248457.267
          },
          "point_estimate": 1247890.296100529,
          "standard_error": 298.28083756863975
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1247003.2366666666,
            "upper_bound": 1248828.9962962964
          },
          "point_estimate": 1248039.8066666666,
          "standard_error": 562.4920285469824
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.91014673302992,
            "upper_bound": 1704.2338437437872
          },
          "point_estimate": 1278.1637141969704,
          "standard_error": 423.92403202264614
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1247285.614754345,
            "upper_bound": 1248573.2699296223
          },
          "point_estimate": 1248036.9183549783,
          "standard_error": 326.42940323959016
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 633.1930939086935,
            "upper_bound": 1183.2984381003544
          },
          "point_estimate": 999.8774495081236,
          "standard_error": 140.36292648584117
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 261836.3048100377,
            "upper_bound": 262596.7391606715
          },
          "point_estimate": 262196.1490441932,
          "standard_error": 194.9689164598905
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 261684.25205549845,
            "upper_bound": 262814.7601918465
          },
          "point_estimate": 261974.7649880096,
          "standard_error": 264.6497028289197
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.38562299964607,
            "upper_bound": 1094.8312582607214
          },
          "point_estimate": 648.2860122316296,
          "standard_error": 262.2450059706111
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 261796.13781129292,
            "upper_bound": 262270.26435035
          },
          "point_estimate": 262015.06527141924,
          "standard_error": 121.76233594280106
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307.3652556532505,
            "upper_bound": 800.326442190378
          },
          "point_estimate": 649.3768695682295,
          "standard_error": 123.04152318731688
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 250838.60975862076,
            "upper_bound": 251166.41014511496
          },
          "point_estimate": 251015.22887520524,
          "standard_error": 84.0693797350564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 250846.78068965516,
            "upper_bound": 251194.08954022988
          },
          "point_estimate": 251067.05960591132,
          "standard_error": 84.45398351169761
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.511574823277144,
            "upper_bound": 436.1141100505331
          },
          "point_estimate": 231.28413520425147,
          "standard_error": 101.48510896904918
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 251004.7808646025,
            "upper_bound": 251187.1330010653
          },
          "point_estimate": 251108.761325571,
          "standard_error": 45.58584134867164
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120.38911905544484,
            "upper_bound": 380.6035915659028
          },
          "point_estimate": 278.9226464270567,
          "standard_error": 71.81834986926677
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274440.2966374269,
            "upper_bound": 274817.9315371763
          },
          "point_estimate": 274632.216186299,
          "standard_error": 96.87364067631694
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274414.25187969924,
            "upper_bound": 274902.515037594
          },
          "point_estimate": 274577.36716791976,
          "standard_error": 113.82776058850598
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.065857343219413,
            "upper_bound": 608.6927523515017
          },
          "point_estimate": 307.2536717381558,
          "standard_error": 161.16230311784508
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274527.021895128,
            "upper_bound": 274900.91140319314
          },
          "point_estimate": 274698.4774924324,
          "standard_error": 94.7205832189949
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179.96267017389974,
            "upper_bound": 414.9069307101664
          },
          "point_estimate": 322.4975789027143,
          "standard_error": 61.41209420382144
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 623289.6197469733,
            "upper_bound": 624586.3102763317
          },
          "point_estimate": 623941.4726358622,
          "standard_error": 331.28163256837865
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 623301.5983050847,
            "upper_bound": 624747.9322033898
          },
          "point_estimate": 623914.8851224105,
          "standard_error": 297.8126002471314
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.5030749888211,
            "upper_bound": 1871.1844010171496
          },
          "point_estimate": 883.6245585497898,
          "standard_error": 498.090667125728
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 623566.7037583133,
            "upper_bound": 624442.1427063614
          },
          "point_estimate": 623994.7033237949,
          "standard_error": 219.35928257287264
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481.2981010273849,
            "upper_bound": 1494.5093988158394
          },
          "point_estimate": 1106.3733895667317,
          "standard_error": 254.33893595733508
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246385.57958781777,
            "upper_bound": 247121.7240543221
          },
          "point_estimate": 246662.93393044823,
          "standard_error": 208.0505989491775
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246359.6629129129,
            "upper_bound": 246615.3423423423
          },
          "point_estimate": 246457.87972972973,
          "standard_error": 78.83462864532196
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.258107332201138,
            "upper_bound": 331.0414084978445
          },
          "point_estimate": 189.5351576711205,
          "standard_error": 95.71797597983988
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246307.2650717043,
            "upper_bound": 246572.71264346535
          },
          "point_estimate": 246418.41103896103,
          "standard_error": 68.71243359209471
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.17760000520404,
            "upper_bound": 1065.4094995361231
          },
          "point_estimate": 693.8291971997226,
          "standard_error": 335.56480197445103
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123795.25338516358,
            "upper_bound": 123949.9912215946
          },
          "point_estimate": 123873.73175440016,
          "standard_error": 39.61118522972469
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123768.28174603176,
            "upper_bound": 123991.91836734694
          },
          "point_estimate": 123853.78620019436,
          "standard_error": 59.8738839752156
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.367606923292412,
            "upper_bound": 233.7451093422587
          },
          "point_estimate": 179.24069962397647,
          "standard_error": 56.36172948273763
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123752.37065076928,
            "upper_bound": 123962.6465800844
          },
          "point_estimate": 123872.45003092146,
          "standard_error": 56.481872309870035
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.94611204999215,
            "upper_bound": 163.67333664926127
          },
          "point_estimate": 131.8246904092609,
          "standard_error": 22.168523829887388
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292918.76483492064,
            "upper_bound": 293330.8433558571
          },
          "point_estimate": 293140.87984634924,
          "standard_error": 106.10558961705776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292845.2522222222,
            "upper_bound": 293400.41260000004
          },
          "point_estimate": 293318.601,
          "standard_error": 146.64619803695908
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.12021291853815,
            "upper_bound": 580.0636814618147
          },
          "point_estimate": 178.32060139416694,
          "standard_error": 151.30210178519914
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292977.5988000752,
            "upper_bound": 293396.0159042857
          },
          "point_estimate": 293214.4865038961,
          "standard_error": 106.20409452941692
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.0111678490527,
            "upper_bound": 436.2398594020441
          },
          "point_estimate": 353.98144917754007,
          "standard_error": 76.66231500755141
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56310.16085825028,
            "upper_bound": 56454.15368973328
          },
          "point_estimate": 56375.948884766825,
          "standard_error": 36.91262453135165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56268.99031007752,
            "upper_bound": 56442.47820844099
          },
          "point_estimate": 56370.04209302325,
          "standard_error": 41.584904865683754
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.72123408240676,
            "upper_bound": 185.4011379875384
          },
          "point_estimate": 120.57551724307044,
          "standard_error": 45.69932182580291
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56314.85811800172,
            "upper_bound": 56434.18349599404
          },
          "point_estimate": 56368.96802979966,
          "standard_error": 30.019636496237293
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.182772242738274,
            "upper_bound": 168.0869959975109
          },
          "point_estimate": 123.22711093558988,
          "standard_error": 32.002853447290605
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/regex_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72169.51689188679,
            "upper_bound": 72335.40902575817
          },
          "point_estimate": 72253.8920802171,
          "standard_error": 42.5241555637823
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72127.68323392975,
            "upper_bound": 72361.16600397615
          },
          "point_estimate": 72265.77364385118,
          "standard_error": 57.37231292755776
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.691966371468567,
            "upper_bound": 247.08992493733223
          },
          "point_estimate": 152.12465252788124,
          "standard_error": 53.84748959935125
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72231.02449159305,
            "upper_bound": 72395.51529795585
          },
          "point_estimate": 72332.5824378405,
          "standard_error": 41.49729243300628
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.8575843778757,
            "upper_bound": 178.5657678871915
          },
          "point_estimate": 141.12864174190466,
          "standard_error": 25.48428188334694
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/regex_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 767643.8830109126,
            "upper_bound": 768443.2649131943
          },
          "point_estimate": 768051.9360540673,
          "standard_error": 205.12331955888948
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 767512.6362847222,
            "upper_bound": 768710.5708333333
          },
          "point_estimate": 768057.0770833334,
          "standard_error": 319.25756272218746
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189.00987539448448,
            "upper_bound": 1196.3320548024503
          },
          "point_estimate": 791.1784447037764,
          "standard_error": 257.7316050550066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 767294.8482253087,
            "upper_bound": 768263.5786436783
          },
          "point_estimate": 767772.8221320346,
          "standard_error": 249.581168361222
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 407.3063398247976,
            "upper_bound": 849.2258031613193
          },
          "point_estimate": 683.3960632371162,
          "standard_error": 115.20804189560762
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/regex_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1581.051938709717,
            "upper_bound": 1586.013323116111
          },
          "point_estimate": 1583.2181655439426,
          "standard_error": 1.288353897364469
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1580.4296767537826,
            "upper_bound": 1585.4631619669876
          },
          "point_estimate": 1582.0517618423314,
          "standard_error": 1.0608091485961808
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4639657177774598,
            "upper_bound": 4.90585838178256
          },
          "point_estimate": 2.1375108216414604,
          "standard_error": 1.2188725499106197
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1580.6606185599,
            "upper_bound": 1585.0101990666772
          },
          "point_estimate": 1582.2844155844157,
          "standard_error": 1.1145215752115591
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0340358007900656,
            "upper_bound": 5.917972846079762
          },
          "point_estimate": 4.293606794768237,
          "standard_error": 1.3441509279722086
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48163.031543987614,
            "upper_bound": 48212.136063218386
          },
          "point_estimate": 48191.08037982611,
          "standard_error": 12.428183277387976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48162.003713527854,
            "upper_bound": 48212.43213969938
          },
          "point_estimate": 48210.63179708223,
          "standard_error": 11.692673221007158
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1310875308288101,
            "upper_bound": 61.64860867562955
          },
          "point_estimate": 3.9467833782066735,
          "standard_error": 12.217939607345787
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48140.200966360586,
            "upper_bound": 48215.51405543492
          },
          "point_estimate": 48182.12906403941,
          "standard_error": 20.043784709918555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.592541014832967,
            "upper_bound": 51.922350980402655
          },
          "point_estimate": 41.38035488141204,
          "standard_error": 12.363143250641
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49471.12762487852,
            "upper_bound": 49572.62134038575
          },
          "point_estimate": 49520.75147786416,
          "standard_error": 26.023541646584615
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49447.42517006803,
            "upper_bound": 49587.47508503401
          },
          "point_estimate": 49519.79188964475,
          "standard_error": 34.47976951167471
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.20505831210796,
            "upper_bound": 157.93125586282696
          },
          "point_estimate": 80.53800036608378,
          "standard_error": 34.21256880085564
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49491.65488268777,
            "upper_bound": 49576.83170581651
          },
          "point_estimate": 49534.921258061666,
          "standard_error": 21.449076177435437
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.641081571964726,
            "upper_bound": 108.89618308438456
          },
          "point_estimate": 86.54552671388099,
          "standard_error": 15.458165880188918
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51910.00033149922,
            "upper_bound": 51982.690981138054
          },
          "point_estimate": 51948.48149333152,
          "standard_error": 18.57452695550946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51910.9869234427,
            "upper_bound": 51999.37181011254
          },
          "point_estimate": 51955.23509272468,
          "standard_error": 23.33986777346394
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.013288274906948,
            "upper_bound": 104.85973080598502
          },
          "point_estimate": 51.23938425436731,
          "standard_error": 21.603528353616813
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51919.3050876226,
            "upper_bound": 52004.79092323598
          },
          "point_estimate": 51974.52041425052,
          "standard_error": 21.747620327596024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.988381209394245,
            "upper_bound": 82.54541849854515
          },
          "point_estimate": 61.979689945612385,
          "standard_error": 14.206496492305543
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20268.52096702754,
            "upper_bound": 20294.390219215707
          },
          "point_estimate": 20281.507554417356,
          "standard_error": 6.640237845851698
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20263.425569760977,
            "upper_bound": 20299.05293064048
          },
          "point_estimate": 20284.55197331851,
          "standard_error": 8.918305493779311
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.822410290520355,
            "upper_bound": 37.592988437649375
          },
          "point_estimate": 25.05216918558485,
          "standard_error": 8.600352811073575
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20272.24693497768,
            "upper_bound": 20295.832109956624
          },
          "point_estimate": 20283.54398330963,
          "standard_error": 6.005380954333603
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.752554532561314,
            "upper_bound": 27.640039008237117
          },
          "point_estimate": 22.09827969654925,
          "standard_error": 3.820236802037789
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24332.874454245557,
            "upper_bound": 24362.878400984893
          },
          "point_estimate": 24346.29564774973,
          "standard_error": 7.754166616338326
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24332.018473895583,
            "upper_bound": 24362.328647925035
          },
          "point_estimate": 24335.978012048192,
          "standard_error": 7.644363802529778
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0659536557758815,
            "upper_bound": 39.06745205742109
          },
          "point_estimate": 7.43069417028702,
          "standard_error": 10.39912045177333
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24333.69550423918,
            "upper_bound": 24362.358945743505
          },
          "point_estimate": 24342.92263252143,
          "standard_error": 7.657817544602
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.915821913259693,
            "upper_bound": 35.5780532210272
          },
          "point_estimate": 25.793960507142895,
          "standard_error": 7.340925949260898
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17681.787091233185,
            "upper_bound": 17701.38889101095
          },
          "point_estimate": 17691.017328479596,
          "standard_error": 5.029893030903118
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17677.815264948953,
            "upper_bound": 17702.47751579971
          },
          "point_estimate": 17687.698914276454,
          "standard_error": 6.411911120830149
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.027049400299268,
            "upper_bound": 27.54057786140729
          },
          "point_estimate": 16.995602566524244,
          "standard_error": 5.9297659728742795
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17686.4015822887,
            "upper_bound": 17708.599416626155
          },
          "point_estimate": 17698.569101389618,
          "standard_error": 5.5915807605978705
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.241655190005162,
            "upper_bound": 21.83322026111888
          },
          "point_estimate": 16.812591597510227,
          "standard_error": 3.5997348925574704
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17516.244676198978,
            "upper_bound": 17535.35299797098
          },
          "point_estimate": 17525.712304854376,
          "standard_error": 4.880020557900274
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17512.36770448831,
            "upper_bound": 17537.6006860857
          },
          "point_estimate": 17526.80554485636,
          "standard_error": 6.5179432658523035
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4005991813228875,
            "upper_bound": 28.07504380825775
          },
          "point_estimate": 17.063402754354136,
          "standard_error": 5.981144626126908
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17517.287927659745,
            "upper_bound": 17535.613924766072
          },
          "point_estimate": 17526.710927974287,
          "standard_error": 4.7286542106561935
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.031014178976596,
            "upper_bound": 20.920071913686705
          },
          "point_estimate": 16.27746387139016,
          "standard_error": 3.071839470926755
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17323.893311474363,
            "upper_bound": 17350.703739445733
          },
          "point_estimate": 17336.816060968875,
          "standard_error": 6.868844093871075
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17316.209088020336,
            "upper_bound": 17357.74773593899
          },
          "point_estimate": 17331.850734164815,
          "standard_error": 10.896724298697563
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.213067900776127,
            "upper_bound": 37.771075605250026
          },
          "point_estimate": 23.93484306689191,
          "standard_error": 8.470832824743015
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17324.80030217458,
            "upper_bound": 17362.18776171501
          },
          "point_estimate": 17343.655742636773,
          "standard_error": 10.175754718392463
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.35453391032065,
            "upper_bound": 28.562187568964433
          },
          "point_estimate": 23.016954882214,
          "standard_error": 3.960775803572912
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25536.802040177183,
            "upper_bound": 25595.577750766966
          },
          "point_estimate": 25564.751681478145,
          "standard_error": 15.091319671762104
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25529.496223489397,
            "upper_bound": 25613.789915966387
          },
          "point_estimate": 25547.539716581075,
          "standard_error": 20.052294213712884
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.962963810205046,
            "upper_bound": 82.19498002359079
          },
          "point_estimate": 31.428775996649676,
          "standard_error": 20.45965709966891
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25536.296985323053,
            "upper_bound": 25563.541661391813
          },
          "point_estimate": 25550.34217687075,
          "standard_error": 6.911377840455815
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.245819193275167,
            "upper_bound": 63.46205395464572
          },
          "point_estimate": 50.27555088425927,
          "standard_error": 9.98978585064251
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26661.513369807748,
            "upper_bound": 26704.15117434883
          },
          "point_estimate": 26679.88050503474,
          "standard_error": 11.073701023910292
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26654.12406733026,
            "upper_bound": 26691.242868414665
          },
          "point_estimate": 26674.492279679533,
          "standard_error": 8.69319431845563
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9948681442084215,
            "upper_bound": 42.80804956775486
          },
          "point_estimate": 22.448437698082937,
          "standard_error": 10.97518863558465
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26656.289215060977,
            "upper_bound": 26684.643302488017
          },
          "point_estimate": 26668.22246857294,
          "standard_error": 7.322083845738455
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.949385213386348,
            "upper_bound": 53.58917022096237
          },
          "point_estimate": 37.008847974290504,
          "standard_error": 12.556988461155363
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41654.24736822488,
            "upper_bound": 41694.51116023473
          },
          "point_estimate": 41674.70648111784,
          "standard_error": 10.367021053453383
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41640.22542955326,
            "upper_bound": 41709.904352806414
          },
          "point_estimate": 41679.204324169536,
          "standard_error": 20.213212647544296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5064940448482504,
            "upper_bound": 54.0121671453274
          },
          "point_estimate": 46.25486895543691,
          "standard_error": 14.544495384009233
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41649.3256066871,
            "upper_bound": 41702.14723558904
          },
          "point_estimate": 41677.90408354532,
          "standard_error": 13.836571813107811
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.374761177939337,
            "upper_bound": 39.60726272247624
          },
          "point_estimate": 34.572824960059606,
          "standard_error": 4.383809689330315
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17641.3727388993,
            "upper_bound": 17654.45248964552
          },
          "point_estimate": 17647.920543851327,
          "standard_error": 3.3603019857473964
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17636.64838840298,
            "upper_bound": 17658.844239283015
          },
          "point_estimate": 17647.828646339487,
          "standard_error": 7.180838640626415
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5230972032799812,
            "upper_bound": 17.829480372242404
          },
          "point_estimate": 15.252186194641853,
          "standard_error": 4.572105192672795
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17637.719184985635,
            "upper_bound": 17655.17058934585
          },
          "point_estimate": 17646.80244595055,
          "standard_error": 4.583379163825907
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.694518241774324,
            "upper_bound": 12.51800165998197
          },
          "point_estimate": 11.16518164846332,
          "standard_error": 1.256058131526519
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25476.884807319533,
            "upper_bound": 25509.02670964455
          },
          "point_estimate": 25493.06011719226,
          "standard_error": 8.233853310537532
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25473.247266806233,
            "upper_bound": 25522.24021090176
          },
          "point_estimate": 25489.558478715982,
          "standard_error": 10.607595938846066
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.917607763972288,
            "upper_bound": 51.701728184229296
          },
          "point_estimate": 24.527528371250963,
          "standard_error": 12.967716470428954
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25464.044206512124,
            "upper_bound": 25517.10621137044
          },
          "point_estimate": 25490.324920020663,
          "standard_error": 14.153307503486236
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.256040324224209,
            "upper_bound": 34.7159485234258
          },
          "point_estimate": 27.424092947872683,
          "standard_error": 4.99679208403046
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70139.37745325596,
            "upper_bound": 70267.64024868749
          },
          "point_estimate": 70199.68115317308,
          "standard_error": 32.80388565223151
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70130.85783365571,
            "upper_bound": 70276.63332412268
          },
          "point_estimate": 70171.72130883302,
          "standard_error": 42.35447525081803
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.232106535178086,
            "upper_bound": 177.5150118194575
          },
          "point_estimate": 64.3927294177249,
          "standard_error": 46.51036241464267
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70132.58156786893,
            "upper_bound": 70211.26249424335
          },
          "point_estimate": 70168.30119822151,
          "standard_error": 20.05707684989817
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.34361527427413,
            "upper_bound": 147.51879313608785
          },
          "point_estimate": 109.80330458929332,
          "standard_error": 25.961383964157687
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34391.34709379518,
            "upper_bound": 34463.14669587795
          },
          "point_estimate": 34424.60179862806,
          "standard_error": 18.48026441615963
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34382.13336100569,
            "upper_bound": 34455.769555133884
          },
          "point_estimate": 34419.30174166441,
          "standard_error": 16.426569316125697
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.544646426784291,
            "upper_bound": 93.4060366876979
          },
          "point_estimate": 49.35595786822381,
          "standard_error": 22.17939547950248
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34396.726124521934,
            "upper_bound": 34440.303244682116
          },
          "point_estimate": 34420.85156115232,
          "standard_error": 10.996659514102184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.667771420438356,
            "upper_bound": 85.44115194362891
          },
          "point_estimate": 61.60414765801285,
          "standard_error": 16.811055602135976
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60458.12377981143,
            "upper_bound": 60519.6127485936
          },
          "point_estimate": 60489.00048477405,
          "standard_error": 15.768531258557044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60445.70216306156,
            "upper_bound": 60539.879275281935
          },
          "point_estimate": 60489.434484193014,
          "standard_error": 25.68096912270188
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.25842029815307,
            "upper_bound": 87.08247885330563
          },
          "point_estimate": 69.81349204952436,
          "standard_error": 18.69987221520964
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60464.07323633543,
            "upper_bound": 60522.26881804121
          },
          "point_estimate": 60491.66487023792,
          "standard_error": 14.950504064767976
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.62398700990772,
            "upper_bound": 63.16485154852019
          },
          "point_estimate": 52.58369494377245,
          "standard_error": 7.766717844970738
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26168.37642848597,
            "upper_bound": 26202.1014496462
          },
          "point_estimate": 26184.107061754352,
          "standard_error": 8.633988233743029
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26159.40209978464,
            "upper_bound": 26200.969633883706
          },
          "point_estimate": 26180.37691860664,
          "standard_error": 11.610048230183184
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.742328747468653,
            "upper_bound": 48.467991969744425
          },
          "point_estimate": 30.24356271683737,
          "standard_error": 11.04299043894992
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26164.410278301475,
            "upper_bound": 26190.739059442873
          },
          "point_estimate": 26177.050501114103,
          "standard_error": 6.834778956279952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.17228358013386,
            "upper_bound": 38.17967396608054
          },
          "point_estimate": 28.892209824306143,
          "standard_error": 6.59810365493889
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22869.17968455003,
            "upper_bound": 22892.422957517956
          },
          "point_estimate": 22881.224405710887,
          "standard_error": 5.98073323726194
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22860.631843927,
            "upper_bound": 22896.75519194462
          },
          "point_estimate": 22888.94259142717,
          "standard_error": 8.85342101255446
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.470755588609414,
            "upper_bound": 32.41014237614713
          },
          "point_estimate": 19.1040641982816,
          "standard_error": 8.157147061075351
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22860.899132948853,
            "upper_bound": 22891.19481310105
          },
          "point_estimate": 22874.20217894126,
          "standard_error": 7.780809947711742
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.315642780535578,
            "upper_bound": 24.485800486654053
          },
          "point_estimate": 19.91723753179034,
          "standard_error": 3.533849882893041
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28754.3517009277,
            "upper_bound": 28787.060831485443
          },
          "point_estimate": 28771.1149195074,
          "standard_error": 8.342754380857098
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28754.79964483031,
            "upper_bound": 28788.26863857882
          },
          "point_estimate": 28770.52545382794,
          "standard_error": 9.05270271457506
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7300250153244403,
            "upper_bound": 47.86823399626102
          },
          "point_estimate": 25.44103273696631,
          "standard_error": 11.275701894520406
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28757.738752152112,
            "upper_bound": 28790.73292640503
          },
          "point_estimate": 28774.24957820396,
          "standard_error": 8.350162801763641
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.99765683580403,
            "upper_bound": 37.28712125519317
          },
          "point_estimate": 27.87660006869305,
          "standard_error": 6.3923946352704535
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector-freq_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415670.5769195527,
            "upper_bound": 416180.5787423904
          },
          "point_estimate": 415921.4451564754,
          "standard_error": 130.3459531083088
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415533.2428977273,
            "upper_bound": 416368.2784090909
          },
          "point_estimate": 415871.44431818184,
          "standard_error": 205.83454987468292
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.36034386805083,
            "upper_bound": 723.4227434067159
          },
          "point_estimate": 596.7319114418698,
          "standard_error": 163.98137242553378
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415522.15034066816,
            "upper_bound": 416095.161411864
          },
          "point_estimate": 415781.3739964581,
          "standard_error": 147.5348866545715
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 260.1299729413857,
            "upper_bound": 527.5177462424776
          },
          "point_estimate": 433.87901741582226,
          "standard_error": 67.50970285051861
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector-repeated_ra"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2696621.3760505957,
            "upper_bound": 2702647.5048809526
          },
          "point_estimate": 2699188.004404762,
          "standard_error": 1572.9027902321416
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2696011.95,
            "upper_bound": 2700381.9523809524
          },
          "point_estimate": 2698467.913690476,
          "standard_error": 1211.1891858346846
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 655.5527583615923,
            "upper_bound": 5596.862555635403
          },
          "point_estimate": 3084.8131127334254,
          "standard_error": 1246.0967974811513
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2696621.9112426033,
            "upper_bound": 2699774.1615784094
          },
          "point_estimate": 2697918.5090909093,
          "standard_error": 826.8280769068955
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1617.2328658572674,
            "upper_bound": 7694.201470251816
          },
          "point_estimate": 5237.460564779527,
          "standard_error": 1894.6752464042863
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117612.0775891894,
            "upper_bound": 117754.85017972723
          },
          "point_estimate": 117683.63525363436,
          "standard_error": 36.41505826055494
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117593.27040632868,
            "upper_bound": 117756.80131761442
          },
          "point_estimate": 117707.5072815534,
          "standard_error": 44.69886146339169
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.100683856560693,
            "upper_bound": 212.94309932629216
          },
          "point_estimate": 130.24742061105314,
          "standard_error": 50.55248704578095
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117625.46165752076,
            "upper_bound": 117796.77360560728
          },
          "point_estimate": 117706.80212667592,
          "standard_error": 43.49056229749428
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.041001328381725,
            "upper_bound": 160.83557815722128
          },
          "point_estimate": 121.65233889277312,
          "standard_error": 25.75460115577911
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/sliceslice_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9275.658380612073,
            "upper_bound": 9291.501860703956
          },
          "point_estimate": 9284.123906886814,
          "standard_error": 4.078743257721481
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9271.75024105269,
            "upper_bound": 9295.587161817251
          },
          "point_estimate": 9288.12611345074,
          "standard_error": 5.447611481058939
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.547229245120285,
            "upper_bound": 22.38962675070888
          },
          "point_estimate": 11.133420462755614,
          "standard_error": 5.132466863981536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9278.985001992056,
            "upper_bound": 9294.537651612987
          },
          "point_estimate": 9288.076987331198,
          "standard_error": 4.19182636998614
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.058733451243389,
            "upper_bound": 17.577425490398834
          },
          "point_estimate": 13.5762128556153,
          "standard_error": 3.0425051484693357
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/sliceslice_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8986.514657679276,
            "upper_bound": 8999.203141514623
          },
          "point_estimate": 8992.603916043194,
          "standard_error": 3.234260682069811
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8986.830842391304,
            "upper_bound": 8998.437611754187
          },
          "point_estimate": 8990.894602272727,
          "standard_error": 3.341474799127669
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9589145156177747,
            "upper_bound": 17.63153869389334
          },
          "point_estimate": 8.911684537932715,
          "standard_error": 3.897664553268525
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8988.591767044722,
            "upper_bound": 8995.354311223733
          },
          "point_estimate": 8991.664523125097,
          "standard_error": 1.7075433827960504
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.468584017204462,
            "upper_bound": 14.904452183090518
          },
          "point_estimate": 10.742479239089528,
          "standard_error": 2.725850966412563
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/sliceslice_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14484.023002720276,
            "upper_bound": 14510.605501924098
          },
          "point_estimate": 14496.233853834923,
          "standard_error": 6.8060952196313345
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14486.482583598725,
            "upper_bound": 14503.488130307856
          },
          "point_estimate": 14491.29635250796,
          "standard_error": 4.707412069367212
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4703162905769478,
            "upper_bound": 34.412911583515466
          },
          "point_estimate": 9.652503426363936,
          "standard_error": 8.500566636515554
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14487.621774525236,
            "upper_bound": 14496.147539706615
          },
          "point_estimate": 14491.187632351724,
          "standard_error": 2.1537843527204563
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.458583880492498,
            "upper_bound": 31.90341287247046
          },
          "point_estimate": 22.630880192779028,
          "standard_error": 6.691785564076648
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/sliceslice_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.55827836021503,
            "upper_bound": 49.61573403861355
          },
          "point_estimate": 49.58595649820179,
          "standard_error": 0.014748849812056814
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.54577702171419,
            "upper_bound": 49.63668835031513
          },
          "point_estimate": 49.56278649241346,
          "standard_error": 0.027659697267254285
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006435548647978855,
            "upper_bound": 0.07251772403538205
          },
          "point_estimate": 0.037353045517962,
          "standard_error": 0.02021409092179275
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.55315091369009,
            "upper_bound": 49.62973270647953
          },
          "point_estimate": 49.59201316629473,
          "standard_error": 0.019895366254703974
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029313928605754415,
            "upper_bound": 0.05723589174778918
          },
          "point_estimate": 0.04899655023804852,
          "standard_error": 0.007176601893566434
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.28931529153788,
            "upper_bound": 29.315114267887186
          },
          "point_estimate": 29.30130627305032,
          "standard_error": 0.006602995216797137
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.284593432151873,
            "upper_bound": 29.3140051930043
          },
          "point_estimate": 29.29810838656132,
          "standard_error": 0.007783927912226211
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004731055830158609,
            "upper_bound": 0.03448685990180223
          },
          "point_estimate": 0.01794236471326484,
          "standard_error": 0.007541032032523233
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.28204025592845,
            "upper_bound": 29.310905652169257
          },
          "point_estimate": 29.294657978928974,
          "standard_error": 0.007427309617233931
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010323912969028496,
            "upper_bound": 0.029596605259070673
          },
          "point_estimate": 0.02203754391980813,
          "standard_error": 0.0053597508465887496
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.53541628654617,
            "upper_bound": 29.565627278597567
          },
          "point_estimate": 29.551668516258957,
          "standard_error": 0.007750565921273904
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.538028307037003,
            "upper_bound": 29.567463052174148
          },
          "point_estimate": 29.559466955815186,
          "standard_error": 0.008164874208072019
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013756263979115555,
            "upper_bound": 0.03892888396844978
          },
          "point_estimate": 0.024456194299092543,
          "standard_error": 0.010059009809269003
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.547204713709945,
            "upper_bound": 29.576877993713428
          },
          "point_estimate": 29.563078394422124,
          "standard_error": 0.007818161281820378
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011116794973639006,
            "upper_bound": 0.035736936795980855
          },
          "point_estimate": 0.02587983664348016,
          "standard_error": 0.00685622779820702
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.711299361951752,
            "upper_bound": 27.75842096043801
          },
          "point_estimate": 27.734931065467457,
          "standard_error": 0.01208207840900282
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.694108417523776,
            "upper_bound": 27.774122810351688
          },
          "point_estimate": 27.739151215915225,
          "standard_error": 0.019407759460179055
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010528991542265834,
            "upper_bound": 0.0683129735414699
          },
          "point_estimate": 0.057783981999206695,
          "standard_error": 0.015083219266907463
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.724951071967496,
            "upper_bound": 27.77663592172797
          },
          "point_estimate": 27.757406402274476,
          "standard_error": 0.013255975382322356
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02495947261853947,
            "upper_bound": 0.04822768722862643
          },
          "point_estimate": 0.04036716520759996,
          "standard_error": 0.0059024162117098005
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.303094343617943,
            "upper_bound": 28.33596538362004
          },
          "point_estimate": 28.318134249354795,
          "standard_error": 0.008422482628192379
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.30242167787441,
            "upper_bound": 28.328317117675724
          },
          "point_estimate": 28.313842123020155,
          "standard_error": 0.007604851567238028
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004113614118937429,
            "upper_bound": 0.0392188536910728
          },
          "point_estimate": 0.017757493374008955,
          "standard_error": 0.008584272114823207
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.307029398329984,
            "upper_bound": 28.33162668450763
          },
          "point_estimate": 28.318347251086923,
          "standard_error": 0.00632773767825614
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010112344218367583,
            "upper_bound": 0.03991852757556259
          },
          "point_estimate": 0.028110355730505604,
          "standard_error": 0.00834916756792003
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.7597293802104,
            "upper_bound": 27.790919089695375
          },
          "point_estimate": 27.775283638950885,
          "standard_error": 0.007999302449258987
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.75439481609441,
            "upper_bound": 27.799064930035364
          },
          "point_estimate": 27.77265297438876,
          "standard_error": 0.011172128455005514
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008848437075381816,
            "upper_bound": 0.04747138670422151
          },
          "point_estimate": 0.028937021903104057,
          "standard_error": 0.0101897219216976
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.765463416505145,
            "upper_bound": 27.79739141393841
          },
          "point_estimate": 27.778061878806675,
          "standard_error": 0.008106809125661595
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01572808770189541,
            "upper_bound": 0.032794639366455526
          },
          "point_estimate": 0.02665116191120432,
          "standard_error": 0.0043649641728523815
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.70367780507463,
            "upper_bound": 43.75076873600677
          },
          "point_estimate": 43.72886221903959,
          "standard_error": 0.01207995035659796
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.70832784265606,
            "upper_bound": 43.75437632893002
          },
          "point_estimate": 43.73308440449503,
          "standard_error": 0.01139214487082491
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006623193359398743,
            "upper_bound": 0.06303006390635776
          },
          "point_estimate": 0.02423371685763894,
          "standard_error": 0.013824738336680402
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.70918739395512,
            "upper_bound": 43.74400927264801
          },
          "point_estimate": 43.727224967946995,
          "standard_error": 0.008945899990684939
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015949976071309366,
            "upper_bound": 0.05557417933803773
          },
          "point_estimate": 0.04020996416673352,
          "standard_error": 0.010753810480740145
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.555454538118966,
            "upper_bound": 36.63148021169892
          },
          "point_estimate": 36.590109275414434,
          "standard_error": 0.01948996401761962
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.54725184280376,
            "upper_bound": 36.65025977680425
          },
          "point_estimate": 36.5618136021355,
          "standard_error": 0.022814647472994852
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0049226270474457415,
            "upper_bound": 0.08973674078459976
          },
          "point_estimate": 0.03901935926122764,
          "standard_error": 0.02229284082839503
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.561318503422534,
            "upper_bound": 36.61295652883626
          },
          "point_estimate": 36.586369460399965,
          "standard_error": 0.012943181997117048
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020529011267492767,
            "upper_bound": 0.08098176320694639
          },
          "point_estimate": 0.06490249690797002,
          "standard_error": 0.01577070331228515
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.65189993119502,
            "upper_bound": 29.67104783784795
          },
          "point_estimate": 29.661849760645634,
          "standard_error": 0.004882319944708182
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.651782589492353,
            "upper_bound": 29.670446994634226
          },
          "point_estimate": 29.663575594194576,
          "standard_error": 0.0045635513731845
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001246407525919041,
            "upper_bound": 0.02771191226862372
          },
          "point_estimate": 0.011646407692068749,
          "standard_error": 0.006419636734877052
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.659657951499465,
            "upper_bound": 29.679253729112972
          },
          "point_estimate": 29.669765093246767,
          "standard_error": 0.005023258893911129
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0069548496334618575,
            "upper_bound": 0.021972133778064576
          },
          "point_estimate": 0.01623564878645416,
          "standard_error": 0.003968518829048197
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.376255657371175,
            "upper_bound": 39.43916169958489
          },
          "point_estimate": 39.40761019631408,
          "standard_error": 0.0160931563919654
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.36906835548032,
            "upper_bound": 39.451620877729795
          },
          "point_estimate": 39.4065702435463,
          "standard_error": 0.02436674780984984
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010290426168007112,
            "upper_bound": 0.08987170442753684
          },
          "point_estimate": 0.060338602393318515,
          "standard_error": 0.021172142388004668
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.3914541934779,
            "upper_bound": 39.4618032083335
          },
          "point_estimate": 39.42713945263521,
          "standard_error": 0.018596444519946284
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03153602252151016,
            "upper_bound": 0.06658558221412215
          },
          "point_estimate": 0.05341181846220162,
          "standard_error": 0.008917981370856354
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.64109032786521,
            "upper_bound": 33.68835532885534
          },
          "point_estimate": 33.66561550993607,
          "standard_error": 0.012122923589071162
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.63003293724627,
            "upper_bound": 33.70380357950316
          },
          "point_estimate": 33.66518203511659,
          "standard_error": 0.01848350959362907
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00724947148238636,
            "upper_bound": 0.07347993316690524
          },
          "point_estimate": 0.050853772943011695,
          "standard_error": 0.018569550619133936
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.64271708361468,
            "upper_bound": 33.67879526847163
          },
          "point_estimate": 33.658896312800415,
          "standard_error": 0.009094936070075667
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022891374597829972,
            "upper_bound": 0.04920359727682524
          },
          "point_estimate": 0.04049199501030066,
          "standard_error": 0.006749279152194446
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.581365683858422,
            "upper_bound": 27.61633786926
          },
          "point_estimate": 27.59710792562467,
          "standard_error": 0.009011936668404892
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.57614571696393,
            "upper_bound": 27.616660757985336
          },
          "point_estimate": 27.58886849784377,
          "standard_error": 0.008350501985652932
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004617371836495305,
            "upper_bound": 0.042582147350548126
          },
          "point_estimate": 0.01716685062260177,
          "standard_error": 0.010041343942850736
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.58107160073556,
            "upper_bound": 27.610407015159357
          },
          "point_estimate": 27.59271403704411,
          "standard_error": 0.007533637136775555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009632262007454113,
            "upper_bound": 0.03989139681531976
          },
          "point_estimate": 0.030094075147331815,
          "standard_error": 0.00804374158492148
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.651325922570386,
            "upper_bound": 44.71644412986255
          },
          "point_estimate": 44.683580858999285,
          "standard_error": 0.016619500754181688
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.64995869048209,
            "upper_bound": 44.72109544886511
          },
          "point_estimate": 44.676907156659425,
          "standard_error": 0.02108892700244626
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006624429749107566,
            "upper_bound": 0.0966137583458242
          },
          "point_estimate": 0.04298439105177328,
          "standard_error": 0.022559090331862815
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.66664417923716,
            "upper_bound": 44.7128860723233
          },
          "point_estimate": 44.69002140479547,
          "standard_error": 0.011913324646500686
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02866137917525979,
            "upper_bound": 0.07264876064490385
          },
          "point_estimate": 0.055561905918164345,
          "standard_error": 0.011306380487378026
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47702.50481505752,
            "upper_bound": 47798.42662006698
          },
          "point_estimate": 47741.925718290375,
          "standard_error": 25.472108262776196
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47697.863215377896,
            "upper_bound": 47759.69410222805
          },
          "point_estimate": 47712.85697903014,
          "standard_error": 17.19689057828711
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.09462383377241,
            "upper_bound": 81.05061213905282
          },
          "point_estimate": 36.32991733666632,
          "standard_error": 20.25821969386342
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47692.81132543157,
            "upper_bound": 47737.31070496091
          },
          "point_estimate": 47712.48760701945,
          "standard_error": 11.534366595782496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.36072925218183,
            "upper_bound": 126.48400680323586
          },
          "point_estimate": 84.90512697035123,
          "standard_error": 33.293606406690515
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49445.55077367455,
            "upper_bound": 49532.34445092323
          },
          "point_estimate": 49488.21009772163,
          "standard_error": 22.257674530201704
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49424.54497354497,
            "upper_bound": 49555.872653061226
          },
          "point_estimate": 49473.937585034015,
          "standard_error": 32.81357664775314
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.906839628010006,
            "upper_bound": 123.2699511448565
          },
          "point_estimate": 91.34650477283417,
          "standard_error": 29.154090283546687
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49454.6444631437,
            "upper_bound": 49520.270686399
          },
          "point_estimate": 49478.925912183055,
          "standard_error": 16.700428313451845
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.62144001166806,
            "upper_bound": 91.83353745222216
          },
          "point_estimate": 74.19147825673546,
          "standard_error": 12.35285627328133
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51701.39845187148,
            "upper_bound": 51783.883846206096
          },
          "point_estimate": 51744.697244129704,
          "standard_error": 21.088662849267724
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51699.008707798384,
            "upper_bound": 51786.04650499287
          },
          "point_estimate": 51754.6336865702,
          "standard_error": 17.380983691512654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.064182150141344,
            "upper_bound": 124.62888865757876
          },
          "point_estimate": 36.6902251403434,
          "standard_error": 28.980943743415494
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51717.64065868529,
            "upper_bound": 51773.10881177158
          },
          "point_estimate": 51753.02774144543,
          "standard_error": 14.322407632392371
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.36611781879899,
            "upper_bound": 93.97847379610162
          },
          "point_estimate": 70.22393136048848,
          "standard_error": 17.24512735300667
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20190.781239250995,
            "upper_bound": 20229.88429924941
          },
          "point_estimate": 20209.710100546676,
          "standard_error": 9.983757812100537
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20188.336821705427,
            "upper_bound": 20224.3784606866
          },
          "point_estimate": 20211.181621227304,
          "standard_error": 7.986396621683182
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.341117941351079,
            "upper_bound": 53.435333004819675
          },
          "point_estimate": 17.95066537898565,
          "standard_error": 12.609755919241213
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20200.386036336393,
            "upper_bound": 20220.967172915676
          },
          "point_estimate": 20212.12692324287,
          "standard_error": 5.170243369960875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.746249249764452,
            "upper_bound": 46.790210913160934
          },
          "point_estimate": 33.18995838322576,
          "standard_error": 8.839912285418263
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24237.610080265153,
            "upper_bound": 24263.509466893272
          },
          "point_estimate": 24250.217538385383,
          "standard_error": 6.641181010767543
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24230.891594396264,
            "upper_bound": 24269.067637684384
          },
          "point_estimate": 24244.90067187649,
          "standard_error": 10.53106833694255
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.206507352531436,
            "upper_bound": 35.250659244648055
          },
          "point_estimate": 22.585060484068855,
          "standard_error": 8.186343296642706
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24242.00426193882,
            "upper_bound": 24269.295879323123
          },
          "point_estimate": 24258.852062413906,
          "standard_error": 6.960051575813575
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.75861680599201,
            "upper_bound": 27.033541691937305
          },
          "point_estimate": 22.12466662675396,
          "standard_error": 3.632376245606739
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17650.72109405746,
            "upper_bound": 17667.189058499433
          },
          "point_estimate": 17658.63687594046,
          "standard_error": 4.211635138637931
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17650.443607194946,
            "upper_bound": 17667.368011667477
          },
          "point_estimate": 17655.71060606061,
          "standard_error": 4.543010860701244
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5105053353634943,
            "upper_bound": 23.476481668769843
          },
          "point_estimate": 11.557326192482629,
          "standard_error": 5.330575872316178
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17648.120314246396,
            "upper_bound": 17663.019825060284
          },
          "point_estimate": 17654.822068451722,
          "standard_error": 3.734408308749564
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.590255303172293,
            "upper_bound": 18.71368782523243
          },
          "point_estimate": 13.996460968322063,
          "standard_error": 3.1650684911941394
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17280.394948259855,
            "upper_bound": 17300.552300417392
          },
          "point_estimate": 17290.422914789153,
          "standard_error": 5.146635489300046
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17279.121160247265,
            "upper_bound": 17303.202818252837
          },
          "point_estimate": 17289.076002536058,
          "standard_error": 5.647965633510209
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4398304071528685,
            "upper_bound": 30.42189083730075
          },
          "point_estimate": 17.272783188353582,
          "standard_error": 8.053204314847846
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17288.623997077906,
            "upper_bound": 17308.72842253296
          },
          "point_estimate": 17301.384146333934,
          "standard_error": 4.909970970681108
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.241459368263447,
            "upper_bound": 22.06583918783274
          },
          "point_estimate": 17.10573384343372,
          "standard_error": 3.28902525108841
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17289.06091683846,
            "upper_bound": 17320.81056822867
          },
          "point_estimate": 17305.52920334753,
          "standard_error": 8.141647309463892
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17290.054247277723,
            "upper_bound": 17322.00149659055
          },
          "point_estimate": 17308.27293054234,
          "standard_error": 6.759618092276313
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.400895592659831,
            "upper_bound": 44.357329302890655
          },
          "point_estimate": 13.15424976012477,
          "standard_error": 10.600357601787396
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17290.831107203507,
            "upper_bound": 17309.564765956562
          },
          "point_estimate": 17302.264149171475,
          "standard_error": 4.770736887532295
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.842627438763532,
            "upper_bound": 37.11364099389784
          },
          "point_estimate": 27.1341722293722,
          "standard_error": 6.749036081468047
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25438.78266050244,
            "upper_bound": 25476.268854943428
          },
          "point_estimate": 25456.865268528672,
          "standard_error": 9.598412557809482
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25431.643841591067,
            "upper_bound": 25481.91547452896
          },
          "point_estimate": 25453.026762037684,
          "standard_error": 12.967527445830797
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.80946833627191,
            "upper_bound": 54.884225944659214
          },
          "point_estimate": 35.40651808622652,
          "standard_error": 11.996629162874632
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25437.393899296796,
            "upper_bound": 25475.34994612301
          },
          "point_estimate": 25456.43050724572,
          "standard_error": 9.574229818013093
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.316065111263914,
            "upper_bound": 39.84126043080822
          },
          "point_estimate": 32.1537238684813,
          "standard_error": 5.624939831827956
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26575.965919123377,
            "upper_bound": 26600.826429692643
          },
          "point_estimate": 26588.848961991334,
          "standard_error": 6.325086337783706
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26582.882272727275,
            "upper_bound": 26601.404363636364
          },
          "point_estimate": 26587.61809090909,
          "standard_error": 4.395200197904363
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7183333694929668,
            "upper_bound": 32.431672251495165
          },
          "point_estimate": 7.779583943701394,
          "standard_error": 7.878792768297923
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26576.075800928997,
            "upper_bound": 26592.92074742386
          },
          "point_estimate": 26586.088132231405,
          "standard_error": 4.2111524753975065
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.165273872150271,
            "upper_bound": 30.09236315011443
          },
          "point_estimate": 21.021932190233812,
          "standard_error": 6.200236315541224
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41520.37365908536,
            "upper_bound": 41577.016044897384
          },
          "point_estimate": 41545.73586876256,
          "standard_error": 14.578504820887366
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41508.617825921705,
            "upper_bound": 41570.74388065374
          },
          "point_estimate": 41531.75500895911,
          "standard_error": 16.401830869565824
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.0173017682277194,
            "upper_bound": 71.11908181605541
          },
          "point_estimate": 43.466776993418485,
          "standard_error": 15.754840957052062
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41515.78201387767,
            "upper_bound": 41566.18409452885
          },
          "point_estimate": 41540.96522679145,
          "standard_error": 13.317984104163868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.1667198008601,
            "upper_bound": 67.225667225923
          },
          "point_estimate": 48.37843422517048,
          "standard_error": 13.667136113980376
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17572.303424047237,
            "upper_bound": 17614.68523236331
          },
          "point_estimate": 17590.143414347054,
          "standard_error": 11.134049864487707
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17569.95018115942,
            "upper_bound": 17597.408937198066
          },
          "point_estimate": 17582.16108427268,
          "standard_error": 6.607513733296524
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5921038686760671,
            "upper_bound": 36.52641855442357
          },
          "point_estimate": 19.431263234735702,
          "standard_error": 9.905726260159703
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17576.13012646123,
            "upper_bound": 17589.87664783527
          },
          "point_estimate": 17582.791776146558,
          "standard_error": 3.428513983630072
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.297283276262409,
            "upper_bound": 55.171844500648135
          },
          "point_estimate": 37.26615682005104,
          "standard_error": 14.037511910975471
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25384.931865383132,
            "upper_bound": 25409.64047326606
          },
          "point_estimate": 25398.14916879301,
          "standard_error": 6.295394487649871
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25389.395441892833,
            "upper_bound": 25410.763674321504
          },
          "point_estimate": 25399.32814312225,
          "standard_error": 5.710352508470774
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.843512800367676,
            "upper_bound": 32.36880975936462
          },
          "point_estimate": 14.1316779815426,
          "standard_error": 7.0291286866637614
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25392.071636114873,
            "upper_bound": 25406.85543261324
          },
          "point_estimate": 25400.02438160309,
          "standard_error": 3.853454064618565
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.880399393492835,
            "upper_bound": 29.44652100784015
          },
          "point_estimate": 21.015206144117347,
          "standard_error": 5.771878525628089
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69998.19251117266,
            "upper_bound": 70081.57508269108
          },
          "point_estimate": 70041.74402965105,
          "standard_error": 21.377261457898136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69988.88669449797,
            "upper_bound": 70098.01348747592
          },
          "point_estimate": 70063.67738439306,
          "standard_error": 28.030453811786465
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.158758254403748,
            "upper_bound": 117.45272209591484
          },
          "point_estimate": 82.86816551659325,
          "standard_error": 29.483865270656633
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69971.36838941027,
            "upper_bound": 70062.45821500586
          },
          "point_estimate": 70023.53916372644,
          "standard_error": 23.336628344194605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.11326300991302,
            "upper_bound": 91.6617953597219
          },
          "point_estimate": 71.0684904819834,
          "standard_error": 14.276696071705365
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34244.69643205206,
            "upper_bound": 34303.71750012891
          },
          "point_estimate": 34275.537670724596,
          "standard_error": 15.059310500176464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34236.373940677964,
            "upper_bound": 34313.81344161958
          },
          "point_estimate": 34282.22408976774,
          "standard_error": 15.915004546172623
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.647700747187513,
            "upper_bound": 92.32045897774404
          },
          "point_estimate": 38.83342211424111,
          "standard_error": 22.356236830411845
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34278.51009119197,
            "upper_bound": 34312.380808901486
          },
          "point_estimate": 34294.40982219287,
          "standard_error": 8.621537849346904
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.1322173455029,
            "upper_bound": 63.87607612372024
          },
          "point_estimate": 50.09887591221919,
          "standard_error": 10.265256177973702
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60376.5278820598,
            "upper_bound": 60477.4334979236
          },
          "point_estimate": 60430.45258859357,
          "standard_error": 25.899743009228192
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60389.808554817275,
            "upper_bound": 60481.2030730897
          },
          "point_estimate": 60446.308139534885,
          "standard_error": 22.85246307946061
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.925135817046293,
            "upper_bound": 128.77357678357865
          },
          "point_estimate": 61.5844404764271,
          "standard_error": 29.6100388898827
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60421.086133994315,
            "upper_bound": 60499.26817109484
          },
          "point_estimate": 60462.77381887216,
          "standard_error": 19.72429998522062
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.02057721605166,
            "upper_bound": 120.64279906866506
          },
          "point_estimate": 86.50862771739766,
          "standard_error": 23.79358420168371
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26243.731720367992,
            "upper_bound": 26277.05277392372
          },
          "point_estimate": 26259.76910086217,
          "standard_error": 8.51559256549345
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26235.773826652963,
            "upper_bound": 26281.289442446043
          },
          "point_estimate": 26259.911375899283,
          "standard_error": 11.719917567717747
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.809993152019685,
            "upper_bound": 47.41364493162069
          },
          "point_estimate": 31.653893547746577,
          "standard_error": 11.72844338245167
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26249.49219517375,
            "upper_bound": 26293.194613354543
          },
          "point_estimate": 26272.914082033076,
          "standard_error": 11.544276425090231
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.039331302464165,
            "upper_bound": 35.778380852608045
          },
          "point_estimate": 28.38479989431532,
          "standard_error": 5.339916516201282
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22941.513670662465,
            "upper_bound": 22971.344366430072
          },
          "point_estimate": 22956.432549747133,
          "standard_error": 7.651019994870201
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22931.20112162636,
            "upper_bound": 22982.203154574137
          },
          "point_estimate": 22957.735973411443,
          "standard_error": 12.91873107871958
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.096682935929295,
            "upper_bound": 42.31984707622017
          },
          "point_estimate": 37.80780635296201,
          "standard_error": 9.69814387722611
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22939.160635369983,
            "upper_bound": 22981.48821929855
          },
          "point_estimate": 22963.0484182064,
          "standard_error": 11.012301406536464
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.00292826871416,
            "upper_bound": 30.10025545401806
          },
          "point_estimate": 25.45868275581665,
          "standard_error": 3.5709850092000797
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28572.94194419526,
            "upper_bound": 28621.116970795643
          },
          "point_estimate": 28597.21980112007,
          "standard_error": 12.3736094408516
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28554.259288330715,
            "upper_bound": 28640.155494505496
          },
          "point_estimate": 28603.23657210137,
          "standard_error": 25.77597046728551
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4251778415582907,
            "upper_bound": 64.94300759898852
          },
          "point_estimate": 61.4844998581929,
          "standard_error": 18.33792746917719
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28571.794734241223,
            "upper_bound": 28628.230558837335
          },
          "point_estimate": 28605.015835185222,
          "standard_error": 14.547752084568671
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.974275883534418,
            "upper_bound": 46.39057304145472
          },
          "point_estimate": 41.183605245063475,
          "standard_error": 4.825176401970557
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415395.3016698232,
            "upper_bound": 416554.8190273494
          },
          "point_estimate": 415870.7685529401,
          "standard_error": 311.0329343967788
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415300.945211039,
            "upper_bound": 416104.7154356061
          },
          "point_estimate": 415494.0846590909,
          "standard_error": 231.16286417134424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.225527617998,
            "upper_bound": 917.091416445739
          },
          "point_estimate": 399.39061109120325,
          "standard_error": 261.1683819138683
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415368.653776068,
            "upper_bound": 415805.39652615663
          },
          "point_estimate": 415551.9528335301,
          "standard_error": 112.69085906372504
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.30507974286363,
            "upper_bound": 1548.3419102308276
          },
          "point_estimate": 1038.1617347763909,
          "standard_error": 416.1486451845488
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2697337.5041398806,
            "upper_bound": 2699743.897959183
          },
          "point_estimate": 2698607.201122449,
          "standard_error": 618.7747658895472
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2697049.15,
            "upper_bound": 2700050.163690476
          },
          "point_estimate": 2699188.9375,
          "standard_error": 729.2568195897522
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 211.82205873966973,
            "upper_bound": 3328.300153410729
          },
          "point_estimate": 1745.859804719135,
          "standard_error": 861.48872066851
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2697514.262963503,
            "upper_bound": 2699917.0643776823
          },
          "point_estimate": 2698671.1155844154,
          "standard_error": 619.1580385864057
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 935.2194911237764,
            "upper_bound": 2611.238893102892
          },
          "point_estimate": 2059.3234364981618,
          "standard_error": 418.127130427501
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117574.23248694318,
            "upper_bound": 117721.0130747568
          },
          "point_estimate": 117650.84281579622,
          "standard_error": 37.51578432801278
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117596.08732718894,
            "upper_bound": 117751.74112903226
          },
          "point_estimate": 117647.9561155914,
          "standard_error": 33.00630296645591
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.521012848661403,
            "upper_bound": 209.9807405516764
          },
          "point_estimate": 88.76337714187126,
          "standard_error": 53.10016810493122
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117615.52110303116,
            "upper_bound": 117714.65679784682
          },
          "point_estimate": 117665.82226225387,
          "standard_error": 25.206218406579083
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.5982915073274,
            "upper_bound": 173.39725821228097
          },
          "point_estimate": 125.16710325315698,
          "standard_error": 31.77155209152425
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9164.38461163166,
            "upper_bound": 9180.10762838292
          },
          "point_estimate": 9172.51256260556,
          "standard_error": 4.035796634297462
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9162.446848209784,
            "upper_bound": 9183.478032022187
          },
          "point_estimate": 9173.805833963692,
          "standard_error": 6.234665771224331
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6660756208471197,
            "upper_bound": 23.683090151485455
          },
          "point_estimate": 15.218462566428943,
          "standard_error": 5.166228568350609
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9167.385054953618,
            "upper_bound": 9181.588441905822
          },
          "point_estimate": 9175.012977844142,
          "standard_error": 3.6472606692223497
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.772804664899968,
            "upper_bound": 17.208945522466998
          },
          "point_estimate": 13.476041653201555,
          "standard_error": 2.5318280678585467
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8844.548788965712,
            "upper_bound": 8861.779778341774
          },
          "point_estimate": 8853.15878479103,
          "standard_error": 4.420059966693232
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8841.70649793137,
            "upper_bound": 8865.85956132879
          },
          "point_estimate": 8853.485483085908,
          "standard_error": 5.878330958580484
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9244301233066645,
            "upper_bound": 26.303843071586435
          },
          "point_estimate": 14.509330650170968,
          "standard_error": 5.875922072693955
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8849.935800730667,
            "upper_bound": 8859.885438132154
          },
          "point_estimate": 8855.098182956008,
          "standard_error": 2.5116244760759576
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.524594737913315,
            "upper_bound": 18.514631844114444
          },
          "point_estimate": 14.769213967078564,
          "standard_error": 2.567924702692845
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14280.893906601805,
            "upper_bound": 14300.557047658029
          },
          "point_estimate": 14291.14626645773,
          "standard_error": 5.024593695325527
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14280.975982318272,
            "upper_bound": 14306.829360401658
          },
          "point_estimate": 14290.079324539243,
          "standard_error": 7.056673470820626
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.2996830455445827,
            "upper_bound": 31.9295995870335
          },
          "point_estimate": 17.090419565608663,
          "standard_error": 6.764432163997416
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14285.66245783467,
            "upper_bound": 14304.279931561565
          },
          "point_estimate": 14297.63285892889,
          "standard_error": 4.707163138849903
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.155425015904765,
            "upper_bound": 21.98612420563339
          },
          "point_estimate": 16.790475670679825,
          "standard_error": 3.471436116523657
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.143356024308275,
            "upper_bound": 28.223528386353266
          },
          "point_estimate": 28.18489791284972,
          "standard_error": 0.020502906124706708
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.13604140563525,
            "upper_bound": 28.249464380454057
          },
          "point_estimate": 28.18246768216357,
          "standard_error": 0.027118012110053137
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014641287822169871,
            "upper_bound": 0.1208610356414144
          },
          "point_estimate": 0.07162690137131691,
          "standard_error": 0.02688657277262777
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.164781820523977,
            "upper_bound": 28.23306247753816
          },
          "point_estimate": 28.19544388385716,
          "standard_error": 0.017630688029089445
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03800146899564286,
            "upper_bound": 0.08835065639635534
          },
          "point_estimate": 0.06816476832353228,
          "standard_error": 0.013517941342473376
        }
      }
    },
    "memmem/sliceslice/prebuilt/sliceslice-i386/words": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/sliceslice_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26375884.1058125,
            "upper_bound": 26398875.82918651
          },
          "point_estimate": 26387348.98575397,
          "standard_error": 5894.627608614885
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26373426.983333334,
            "upper_bound": 26404451.5
          },
          "point_estimate": 26382662.785714284,
          "standard_error": 9190.611571926276
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3624.256818990219,
            "upper_bound": 33485.72501800954
          },
          "point_estimate": 24458.92949243301,
          "standard_error": 8018.304907135061
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26374782.23109244,
            "upper_bound": 26399297.849820144
          },
          "point_estimate": 26389496.35064935,
          "standard_error": 6318.289660365182
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11764.015705512546,
            "upper_bound": 24474.585048123543
          },
          "point_estimate": 19601.641474450076,
          "standard_error": 3274.126139279172
        }
      }
    },
    "memmem/sliceslice/prebuilt/sliceslice-words/words": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/sliceslice_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76999110.53750001,
            "upper_bound": 77628186.26666668
          },
          "point_estimate": 77231047.36666667,
          "standard_error": 181679.38143034512
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76961184.33333334,
            "upper_bound": 77149457.83333334
          },
          "point_estimate": 77071050.0,
          "standard_error": 57687.84651929267
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18606.62966966629,
            "upper_bound": 236849.29939508068
          },
          "point_estimate": 100693.98951232064,
          "standard_error": 71840.58110687978
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65666.39033558252,
            "upper_bound": 924902.5496894312
          },
          "point_estimate": 600874.1136543389,
          "standard_error": 302142.9922918604
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.268459642080329,
            "upper_bound": 5.297881299352225
          },
          "point_estimate": 5.28438573319144,
          "standard_error": 0.007560994885876877
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.26912660956895,
            "upper_bound": 5.306269985553589
          },
          "point_estimate": 5.293289444181642,
          "standard_error": 0.009606420700243094
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001975202232069652,
            "upper_bound": 0.0407179192780855
          },
          "point_estimate": 0.0202258998198177,
          "standard_error": 0.009427238605991642
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.2840325688688115,
            "upper_bound": 5.303454050603025
          },
          "point_estimate": 5.295504280935957,
          "standard_error": 0.004985327774893829
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011093527924958378,
            "upper_bound": 0.033365993246095536
          },
          "point_estimate": 0.025172650971714818,
          "standard_error": 0.006063459033372702
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.170296516875203,
            "upper_bound": 5.181928712081926
          },
          "point_estimate": 5.175848844310548,
          "standard_error": 0.0029780586271981793
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.169226628147654,
            "upper_bound": 5.183517524147551
          },
          "point_estimate": 5.17496885136322,
          "standard_error": 0.0025986148504412816
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00013655951784651853,
            "upper_bound": 0.019165884120885
          },
          "point_estimate": 0.0034470570490077964,
          "standard_error": 0.005097358990607139
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.166987822704997,
            "upper_bound": 5.1778082011352655
          },
          "point_estimate": 5.172123493221576,
          "standard_error": 0.0027794760015339066
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004429078027432029,
            "upper_bound": 0.012874993268533962
          },
          "point_estimate": 0.009952635345093865,
          "standard_error": 0.0021746674821249354
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.39273271167217,
            "upper_bound": 5.40146635611037
          },
          "point_estimate": 5.397134753912836,
          "standard_error": 0.002234660535327138
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.391359589080745,
            "upper_bound": 5.403613944947157
          },
          "point_estimate": 5.397400820026359,
          "standard_error": 0.002842458704191862
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016949477478249194,
            "upper_bound": 0.013677681153520057
          },
          "point_estimate": 0.00717442508201676,
          "standard_error": 0.003038810265357213
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.395469975960471,
            "upper_bound": 5.401771379815808
          },
          "point_estimate": 5.398783667444555,
          "standard_error": 0.0015877269245185605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0042595317282057765,
            "upper_bound": 0.009293408425618526
          },
          "point_estimate": 0.007463653546759879,
          "standard_error": 0.0012938816367104669
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.199303762915273,
            "upper_bound": 5.239568854182497
          },
          "point_estimate": 5.220735244304931,
          "standard_error": 0.010355536893812555
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.193929734521371,
            "upper_bound": 5.247944723193504
          },
          "point_estimate": 5.224653429235222,
          "standard_error": 0.01131617278970795
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004945696657651407,
            "upper_bound": 0.05733722038105509
          },
          "point_estimate": 0.0261168196882128,
          "standard_error": 0.013733867480221892
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.209222131754599,
            "upper_bound": 5.2373886652155965
          },
          "point_estimate": 5.223224900901288,
          "standard_error": 0.0070318471035692234
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014557289023674026,
            "upper_bound": 0.043986975555102885
          },
          "point_estimate": 0.03454690292178852,
          "standard_error": 0.007614916196527093
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.520654159497407,
            "upper_bound": 4.5303492245140875
          },
          "point_estimate": 4.52485742203752,
          "standard_error": 0.002528738598499936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.519250281036117,
            "upper_bound": 4.5281931812224165
          },
          "point_estimate": 4.522641361415685,
          "standard_error": 0.0018229155801593355
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006156102861125467,
            "upper_bound": 0.010714667928048842
          },
          "point_estimate": 0.0027853748341666487,
          "standard_error": 0.0026517663084418045
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.520450124161786,
            "upper_bound": 4.526525831639177
          },
          "point_estimate": 4.523281231715006,
          "standard_error": 0.00154124138763373
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023593710769589226,
            "upper_bound": 0.012067118813324267
          },
          "point_estimate": 0.00843484510269283,
          "standard_error": 0.0028292544737667695
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.34469507341551,
            "upper_bound": 18.370977250302833
          },
          "point_estimate": 18.357389806823495,
          "standard_error": 0.006716423857187895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.341345283040763,
            "upper_bound": 18.370746218719734
          },
          "point_estimate": 18.356213674640145,
          "standard_error": 0.006960777016098042
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0055388486268855485,
            "upper_bound": 0.03726783138168955
          },
          "point_estimate": 0.01486178757073773,
          "standard_error": 0.00813773125646126
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.348393224386697,
            "upper_bound": 18.365941776885816
          },
          "point_estimate": 18.357359963143633,
          "standard_error": 0.004474972022016814
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010383890552631257,
            "upper_bound": 0.030111139284929585
          },
          "point_estimate": 0.022402808829819376,
          "standard_error": 0.005117573271239475
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.36860351784049,
            "upper_bound": 7.38026527797199
          },
          "point_estimate": 7.374373850306489,
          "standard_error": 0.002983156432587496
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.364353263869101,
            "upper_bound": 7.383429526358011
          },
          "point_estimate": 7.373277200682717,
          "standard_error": 0.0048948010343007236
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025520592515980677,
            "upper_bound": 0.016838856981857622
          },
          "point_estimate": 0.013684122228400484,
          "standard_error": 0.0036363647906261978
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.367468877484216,
            "upper_bound": 7.381019297305387
          },
          "point_estimate": 7.375042323238375,
          "standard_error": 0.0033938864927640583
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006180474536343352,
            "upper_bound": 0.011859817906308736
          },
          "point_estimate": 0.009951271303229212,
          "standard_error": 0.001436525382672847
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.114724971262875,
            "upper_bound": 6.1235145224324485
          },
          "point_estimate": 6.119121589890583,
          "standard_error": 0.0022496641870398677
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.112366466974627,
            "upper_bound": 6.126221172090729
          },
          "point_estimate": 6.11833961423287,
          "standard_error": 0.003884927611418315
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002202868146109899,
            "upper_bound": 0.012233906269993893
          },
          "point_estimate": 0.009182726969954114,
          "standard_error": 0.002663835887063368
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.114813035947749,
            "upper_bound": 6.126901550291375
          },
          "point_estimate": 6.122017144418701,
          "standard_error": 0.003068929016936709
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004749996694790215,
            "upper_bound": 0.009061067081328356
          },
          "point_estimate": 0.007512312928090459,
          "standard_error": 0.001101927349063055
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.785906676342906,
            "upper_bound": 9.800584200980738
          },
          "point_estimate": 9.792769768420351,
          "standard_error": 0.003753656000746121
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.782583474392704,
            "upper_bound": 9.799087029789744
          },
          "point_estimate": 9.792839027513024,
          "standard_error": 0.004942738732690867
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015770528811944432,
            "upper_bound": 0.021593888197745853
          },
          "point_estimate": 0.011227465868468732,
          "standard_error": 0.004754551352284137
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.7867600877474,
            "upper_bound": 9.805399547123544
          },
          "point_estimate": 9.795332040055555,
          "standard_error": 0.004790812693351044
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006195513778679186,
            "upper_bound": 0.017059180556703954
          },
          "point_estimate": 0.012515851341421906,
          "standard_error": 0.0030838889978048933
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.339626972915546,
            "upper_bound": 7.351720380748407
          },
          "point_estimate": 7.345761760614866,
          "standard_error": 0.0030943080489010183
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.338853196252071,
            "upper_bound": 7.354146257253502
          },
          "point_estimate": 7.345108695087838,
          "standard_error": 0.0035511813791479236
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015681443738892057,
            "upper_bound": 0.01884196302495532
          },
          "point_estimate": 0.010260676029983491,
          "standard_error": 0.004452717100254245
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.340299618391338,
            "upper_bound": 7.352012158308139
          },
          "point_estimate": 7.345882877374444,
          "standard_error": 0.002987008084594603
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00559468503809491,
            "upper_bound": 0.01348606772595777
          },
          "point_estimate": 0.010340906339205624,
          "standard_error": 0.002071565574946112
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.153987722850737,
            "upper_bound": 4.159029725637271
          },
          "point_estimate": 4.156530634047894,
          "standard_error": 0.0012937894238480002
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.153071650121943,
            "upper_bound": 4.160550790169623
          },
          "point_estimate": 4.156606015376727,
          "standard_error": 0.0018999696508896775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001387812067219689,
            "upper_bound": 0.007366660017802474
          },
          "point_estimate": 0.00549715396748287,
          "standard_error": 0.0015773966641805437
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.156387855710874,
            "upper_bound": 4.1611978972248425
          },
          "point_estimate": 4.159567689299827,
          "standard_error": 0.0012280404785568644
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002556943804373943,
            "upper_bound": 0.005243527856116357
          },
          "point_estimate": 0.004299733633442055,
          "standard_error": 0.0006836624915941154
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.64453811308423,
            "upper_bound": 15.670696241642052
          },
          "point_estimate": 15.657045467301632,
          "standard_error": 0.0067243059661773324
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.64232232030612,
            "upper_bound": 15.675063029342104
          },
          "point_estimate": 15.650893746554855,
          "standard_error": 0.007918432542298718
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014316563845465369,
            "upper_bound": 0.037508491171479016
          },
          "point_estimate": 0.01330898573228294,
          "standard_error": 0.010402898886968456
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.648716827706943,
            "upper_bound": 15.666976054069698
          },
          "point_estimate": 15.656140741662242,
          "standard_error": 0.004719018544189667
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00998644606728718,
            "upper_bound": 0.02814552469822365
          },
          "point_estimate": 0.022384593689786788,
          "standard_error": 0.004435210118428577
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1225236.4815317462,
            "upper_bound": 1227378.9915984126
          },
          "point_estimate": 1226166.8896984127,
          "standard_error": 557.353489095078
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1225150.4222222222,
            "upper_bound": 1226926.925
          },
          "point_estimate": 1225590.620833333,
          "standard_error": 393.6712066938008
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.83587718005347,
            "upper_bound": 2320.7481562985727
          },
          "point_estimate": 721.8626893510528,
          "standard_error": 564.6355801299941
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1224886.1766458766,
            "upper_bound": 1226495.5618307428
          },
          "point_estimate": 1225520.9356709956,
          "standard_error": 408.364666491561
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 427.96053026741026,
            "upper_bound": 2604.714871606701
          },
          "point_estimate": 1859.237236962421,
          "standard_error": 600.4179151981746
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1367702.7997993827,
            "upper_bound": 1370532.8800558497
          },
          "point_estimate": 1369063.800117578,
          "standard_error": 729.4405620689112
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1367222.7777777778,
            "upper_bound": 1371397.8744855963
          },
          "point_estimate": 1368149.625,
          "standard_error": 1209.408695613506
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.63185068670533,
            "upper_bound": 3794.2661918975073
          },
          "point_estimate": 2334.365098371635,
          "standard_error": 1050.7504058641823
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1367593.1961698227,
            "upper_bound": 1369696.1080199056
          },
          "point_estimate": 1368586.7328523328,
          "standard_error": 541.7125573714472
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1349.3360580255828,
            "upper_bound": 2941.682392145543
          },
          "point_estimate": 2431.8712997288585,
          "standard_error": 399.4451100314846
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1265170.2909578546,
            "upper_bound": 1269384.905539135
          },
          "point_estimate": 1267105.4157197592,
          "standard_error": 1086.3847517145168
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1264364.124137931,
            "upper_bound": 1268965.0591133004
          },
          "point_estimate": 1266600.3060344828,
          "standard_error": 1299.1250019076226
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 444.04508263418666,
            "upper_bound": 5623.407850854065
          },
          "point_estimate": 3366.7250852287852,
          "standard_error": 1226.571757730238
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1265159.5484024044,
            "upper_bound": 1268111.4687237632
          },
          "point_estimate": 1266554.3035378414,
          "standard_error": 770.3075080574456
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1630.754419250047,
            "upper_bound": 4945.8976138448825
          },
          "point_estimate": 3616.233444334704,
          "standard_error": 939.924698387218
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/stud_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332333.48340909096,
            "upper_bound": 333233.42167126626
          },
          "point_estimate": 332682.85347402596,
          "standard_error": 248.03372359680887
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332250.89090909093,
            "upper_bound": 332688.2128787879
          },
          "point_estimate": 332453.12873376627,
          "standard_error": 117.8945163088669
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.81153947791303,
            "upper_bound": 523.7845997918645
          },
          "point_estimate": 284.64459358289963,
          "standard_error": 123.90149120634126
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332360.6509427536,
            "upper_bound": 332558.93341750844
          },
          "point_estimate": 332469.44814639905,
          "standard_error": 50.75562195622652
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146.1665519479539,
            "upper_bound": 1261.3179517672693
          },
          "point_estimate": 827.0814604874971,
          "standard_error": 376.5545278547402
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/stud_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 260173.06741101187,
            "upper_bound": 260539.39237074833
          },
          "point_estimate": 260344.93596201815,
          "standard_error": 94.32198921354748
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 260135.38821428572,
            "upper_bound": 260566.80992063493
          },
          "point_estimate": 260275.2369047619,
          "standard_error": 97.15888953276296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.27390610747627,
            "upper_bound": 525.1463240101396
          },
          "point_estimate": 234.19273994938055,
          "standard_error": 119.1695860363932
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 260110.8528142589,
            "upper_bound": 260394.1141630521
          },
          "point_estimate": 260229.17740259744,
          "standard_error": 72.35984313130538
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122.70001133840071,
            "upper_bound": 401.0569872084748
          },
          "point_estimate": 314.1896735370703,
          "standard_error": 70.01629946911041
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 457511.5497321429,
            "upper_bound": 458028.2702787698
          },
          "point_estimate": 457778.83303769847,
          "standard_error": 131.98377945209694
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 457347.7986607143,
            "upper_bound": 458177.02
          },
          "point_estimate": 457922.6145833334,
          "standard_error": 189.95743563293092
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.430708564794893,
            "upper_bound": 746.4091322486255
          },
          "point_estimate": 467.24903003801495,
          "standard_error": 183.14301199525775
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 457592.399612636,
            "upper_bound": 458112.8542040359
          },
          "point_estimate": 457930.13775974023,
          "standard_error": 132.49207642977757
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232.16940459757296,
            "upper_bound": 535.8838526260156
          },
          "point_estimate": 440.6029819650972,
          "standard_error": 75.99215855907023
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/stud_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243039.5939761905,
            "upper_bound": 243394.0387718056
          },
          "point_estimate": 243219.3318976191,
          "standard_error": 90.74479021243928
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 242973.25333333333,
            "upper_bound": 243531.76222222223
          },
          "point_estimate": 243227.8400833333,
          "standard_error": 137.96815711260504
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.9863372643686,
            "upper_bound": 505.0569061001218
          },
          "point_estimate": 414.0226319829709,
          "standard_error": 111.86687640234524
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 242962.05158573284,
            "upper_bound": 243375.34969904425
          },
          "point_estimate": 243189.4867878788,
          "standard_error": 104.1816936655428
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182.04574422523035,
            "upper_bound": 372.7917243083024
          },
          "point_estimate": 301.4204587025566,
          "standard_error": 49.115573329438995
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/stud_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 619949.626779661,
            "upper_bound": 621088.7331567797
          },
          "point_estimate": 620517.9802118645,
          "standard_error": 291.87886182446454
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 619702.6553672317,
            "upper_bound": 621221.3711158192
          },
          "point_estimate": 620510.470338983,
          "standard_error": 335.42932259200154
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223.3407028145295,
            "upper_bound": 1809.3556892757856
          },
          "point_estimate": 830.2057276337761,
          "standard_error": 415.3922068034925
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 619985.7031572512,
            "upper_bound": 621114.6827013493
          },
          "point_estimate": 620694.7667180278,
          "standard_error": 285.04947146455635
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 535.7056054125675,
            "upper_bound": 1234.937263951252
          },
          "point_estimate": 972.5092353777422,
          "standard_error": 180.33980737775136
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114945.92077953588,
            "upper_bound": 115145.49824920886
          },
          "point_estimate": 115029.71585706752,
          "standard_error": 52.1425295534931
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114926.4868670886,
            "upper_bound": 115041.93459915611
          },
          "point_estimate": 115021.83227848102,
          "standard_error": 32.40766126906003
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.312420004688923,
            "upper_bound": 181.152603809219
          },
          "point_estimate": 67.23555692341432,
          "standard_error": 45.394409508570725
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114945.48229728162,
            "upper_bound": 115128.76289367852
          },
          "point_estimate": 115018.46722012165,
          "standard_error": 46.47058306762119
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.51807119256235,
            "upper_bound": 258.17027322912486
          },
          "point_estimate": 173.52392166476127,
          "standard_error": 66.33766203996375
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123358.83322824858,
            "upper_bound": 123616.89391631358
          },
          "point_estimate": 123484.09334180792,
          "standard_error": 66.19804382951165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123311.03131355932,
            "upper_bound": 123669.73220338984
          },
          "point_estimate": 123437.99788135591,
          "standard_error": 100.89935247180333
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.09689952053525,
            "upper_bound": 367.6394569052937
          },
          "point_estimate": 221.64440322604284,
          "standard_error": 87.6545777341035
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123341.52160358724,
            "upper_bound": 123597.85887762337
          },
          "point_estimate": 123432.2858727713,
          "standard_error": 65.62185722735394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123.7874804600227,
            "upper_bound": 273.3904084882953
          },
          "point_estimate": 220.99261382187595,
          "standard_error": 37.90246831436718
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205341.4057062147,
            "upper_bound": 205738.06744608103
          },
          "point_estimate": 205557.7883021702,
          "standard_error": 101.97700235508722
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205396.71581920903,
            "upper_bound": 205816.97013720745
          },
          "point_estimate": 205586.9599576271,
          "standard_error": 114.44094318563526
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.84075368504233,
            "upper_bound": 518.2233227017686
          },
          "point_estimate": 311.53452040136415,
          "standard_error": 108.12393683529908
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205320.7721818671,
            "upper_bound": 205800.2263106218
          },
          "point_estimate": 205574.6090688972,
          "standard_error": 121.59704145940104
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150.7846429229187,
            "upper_bound": 468.837002032306
          },
          "point_estimate": 339.27359576411027,
          "standard_error": 92.57779681432856
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279017.89603023144,
            "upper_bound": 279475.9323791348
          },
          "point_estimate": 279260.32045044226,
          "standard_error": 117.73131113076929
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278993.8538107355,
            "upper_bound": 279529.1921119593
          },
          "point_estimate": 279388.85515267175,
          "standard_error": 179.71300001119454
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.02071174512695,
            "upper_bound": 720.7905763856617
          },
          "point_estimate": 377.6665644146733,
          "standard_error": 177.50367207674464
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279115.0191266629,
            "upper_bound": 279516.49257365894
          },
          "point_estimate": 279332.5086943591,
          "standard_error": 104.31845270452868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.98512626607223,
            "upper_bound": 517.2833706717931
          },
          "point_estimate": 391.59756365268777,
          "standard_error": 85.15641910506326
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 304488.6300045914,
            "upper_bound": 305481.29391118983
          },
          "point_estimate": 305073.81250852684,
          "standard_error": 263.8264139042759
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 304978.75564738293,
            "upper_bound": 305536.8484848485
          },
          "point_estimate": 305267.40444214875,
          "standard_error": 172.59966124240952
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.50203488730433,
            "upper_bound": 779.8293040616468
          },
          "point_estimate": 394.98525869010217,
          "standard_error": 180.1712530748785
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 305257.031643595,
            "upper_bound": 305642.2368129125
          },
          "point_estimate": 305479.0288290222,
          "standard_error": 98.09930041353252
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220.80381203220372,
            "upper_bound": 1323.0245082635356
          },
          "point_estimate": 881.0212779825321,
          "standard_error": 354.4403021632446
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325784.92910071224,
            "upper_bound": 326798.9450425702
          },
          "point_estimate": 326287.4160629251,
          "standard_error": 259.80937284234335
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325608.56696428574,
            "upper_bound": 326922.21279761905
          },
          "point_estimate": 326327.54873511905,
          "standard_error": 332.4033401458043
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218.63495861841247,
            "upper_bound": 1558.9772585726078
          },
          "point_estimate": 912.2076916175516,
          "standard_error": 344.5919015346532
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325437.75404355815,
            "upper_bound": 326655.4936816863
          },
          "point_estimate": 326036.0865955473,
          "standard_error": 325.15333660361455
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 494.4317385670227,
            "upper_bound": 1083.2042830812993
          },
          "point_estimate": 866.530586337978,
          "standard_error": 151.15809608270604
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406904.6606539352,
            "upper_bound": 407837.83781966486
          },
          "point_estimate": 407344.890888448,
          "standard_error": 239.32985705647553
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406737.92291666666,
            "upper_bound": 407765.2
          },
          "point_estimate": 407298.6546296296,
          "standard_error": 290.1113545479423
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.04389949753204,
            "upper_bound": 1300.1413369179002
          },
          "point_estimate": 687.524163905115,
          "standard_error": 276.14734884196406
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406961.3100694445,
            "upper_bound": 408121.4555555555
          },
          "point_estimate": 407554.0051370851,
          "standard_error": 289.4445053094142
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.82371397665776,
            "upper_bound": 1086.651593166868
          },
          "point_estimate": 798.883528095582,
          "standard_error": 195.22295099084965
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279928.23356849817,
            "upper_bound": 280321.5972538462
          },
          "point_estimate": 280117.91317582416,
          "standard_error": 100.62012571386552
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279816.82499999995,
            "upper_bound": 280387.9314102564
          },
          "point_estimate": 280086.6046153846,
          "standard_error": 149.88042710607888
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.08337658418058,
            "upper_bound": 579.4825630967454
          },
          "point_estimate": 382.9194585864139,
          "standard_error": 127.66599703728768
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279881.888347939,
            "upper_bound": 280232.34001554
          },
          "point_estimate": 280060.6035764236,
          "standard_error": 87.95463507788072
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.5178442156982,
            "upper_bound": 406.76756981601
          },
          "point_estimate": 334.22460284192715,
          "standard_error": 54.74224517433018
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125912.17728002966,
            "upper_bound": 126072.4671377767
          },
          "point_estimate": 125991.92576522769,
          "standard_error": 40.939132376689535
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125871.65951557094,
            "upper_bound": 126139.43425605536
          },
          "point_estimate": 125984.0104053386,
          "standard_error": 58.42029927958412
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.06472231045512,
            "upper_bound": 232.53516803141343
          },
          "point_estimate": 174.98890038813133,
          "standard_error": 56.890853468009446
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125929.11510164235,
            "upper_bound": 126011.16363398705
          },
          "point_estimate": 125970.42295420842,
          "standard_error": 20.517799727527077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.34955151494366,
            "upper_bound": 169.0319945658436
          },
          "point_estimate": 136.45456244071357,
          "standard_error": 22.64770274562636
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207846.6454603175,
            "upper_bound": 208463.3464952381
          },
          "point_estimate": 208097.59228344672,
          "standard_error": 164.4785384617598
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207816.57904761905,
            "upper_bound": 208185.8688571429
          },
          "point_estimate": 207946.76952380955,
          "standard_error": 95.0337263881025
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.29061242757758,
            "upper_bound": 491.095104138456
          },
          "point_estimate": 210.48467119643533,
          "standard_error": 111.96545030590816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207812.5998265948,
            "upper_bound": 208034.8871181567
          },
          "point_estimate": 207919.63943599255,
          "standard_error": 58.424269281972066
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.55719041769956,
            "upper_bound": 820.67810217492
          },
          "point_estimate": 546.7029338483288,
          "standard_error": 222.30509671540975
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154872.11892046605,
            "upper_bound": 155019.20523226104
          },
          "point_estimate": 154940.42197585278,
          "standard_error": 37.750067535941405
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154832.28936170213,
            "upper_bound": 155048.3866261398
          },
          "point_estimate": 154893.7570212766,
          "standard_error": 55.40584410653654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.415962705068653,
            "upper_bound": 221.35322755954897
          },
          "point_estimate": 105.5479131722589,
          "standard_error": 50.72872645150514
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154857.54486838065,
            "upper_bound": 154998.7362004142
          },
          "point_estimate": 154912.7031334623,
          "standard_error": 35.93063541432439
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.28959030746887,
            "upper_bound": 163.46369046962013
          },
          "point_estimate": 125.85708931691124,
          "standard_error": 27.396882945657765
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44338.805412620895,
            "upper_bound": 44435.837950268426
          },
          "point_estimate": 44384.71272336571,
          "standard_error": 24.86963069935896
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44336.35086996337,
            "upper_bound": 44440.67602767603
          },
          "point_estimate": 44361.41436711437,
          "standard_error": 23.517601593182476
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.403368164708132,
            "upper_bound": 139.09128155994568
          },
          "point_estimate": 45.90376467649504,
          "standard_error": 35.23689065996156
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44344.22758690862,
            "upper_bound": 44402.21476159905
          },
          "point_estimate": 44370.67461427461,
          "standard_error": 14.882960397795308
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.33752752319116,
            "upper_bound": 108.49146028949184
          },
          "point_estimate": 82.8584011645904,
          "standard_error": 19.017990175567785
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1226677.010616435,
            "upper_bound": 1229758.47275
          },
          "point_estimate": 1228033.360935185,
          "standard_error": 795.6548516626538
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1226289.433333333,
            "upper_bound": 1228732.0733333332
          },
          "point_estimate": 1227836.6446759258,
          "standard_error": 674.4472533713767
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289.06375236808105,
            "upper_bound": 3489.190931804536
          },
          "point_estimate": 1510.7006656521305,
          "standard_error": 772.5725400863613
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1226638.382729131,
            "upper_bound": 1228519.371321685
          },
          "point_estimate": 1227718.035238095,
          "standard_error": 487.4626691584568
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 961.3985407729964,
            "upper_bound": 3813.113475420817
          },
          "point_estimate": 2647.762768706686,
          "standard_error": 859.0052088511711
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103862.37322460316,
            "upper_bound": 103989.35227581636
          },
          "point_estimate": 103923.33736201817,
          "standard_error": 32.39127801937226
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103848.3455357143,
            "upper_bound": 103989.03761904762
          },
          "point_estimate": 103910.59,
          "standard_error": 44.55826386974411
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.913553859506601,
            "upper_bound": 195.5485068854522
          },
          "point_estimate": 104.05310215268638,
          "standard_error": 44.319718856065904
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103814.94895003302,
            "upper_bound": 103974.05891537794
          },
          "point_estimate": 103876.62848979593,
          "standard_error": 40.11095587724593
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.58153799115074,
            "upper_bound": 144.46808792329466
          },
          "point_estimate": 107.9442807799532,
          "standard_error": 24.089543804272804
        }
      }
    },
    "memmem/stud/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/stud_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27102.68171882255,
            "upper_bound": 27143.85752563225
          },
          "point_estimate": 27123.03011839612,
          "standard_error": 10.58806809829606
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27090.19141791045,
            "upper_bound": 27158.539029850745
          },
          "point_estimate": 27118.77209784411,
          "standard_error": 19.185927549138224
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.397305034658364,
            "upper_bound": 57.45664855830419
          },
          "point_estimate": 46.06666163240413,
          "standard_error": 12.85676586433405
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27103.49171894536,
            "upper_bound": 27156.448646627006
          },
          "point_estimate": 27134.612523744912,
          "standard_error": 13.43040822924046
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.722499495950206,
            "upper_bound": 41.12783491345828
          },
          "point_estimate": 35.28610969391688,
          "standard_error": 4.690992228991718
        }
      }
    },
    "memmem/stud/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/stud_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28276.297438102687,
            "upper_bound": 28370.088321433228
          },
          "point_estimate": 28321.78021299502,
          "standard_error": 24.086667823127332
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28249.690687168797,
            "upper_bound": 28391.940422204847
          },
          "point_estimate": 28315.53199113891,
          "standard_error": 38.19087901261242
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.231006343496677,
            "upper_bound": 140.2866824585926
          },
          "point_estimate": 102.86009686034124,
          "standard_error": 30.12562581919808
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28258.83862843568,
            "upper_bound": 28350.013794258903
          },
          "point_estimate": 28289.7453834672,
          "standard_error": 23.335346466812528
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.7475559973668,
            "upper_bound": 100.26256465791722
          },
          "point_estimate": 80.35006233636493,
          "standard_error": 14.052697464789253
        }
      }
    },
    "memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/stud_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 564837.736025641,
            "upper_bound": 566257.2051822344
          },
          "point_estimate": 565550.1729670329,
          "standard_error": 362.2359958804592
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 564383.6051282051,
            "upper_bound": 566706.6835164835
          },
          "point_estimate": 565484.9717948718,
          "standard_error": 637.9454434015122
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 235.53300043387324,
            "upper_bound": 2026.44425479269
          },
          "point_estimate": 1575.7767658704197,
          "standard_error": 448.63747697417335
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 565045.9717310446,
            "upper_bound": 566289.234026141
          },
          "point_estimate": 565727.8996603397,
          "standard_error": 318.12593723206436
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 777.696429410563,
            "upper_bound": 1416.7005739238505
          },
          "point_estimate": 1208.199744430555,
          "standard_error": 162.6897981982249
        }
      }
    },
    "memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/stud_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128.6234945784504,
            "upper_bound": 1130.1182758656864
          },
          "point_estimate": 1129.3591277562905,
          "standard_error": 0.38212651919628
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128.262368281993,
            "upper_bound": 1130.3653112608072
          },
          "point_estimate": 1129.254691077886,
          "standard_error": 0.4314364952692302
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.23673855536948152,
            "upper_bound": 2.4589651921159303
          },
          "point_estimate": 0.865001681046754,
          "standard_error": 0.5783263440536708
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1129.0148055383609,
            "upper_bound": 1130.144599570187
          },
          "point_estimate": 1129.584394189007,
          "standard_error": 0.284092891036095
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6829319452021441,
            "upper_bound": 1.6179596094918374
          },
          "point_estimate": 1.2748364205741396,
          "standard_error": 0.24153262394938416
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.46768452587045,
            "upper_bound": 34.50967730940168
          },
          "point_estimate": 34.48918892444158,
          "standard_error": 0.010783803534719043
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.45332175416496,
            "upper_bound": 34.51788886654423
          },
          "point_estimate": 34.494717808099054,
          "standard_error": 0.015339927614773544
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00817656699504672,
            "upper_bound": 0.06179205132321113
          },
          "point_estimate": 0.04160360736439706,
          "standard_error": 0.015153662926496989
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.46176737458821,
            "upper_bound": 34.510769685727475
          },
          "point_estimate": 34.48812471184923,
          "standard_error": 0.012253060735611
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0199495541089074,
            "upper_bound": 0.043427853488078955
          },
          "point_estimate": 0.03590114682022598,
          "standard_error": 0.0057715627665113085
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.91307627136095,
            "upper_bound": 37.94110573485676
          },
          "point_estimate": 37.92624699343085,
          "standard_error": 0.007236643974898783
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.90833193238737,
            "upper_bound": 37.94710662927631
          },
          "point_estimate": 37.91780246125341,
          "standard_error": 0.008412796485571054
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00195094682599143,
            "upper_bound": 0.04144465326781632
          },
          "point_estimate": 0.021780331181467303,
          "standard_error": 0.009845872563440536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.91131474390333,
            "upper_bound": 37.93457823920864
          },
          "point_estimate": 37.919957334285435,
          "standard_error": 0.0058917388562246655
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010097427947299212,
            "upper_bound": 0.03047759080011326
          },
          "point_estimate": 0.024108727569387423,
          "standard_error": 0.005153430051696124
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.396630582110692,
            "upper_bound": 26.422244173123204
          },
          "point_estimate": 26.408784472455142,
          "standard_error": 0.00657939294384843
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.391683712615425,
            "upper_bound": 26.427171929969877
          },
          "point_estimate": 26.399948013618904,
          "standard_error": 0.009225444854783394
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001740785628267475,
            "upper_bound": 0.03623256951474152
          },
          "point_estimate": 0.021057473676559524,
          "standard_error": 0.00925108558099886
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.391804906470224,
            "upper_bound": 26.415615216317292
          },
          "point_estimate": 26.40297605719229,
          "standard_error": 0.006164063367833442
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010875857490821495,
            "upper_bound": 0.026939769363463227
          },
          "point_estimate": 0.02195957581413227,
          "standard_error": 0.003952538812225096
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.59444395541824,
            "upper_bound": 32.649510149258944
          },
          "point_estimate": 32.6195379810304,
          "standard_error": 0.014129416308602262
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.59164788600979,
            "upper_bound": 32.6417159167605
          },
          "point_estimate": 32.60838091299113,
          "standard_error": 0.01406728280197511
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010528034429253734,
            "upper_bound": 0.06758047161222297
          },
          "point_estimate": 0.03159308171274089,
          "standard_error": 0.016469151450511366
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.60195013147304,
            "upper_bound": 32.63380926441369
          },
          "point_estimate": 32.617839527543985,
          "standard_error": 0.008218116641308068
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018214617744689247,
            "upper_bound": 0.06611419207079744
          },
          "point_estimate": 0.04726018292713638,
          "standard_error": 0.013418211697850206
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.33485173733537,
            "upper_bound": 40.41422887841972
          },
          "point_estimate": 40.36994265100968,
          "standard_error": 0.02045207193047612
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.3187943856674,
            "upper_bound": 40.39567428956376
          },
          "point_estimate": 40.35926431713879,
          "standard_error": 0.022675281228733066
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0111311196095132,
            "upper_bound": 0.09879008741976876
          },
          "point_estimate": 0.051909699184183984,
          "standard_error": 0.02067993607425152
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.36257014410817,
            "upper_bound": 40.40450643342675
          },
          "point_estimate": 40.38282809625763,
          "standard_error": 0.010477941113656993
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027940616897520816,
            "upper_bound": 0.09710750553949414
          },
          "point_estimate": 0.06818368462848161,
          "standard_error": 0.02104054721615113
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.88757154588387,
            "upper_bound": 73.00383239480382
          },
          "point_estimate": 72.9464381506531,
          "standard_error": 0.02971877911063976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.82942866290102,
            "upper_bound": 73.02083497921676
          },
          "point_estimate": 72.94856234171948,
          "standard_error": 0.04512645487783376
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0050143235781966175,
            "upper_bound": 0.18058624081613525
          },
          "point_estimate": 0.11227766882097594,
          "standard_error": 0.04550237317789963
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.87696202580759,
            "upper_bound": 73.00535203326228
          },
          "point_estimate": 72.95621027826539,
          "standard_error": 0.03286897181220904
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05871689099803807,
            "upper_bound": 0.12161374191790256
          },
          "point_estimate": 0.09936344219446563,
          "standard_error": 0.016075892580049402
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.05471065680754,
            "upper_bound": 66.27733702382923
          },
          "point_estimate": 66.1543996921755,
          "standard_error": 0.05734983143955051
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.01862541466502,
            "upper_bound": 66.23932539834237
          },
          "point_estimate": 66.11115074207365,
          "standard_error": 0.06344082512930173
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019365362756214997,
            "upper_bound": 0.2695666974645782
          },
          "point_estimate": 0.14642890857815377,
          "standard_error": 0.05898348387516346
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.00980494907093,
            "upper_bound": 66.18878324677476
          },
          "point_estimate": 66.0778535327333,
          "standard_error": 0.04580336285192753
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07774424151273858,
            "upper_bound": 0.26929664139385057
          },
          "point_estimate": 0.19115774015601025,
          "standard_error": 0.05679100039159283
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.4795537884821,
            "upper_bound": 56.569196459778354
          },
          "point_estimate": 56.52274905284274,
          "standard_error": 0.02297043292308887
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.449296799902946,
            "upper_bound": 56.57840441722134
          },
          "point_estimate": 56.528107070678786,
          "standard_error": 0.03005745561451689
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014330937283488185,
            "upper_bound": 0.13321349329072954
          },
          "point_estimate": 0.09042855247922016,
          "standard_error": 0.03471186819485106
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.48028561894479,
            "upper_bound": 56.553419222725935
          },
          "point_estimate": 56.514017905294864,
          "standard_error": 0.018572971803182257
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04243660352292757,
            "upper_bound": 0.09789470714384
          },
          "point_estimate": 0.07649961418371012,
          "standard_error": 0.014840017310041588
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.50629107179276,
            "upper_bound": 108.65432077031156
          },
          "point_estimate": 108.57665431142404,
          "standard_error": 0.03799388149601198
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.47525020327149,
            "upper_bound": 108.65834241744255
          },
          "point_estimate": 108.5545346478222,
          "standard_error": 0.05062027268163239
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026717267048866088,
            "upper_bound": 0.21869026956243867
          },
          "point_estimate": 0.11873797121559362,
          "standard_error": 0.046385934511168056
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.4829154305578,
            "upper_bound": 108.64423082671703
          },
          "point_estimate": 108.56196229337432,
          "standard_error": 0.04253564193129002
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06392798218195606,
            "upper_bound": 0.16429659766120697
          },
          "point_estimate": 0.12631522885409197,
          "standard_error": 0.026481464711941325
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.75330776573732,
            "upper_bound": 45.817921079260806
          },
          "point_estimate": 45.78538904775644,
          "standard_error": 0.01652383706630312
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.74821676726978,
            "upper_bound": 45.82504258316233
          },
          "point_estimate": 45.780983397021345,
          "standard_error": 0.022627997153594863
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003915468327129907,
            "upper_bound": 0.09447769781859468
          },
          "point_estimate": 0.049139412753817925,
          "standard_error": 0.021945582206399535
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.75727514006864,
            "upper_bound": 45.817986455899
          },
          "point_estimate": 45.785062218096066,
          "standard_error": 0.015686906427720065
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029243601156066,
            "upper_bound": 0.07158082832484752
          },
          "point_estimate": 0.05495318529233796,
          "standard_error": 0.01098363865162733
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.87764747109296,
            "upper_bound": 49.93825242154916
          },
          "point_estimate": 49.9106562147667,
          "standard_error": 0.015575442475493188
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.88177629137212,
            "upper_bound": 49.94907977547098
          },
          "point_estimate": 49.924497840410005,
          "standard_error": 0.015468444001229317
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006968588705044341,
            "upper_bound": 0.08039462554575658
          },
          "point_estimate": 0.03383212477982595,
          "standard_error": 0.018231296656585363
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.8927870767364,
            "upper_bound": 49.93737902262689
          },
          "point_estimate": 49.917774578715374,
          "standard_error": 0.011282785989964432
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01981703285403183,
            "upper_bound": 0.07026328177923204
          },
          "point_estimate": 0.051829766690401693,
          "standard_error": 0.013761384008596382
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.64113501930387,
            "upper_bound": 90.78795407783588
          },
          "point_estimate": 90.71260671806333,
          "standard_error": 0.037538143096729315
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.6232208816485,
            "upper_bound": 90.7831076652694
          },
          "point_estimate": 90.71844669381112,
          "standard_error": 0.030972409233465385
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00566408848551233,
            "upper_bound": 0.22915911248493695
          },
          "point_estimate": 0.06022287265910685,
          "standard_error": 0.0651540720067749
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.7057190865006,
            "upper_bound": 90.87135860144494
          },
          "point_estimate": 90.7801310108466,
          "standard_error": 0.04888275046693603
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.057990058278913285,
            "upper_bound": 0.16730877785416232
          },
          "point_estimate": 0.1253404722112391,
          "standard_error": 0.02825991183814028
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1110617.0376222944,
            "upper_bound": 1113510.2646681096
          },
          "point_estimate": 1112067.4326875904,
          "standard_error": 740.6333873329758
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1109948.0126262626,
            "upper_bound": 1114397.9696969695
          },
          "point_estimate": 1111900.2272727273,
          "standard_error": 1014.830925487266
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 600.5749347924406,
            "upper_bound": 4545.994089747216
          },
          "point_estimate": 2824.074400771766,
          "standard_error": 1066.092014571626
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1111559.8714513555,
            "upper_bound": 1114167.30846395
          },
          "point_estimate": 1112928.0613931522,
          "standard_error": 659.9348405073936
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1470.415045091764,
            "upper_bound": 3031.191456892709
          },
          "point_estimate": 2473.3348387324745,
          "standard_error": 401.5877277354169
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1294360.505307266,
            "upper_bound": 1296865.5688916254
          },
          "point_estimate": 1295513.2836042691,
          "standard_error": 640.8551732016044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1293962.625862069,
            "upper_bound": 1296432.83045977
          },
          "point_estimate": 1295051.859482759,
          "standard_error": 575.957490780731
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.3113476306696,
            "upper_bound": 3123.954877986901
          },
          "point_estimate": 1658.6923772763364,
          "standard_error": 823.3511768997624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1294250.8137469727,
            "upper_bound": 1296169.8883773373
          },
          "point_estimate": 1295316.3424093148,
          "standard_error": 490.4378222100265
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 859.9344854089072,
            "upper_bound": 2977.445320938042
          },
          "point_estimate": 2142.6145510890237,
          "standard_error": 595.1225658323131
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1381797.8242960025,
            "upper_bound": 1383963.3750440918
          },
          "point_estimate": 1382875.1042519107,
          "standard_error": 553.9603078250464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1381301.3851851851,
            "upper_bound": 1384165.5601851852
          },
          "point_estimate": 1382902.224537037,
          "standard_error": 646.4108783444887
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.48169505601913,
            "upper_bound": 3302.3747552601835
          },
          "point_estimate": 1804.224717845262,
          "standard_error": 751.697948249042
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1381587.8468491323,
            "upper_bound": 1383946.983804143
          },
          "point_estimate": 1382886.353246753,
          "standard_error": 587.9154108672894
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 987.4576657059414,
            "upper_bound": 2393.8970279496807
          },
          "point_estimate": 1842.1462534241236,
          "standard_error": 364.8961512219176
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1719024.1910200214,
            "upper_bound": 1721191.7836271643
          },
          "point_estimate": 1720154.3216233763,
          "standard_error": 552.8915045508132
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1719454.905844156,
            "upper_bound": 1721136.1227272726
          },
          "point_estimate": 1720318.4848484849,
          "standard_error": 399.7989700654272
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194.60809427227272,
            "upper_bound": 2885.845673118098
          },
          "point_estimate": 1026.537620411657,
          "standard_error": 660.375785827911
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1719226.4494385254,
            "upper_bound": 1721842.366976474
          },
          "point_estimate": 1720672.7017709564,
          "standard_error": 668.4229211144509
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 563.2941856562152,
            "upper_bound": 2623.9719772954327
          },
          "point_estimate": 1843.7986317200769,
          "standard_error": 523.3153286662174
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1727776.822478316,
            "upper_bound": 1730591.051985828
          },
          "point_estimate": 1729165.6491780046,
          "standard_error": 720.5384439518275
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1727369.5345238096,
            "upper_bound": 1730630.8015873015
          },
          "point_estimate": 1729145.6023809523,
          "standard_error": 812.6708823009783
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 364.8563810224633,
            "upper_bound": 4059.8661654229263
          },
          "point_estimate": 1735.93579822868,
          "standard_error": 959.0857075815114
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1727286.3188795515,
            "upper_bound": 1729982.5549998395
          },
          "point_estimate": 1728614.2598639457,
          "standard_error": 722.8257951603525
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1117.558144896543,
            "upper_bound": 3202.6187183456227
          },
          "point_estimate": 2406.7404444306167,
          "standard_error": 518.9776857808041
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 348882.3442614513,
            "upper_bound": 349769.8618707483
          },
          "point_estimate": 349323.27707709756,
          "standard_error": 227.2286341323306
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 348681.724829932,
            "upper_bound": 349923.20793650794
          },
          "point_estimate": 349321.61904761905,
          "standard_error": 314.7681981814575
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 275.87514310225066,
            "upper_bound": 1306.5624068038694
          },
          "point_estimate": 756.6410135669007,
          "standard_error": 267.79008405824516
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 348869.96613011404,
            "upper_bound": 349940.84771258506
          },
          "point_estimate": 349314.43025355594,
          "standard_error": 276.4402882712542
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 438.7715812926485,
            "upper_bound": 941.6886156016974
          },
          "point_estimate": 755.1195502748791,
          "standard_error": 129.08812039159957
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 439005.834442532,
            "upper_bound": 439801.5597474541
          },
          "point_estimate": 439373.75923933834,
          "standard_error": 204.2909418348727
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 438914.62168674695,
            "upper_bound": 439844.1295180723
          },
          "point_estimate": 439103.5246820616,
          "standard_error": 248.3625601325519
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.42471881779761,
            "upper_bound": 1090.5573725664674
          },
          "point_estimate": 590.0849116925674,
          "standard_error": 263.4320516877514
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 438953.9679788862,
            "upper_bound": 439447.1346140116
          },
          "point_estimate": 439152.7936160225,
          "standard_error": 126.60539960689523
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 293.86321050888046,
            "upper_bound": 910.5589642893196
          },
          "point_estimate": 682.351252710679,
          "standard_error": 167.08692077523645
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 700364.4488293651,
            "upper_bound": 702761.6029749885
          },
          "point_estimate": 701375.6819146825,
          "standard_error": 626.3336536497632
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 700169.8138736264,
            "upper_bound": 701984.7384615385
          },
          "point_estimate": 700568.266826923,
          "standard_error": 465.5109251959768
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.3365729307033,
            "upper_bound": 2269.261817404817
          },
          "point_estimate": 624.7372957995791,
          "standard_error": 584.971908743603
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 700098.6385325561,
            "upper_bound": 700755.3853021978
          },
          "point_estimate": 700420.1886613386,
          "standard_error": 170.43923978883134
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414.135542473031,
            "upper_bound": 3004.6173639760827
          },
          "point_estimate": 2087.043801542502,
          "standard_error": 737.202873259574
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1043773.8435356292,
            "upper_bound": 1044631.4652040816
          },
          "point_estimate": 1044194.9933775512,
          "standard_error": 219.55996305381265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1043555.8857142857,
            "upper_bound": 1044752.178095238
          },
          "point_estimate": 1044202.2476190476,
          "standard_error": 279.99771699176716
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200.95281071805704,
            "upper_bound": 1267.9124374900612
          },
          "point_estimate": 726.0473613958047,
          "standard_error": 294.60083492017964
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1043562.2953601952,
            "upper_bound": 1045020.2425324676
          },
          "point_estimate": 1044308.801929499,
          "standard_error": 385.3985208111194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409.81744653097184,
            "upper_bound": 927.1581652631512
          },
          "point_estimate": 732.8597942650354,
          "standard_error": 134.93649476079648
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 647740.4894897661,
            "upper_bound": 648889.6660553991
          },
          "point_estimate": 648340.8325543024,
          "standard_error": 294.61853957864093
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 647501.9122807018,
            "upper_bound": 649202.6432748538
          },
          "point_estimate": 648612.0325814537,
          "standard_error": 456.5265967733865
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223.16814428368943,
            "upper_bound": 1653.854437085565
          },
          "point_estimate": 974.0772863908978,
          "standard_error": 365.0465588012865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 647694.6999119694,
            "upper_bound": 648902.5035679085
          },
          "point_estimate": 648295.3943039416,
          "standard_error": 308.38158356522314
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 544.8380084526427,
            "upper_bound": 1233.9874399177295
          },
          "point_estimate": 981.6085358967248,
          "standard_error": 180.9155588520674
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 312755.1977938034,
            "upper_bound": 313238.9288732194
          },
          "point_estimate": 313008.35578347585,
          "standard_error": 124.20578635108603
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 312654.5060541311,
            "upper_bound": 313371.74074074073
          },
          "point_estimate": 313107.2663817664,
          "standard_error": 198.5316636657975
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.58044352344376,
            "upper_bound": 700.767773285372
          },
          "point_estimate": 408.1210181817022,
          "standard_error": 166.62213425212866
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 312699.8582173382,
            "upper_bound": 313313.7026589166
          },
          "point_estimate": 313036.4541902542,
          "standard_error": 162.69545775441895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226.0135849393222,
            "upper_bound": 502.8305554751512
          },
          "point_estimate": 414.5641450140872,
          "standard_error": 69.92277101763922
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 724663.3179147213,
            "upper_bound": 726333.8609478682
          },
          "point_estimate": 725450.937457205,
          "standard_error": 428.8287312595568
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 724184.2875816993,
            "upper_bound": 726385.2705882352
          },
          "point_estimate": 725367.4601618425,
          "standard_error": 623.8517491588661
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.20375340053505,
            "upper_bound": 2481.8122767232944
          },
          "point_estimate": 1477.731180889219,
          "standard_error": 581.9433652511337
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 724282.2739523207,
            "upper_bound": 725648.4709680921
          },
          "point_estimate": 725090.6271454036,
          "standard_error": 351.22140159205765
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 745.2471671754904,
            "upper_bound": 1855.032915634116
          },
          "point_estimate": 1427.2926587483182,
          "standard_error": 304.4513023166893
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184822.80478940048,
            "upper_bound": 185217.7958456007
          },
          "point_estimate": 185017.4458593183,
          "standard_error": 101.21147396780214
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184766.90609137056,
            "upper_bound": 185302.4179357022
          },
          "point_estimate": 185033.40366207395,
          "standard_error": 117.31003127049782
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.440434878567274,
            "upper_bound": 637.3067620865412
          },
          "point_estimate": 291.3367005129905,
          "standard_error": 158.74355997642704
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184917.61201061905,
            "upper_bound": 185145.15077154952
          },
          "point_estimate": 185045.3582174171,
          "standard_error": 56.99957218329166
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184.1423222812768,
            "upper_bound": 428.2827927269979
          },
          "point_estimate": 338.5679630908409,
          "standard_error": 63.084571038808235
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/stud_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156621.6415706792,
            "upper_bound": 156946.6821537911
          },
          "point_estimate": 156764.46045439062,
          "standard_error": 84.19418560809784
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156593.85932284215,
            "upper_bound": 156868.41416309014
          },
          "point_estimate": 156688.10987124464,
          "standard_error": 83.24381739524706
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.96952772412919,
            "upper_bound": 365.0854712008495
          },
          "point_estimate": 197.15680908916008,
          "standard_error": 83.13619272404505
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156565.17559155764,
            "upper_bound": 156730.35115491302
          },
          "point_estimate": 156639.44025416643,
          "standard_error": 41.45489238721859
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.62973961187863,
            "upper_bound": 399.77304262309127
          },
          "point_estimate": 279.3459399067151,
          "standard_error": 88.75489678721733
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/stud_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 623621.5117917676,
            "upper_bound": 624408.9100451977
          },
          "point_estimate": 624022.3034947538,
          "standard_error": 201.88744604549905
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 623479.0508474576,
            "upper_bound": 624690.1751412429
          },
          "point_estimate": 624050.6553672317,
          "standard_error": 286.3858247870771
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126.58220792221786,
            "upper_bound": 1195.387691319852
          },
          "point_estimate": 865.7357753081515,
          "standard_error": 274.08983364015904
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 623497.1172184452,
            "upper_bound": 624274.6466604742
          },
          "point_estimate": 623887.8901606868,
          "standard_error": 194.0153771063516
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390.87072068176246,
            "upper_bound": 851.2374245167649
          },
          "point_estimate": 671.7463306211158,
          "standard_error": 121.06484933972968
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/stud_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1292.8840610972811,
            "upper_bound": 1294.232860713713
          },
          "point_estimate": 1293.5572675456162,
          "standard_error": 0.3450753752103157
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1292.6783479438386,
            "upper_bound": 1294.488739220298
          },
          "point_estimate": 1293.5508487159386,
          "standard_error": 0.657911916392219
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08289499532121412,
            "upper_bound": 1.7177106494275105
          },
          "point_estimate": 1.3212963862781175,
          "standard_error": 0.5092758905188849
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1292.4880581043697,
            "upper_bound": 1294.5675287159036
          },
          "point_estimate": 1293.3796431278793,
          "standard_error": 0.5294653551520774
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7424691093633848,
            "upper_bound": 1.402164402564133
          },
          "point_estimate": 1.1505678416352862,
          "standard_error": 0.17418842514187105
        }
      }
    },
    "memmem/stud/prebuilt/sliceslice-i386/words": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/stud/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/stud_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 305030691.7,
            "upper_bound": 305456694.6
          },
          "point_estimate": 305250399.9,
          "standard_error": 109457.13119606378
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 304876054.0,
            "upper_bound": 305628327.0
          },
          "point_estimate": 305299802.5,
          "standard_error": 160638.76894402853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40012.40808963776,
            "upper_bound": 581923.4548687935
          },
          "point_estimate": 499622.1064299345,
          "standard_error": 158868.66867991287
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195607.30789200333,
            "upper_bound": 449967.49602108693
          },
          "point_estimate": 364596.1105977255,
          "standard_error": 62662.34554510762
        }
      }
    },
    "memmem/stud/prebuilt/sliceslice-words/words": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/stud/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/stud/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/stud_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266681901.5,
            "upper_bound": 267082819.2
          },
          "point_estimate": 266877239.9,
          "standard_error": 103027.47379964712
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266599000.0,
            "upper_bound": 267010362.5
          },
          "point_estimate": 266908380.0,
          "standard_error": 88173.80738462608
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29568.973875045776,
            "upper_bound": 598173.4918802977
          },
          "point_estimate": 190963.32440972328,
          "standard_error": 145309.36830316734
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120409.69811624163,
            "upper_bound": 472688.9295006695
          },
          "point_estimate": 345202.1067147798,
          "standard_error": 86374.42057577851
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159392.18473556527,
            "upper_bound": 159917.01797598254
          },
          "point_estimate": 159640.69073889236,
          "standard_error": 133.99545252224146
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159301.44143619604,
            "upper_bound": 159929.8169577875
          },
          "point_estimate": 159579.7592014972,
          "standard_error": 164.16628810871927
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.64079202662612,
            "upper_bound": 728.9225430416066
          },
          "point_estimate": 354.4656990344873,
          "standard_error": 156.59286854013362
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159334.03630591062,
            "upper_bound": 159745.27711828044
          },
          "point_estimate": 159555.20955027506,
          "standard_error": 106.08241658511928
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217.6077244999495,
            "upper_bound": 593.3890237104229
          },
          "point_estimate": 447.1997460136632,
          "standard_error": 100.89451127050464
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226078.95298210587,
            "upper_bound": 226785.31121118012
          },
          "point_estimate": 226408.8944905353,
          "standard_error": 182.27840176927737
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 225948.1884057971,
            "upper_bound": 226843.6875
          },
          "point_estimate": 226288.9153726708,
          "standard_error": 204.86515469011024
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.35740969547118,
            "upper_bound": 995.3819715209696
          },
          "point_estimate": 517.8595855887747,
          "standard_error": 221.08973809904504
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 225872.3095868215,
            "upper_bound": 226771.0256461631
          },
          "point_estimate": 226230.27045252884,
          "standard_error": 233.6104828629569
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253.45457352280008,
            "upper_bound": 775.7360681770084
          },
          "point_estimate": 606.9985829166634,
          "standard_error": 133.4914177556409
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 225920.57814551907,
            "upper_bound": 226188.308542098
          },
          "point_estimate": 226054.2326252095,
          "standard_error": 68.2564918603264
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 225919.2961401952,
            "upper_bound": 226152.466873706
          },
          "point_estimate": 226079.9645962733,
          "standard_error": 55.92916515503613
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.122722846014396,
            "upper_bound": 394.5455989736812
          },
          "point_estimate": 137.1321074204741,
          "standard_error": 103.22599439394904
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 225937.9248688604,
            "upper_bound": 226268.833882196
          },
          "point_estimate": 226103.95511817376,
          "standard_error": 83.34837167139845
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.66221390817174,
            "upper_bound": 309.7224951646311
          },
          "point_estimate": 228.32137788936635,
          "standard_error": 53.90238603392585
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135545.50554326584,
            "upper_bound": 135844.819801417
          },
          "point_estimate": 135680.9058140843,
          "standard_error": 76.83278511976104
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135504.8444029851,
            "upper_bound": 135768.40391791044
          },
          "point_estimate": 135681.89019189763,
          "standard_error": 73.4637756487877
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.18604960612505,
            "upper_bound": 351.9105014015801
          },
          "point_estimate": 181.42702501285655,
          "standard_error": 82.85817661098666
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135506.83665051532,
            "upper_bound": 135702.98263633176
          },
          "point_estimate": 135594.98328164374,
          "standard_error": 49.42935211342732
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.02004773403216,
            "upper_bound": 363.7288943644553
          },
          "point_estimate": 256.6643578714145,
          "standard_error": 76.6863814175748
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100297.00950075848,
            "upper_bound": 101126.96992200895
          },
          "point_estimate": 100618.45728586818,
          "standard_error": 229.78272444829497
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100239.97973882074,
            "upper_bound": 100621.01090720222
          },
          "point_estimate": 100412.46660510926,
          "standard_error": 86.43113616134148
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.220005387145545,
            "upper_bound": 478.21017446713387
          },
          "point_estimate": 101.31575701939056,
          "standard_error": 145.3900090830545
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100232.90379831645,
            "upper_bound": 100502.79688610516
          },
          "point_estimate": 100353.20489261432,
          "standard_error": 70.7883477591654
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.06800855189924,
            "upper_bound": 1172.82463008032
          },
          "point_estimate": 768.9770858831326,
          "standard_error": 354.2383200621422
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85656.17061413903,
            "upper_bound": 85935.84020151584
          },
          "point_estimate": 85783.88569914617,
          "standard_error": 71.91077032474054
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85598.29383886256,
            "upper_bound": 85949.81575829384
          },
          "point_estimate": 85679.89435934326,
          "standard_error": 83.85794912354781
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.78263608831735,
            "upper_bound": 372.9302032922801
          },
          "point_estimate": 155.39891407168227,
          "standard_error": 90.10781713994491
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85613.25559768299,
            "upper_bound": 85818.14461535872
          },
          "point_estimate": 85702.43827168092,
          "standard_error": 53.81989047194546
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.54806672766595,
            "upper_bound": 314.8484630894325
          },
          "point_estimate": 240.69427215359696,
          "standard_error": 59.51532666300827
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56907.4753413723,
            "upper_bound": 56980.78092108205
          },
          "point_estimate": 56944.20707522915,
          "standard_error": 18.687625093698824
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56901.24817423057,
            "upper_bound": 56979.59947183098
          },
          "point_estimate": 56950.40182576943,
          "standard_error": 19.010993652588645
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.966280403905074,
            "upper_bound": 109.28631214428904
          },
          "point_estimate": 50.40064304887469,
          "standard_error": 26.093882437591535
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56912.655951582514,
            "upper_bound": 56964.88887581135
          },
          "point_estimate": 56942.899054935675,
          "standard_error": 13.29979509255662
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.240257061222337,
            "upper_bound": 81.53675669474787
          },
          "point_estimate": 62.072197237724055,
          "standard_error": 12.87518984248453
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122173.1699496644,
            "upper_bound": 122279.01614653243
          },
          "point_estimate": 122225.02864653242,
          "standard_error": 27.07955188226917
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122152.31124161072,
            "upper_bound": 122324.91163310962
          },
          "point_estimate": 122204.64161073824,
          "standard_error": 43.83837982609455
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.04615090737443,
            "upper_bound": 150.6842307476398
          },
          "point_estimate": 77.71871152290544,
          "standard_error": 37.37229174946635
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122156.34587191213,
            "upper_bound": 122295.75110113384
          },
          "point_estimate": 122227.00085417938,
          "standard_error": 35.42832948161417
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.87791182525658,
            "upper_bound": 108.96581989690635
          },
          "point_estimate": 90.354181606679,
          "standard_error": 14.004892926450614
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79165.89231559342,
            "upper_bound": 79402.83509021916
          },
          "point_estimate": 79288.58554244382,
          "standard_error": 60.59249680602449
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79090.409190372,
            "upper_bound": 79473.31925601751
          },
          "point_estimate": 79328.29932965164,
          "standard_error": 110.53784271904424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.277010829889317,
            "upper_bound": 328.550225458105
          },
          "point_estimate": 232.57993113527925,
          "standard_error": 85.64901955866982
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79249.93369330793,
            "upper_bound": 79452.45550908722
          },
          "point_estimate": 79379.37703827901,
          "standard_error": 51.85796192903018
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122.98964746424532,
            "upper_bound": 243.1403765095132
          },
          "point_estimate": 202.32318252336384,
          "standard_error": 30.962219162065267
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79011.8925947453,
            "upper_bound": 79114.46982268216
          },
          "point_estimate": 79059.64050636304,
          "standard_error": 26.395957458302508
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78991.38816267248,
            "upper_bound": 79118.48169545596
          },
          "point_estimate": 79037.2720043573,
          "standard_error": 30.265712872394747
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.362713595566813,
            "upper_bound": 146.19854277482952
          },
          "point_estimate": 70.37330712390724,
          "standard_error": 32.512271184023845
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79020.13549745824,
            "upper_bound": 79115.4719926304
          },
          "point_estimate": 79057.05645247998,
          "standard_error": 25.297389938761544
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.46353909532749,
            "upper_bound": 112.4546817926102
          },
          "point_estimate": 88.070888224387,
          "standard_error": 19.38460223960969
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85037.30257605977,
            "upper_bound": 85294.39906084735
          },
          "point_estimate": 85175.87975583567,
          "standard_error": 65.99183123297145
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85063.99333333333,
            "upper_bound": 85345.68294117648
          },
          "point_estimate": 85194.75152941176,
          "standard_error": 70.94517529369594
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.474677485928154,
            "upper_bound": 347.19435095764226
          },
          "point_estimate": 208.81650258688975,
          "standard_error": 75.39929067253995
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85121.2010434465,
            "upper_bound": 85326.18571002486
          },
          "point_estimate": 85222.03597860962,
          "standard_error": 51.594471440632375
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.25952380229664,
            "upper_bound": 302.112537263218
          },
          "point_estimate": 219.7934235141011,
          "standard_error": 57.69079794484923
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74968.57095385676,
            "upper_bound": 75172.37757305194
          },
          "point_estimate": 75066.89784681228,
          "standard_error": 51.9079376696833
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74975.90909090909,
            "upper_bound": 75169.80553915782
          },
          "point_estimate": 75023.43560606061,
          "standard_error": 56.60831864734618
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.53964719151193,
            "upper_bound": 295.3666912438142
          },
          "point_estimate": 141.0283913598877,
          "standard_error": 68.5884778120304
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74989.94437215819,
            "upper_bound": 75150.52490246907
          },
          "point_estimate": 75060.84735429859,
          "standard_error": 41.732762091736795
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.37898315595503,
            "upper_bound": 234.21442468658415
          },
          "point_estimate": 172.7926567535558,
          "standard_error": 41.20606056950032
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 235937.72020238097,
            "upper_bound": 236328.5311896645
          },
          "point_estimate": 236128.59098484847,
          "standard_error": 100.19007452136697
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 235865.30735930736,
            "upper_bound": 236380.82564935065
          },
          "point_estimate": 236120.46536796537,
          "standard_error": 113.84307957901471
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.72254885094712,
            "upper_bound": 601.7136806810827
          },
          "point_estimate": 283.50713133037283,
          "standard_error": 150.99024244565587
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 235884.2006754681,
            "upper_bound": 236362.44225175865
          },
          "point_estimate": 236151.57986169675,
          "standard_error": 123.37597946619626
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179.99785940415026,
            "upper_bound": 427.4645911317898
          },
          "point_estimate": 334.3368359263985,
          "standard_error": 64.27474387457848
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157288.65796536798,
            "upper_bound": 157596.75973587926
          },
          "point_estimate": 157450.96376073663,
          "standard_error": 78.98972166685463
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157180.54761904763,
            "upper_bound": 157649.1218305504
          },
          "point_estimate": 157525.91071428574,
          "standard_error": 109.44632264866236
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.74552422335348,
            "upper_bound": 428.7570727920707
          },
          "point_estimate": 185.6945005993223,
          "standard_error": 104.50476295352276
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157228.59994724512,
            "upper_bound": 157678.8221387018
          },
          "point_estimate": 157464.98847472874,
          "standard_error": 119.8899593355928
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.61929217298984,
            "upper_bound": 322.7775085889729
          },
          "point_estimate": 263.2916464341782,
          "standard_error": 48.80198377390846
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75511.21261657774,
            "upper_bound": 75820.5963001383
          },
          "point_estimate": 75641.0393062142,
          "standard_error": 81.55909796900295
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75472.69119739182,
            "upper_bound": 75745.47060857539
          },
          "point_estimate": 75531.82611514523,
          "standard_error": 79.24545826770546
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.418502622657543,
            "upper_bound": 320.926963576256
          },
          "point_estimate": 121.32414060021692,
          "standard_error": 79.19808657227858
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75481.7194787002,
            "upper_bound": 75603.05189742669
          },
          "point_estimate": 75519.77858490057,
          "standard_error": 31.79667986770368
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.18595449842424,
            "upper_bound": 399.4447120612126
          },
          "point_estimate": 271.6721603250114,
          "standard_error": 99.27669135883424
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231101.83573779193,
            "upper_bound": 232349.85811040335
          },
          "point_estimate": 231696.04369957536,
          "standard_error": 320.5112219220777
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231049.32537154987,
            "upper_bound": 232720.74840764332
          },
          "point_estimate": 231322.44121549895,
          "standard_error": 431.81268415568377
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.56295947095997,
            "upper_bound": 1732.927417880924
          },
          "point_estimate": 416.11029380687165,
          "standard_error": 486.1403820087887
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230986.84138690672,
            "upper_bound": 232355.66828970463
          },
          "point_estimate": 231634.1222102738,
          "standard_error": 366.043759076152
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 494.5161557208101,
            "upper_bound": 1324.0193985544706
          },
          "point_estimate": 1068.8984688085425,
          "standard_error": 202.06856803482987
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59594.63856558335,
            "upper_bound": 59673.83919231257
          },
          "point_estimate": 59634.76751130074,
          "standard_error": 20.239628680269593
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59576.41338847323,
            "upper_bound": 59682.44762684124
          },
          "point_estimate": 59643.96031096563,
          "standard_error": 27.464713107894124
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.783960183589253,
            "upper_bound": 112.63649892092462
          },
          "point_estimate": 61.76110422267273,
          "standard_error": 26.49643559121414
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59588.50904058277,
            "upper_bound": 59661.35929377645
          },
          "point_estimate": 59632.355856909046,
          "standard_error": 19.109586803418875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.83561094387745,
            "upper_bound": 85.116893777238
          },
          "point_estimate": 67.32582097847093,
          "standard_error": 12.182379648624517
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65553.58945180179,
            "upper_bound": 65703.04376370121
          },
          "point_estimate": 65622.06028778778,
          "standard_error": 38.4718812298431
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65544.01026026026,
            "upper_bound": 65699.34630630631
          },
          "point_estimate": 65567.81914414414,
          "standard_error": 43.38970325969654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.923882865058128,
            "upper_bound": 201.5419282582292
          },
          "point_estimate": 67.79039229196954,
          "standard_error": 52.76120031763213
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65562.74981884057,
            "upper_bound": 65694.89724942333
          },
          "point_estimate": 65633.73715221715,
          "standard_error": 34.165721344561206
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.33481967083646,
            "upper_bound": 171.17972489833838
          },
          "point_estimate": 128.01397507841483,
          "standard_error": 32.30653505113554
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70213.65870709995,
            "upper_bound": 70382.6935833793
          },
          "point_estimate": 70305.1403057394,
          "standard_error": 43.37726364538412
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70252.41450289576,
            "upper_bound": 70403.44401544401
          },
          "point_estimate": 70308.01949806951,
          "standard_error": 39.53397517930568
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.563067599686857,
            "upper_bound": 199.87114059570456
          },
          "point_estimate": 98.472973657157,
          "standard_error": 46.20833829373595
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70254.279171297,
            "upper_bound": 70402.87532130956
          },
          "point_estimate": 70309.30494910495,
          "standard_error": 38.48893871196939
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.7704224739437,
            "upper_bound": 205.04009768428187
          },
          "point_estimate": 144.78517710182052,
          "standard_error": 42.19320558609518
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94700.11208333334,
            "upper_bound": 94823.9912862594
          },
          "point_estimate": 94764.13290002893,
          "standard_error": 31.739090712306343
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94701.98502604169,
            "upper_bound": 94828.07112630208
          },
          "point_estimate": 94779.57696759258,
          "standard_error": 30.22890597863627
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.808562287409488,
            "upper_bound": 180.1475868625057
          },
          "point_estimate": 82.13468721368922,
          "standard_error": 46.75846318133024
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94711.10543291284,
            "upper_bound": 94823.80888972829
          },
          "point_estimate": 94778.57190205628,
          "standard_error": 28.321896029098728
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.17980103624685,
            "upper_bound": 138.49798745876996
          },
          "point_estimate": 105.64572087522532,
          "standard_error": 22.80425184059104
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66457.3859752365,
            "upper_bound": 66534.38971893227
          },
          "point_estimate": 66492.887068135,
          "standard_error": 19.759489465787063
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66447.97473230609,
            "upper_bound": 66527.00690635791
          },
          "point_estimate": 66476.42400975016,
          "standard_error": 16.94609393058278
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5386002712766904,
            "upper_bound": 108.7087616349237
          },
          "point_estimate": 34.79590312082368,
          "standard_error": 30.209655420477898
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66465.53692716124,
            "upper_bound": 66485.87097423672
          },
          "point_estimate": 66475.25971176903,
          "standard_error": 5.018499450695536
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.155608319382456,
            "upper_bound": 87.0135897206011
          },
          "point_estimate": 65.85803047746478,
          "standard_error": 16.22602315347268
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93407.05344932062,
            "upper_bound": 93683.41250350408
          },
          "point_estimate": 93511.38123474923,
          "standard_error": 78.30608937623921
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93391.94935732648,
            "upper_bound": 93498.87141939036
          },
          "point_estimate": 93425.51805912596,
          "standard_error": 34.980440651923175
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.460393116399048,
            "upper_bound": 124.71238413552248
          },
          "point_estimate": 62.356192067761235,
          "standard_error": 35.236281261119444
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93394.59195433852,
            "upper_bound": 93485.32189378672
          },
          "point_estimate": 93435.06296531232,
          "standard_error": 23.324959098862767
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.8704360150323,
            "upper_bound": 400.330252162471
          },
          "point_estimate": 261.04797205743057,
          "standard_error": 126.28092686165374
        }
      }
    },
    "memmem/twoway/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/twoway_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20453.4386382563,
            "upper_bound": 20524.744911641857
          },
          "point_estimate": 20485.01104009081,
          "standard_error": 18.42710585062462
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20443.46091011871,
            "upper_bound": 20500.32546636518
          },
          "point_estimate": 20483.979811031255,
          "standard_error": 14.231242588620502
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.612048762542956,
            "upper_bound": 78.6875781441949
          },
          "point_estimate": 29.704087426291917,
          "standard_error": 18.564644352172586
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20449.387224420578,
            "upper_bound": 20487.50988604201
          },
          "point_estimate": 20467.526676602087,
          "standard_error": 9.566033497481834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.393452832911706,
            "upper_bound": 88.20889870286153
          },
          "point_estimate": 61.07878012286295,
          "standard_error": 20.12945971173476
        }
      }
    },
    "memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/twoway_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20177.419924179183,
            "upper_bound": 20254.33566536821
          },
          "point_estimate": 20213.88526850306,
          "standard_error": 19.65178508375179
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20168.86449638286,
            "upper_bound": 20238.95512196253
          },
          "point_estimate": 20212.244221851233,
          "standard_error": 14.441555218778976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.0929056124746905,
            "upper_bound": 107.05757880051564
          },
          "point_estimate": 29.618172762988053,
          "standard_error": 28.001732580595625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20177.75233437227,
            "upper_bound": 20233.48598545472
          },
          "point_estimate": 20206.69513402569,
          "standard_error": 14.006149912042616
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.873580690481024,
            "upper_bound": 90.29004922905963
          },
          "point_estimate": 65.45149599795363,
          "standard_error": 17.01639394357469
        }
      }
    },
    "memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/twoway_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272765.3622384533,
            "upper_bound": 275035.7991407089
          },
          "point_estimate": 274188.0351933405,
          "standard_error": 647.7869089056296
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274286.4731471536,
            "upper_bound": 275136.27255639096
          },
          "point_estimate": 275000.05169172934,
          "standard_error": 273.9863104263521
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.78030062732779,
            "upper_bound": 922.1827573122478
          },
          "point_estimate": 322.6753434818719,
          "standard_error": 308.5903420694669
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 270527.1192496183,
            "upper_bound": 275105.72379294416
          },
          "point_estimate": 273204.57293233083,
          "standard_error": 1409.518248651186
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223.36340247843123,
            "upper_bound": 3321.648602605592
          },
          "point_estimate": 2164.631534451082,
          "standard_error": 1061.9831818321372
        }
      }
    },
    "memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/twoway_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 569.1801711317918,
            "upper_bound": 570.1413127967423
          },
          "point_estimate": 569.7424822290263,
          "standard_error": 0.2531132577498107
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 569.5930312715423,
            "upper_bound": 570.1987873660462
          },
          "point_estimate": 569.8714267809023,
          "standard_error": 0.15388047147149414
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0893489064164955,
            "upper_bound": 0.8221352764833207
          },
          "point_estimate": 0.417998714143359,
          "standard_error": 0.18484688346689696
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 569.8361002621378,
            "upper_bound": 570.2204102168207
          },
          "point_estimate": 570.0272550254783,
          "standard_error": 0.09716563851950603
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.21784067150213104,
            "upper_bound": 1.2535230472227887
          },
          "point_estimate": 0.8399796495389475,
          "standard_error": 0.3264283346715254
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.1579940205169,
            "upper_bound": 44.20277436553973
          },
          "point_estimate": 44.17917193926635,
          "standard_error": 0.011456692944432127
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.14463079786833,
            "upper_bound": 44.20510741833377
          },
          "point_estimate": 44.17253166597409,
          "standard_error": 0.014256668753948654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006410198144316779,
            "upper_bound": 0.06246243527155645
          },
          "point_estimate": 0.03979941774477829,
          "standard_error": 0.013895926527767662
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.16122369008635,
            "upper_bound": 44.213837233172335
          },
          "point_estimate": 44.18856880976519,
          "standard_error": 0.013163712989935506
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01959588015336136,
            "upper_bound": 0.05020537351896425
          },
          "point_estimate": 0.038359180019081514,
          "standard_error": 0.008243359849914226
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.39242311421071,
            "upper_bound": 47.45950308418045
          },
          "point_estimate": 47.42318002009785,
          "standard_error": 0.017249884562816797
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.379661358538605,
            "upper_bound": 47.45685271032062
          },
          "point_estimate": 47.42002529371594,
          "standard_error": 0.016104973794019786
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014001650574165833,
            "upper_bound": 0.09111143742740958
          },
          "point_estimate": 0.04366520973462202,
          "standard_error": 0.024868426361886423
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.381171527866954,
            "upper_bound": 47.42791193661577
          },
          "point_estimate": 47.40032181043025,
          "standard_error": 0.011919838799455093
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022355478515386832,
            "upper_bound": 0.07662315151579145
          },
          "point_estimate": 0.05742062990008263,
          "standard_error": 0.01464068209697889
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.907344753043255,
            "upper_bound": 26.94654725608283
          },
          "point_estimate": 26.924382351456114,
          "standard_error": 0.01017322256959095
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.902653288801567,
            "upper_bound": 26.936684757256103
          },
          "point_estimate": 26.91197177675529,
          "standard_error": 0.009188028974992704
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003903189536467547,
            "upper_bound": 0.04251161078987782
          },
          "point_estimate": 0.014724072575833767,
          "standard_error": 0.010462227219838915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.906177468778232,
            "upper_bound": 26.922434787695032
          },
          "point_estimate": 26.912205579300277,
          "standard_error": 0.00414546443775182
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008264644116417905,
            "upper_bound": 0.047912026104011914
          },
          "point_estimate": 0.03391822301431365,
          "standard_error": 0.010947165106828973
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.97995034373805,
            "upper_bound": 27.02987286711676
          },
          "point_estimate": 27.00287934456734,
          "standard_error": 0.012782281720183196
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.97538843897614,
            "upper_bound": 27.031976894879108
          },
          "point_estimate": 26.988606595820833,
          "standard_error": 0.012107233715775888
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006838870023329204,
            "upper_bound": 0.06728900598130115
          },
          "point_estimate": 0.021939615561916224,
          "standard_error": 0.016334583906680942
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.98010673244979,
            "upper_bound": 27.010485279353528
          },
          "point_estimate": 26.99386867954869,
          "standard_error": 0.007714555806654159
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01387832973153744,
            "upper_bound": 0.054584358304906225
          },
          "point_estimate": 0.04258355431629598,
          "standard_error": 0.010440822592942711
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.76968665181047,
            "upper_bound": 36.81351894210606
          },
          "point_estimate": 36.79144094035077,
          "standard_error": 0.011204880557551982
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.764516204099515,
            "upper_bound": 36.821338357138735
          },
          "point_estimate": 36.790815687115675,
          "standard_error": 0.015285233377874892
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01205437229892246,
            "upper_bound": 0.06335677378840165
          },
          "point_estimate": 0.042122261300154736,
          "standard_error": 0.01327754097443194
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.77045734989024,
            "upper_bound": 36.811003836282694
          },
          "point_estimate": 36.79222562837436,
          "standard_error": 0.010274148741709618
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02051150297046852,
            "upper_bound": 0.04809326872681525
          },
          "point_estimate": 0.03731878476733656,
          "standard_error": 0.007146254515819191
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.850711072186,
            "upper_bound": 50.92936653648839
          },
          "point_estimate": 50.890511449229024,
          "standard_error": 0.02019864624314676
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.84012799675051,
            "upper_bound": 50.94370738169721
          },
          "point_estimate": 50.90004328392529,
          "standard_error": 0.02762387544901746
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006895907909488016,
            "upper_bound": 0.1107361594308402
          },
          "point_estimate": 0.07453762429569047,
          "standard_error": 0.026315173208149453
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.86686148559085,
            "upper_bound": 50.94406993325916
          },
          "point_estimate": 50.90954508501233,
          "standard_error": 0.019614416111714113
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03718271950285639,
            "upper_bound": 0.08599026533455251
          },
          "point_estimate": 0.06725906486670581,
          "standard_error": 0.012488811339868403
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.56148684397581,
            "upper_bound": 68.68896043484511
          },
          "point_estimate": 68.62398877404527,
          "standard_error": 0.03271237166132678
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.5318770095275,
            "upper_bound": 68.71758495297061
          },
          "point_estimate": 68.60224085309298,
          "standard_error": 0.05606080934367882
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020535610931101712,
            "upper_bound": 0.1755938544019355
          },
          "point_estimate": 0.1411543141147338,
          "standard_error": 0.03980964532299536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.5582782500118,
            "upper_bound": 68.67408095942159
          },
          "point_estimate": 68.62394053834122,
          "standard_error": 0.029943553645527095
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0672230729245906,
            "upper_bound": 0.13258442719818303
          },
          "point_estimate": 0.10905563919651094,
          "standard_error": 0.016795319372446223
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.156681301036336,
            "upper_bound": 45.21079016818563
          },
          "point_estimate": 45.18045113018037,
          "standard_error": 0.0139891286092991
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.1471396464659,
            "upper_bound": 45.19429925023477
          },
          "point_estimate": 45.179517410055496,
          "standard_error": 0.01383331042457912
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003189880468676333,
            "upper_bound": 0.062035874626748186
          },
          "point_estimate": 0.03613937036797393,
          "standard_error": 0.014171028617483598
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.15510369871405,
            "upper_bound": 45.18546912018007
          },
          "point_estimate": 45.17340155683095,
          "standard_error": 0.007747285525859565
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01835697274958018,
            "upper_bound": 0.06680465869409855
          },
          "point_estimate": 0.0465557592023868,
          "standard_error": 0.014804004242871153
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.4782930953667,
            "upper_bound": 63.55456144297041
          },
          "point_estimate": 63.51833387157642,
          "standard_error": 0.01953016729245144
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.46558688645831,
            "upper_bound": 63.570309943206766
          },
          "point_estimate": 63.5321455760742,
          "standard_error": 0.022268664482268597
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007337298427686386,
            "upper_bound": 0.11708561885289805
          },
          "point_estimate": 0.04408659704677881,
          "standard_error": 0.027029114715758303
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.44971460404601,
            "upper_bound": 63.557862409044134
          },
          "point_estimate": 63.49913726199486,
          "standard_error": 0.0286638720122214
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029774499201221123,
            "upper_bound": 0.08222555779699774
          },
          "point_estimate": 0.06513812452351662,
          "standard_error": 0.013231511364369343
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.16030874187841,
            "upper_bound": 53.25599233003414
          },
          "point_estimate": 53.20281569132825,
          "standard_error": 0.024760896623103996
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.15575498354737,
            "upper_bound": 53.250685323048494
          },
          "point_estimate": 53.17363949291304,
          "standard_error": 0.023146590404823766
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007755696218756671,
            "upper_bound": 0.11616775791079798
          },
          "point_estimate": 0.032764275044649134,
          "standard_error": 0.030666441707147187
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.14175200214072,
            "upper_bound": 53.187211692910495
          },
          "point_estimate": 53.16334236040457,
          "standard_error": 0.011472981311883138
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028492358794875935,
            "upper_bound": 0.11516358603905602
          },
          "point_estimate": 0.08257298759104752,
          "standard_error": 0.024484791157531497
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.26189820508101,
            "upper_bound": 39.319849265445946
          },
          "point_estimate": 39.291216363207106,
          "standard_error": 0.014883986219192388
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.24973699039186,
            "upper_bound": 39.33198540909163
          },
          "point_estimate": 39.30367005138415,
          "standard_error": 0.021707141375560943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024382478317327137,
            "upper_bound": 0.08334669166807002
          },
          "point_estimate": 0.06913320980650729,
          "standard_error": 0.02293274045896635
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.27305753649503,
            "upper_bound": 39.329182810888895
          },
          "point_estimate": 39.29924102473746,
          "standard_error": 0.01424984465346646
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02895173438421004,
            "upper_bound": 0.06178164732522346
          },
          "point_estimate": 0.049600908365625695,
          "standard_error": 0.008398654374191794
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.4722831307367,
            "upper_bound": 64.58458897782346
          },
          "point_estimate": 64.52293744197722,
          "standard_error": 0.028802990056866005
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.45450885811167,
            "upper_bound": 64.5618875811333
          },
          "point_estimate": 64.51734943014432,
          "standard_error": 0.026865509543200848
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01854151142111355,
            "upper_bound": 0.14118985914379886
          },
          "point_estimate": 0.0708420802905897,
          "standard_error": 0.031335588723966835
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.49032253155386,
            "upper_bound": 64.56660744989516
          },
          "point_estimate": 64.5223859129321,
          "standard_error": 0.019339530697699617
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.039227321896879185,
            "upper_bound": 0.13487309963145613
          },
          "point_estimate": 0.09597951623467434,
          "standard_error": 0.02795584224066213
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 229167.3943451507,
            "upper_bound": 229451.80729503595
          },
          "point_estimate": 229298.4078364281,
          "standard_error": 73.19310515530292
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 229125.1283018868,
            "upper_bound": 229465.25550314464
          },
          "point_estimate": 229188.05552560647,
          "standard_error": 96.87859378676433
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.708978861378327,
            "upper_bound": 393.079498314975
          },
          "point_estimate": 160.2495477996526,
          "standard_error": 100.55604219015824
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 229115.88474574295,
            "upper_bound": 229336.5525933048
          },
          "point_estimate": 229219.20682839173,
          "standard_error": 57.381266831673365
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.15996277331148,
            "upper_bound": 317.6850608761199
          },
          "point_estimate": 243.49346499180652,
          "standard_error": 56.56923890139544
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169960.5425508188,
            "upper_bound": 170538.25330997095
          },
          "point_estimate": 170232.61859434383,
          "standard_error": 147.8192352970449
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169803.4516431925,
            "upper_bound": 170601.71948356807
          },
          "point_estimate": 170206.6127543036,
          "standard_error": 203.5497194517381
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.05912155415896,
            "upper_bound": 832.4697053967774
          },
          "point_estimate": 552.7374581446903,
          "standard_error": 192.3978834504627
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170093.44761204443,
            "upper_bound": 170449.65908169537
          },
          "point_estimate": 170258.58981769404,
          "standard_error": 88.97140372770377
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 251.25958827734792,
            "upper_bound": 643.2233648942957
          },
          "point_estimate": 495.159049476888,
          "standard_error": 105.0461780575922
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297434.01668621757,
            "upper_bound": 297907.9406228223
          },
          "point_estimate": 297668.79529036005,
          "standard_error": 121.05237127930953
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297396.3449477352,
            "upper_bound": 297981.8038617886
          },
          "point_estimate": 297657.31178861787,
          "standard_error": 115.94987821747706
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.44521487774553,
            "upper_bound": 714.1725830525957
          },
          "point_estimate": 234.72189298735137,
          "standard_error": 184.98737426712393
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297414.0746307167,
            "upper_bound": 297992.8406398449
          },
          "point_estimate": 297690.2952592123,
          "standard_error": 146.69432161656923
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201.0570122223129,
            "upper_bound": 528.4046653251623
          },
          "point_estimate": 405.2008288803888,
          "standard_error": 83.73364419976436
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 361841.9474901776,
            "upper_bound": 362452.020001768
          },
          "point_estimate": 362140.3374461732,
          "standard_error": 155.96839455870855
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 361680.9811881188,
            "upper_bound": 362606.000550055
          },
          "point_estimate": 362164.8224893918,
          "standard_error": 242.7552766200924
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.00190157521516,
            "upper_bound": 891.1846579407295
          },
          "point_estimate": 649.1339602577333,
          "standard_error": 221.8842831337299
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 361807.076530153,
            "upper_bound": 362496.9561524168
          },
          "point_estimate": 362138.0187733059,
          "standard_error": 179.14768383722964
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 304.8669393268404,
            "upper_bound": 638.2693430199699
          },
          "point_estimate": 519.7534979092064,
          "standard_error": 85.11952153245137
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499352.1242617961,
            "upper_bound": 500765.62598173507
          },
          "point_estimate": 499929.9099124811,
          "standard_error": 374.66117088194414
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499308.44315068494,
            "upper_bound": 499980.3525114155
          },
          "point_estimate": 499805.9520547945,
          "standard_error": 199.3897578147206
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.754174448873668,
            "upper_bound": 1032.417856648048
          },
          "point_estimate": 520.434225417963,
          "standard_error": 272.1322668813561
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499123.62239903753,
            "upper_bound": 499969.73759670666
          },
          "point_estimate": 499581.1539227896,
          "standard_error": 220.10558283293693
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253.74754262515367,
            "upper_bound": 1881.4267534758576
          },
          "point_estimate": 1251.580981861331,
          "standard_error": 516.3408204602181
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97674.6054133598,
            "upper_bound": 97769.81162985633
          },
          "point_estimate": 97724.55505621692,
          "standard_error": 24.328470780262617
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97668.7432795699,
            "upper_bound": 97780.63485663084
          },
          "point_estimate": 97732.09210829494,
          "standard_error": 27.720956449539816
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.403054478389897,
            "upper_bound": 131.91652467415346
          },
          "point_estimate": 82.94522460269606,
          "standard_error": 28.768929058507965
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97692.1247730764,
            "upper_bound": 97765.10697616426
          },
          "point_estimate": 97726.35330261136,
          "standard_error": 18.457796735423138
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.23148961656853,
            "upper_bound": 108.35476808956672
          },
          "point_estimate": 81.09676373660969,
          "standard_error": 18.551345681079145
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157042.15535329434,
            "upper_bound": 157274.36011699509
          },
          "point_estimate": 157150.58264111247,
          "standard_error": 59.53574878214777
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157036.73038793105,
            "upper_bound": 157316.7684729064
          },
          "point_estimate": 157073.5875538793,
          "standard_error": 70.23661465625973
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.65882012199979,
            "upper_bound": 306.7087956000826
          },
          "point_estimate": 69.40354558896392,
          "standard_error": 88.38373222758572
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156984.29801148706,
            "upper_bound": 157218.30188367667
          },
          "point_estimate": 157079.8946372593,
          "standard_error": 60.04414870923827
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.58186030026783,
            "upper_bound": 250.1913821113949
          },
          "point_estimate": 198.48373550596543,
          "standard_error": 42.66225732100545
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298665.3067359485,
            "upper_bound": 299609.8619086651
          },
          "point_estimate": 299159.1078054254,
          "standard_error": 241.21928403834372
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298797.9864754098,
            "upper_bound": 300040.2322404372
          },
          "point_estimate": 299089.41557377053,
          "standard_error": 307.52986652482514
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.7183667949694,
            "upper_bound": 1528.1627172754172
          },
          "point_estimate": 489.6989736627282,
          "standard_error": 372.1172164031086
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298835.004557239,
            "upper_bound": 299900.01647378475
          },
          "point_estimate": 299348.5704066425,
          "standard_error": 276.2229911915769
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 407.73040057443865,
            "upper_bound": 1075.5377932650608
          },
          "point_estimate": 802.9296418980389,
          "standard_error": 184.77594802517535
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271449.93865982594,
            "upper_bound": 271880.54913764505
          },
          "point_estimate": 271657.617767709,
          "standard_error": 110.25720174525097
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271398.77611940296,
            "upper_bound": 271974.6449004975
          },
          "point_estimate": 271568.25290215586,
          "standard_error": 162.49167448710182
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.59375447462556,
            "upper_bound": 615.9741883179549
          },
          "point_estimate": 260.6890025857403,
          "standard_error": 162.61876351623826
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271427.39808892045,
            "upper_bound": 271763.7011289949
          },
          "point_estimate": 271584.8135103702,
          "standard_error": 86.17801786586145
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 185.24513800468216,
            "upper_bound": 454.5537273128784
          },
          "point_estimate": 368.2999447373888,
          "standard_error": 66.61078917500247
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136750.05202007998,
            "upper_bound": 137042.82683834585
          },
          "point_estimate": 136894.47936522856,
          "standard_error": 74.93919126075363
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136651.42481203008,
            "upper_bound": 137195.65488721806
          },
          "point_estimate": 136865.03989139514,
          "standard_error": 118.15514475661432
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.19938085062952,
            "upper_bound": 441.610299879162
          },
          "point_estimate": 337.2737201525079,
          "standard_error": 114.62521790034936
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136664.9497691994,
            "upper_bound": 136942.12771108333
          },
          "point_estimate": 136779.84862806366,
          "standard_error": 71.027016968494
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149.10669364812713,
            "upper_bound": 296.4703744478703
          },
          "point_estimate": 249.04330415719707,
          "standard_error": 36.751242510256496
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146658.1384855031,
            "upper_bound": 147178.89541666667
          },
          "point_estimate": 146887.8987139977,
          "standard_error": 134.8864085401593
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146611.142281106,
            "upper_bound": 147156.88054435485
          },
          "point_estimate": 146722.94059139787,
          "standard_error": 117.94881071481583
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.777335750252035,
            "upper_bound": 581.9643299906724
          },
          "point_estimate": 180.13364785039062,
          "standard_error": 127.26175055314566
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146613.82736398518,
            "upper_bound": 146838.77483698612
          },
          "point_estimate": 146707.6983242564,
          "standard_error": 57.05736476237789
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.7234886257456,
            "upper_bound": 603.3848195610228
          },
          "point_estimate": 448.3508990390631,
          "standard_error": 135.2813776611354
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153687.6086519071,
            "upper_bound": 153848.73904699113
          },
          "point_estimate": 153770.866192318,
          "standard_error": 41.23734485885538
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153647.41104078764,
            "upper_bound": 153880.53345388788
          },
          "point_estimate": 153782.6394514768,
          "standard_error": 47.63395605103256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.75110149548747,
            "upper_bound": 241.18732019064757
          },
          "point_estimate": 112.07135402298124,
          "standard_error": 58.74179569551913
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153627.2454536876,
            "upper_bound": 153829.39103188275
          },
          "point_estimate": 153734.62186421175,
          "standard_error": 52.6166065888293
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.614878743439,
            "upper_bound": 175.77524827488136
          },
          "point_estimate": 137.73965059679364,
          "standard_error": 27.37021775777517
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82673.42419767148,
            "upper_bound": 82811.36569024119
          },
          "point_estimate": 82743.45074529052,
          "standard_error": 35.39416715127165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82657.85614401418,
            "upper_bound": 82820.74202733485
          },
          "point_estimate": 82753.09030941533,
          "standard_error": 41.45048434067488
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.38688797251981,
            "upper_bound": 203.54341483741305
          },
          "point_estimate": 103.5359434921728,
          "standard_error": 46.33906767328163
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82717.68539804917,
            "upper_bound": 82807.01601147208
          },
          "point_estimate": 82767.53069845872,
          "standard_error": 23.416706828252902
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.95459182925175,
            "upper_bound": 153.7711154504118
          },
          "point_estimate": 118.14643227940525,
          "standard_error": 23.862039254507906
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/twoway_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28329.54932047601,
            "upper_bound": 28401.915099531616
          },
          "point_estimate": 28362.033989349835,
          "standard_error": 18.65908406161816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28317.96971116315,
            "upper_bound": 28390.760977751757
          },
          "point_estimate": 28348.698918255828,
          "standard_error": 20.18697773322249
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.32901287534998,
            "upper_bound": 86.4683129457551
          },
          "point_estimate": 41.26415609801667,
          "standard_error": 20.19771128497029
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28326.708182138012,
            "upper_bound": 28358.47497722126
          },
          "point_estimate": 28340.68904772043,
          "standard_error": 8.08450729051632
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.5966120101084,
            "upper_bound": 86.42847763946818
          },
          "point_estimate": 62.05574396502972,
          "standard_error": 17.763860209392764
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/twoway_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1988919.9521077692,
            "upper_bound": 1992342.4798934837
          },
          "point_estimate": 1990669.6315601503,
          "standard_error": 875.5524747467668
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1988652.8190789472,
            "upper_bound": 1993055.5157894737
          },
          "point_estimate": 1990923.198245614,
          "standard_error": 1299.4029810760314
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 305.9088656216402,
            "upper_bound": 5126.561500038002
          },
          "point_estimate": 3213.109659929574,
          "standard_error": 1159.890604561369
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1989304.742105263,
            "upper_bound": 1991904.8455202896
          },
          "point_estimate": 1990756.9920710868,
          "standard_error": 662.0722557314976
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1640.664162603324,
            "upper_bound": 3759.45859196175
          },
          "point_estimate": 2918.365348143148,
          "standard_error": 554.2851741603503
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/twoway_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3987.7939320339015,
            "upper_bound": 3991.772225198304
          },
          "point_estimate": 3989.937019556734,
          "standard_error": 1.0225441717268011
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3987.429024376853,
            "upper_bound": 3992.6853183753647
          },
          "point_estimate": 3990.730254592229,
          "standard_error": 1.2807800718101214
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5157461567967169,
            "upper_bound": 5.6813191919882025
          },
          "point_estimate": 3.0774199903852844,
          "standard_error": 1.2324784312693475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3988.4179744394687,
            "upper_bound": 3991.576809755728
          },
          "point_estimate": 3990.032050413625,
          "standard_error": 0.7982434443380515
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5295840095874866,
            "upper_bound": 4.5781765957278155
          },
          "point_estimate": 3.416542841116239,
          "standard_error": 0.8359707059080733
        }
      }
    },
    "memrchr1/krate/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr1/krate/empty/never",
        "directory_name": "memrchr1/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8554786841316344,
            "upper_bound": 0.8569778747780538
          },
          "point_estimate": 0.8561853241775197,
          "standard_error": 0.0003849036250026123
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.855513122113067,
            "upper_bound": 0.8571650285342003
          },
          "point_estimate": 0.8558396083759716,
          "standard_error": 0.0003747167222577055
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00008556487832470657,
            "upper_bound": 0.002126612335030461
          },
          "point_estimate": 0.0005268309639512189,
          "standard_error": 0.0005296982752822198
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8555961525381959,
            "upper_bound": 0.8564990643544834
          },
          "point_estimate": 0.8559991864450547,
          "standard_error": 0.000226088779018019
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004198333024127472,
            "upper_bound": 0.001665773320621589
          },
          "point_estimate": 0.001284695391768439,
          "standard_error": 0.0002961376623998644
        }
      }
    },
    "memrchr1/krate/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/common",
        "directory_name": "memrchr1/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223281.4844958796,
            "upper_bound": 223697.99233741112
          },
          "point_estimate": 223478.6613713604,
          "standard_error": 106.67294167440409
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223123.85398773005,
            "upper_bound": 223666.0920245399
          },
          "point_estimate": 223457.88212094657,
          "standard_error": 131.03883641355236
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.39628919203432,
            "upper_bound": 626.046535613472
          },
          "point_estimate": 339.944841204039,
          "standard_error": 142.2648754930522
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223275.6297453058,
            "upper_bound": 223529.707156049
          },
          "point_estimate": 223421.38635965265,
          "standard_error": 63.86885190581542
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 181.88096205356143,
            "upper_bound": 478.3261049218507
          },
          "point_estimate": 356.9674570508948,
          "standard_error": 82.58634493107235
        }
      }
    },
    "memrchr1/krate/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/never",
        "directory_name": "memrchr1/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8826.812636602852,
            "upper_bound": 8834.197115808822
          },
          "point_estimate": 8830.504848494495,
          "standard_error": 1.889736639319494
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8826.327675533023,
            "upper_bound": 8835.263909820125
          },
          "point_estimate": 8830.164735591206,
          "standard_error": 2.2345831526200386
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9214697471219304,
            "upper_bound": 10.522785876016426
          },
          "point_estimate": 5.391295155257373,
          "standard_error": 2.322577886266876
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8826.88605741919,
            "upper_bound": 8834.82567858217
          },
          "point_estimate": 8831.2806987859,
          "standard_error": 2.07081237818548
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.348353741210338,
            "upper_bound": 8.160735885933404
          },
          "point_estimate": 6.298879040151521,
          "standard_error": 1.232700233993748
        }
      }
    },
    "memrchr1/krate/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/rare",
        "directory_name": "memrchr1/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10800.306058245773,
            "upper_bound": 10811.159874997053
          },
          "point_estimate": 10805.486551982358,
          "standard_error": 2.7833954527662694
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10797.960534918277,
            "upper_bound": 10811.045963348191
          },
          "point_estimate": 10804.92405717116,
          "standard_error": 3.282070960860259
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.12001760475879,
            "upper_bound": 15.639918064089102
          },
          "point_estimate": 9.70022792288239,
          "standard_error": 3.2652157470487935
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10799.860158110714,
            "upper_bound": 10807.290165135444
          },
          "point_estimate": 10803.142597016653,
          "standard_error": 1.9460093364755044
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.607831981846413,
            "upper_bound": 12.337041677838377
          },
          "point_estimate": 9.250553277748695,
          "standard_error": 2.063597049727307
        }
      }
    },
    "memrchr1/krate/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/uncommon",
        "directory_name": "memrchr1/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78471.99691487537,
            "upper_bound": 78585.52665226781
          },
          "point_estimate": 78527.81079416504,
          "standard_error": 29.0406484426487
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78440.47423634681,
            "upper_bound": 78620.78887688986
          },
          "point_estimate": 78501.2813774898,
          "standard_error": 44.08881496073155
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.933526562311329,
            "upper_bound": 165.71990693907475
          },
          "point_estimate": 101.62774517199011,
          "standard_error": 38.136790057310755
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78426.9664670944,
            "upper_bound": 78557.72471008473
          },
          "point_estimate": 78483.40674314887,
          "standard_error": 33.83691685746189
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.48846964477697,
            "upper_bound": 117.035069156275
          },
          "point_estimate": 96.52975759439128,
          "standard_error": 15.270041787128005
        }
      }
    },
    "memrchr1/krate/huge/verycommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/verycommon",
        "directory_name": "memrchr1/krate_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463964.045514366,
            "upper_bound": 464776.5134268887
          },
          "point_estimate": 464307.2416114125,
          "standard_error": 212.3153931473398
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463892.12658227846,
            "upper_bound": 464618.0158227848
          },
          "point_estimate": 464058.4148081173,
          "standard_error": 157.05882966762238
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.40994898285103,
            "upper_bound": 759.5569752283153
          },
          "point_estimate": 226.36158110788165,
          "standard_error": 180.82290710617764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463962.0032187908,
            "upper_bound": 464168.9289191821
          },
          "point_estimate": 464082.0672365609,
          "standard_error": 52.74246768223069
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.7147079273365,
            "upper_bound": 1001.0442420824812
          },
          "point_estimate": 705.5774430345701,
          "standard_error": 247.56513286809565
        }
      }
    },
    "memrchr1/krate/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/common",
        "directory_name": "memrchr1/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.3509176475499,
            "upper_bound": 227.70929850151953
          },
          "point_estimate": 227.53291006185427,
          "standard_error": 0.0914637013171738
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.3927650173518,
            "upper_bound": 227.7294243050338
          },
          "point_estimate": 227.50176211147289,
          "standard_error": 0.107649839536547
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013158598564289045,
            "upper_bound": 0.521605162978605
          },
          "point_estimate": 0.1841645302265388,
          "standard_error": 0.1323285926123762
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.5110908582979,
            "upper_bound": 227.7581496098025
          },
          "point_estimate": 227.6282357762448,
          "standard_error": 0.061478452917755
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13556289945083663,
            "upper_bound": 0.41583635489834664
          },
          "point_estimate": 0.3053309817276201,
          "standard_error": 0.07238076144579894
        }
      }
    },
    "memrchr1/krate/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/never",
        "directory_name": "memrchr1/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.426535727365094,
            "upper_bound": 8.485954330737908
          },
          "point_estimate": 8.45215961594199,
          "standard_error": 0.015491971417455196
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.415227245617512,
            "upper_bound": 8.471087411591213
          },
          "point_estimate": 8.433471178818753,
          "standard_error": 0.016118991855851524
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002322173927614315,
            "upper_bound": 0.06631230496762748
          },
          "point_estimate": 0.029516236694838095,
          "standard_error": 0.01615966058756106
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.424900548701915,
            "upper_bound": 8.45540197684478
          },
          "point_estimate": 8.439493197987746,
          "standard_error": 0.007861742915393655
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017488671228728516,
            "upper_bound": 0.07475069069726137
          },
          "point_estimate": 0.05171109217868316,
          "standard_error": 0.01742251534097089
        }
      }
    },
    "memrchr1/krate/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/rare",
        "directory_name": "memrchr1/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.693915508404936,
            "upper_bound": 13.714530619916664
          },
          "point_estimate": 13.703775823799328,
          "standard_error": 0.005277853035441931
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.689727378784774,
            "upper_bound": 13.720381878710414
          },
          "point_estimate": 13.701029503254212,
          "standard_error": 0.006503017691473425
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0031962198655424577,
            "upper_bound": 0.03242223644095848
          },
          "point_estimate": 0.014552326060612448,
          "standard_error": 0.0071925939133848016
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.696059968747468,
            "upper_bound": 13.712636924288857
          },
          "point_estimate": 13.703659975230543,
          "standard_error": 0.004126681367073389
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008438991295466282,
            "upper_bound": 0.02199352959048009
          },
          "point_estimate": 0.017520272509911887,
          "standard_error": 0.0034130568401910173
        }
      }
    },
    "memrchr1/krate/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/uncommon",
        "directory_name": "memrchr1/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.86045448933218,
            "upper_bound": 46.92828969216554
          },
          "point_estimate": 46.89459840161753,
          "standard_error": 0.017401538944834254
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.85490166343582,
            "upper_bound": 46.94510305961517
          },
          "point_estimate": 46.888478660260034,
          "standard_error": 0.021333803072238224
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003549330557439672,
            "upper_bound": 0.09865708629580384
          },
          "point_estimate": 0.050770457975003874,
          "standard_error": 0.024126424084417387
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.868105329832815,
            "upper_bound": 46.91304158527032
          },
          "point_estimate": 46.89242038565238,
          "standard_error": 0.011455730151230732
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03076345631872607,
            "upper_bound": 0.07470088326425926
          },
          "point_estimate": 0.05779487650929688,
          "standard_error": 0.011364864058637613
        }
      }
    },
    "memrchr1/krate/small/verycommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/verycommon",
        "directory_name": "memrchr1/krate_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500.145367882401,
            "upper_bound": 500.87016699730896
          },
          "point_estimate": 500.4895898986462,
          "standard_error": 0.18657153765301285
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499.9828987163425,
            "upper_bound": 501.08346470515346
          },
          "point_estimate": 500.2907392619313,
          "standard_error": 0.2784408931797689
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14828721439594328,
            "upper_bound": 1.0132014992705358
          },
          "point_estimate": 0.6011050753108355,
          "standard_error": 0.23415629926811737
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500.2699368151674,
            "upper_bound": 500.92302467869183
          },
          "point_estimate": 500.5565134346591,
          "standard_error": 0.1664789778576978
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.32562258695204815,
            "upper_bound": 0.7664671602896403
          },
          "point_estimate": 0.6235341800850493,
          "standard_error": 0.11053253151887936
        }
      }
    },
    "memrchr1/krate/tiny/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/common",
        "directory_name": "memrchr1/krate_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.45825194527851,
            "upper_bound": 51.552678426324576
          },
          "point_estimate": 51.50307742799949,
          "standard_error": 0.02426273931275996
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.43298653576964,
            "upper_bound": 51.55867088212024
          },
          "point_estimate": 51.48999186076077,
          "standard_error": 0.03049657954369882
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013322818070938151,
            "upper_bound": 0.1351815016287144
          },
          "point_estimate": 0.0889491235828291,
          "standard_error": 0.03200856208196384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.45374230949498,
            "upper_bound": 51.558676994587636
          },
          "point_estimate": 51.49839266397406,
          "standard_error": 0.02695380916650516
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.036710591589965355,
            "upper_bound": 0.10122198276157228
          },
          "point_estimate": 0.08085181431870832,
          "standard_error": 0.016002942343646208
        }
      }
    },
    "memrchr1/krate/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/never",
        "directory_name": "memrchr1/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.618063576073788,
            "upper_bound": 4.668059801176932
          },
          "point_estimate": 4.64479743876139,
          "standard_error": 0.012850197143979626
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.620661655622147,
            "upper_bound": 4.674122388670464
          },
          "point_estimate": 4.647746491738058,
          "standard_error": 0.011665395578546551
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0027195767047192973,
            "upper_bound": 0.07010524749195947
          },
          "point_estimate": 0.03245442516758334,
          "standard_error": 0.016992514267592778
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.635404137847831,
            "upper_bound": 4.677416525057693
          },
          "point_estimate": 4.656549653777919,
          "standard_error": 0.010775567323335237
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01552914560829414,
            "upper_bound": 0.05810657103565628
          },
          "point_estimate": 0.042938706265007856,
          "standard_error": 0.01101562687095986
        }
      }
    },
    "memrchr1/krate/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/rare",
        "directory_name": "memrchr1/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.195535181028359,
            "upper_bound": 7.209062754118992
          },
          "point_estimate": 7.202507709633624,
          "standard_error": 0.0034846925789995325
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.191852125558427,
            "upper_bound": 7.214050434315252
          },
          "point_estimate": 7.205415849937955,
          "standard_error": 0.00621914367080185
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007502146170586609,
            "upper_bound": 0.019341902654270063
          },
          "point_estimate": 0.013452339591122514,
          "standard_error": 0.0048356769888970785
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.18974669630394,
            "upper_bound": 7.211309403570794
          },
          "point_estimate": 7.200368244396983,
          "standard_error": 0.005618793643717744
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007065999015639582,
            "upper_bound": 0.013674639125495228
          },
          "point_estimate": 0.011609151448872434,
          "standard_error": 0.0016851726803610578
        }
      }
    },
    "memrchr1/krate/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/uncommon",
        "directory_name": "memrchr1/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.325593562345706,
            "upper_bound": 28.38240568464687
          },
          "point_estimate": 28.354093849102536,
          "standard_error": 0.01450497962125903
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.321736572941653,
            "upper_bound": 28.388071185127025
          },
          "point_estimate": 28.358966044657205,
          "standard_error": 0.014189107322348004
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006230554331656278,
            "upper_bound": 0.08369188281787353
          },
          "point_estimate": 0.033862075219737116,
          "standard_error": 0.02268139639727475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.349913604368083,
            "upper_bound": 28.38109721580024
          },
          "point_estimate": 28.364076527081377,
          "standard_error": 0.007867219664989917
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024399541580334617,
            "upper_bound": 0.06294293570039505
          },
          "point_estimate": 0.048198941482069255,
          "standard_error": 0.009817844335727202
        }
      }
    },
    "memrchr1/libc/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr1/libc/empty/never",
        "directory_name": "memrchr1/libc_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4890020087035527,
            "upper_bound": 0.49222438174351735
          },
          "point_estimate": 0.4904099324498099,
          "standard_error": 0.0008342903112153137
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.48874745754446025,
            "upper_bound": 0.49216504692922747
          },
          "point_estimate": 0.489346850487446,
          "standard_error": 0.0006922484383374225
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00021607459990612935,
            "upper_bound": 0.0038647586599869777
          },
          "point_estimate": 0.0009166662788749684,
          "standard_error": 0.0007378677376740743
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4890692172429651,
            "upper_bound": 0.4927028104139955
          },
          "point_estimate": 0.49049673610379174,
          "standard_error": 0.001027488088423082
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000441185180547882,
            "upper_bound": 0.0036111866786029127
          },
          "point_estimate": 0.0027831394128478497,
          "standard_error": 0.0008558219614859282
        }
      }
    },
    "memrchr1/libc/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/common",
        "directory_name": "memrchr1/libc_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262024.9933192874,
            "upper_bound": 262237.3700658616
          },
          "point_estimate": 262127.6603631381,
          "standard_error": 54.052379815640265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262012.33381294968,
            "upper_bound": 262208.4496402878
          },
          "point_estimate": 262124.43802672147,
          "standard_error": 42.52189413882462
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.66569488119343,
            "upper_bound": 288.2396561057685
          },
          "point_estimate": 111.12922320692743,
          "standard_error": 74.35674318439888
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262009.76150642164,
            "upper_bound": 262307.1838464451
          },
          "point_estimate": 262139.5129402971,
          "standard_error": 76.6280860541219
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.50628818463674,
            "upper_bound": 248.35196678358693
          },
          "point_estimate": 180.24306110629004,
          "standard_error": 45.0687666492021
        }
      }
    },
    "memrchr1/libc/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/never",
        "directory_name": "memrchr1/libc_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9717.517247748352,
            "upper_bound": 9741.164999583323
          },
          "point_estimate": 9730.032756015014,
          "standard_error": 6.044208222192835
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9716.902019796682,
            "upper_bound": 9746.092038969146
          },
          "point_estimate": 9732.53931734991,
          "standard_error": 7.766969179753314
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.48204755564700663,
            "upper_bound": 34.274920632804495
          },
          "point_estimate": 13.465350652684362,
          "standard_error": 8.303376440642603
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9724.16848340301,
            "upper_bound": 9738.97955055012
          },
          "point_estimate": 9729.755171527242,
          "standard_error": 3.801304781216737
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.235303685583464,
            "upper_bound": 26.41205061073545
          },
          "point_estimate": 20.10637216895603,
          "standard_error": 4.343971513274842
        }
      }
    },
    "memrchr1/libc/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/rare",
        "directory_name": "memrchr1/libc_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10835.251883795529,
            "upper_bound": 10852.08490306313
          },
          "point_estimate": 10842.716322123137,
          "standard_error": 4.357438855846423
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10833.971160228328,
            "upper_bound": 10850.972476105137
          },
          "point_estimate": 10837.75887967941,
          "standard_error": 3.5493194607339933
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3879075073022669,
            "upper_bound": 19.20257020568112
          },
          "point_estimate": 5.999789135816481,
          "standard_error": 4.250877390425352
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10834.208949997814,
            "upper_bound": 10839.26490442055
          },
          "point_estimate": 10836.629879439555,
          "standard_error": 1.280586648294953
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.946451543874976,
            "upper_bound": 19.201618464052395
          },
          "point_estimate": 14.563974382204137,
          "standard_error": 4.347392448000407
        }
      }
    },
    "memrchr1/libc/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/uncommon",
        "directory_name": "memrchr1/libc_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78619.23481618175,
            "upper_bound": 78767.8221408046
          },
          "point_estimate": 78692.06167128489,
          "standard_error": 38.13872001357803
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78590.94288793103,
            "upper_bound": 78823.26416256158
          },
          "point_estimate": 78674.5447198276,
          "standard_error": 53.93791511809996
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.163494416349124,
            "upper_bound": 220.49144188290128
          },
          "point_estimate": 132.80772695255013,
          "standard_error": 51.15324823690295
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78657.48779194432,
            "upper_bound": 78840.7176942282
          },
          "point_estimate": 78752.10012875056,
          "standard_error": 48.69272018559755
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.74332034764745,
            "upper_bound": 156.9279277521144
          },
          "point_estimate": 127.17011657913264,
          "standard_error": 21.65548648734893
        }
      }
    },
    "memrchr1/libc/huge/verycommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/verycommon",
        "directory_name": "memrchr1/libc_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 546553.8653001362,
            "upper_bound": 547155.1159464582
          },
          "point_estimate": 546826.6487408198,
          "standard_error": 155.44611431962196
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 546496.4701492537,
            "upper_bound": 547165.2369402985
          },
          "point_estimate": 546677.6872778963,
          "standard_error": 165.23763993266763
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.490889473258726,
            "upper_bound": 770.2180624452998
          },
          "point_estimate": 287.1640723646747,
          "standard_error": 190.82442131803612
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 546531.3880738299,
            "upper_bound": 547089.6607757914
          },
          "point_estimate": 546730.8691994572,
          "standard_error": 153.53280124458252
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.34163332535934,
            "upper_bound": 686.3365146994015
          },
          "point_estimate": 518.2657151650424,
          "standard_error": 134.84459722751956
        }
      }
    },
    "memrchr1/libc/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/common",
        "directory_name": "memrchr1/libc_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238.35729029117496,
            "upper_bound": 238.82489071267412
          },
          "point_estimate": 238.57663880999945,
          "standard_error": 0.12010284706818972
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238.20951800209443,
            "upper_bound": 238.90528369526731
          },
          "point_estimate": 238.50836262026215,
          "standard_error": 0.15020466924220757
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04260566469070983,
            "upper_bound": 0.6801865302089124
          },
          "point_estimate": 0.3476259753776267,
          "standard_error": 0.1625428680880815
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238.23738089397605,
            "upper_bound": 238.56283202253823
          },
          "point_estimate": 238.37617099320155,
          "standard_error": 0.08362775507565219
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.17567823065727348,
            "upper_bound": 0.502489142377025
          },
          "point_estimate": 0.4002625226680667,
          "standard_error": 0.08463397739841388
        }
      }
    },
    "memrchr1/libc/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/never",
        "directory_name": "memrchr1/libc_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.083126514102972,
            "upper_bound": 8.099825960906712
          },
          "point_estimate": 8.091356257909355,
          "standard_error": 0.004268394422347406
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.081693245804683,
            "upper_bound": 8.102203298444161
          },
          "point_estimate": 8.09029784569092,
          "standard_error": 0.004699507143625331
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001920727469055332,
            "upper_bound": 0.02723134444353425
          },
          "point_estimate": 0.01320101814776277,
          "standard_error": 0.006814459634910687
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.083199842878292,
            "upper_bound": 8.100473470842392
          },
          "point_estimate": 8.091411996840144,
          "standard_error": 0.004414646127717841
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007730812405241965,
            "upper_bound": 0.017951738512906445
          },
          "point_estimate": 0.014234950165712416,
          "standard_error": 0.002637578565172491
        }
      }
    },
    "memrchr1/libc/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/rare",
        "directory_name": "memrchr1/libc_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.323115212944485,
            "upper_bound": 14.34760973662021
          },
          "point_estimate": 14.334960579389929,
          "standard_error": 0.0062239088896632894
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.322130229661887,
            "upper_bound": 14.355494507325089
          },
          "point_estimate": 14.328298912494557,
          "standard_error": 0.00825534556241669
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002466932377313511,
            "upper_bound": 0.03475676186850113
          },
          "point_estimate": 0.018354163697408145,
          "standard_error": 0.009570239315518416
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.326598718880284,
            "upper_bound": 14.34148352328843
          },
          "point_estimate": 14.333418692639764,
          "standard_error": 0.003821201579751456
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010170666445687164,
            "upper_bound": 0.02641541159072413
          },
          "point_estimate": 0.02075237551944384,
          "standard_error": 0.003953637012835683
        }
      }
    },
    "memrchr1/libc/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/uncommon",
        "directory_name": "memrchr1/libc_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.95404853560856,
            "upper_bound": 48.0197257854052
          },
          "point_estimate": 47.9859798931831,
          "standard_error": 0.016791817567635082
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.945211354548725,
            "upper_bound": 48.01564754683801
          },
          "point_estimate": 47.989090339397094,
          "standard_error": 0.018880324556180068
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01439991306547532,
            "upper_bound": 0.09597250681484772
          },
          "point_estimate": 0.04887153213580524,
          "standard_error": 0.021037928167371232
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.957825988606615,
            "upper_bound": 48.00990151403368
          },
          "point_estimate": 47.98893869385268,
          "standard_error": 0.01315591923444597
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028092286070727123,
            "upper_bound": 0.07473136253862048
          },
          "point_estimate": 0.05612974638952961,
          "standard_error": 0.012186570597768434
        }
      }
    },
    "memrchr1/libc/small/verycommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/verycommon",
        "directory_name": "memrchr1/libc_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 580.2414977327892,
            "upper_bound": 581.5883543832423
          },
          "point_estimate": 580.8644260165297,
          "standard_error": 0.3436364458308205
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 580.0886952868689,
            "upper_bound": 581.2424088447895
          },
          "point_estimate": 580.9106204155664,
          "standard_error": 0.31330013516695915
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1444447049653207,
            "upper_bound": 1.7424438001049318
          },
          "point_estimate": 0.5937854219619577,
          "standard_error": 0.41441999272794744
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 580.4403767097651,
            "upper_bound": 581.0637291010672
          },
          "point_estimate": 580.8044434560398,
          "standard_error": 0.1594458505166479
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4376254706327892,
            "upper_bound": 1.606993261945569
          },
          "point_estimate": 1.145895827217155,
          "standard_error": 0.32118450953449196
        }
      }
    },
    "memrchr1/libc/tiny/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/common",
        "directory_name": "memrchr1/libc_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.59845480672334,
            "upper_bound": 50.68406515629244
          },
          "point_estimate": 50.63898552776805,
          "standard_error": 0.0218878152412237
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.57778902968476,
            "upper_bound": 50.68565132456215
          },
          "point_estimate": 50.625783009682365,
          "standard_error": 0.024792576835644953
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014594063795052204,
            "upper_bound": 0.1207671657741851
          },
          "point_estimate": 0.06850053895085843,
          "standard_error": 0.029584247064201975
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.615819190244466,
            "upper_bound": 50.70284870470076
          },
          "point_estimate": 50.662186272641726,
          "standard_error": 0.022310056221175757
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.035464185254082306,
            "upper_bound": 0.09725222026188672
          },
          "point_estimate": 0.07321917561156267,
          "standard_error": 0.01664863577487871
        }
      }
    },
    "memrchr1/libc/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/never",
        "directory_name": "memrchr1/libc_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9326796202615126,
            "upper_bound": 2.9363900921218
          },
          "point_estimate": 2.9346474072812745,
          "standard_error": 0.0009568968494102332
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9325640656348666,
            "upper_bound": 2.936430465193202
          },
          "point_estimate": 2.935809503904328,
          "standard_error": 0.0010833971798870751
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002728558848167741,
            "upper_bound": 0.005064016376357508
          },
          "point_estimate": 0.0010446691286954116,
          "standard_error": 0.0014456219714887215
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9346923692906497,
            "upper_bound": 2.9377484371297222
          },
          "point_estimate": 2.9363854455895018,
          "standard_error": 0.0008121882687710102
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001199294408713929,
            "upper_bound": 0.004052385539075774
          },
          "point_estimate": 0.003193028762010594,
          "standard_error": 0.0006855769387475861
        }
      }
    },
    "memrchr1/libc/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/rare",
        "directory_name": "memrchr1/libc_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.869268248656582,
            "upper_bound": 5.875041139853014
          },
          "point_estimate": 5.872202684445278,
          "standard_error": 0.0014815708303815055
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.868395868083841,
            "upper_bound": 5.876475109657157
          },
          "point_estimate": 5.872199775818819,
          "standard_error": 0.0022483967918584284
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014564795739780913,
            "upper_bound": 0.008438916798083639
          },
          "point_estimate": 0.00554439741181461,
          "standard_error": 0.0018244636111293924
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.869391823523423,
            "upper_bound": 5.876099807273986
          },
          "point_estimate": 5.873303563137148,
          "standard_error": 0.0017315755932580611
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0029132840886256987,
            "upper_bound": 0.006033584188338427
          },
          "point_estimate": 0.0049326712952900325,
          "standard_error": 0.0007961416871051758
        }
      }
    },
    "memrchr1/libc/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr1/libc/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/uncommon",
        "directory_name": "memrchr1/libc_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.661527969722414,
            "upper_bound": 17.68543820268682
          },
          "point_estimate": 17.67130213273163,
          "standard_error": 0.0063742049694261935
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.660878197010177,
            "upper_bound": 17.675713236108848
          },
          "point_estimate": 17.665453447286122,
          "standard_error": 0.003790145717329342
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017148979091628272,
            "upper_bound": 0.018207239041724513
          },
          "point_estimate": 0.006901231491732867,
          "standard_error": 0.004675921456959944
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.66032601043677,
            "upper_bound": 17.669356422859472
          },
          "point_estimate": 17.664437696508173,
          "standard_error": 0.0023074933637567863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004727492038080843,
            "upper_bound": 0.03196873419298771
          },
          "point_estimate": 0.02128111979658743,
          "standard_error": 0.008723214736426394
        }
      }
    },
    "memrchr2/krate/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr2/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr2/krate/empty/never",
        "directory_name": "memrchr2/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.983201970190971,
            "upper_bound": 0.984569910800817
          },
          "point_estimate": 0.9838515661080398,
          "standard_error": 0.00035129175795825686
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9830206942326384,
            "upper_bound": 0.9848488325310344
          },
          "point_estimate": 0.9835600621670176,
          "standard_error": 0.0004054510778410482
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00010709184953147886,
            "upper_bound": 0.0019977514265682133
          },
          "point_estimate": 0.0008055894076030122,
          "standard_error": 0.0005204404098755149
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.983005104207864,
            "upper_bound": 0.9841685656251123
          },
          "point_estimate": 0.9835356730034488,
          "standard_error": 0.00029235081019987955
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005476149149510668,
            "upper_bound": 0.0015122529545052428
          },
          "point_estimate": 0.0011700853875615635,
          "standard_error": 0.0002462930179956943
        }
      }
    },
    "memrchr2/krate/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr2/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/common",
        "directory_name": "memrchr2/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 445488.89521099493,
            "upper_bound": 445953.7002439025
          },
          "point_estimate": 445736.1037925861,
          "standard_error": 119.02129995548044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 445468.93728222995,
            "upper_bound": 446045.4146341463
          },
          "point_estimate": 445820.6278963415,
          "standard_error": 166.6830476593791
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.18521230884878,
            "upper_bound": 705.2785329666881
          },
          "point_estimate": 379.45368411702407,
          "standard_error": 146.26619301357252
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 445617.950742608,
            "upper_bound": 445921.25466087536
          },
          "point_estimate": 445797.8023756731,
          "standard_error": 77.08450812412077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203.6063339287235,
            "upper_bound": 523.7322333586674
          },
          "point_estimate": 394.9371856655808,
          "standard_error": 88.9691107635044
        }
      }
    },
    "memrchr2/krate/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr2/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/never",
        "directory_name": "memrchr2/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12853.523452044305,
            "upper_bound": 12964.93154088606
          },
          "point_estimate": 12906.264657080575,
          "standard_error": 28.80039543283956
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12828.469718392837,
            "upper_bound": 13035.093319194062
          },
          "point_estimate": 12868.075998586071,
          "standard_error": 49.72430318940831
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.201570603694694,
            "upper_bound": 133.76783155939827
          },
          "point_estimate": 60.05420815969737,
          "standard_error": 35.25484059380583
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12872.031147960122,
            "upper_bound": 13021.51903325132
          },
          "point_estimate": 12973.86759857322,
          "standard_error": 36.569160067944566
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.47203794013508,
            "upper_bound": 109.87956649234266
          },
          "point_estimate": 95.95565636551524,
          "standard_error": 16.11760560003243
        }
      }
    },
    "memrchr2/krate/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr2/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/rare",
        "directory_name": "memrchr2/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16787.749281012526,
            "upper_bound": 16821.04424650852
          },
          "point_estimate": 16803.786617118505,
          "standard_error": 8.54172101388494
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16781.69061922366,
            "upper_bound": 16827.9969038817
          },
          "point_estimate": 16798.917802680222,
          "standard_error": 10.975635986624487
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.562117616014456,
            "upper_bound": 51.40564284661164
          },
          "point_estimate": 26.381612761762874,
          "standard_error": 10.967080845416076
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16776.59632008211,
            "upper_bound": 16810.819810384106
          },
          "point_estimate": 16792.662490097704,
          "standard_error": 8.998275929559775
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.303884479168683,
            "upper_bound": 35.54639715330475
          },
          "point_estimate": 28.44285409614078,
          "standard_error": 5.365112513256584
        }
      }
    },
    "memrchr2/krate/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr2/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/uncommon",
        "directory_name": "memrchr2/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167324.15581742374,
            "upper_bound": 167655.44507241604
          },
          "point_estimate": 167493.44347871406,
          "standard_error": 84.56502693961485
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167269.17780337945,
            "upper_bound": 167744.37096774194
          },
          "point_estimate": 167503.5361751152,
          "standard_error": 130.13804854699106
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.717308982486834,
            "upper_bound": 487.08177482031056
          },
          "point_estimate": 309.61962929577845,
          "standard_error": 107.07711011973824
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167185.82737803427,
            "upper_bound": 167626.69004643543
          },
          "point_estimate": 167388.2288467293,
          "standard_error": 111.89763997916111
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167.07452002638007,
            "upper_bound": 346.5516328953858
          },
          "point_estimate": 281.6331452074637,
          "standard_error": 46.04106336051049
        }
      }
    },
    "memrchr2/krate/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr2/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/common",
        "directory_name": "memrchr2/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 457.9369845464143,
            "upper_bound": 459.1523965730627
          },
          "point_estimate": 458.4696833297286,
          "standard_error": 0.31667890670101667
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 457.7555366304988,
            "upper_bound": 458.95078223568
          },
          "point_estimate": 458.0956813020439,
          "standard_error": 0.3275268820002498
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1638991530326732,
            "upper_bound": 1.4118821746565655
          },
          "point_estimate": 0.6632104291794657,
          "standard_error": 0.31112308463907645
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 458.0133468768725,
            "upper_bound": 458.8152981591191
          },
          "point_estimate": 458.4955209060432,
          "standard_error": 0.20411074306966945
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3843513129666235,
            "upper_bound": 1.514525443806096
          },
          "point_estimate": 1.0572644795050266,
          "standard_error": 0.3413572122267835
        }
      }
    },
    "memrchr2/krate/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr2/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/never",
        "directory_name": "memrchr2/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.961648622777467,
            "upper_bound": 12.979047643475184
          },
          "point_estimate": 12.97020123534196,
          "standard_error": 0.0044391045037163275
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.960727676719548,
            "upper_bound": 12.980391128876944
          },
          "point_estimate": 12.967538005614117,
          "standard_error": 0.005022396084092858
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0021370980017770148,
            "upper_bound": 0.025337874547843225
          },
          "point_estimate": 0.01371051918250068,
          "standard_error": 0.005896692481158575
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.961460463562924,
            "upper_bound": 12.976022344980995
          },
          "point_estimate": 12.967023439860334,
          "standard_error": 0.003713128573898549
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0070516455958524425,
            "upper_bound": 0.019797474834696707
          },
          "point_estimate": 0.014835501620313184,
          "standard_error": 0.0032932395774521927
        }
      }
    },
    "memrchr2/krate/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr2/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/rare",
        "directory_name": "memrchr2/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.54719609950324,
            "upper_bound": 23.598125044036863
          },
          "point_estimate": 23.57023649116144,
          "standard_error": 0.013115073284786416
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.536460404579227,
            "upper_bound": 23.588020322156144
          },
          "point_estimate": 23.561564757590556,
          "standard_error": 0.012230456831168772
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007878147055522793,
            "upper_bound": 0.06318938839955242
          },
          "point_estimate": 0.0370704430974611,
          "standard_error": 0.01429227321051428
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.547302450814396,
            "upper_bound": 23.57319056346666
          },
          "point_estimate": 23.560657125741407,
          "standard_error": 0.006465855169577901
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017301226201224037,
            "upper_bound": 0.06103405007275085
          },
          "point_estimate": 0.04387634691387516,
          "standard_error": 0.012431462368636754
        }
      }
    },
    "memrchr2/krate/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr2/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/uncommon",
        "directory_name": "memrchr2/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.39512449010404,
            "upper_bound": 96.59393795357572
          },
          "point_estimate": 96.48868795617618,
          "standard_error": 0.05095821612004048
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.36701041868412,
            "upper_bound": 96.61779818740897
          },
          "point_estimate": 96.44449281069484,
          "standard_error": 0.05849906263623722
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023455537599262343,
            "upper_bound": 0.28357160743405363
          },
          "point_estimate": 0.1208979298761716,
          "standard_error": 0.06681744433259804
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.43459288249927,
            "upper_bound": 96.6182402684181
          },
          "point_estimate": 96.5154611295968,
          "standard_error": 0.04631304086945435
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07919950149101496,
            "upper_bound": 0.2259770632869436
          },
          "point_estimate": 0.17028565391750516,
          "standard_error": 0.03887078976270713
        }
      }
    },
    "memrchr2/krate/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr2/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/never",
        "directory_name": "memrchr2/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.882467077847104,
            "upper_bound": 4.90865881771583
          },
          "point_estimate": 4.896204778500008,
          "standard_error": 0.0066896089263374856
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.877229321316271,
            "upper_bound": 4.9158751382286905
          },
          "point_estimate": 4.9004014768375574,
          "standard_error": 0.010730525741420688
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001501729473401042,
            "upper_bound": 0.03674362940651587
          },
          "point_estimate": 0.023483206204705856,
          "standard_error": 0.009701232826197472
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.881132886022943,
            "upper_bound": 4.910938396885493
          },
          "point_estimate": 4.896946480419948,
          "standard_error": 0.007661331174639812
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011926143743076014,
            "upper_bound": 0.027594281009902934
          },
          "point_estimate": 0.022225345799427112,
          "standard_error": 0.004007171161596964
        }
      }
    },
    "memrchr2/krate/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr2/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/rare",
        "directory_name": "memrchr2/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.879627187766948,
            "upper_bound": 15.952119165349323
          },
          "point_estimate": 15.915385720871644,
          "standard_error": 0.018515301937562668
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.866325815076085,
            "upper_bound": 15.963352344188175
          },
          "point_estimate": 15.916362108647824,
          "standard_error": 0.026924205280538245
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01585725428300931,
            "upper_bound": 0.10291710079864214
          },
          "point_estimate": 0.0651799167315291,
          "standard_error": 0.023134993367152194
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.881198799949717,
            "upper_bound": 15.931050093800856
          },
          "point_estimate": 15.903374565984803,
          "standard_error": 0.012697574462627693
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0354981729110264,
            "upper_bound": 0.07653809697265784
          },
          "point_estimate": 0.06159405212894469,
          "standard_error": 0.010457036687999265
        }
      }
    },
    "memrchr2/krate/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr2/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/uncommon",
        "directory_name": "memrchr2/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.428268922652975,
            "upper_bound": 50.49542740000764
          },
          "point_estimate": 50.464907930033846,
          "standard_error": 0.017272489639497155
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.435109417486245,
            "upper_bound": 50.5085319174178
          },
          "point_estimate": 50.47722647595796,
          "standard_error": 0.024347858495899148
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006011293516548707,
            "upper_bound": 0.094445883995237
          },
          "point_estimate": 0.049016834261277835,
          "standard_error": 0.023074226505156756
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.44226672858786,
            "upper_bound": 50.50080995853659
          },
          "point_estimate": 50.47363074706337,
          "standard_error": 0.0152954415390205
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027145384025448884,
            "upper_bound": 0.07885987426129065
          },
          "point_estimate": 0.05739034681968818,
          "standard_error": 0.015207615225100618
        }
      }
    },
    "memrchr3/krate/empty/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr3/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr3/krate/empty/never",
        "directory_name": "memrchr3/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2232352584017565,
            "upper_bound": 1.2261517442948426
          },
          "point_estimate": 1.224609034404904,
          "standard_error": 0.0007486328643659139
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2225302833713607,
            "upper_bound": 1.2266095437507416
          },
          "point_estimate": 1.223798396611496,
          "standard_error": 0.001061011690200483
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003929284608078473,
            "upper_bound": 0.004198297195236229
          },
          "point_estimate": 0.001947332042888162,
          "standard_error": 0.000981607180393014
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2227030181202752,
            "upper_bound": 1.2249800256220926
          },
          "point_estimate": 1.2235391763200196,
          "standard_error": 0.0005849833506989747
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010386768319953592,
            "upper_bound": 0.0030350539336083137
          },
          "point_estimate": 0.0025052037565317068,
          "standard_error": 0.0004789830274047884
        }
      }
    },
    "memrchr3/krate/huge/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr3/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/common",
        "directory_name": "memrchr3/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 685937.4605031447,
            "upper_bound": 687420.7256257862
          },
          "point_estimate": 686565.1540536089,
          "standard_error": 387.263827275298
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 685736.2735849057,
            "upper_bound": 686757.4056603773
          },
          "point_estimate": 686525.2536687632,
          "standard_error": 300.443591002878
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.392256059035,
            "upper_bound": 1402.6817741541297
          },
          "point_estimate": 571.4968327784444,
          "standard_error": 355.17114895428625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 685601.0877332992,
            "upper_bound": 686636.5421885183
          },
          "point_estimate": 686163.1370742465,
          "standard_error": 293.98445866200865
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 381.020498215013,
            "upper_bound": 1900.663132701731
          },
          "point_estimate": 1286.2743120852942,
          "standard_error": 474.115311632172
        }
      }
    },
    "memrchr3/krate/huge/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr3/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/never",
        "directory_name": "memrchr3/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16696.284753559947,
            "upper_bound": 16715.992826519672
          },
          "point_estimate": 16705.40337492982,
          "standard_error": 5.046727532906616
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16691.393609962743,
            "upper_bound": 16713.291417853317
          },
          "point_estimate": 16705.385898024804,
          "standard_error": 6.089132867226014
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.490897526449691,
            "upper_bound": 27.927248314483982
          },
          "point_estimate": 15.727171264196624,
          "standard_error": 5.950330553646248
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16693.50583323513,
            "upper_bound": 16710.297412842083
          },
          "point_estimate": 16701.820189823957,
          "standard_error": 4.264361595680867
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.212873513543993,
            "upper_bound": 22.83299961860093
          },
          "point_estimate": 16.798231782389145,
          "standard_error": 4.1709136650526855
        }
      }
    },
    "memrchr3/krate/huge/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr3/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/rare",
        "directory_name": "memrchr3/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22177.259977106223,
            "upper_bound": 22191.002897673312
          },
          "point_estimate": 22184.538878398937,
          "standard_error": 3.527432499042946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22175.1011686726,
            "upper_bound": 22193.64705857414
          },
          "point_estimate": 22187.23656898657,
          "standard_error": 4.494551454906042
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.778969822458308,
            "upper_bound": 18.309498713404853
          },
          "point_estimate": 9.872544924442558,
          "standard_error": 4.4326750287674965
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22180.51749701687,
            "upper_bound": 22195.23282496364
          },
          "point_estimate": 22190.038610595755,
          "standard_error": 3.771266976321197
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.325059678520868,
            "upper_bound": 15.041220397852577
          },
          "point_estimate": 11.743757294076042,
          "standard_error": 2.448526857298541
        }
      }
    },
    "memrchr3/krate/huge/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr3/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/uncommon",
        "directory_name": "memrchr3/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220142.83788965383,
            "upper_bound": 220492.3798978772
          },
          "point_estimate": 220312.33742445972,
          "standard_error": 89.32846034669042
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220084.63654618477,
            "upper_bound": 220466.52194492257
          },
          "point_estimate": 220362.6234939759,
          "standard_error": 120.51698718374236
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.884173997423773,
            "upper_bound": 518.6660167557073
          },
          "point_estimate": 314.8860598714972,
          "standard_error": 129.69609833130812
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220185.59788172296,
            "upper_bound": 220458.54447303544
          },
          "point_estimate": 220316.43013612897,
          "standard_error": 69.25885061190363
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154.49755829899436,
            "upper_bound": 403.054769805262
          },
          "point_estimate": 299.3446777797099,
          "standard_error": 65.8437846129972
        }
      }
    },
    "memrchr3/krate/small/common": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr3/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/common",
        "directory_name": "memrchr3/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 709.6227980105217,
            "upper_bound": 710.3343623177497
          },
          "point_estimate": 709.9805465257081,
          "standard_error": 0.1822306001358872
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 709.400060498429,
            "upper_bound": 710.4905186601875
          },
          "point_estimate": 709.9654462897549,
          "standard_error": 0.2462933218231046
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12873067451635875,
            "upper_bound": 1.0404662931730138
          },
          "point_estimate": 0.8083566209604022,
          "standard_error": 0.2396119911313179
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 709.7388981834504,
            "upper_bound": 710.3731198423876
          },
          "point_estimate": 710.1069525544809,
          "standard_error": 0.15905173791765492
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.352720020685915,
            "upper_bound": 0.757482145526716
          },
          "point_estimate": 0.6076077380058983,
          "standard_error": 0.10388771636685676
        }
      }
    },
    "memrchr3/krate/small/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr3/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/never",
        "directory_name": "memrchr3/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.715907049896352,
            "upper_bound": 15.745867818449591
          },
          "point_estimate": 15.729982335896333,
          "standard_error": 0.007709327327786166
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.71133669153546,
            "upper_bound": 15.747928320986832
          },
          "point_estimate": 15.725822252318183,
          "standard_error": 0.008882480115074901
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001782959419634151,
            "upper_bound": 0.04365786503890541
          },
          "point_estimate": 0.021772981951549024,
          "standard_error": 0.010150356825633868
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.710317559408535,
            "upper_bound": 15.732038989809473
          },
          "point_estimate": 15.719067372154054,
          "standard_error": 0.005569117795811337
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010866739270719563,
            "upper_bound": 0.03237986213457897
          },
          "point_estimate": 0.02567692134405191,
          "standard_error": 0.005317247191366629
        }
      }
    },
    "memrchr3/krate/small/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr3/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/rare",
        "directory_name": "memrchr3/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.89211275486314,
            "upper_bound": 34.979713581482095
          },
          "point_estimate": 34.92867100159552,
          "standard_error": 0.023086111151922283
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.88749159450181,
            "upper_bound": 34.954250965692125
          },
          "point_estimate": 34.90783749295474,
          "standard_error": 0.01698721446475663
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008003860782874014,
            "upper_bound": 0.07772501078067326
          },
          "point_estimate": 0.03400097072148495,
          "standard_error": 0.019425980883289333
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.89158839525238,
            "upper_bound": 34.919819032430055
          },
          "point_estimate": 34.90159312468582,
          "standard_error": 0.007235999629716422
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020801236173955386,
            "upper_bound": 0.11340213146977778
          },
          "point_estimate": 0.07672673988607856,
          "standard_error": 0.02896226798547087
        }
      }
    },
    "memrchr3/krate/small/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr3/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/uncommon",
        "directory_name": "memrchr3/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.96052090790778,
            "upper_bound": 153.17354863587744
          },
          "point_estimate": 153.06469313435872,
          "standard_error": 0.05454921299419288
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.89137356766028,
            "upper_bound": 153.2123030063358
          },
          "point_estimate": 153.06439736556473,
          "standard_error": 0.07077657327609825
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02781067839291387,
            "upper_bound": 0.3256627581035913
          },
          "point_estimate": 0.2067388020130657,
          "standard_error": 0.07847366785816373
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.96895802205486,
            "upper_bound": 153.15410463410225
          },
          "point_estimate": 153.053760905604,
          "standard_error": 0.04725019500095296
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1008763321073273,
            "upper_bound": 0.22658679683664248
          },
          "point_estimate": 0.18216331297257848,
          "standard_error": 0.031930440744516425
        }
      }
    },
    "memrchr3/krate/tiny/never": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr3/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/never",
        "directory_name": "memrchr3/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.621496317612447,
            "upper_bound": 5.63007976396828
          },
          "point_estimate": 5.625682985165102,
          "standard_error": 0.0021907773505300005
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.620170625873355,
            "upper_bound": 5.628729381370614
          },
          "point_estimate": 5.626193459642586,
          "standard_error": 0.0018954730479385604
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010810758202923023,
            "upper_bound": 0.013123933440452762
          },
          "point_estimate": 0.003825360787228199,
          "standard_error": 0.0031090379533526757
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.6246779975106715,
            "upper_bound": 5.628209227679206
          },
          "point_estimate": 5.626779278992376,
          "standard_error": 0.0009165357847089308
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002463014553574302,
            "upper_bound": 0.010000282957597749
          },
          "point_estimate": 0.007307728970395553,
          "standard_error": 0.0018122299751459968
        }
      }
    },
    "memrchr3/krate/tiny/rare": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr3/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/rare",
        "directory_name": "memrchr3/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.02640198162352,
            "upper_bound": 16.0535765666583
          },
          "point_estimate": 16.039770277692433,
          "standard_error": 0.006973626839309822
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.02134332605537,
            "upper_bound": 16.05495678130931
          },
          "point_estimate": 16.041346999665063,
          "standard_error": 0.008484437309475847
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004659541941648566,
            "upper_bound": 0.040924371353761695
          },
          "point_estimate": 0.021941051563974506,
          "standard_error": 0.008807051788429518
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.03300897403749,
            "upper_bound": 16.05128605628694
          },
          "point_estimate": 16.042590163551612,
          "standard_error": 0.004586040556305983
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012519354587036574,
            "upper_bound": 0.030325900175771203
          },
          "point_estimate": 0.02327902407660008,
          "standard_error": 0.004627193882763118
        }
      }
    },
    "memrchr3/krate/tiny/uncommon": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrchr3/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/uncommon",
        "directory_name": "memrchr3/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.49249329670116,
            "upper_bound": 86.59994607710468
          },
          "point_estimate": 86.543060964466,
          "standard_error": 0.027673135378573153
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.46803661312521,
            "upper_bound": 86.61462974691749
          },
          "point_estimate": 86.50637568871343,
          "standard_error": 0.04695865638139417
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00890410389539925,
            "upper_bound": 0.16904091719626055
          },
          "point_estimate": 0.07614401903218736,
          "standard_error": 0.04231278984510292
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.48511860448258,
            "upper_bound": 86.59537428561775
          },
          "point_estimate": 86.53514767746432,
          "standard_error": 0.02847906338643127
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0495831286164952,
            "upper_bound": 0.11843580715412988
          },
          "point_estimate": 0.0924758272664718,
          "standard_error": 0.018437934425185404
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89307.66060276411,
            "upper_bound": 89451.77434934319
          },
          "point_estimate": 89378.34460346782,
          "standard_error": 37.046890167872654
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89256.5749178982,
            "upper_bound": 89481.33004926109
          },
          "point_estimate": 89394.9279864532,
          "standard_error": 66.8001797767186
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.232775698155848,
            "upper_bound": 229.3421889168905
          },
          "point_estimate": 164.72436049919898,
          "standard_error": 57.25375460662261
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89279.56467593559,
            "upper_bound": 89437.71647702184
          },
          "point_estimate": 89360.26517817158,
          "standard_error": 42.487673629813415
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.45599576482027,
            "upper_bound": 151.65353065999676
          },
          "point_estimate": 123.8067507123036,
          "standard_error": 19.38716921935332
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3104599.6779464283,
            "upper_bound": 3108829.6219831347
          },
          "point_estimate": 3106858.75130291,
          "standard_error": 1086.820741925051
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3104999.625,
            "upper_bound": 3108884.5678571425
          },
          "point_estimate": 3107648.841435185,
          "standard_error": 856.3983141223429
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.72871486080724,
            "upper_bound": 6009.579264142018
          },
          "point_estimate": 1642.5474576442532,
          "standard_error": 1449.874108945642
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3104552.7035108022,
            "upper_bound": 3108066.193685072
          },
          "point_estimate": 3106381.520562771,
          "standard_error": 982.53648035446
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1299.6789633995168,
            "upper_bound": 4921.617591303036
          },
          "point_estimate": 3610.21922664626,
          "standard_error": 947.134922835594
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3101121.582077794,
            "upper_bound": 3105101.539548694
          },
          "point_estimate": 3103070.526028439,
          "standard_error": 1017.4085497196552
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3101080.9416666664,
            "upper_bound": 3105281.183333333
          },
          "point_estimate": 3102502.5839120373,
          "standard_error": 1108.8412097382743
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 677.0910529792079,
            "upper_bound": 5896.127125322772
          },
          "point_estimate": 2303.2199170963077,
          "standard_error": 1398.641946703793
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3102436.502944569,
            "upper_bound": 3105047.3475025664
          },
          "point_estimate": 3103757.3162337663,
          "standard_error": 672.2920400767957
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1634.2338985479637,
            "upper_bound": 4447.753328403375
          },
          "point_estimate": 3383.1053593138804,
          "standard_error": 717.7894160293816
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1410875.7935836385,
            "upper_bound": 1413786.0805494506
          },
          "point_estimate": 1412385.9545772283,
          "standard_error": 745.8809029732305
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1410775.273076923,
            "upper_bound": 1414189.4134615385
          },
          "point_estimate": 1412753.013087607,
          "standard_error": 1129.655247421313
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.3236555208022,
            "upper_bound": 4600.2470414060435
          },
          "point_estimate": 2736.0201321951276,
          "standard_error": 1049.347758168655
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1410986.3127355548,
            "upper_bound": 1412895.518343195
          },
          "point_estimate": 1411821.241058941,
          "standard_error": 489.54390695347365
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1363.492823612631,
            "upper_bound": 3264.367748418328
          },
          "point_estimate": 2486.161554767728,
          "standard_error": 506.69336395333914
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 894890.2435356176,
            "upper_bound": 895962.5789916522
          },
          "point_estimate": 895412.0466656986,
          "standard_error": 274.8870647384726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 894639.6390243903,
            "upper_bound": 896266.6138211382
          },
          "point_estimate": 895402.0996902826,
          "standard_error": 386.54178027656246
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.10646891587749,
            "upper_bound": 1611.6111396808972
          },
          "point_estimate": 800.0258550243344,
          "standard_error": 394.7473253145468
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 895107.2711198428,
            "upper_bound": 895911.895343828
          },
          "point_estimate": 895427.489135255,
          "standard_error": 207.0690039396088
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 507.4690258388581,
            "upper_bound": 1134.9652334116324
          },
          "point_estimate": 914.018684846001,
          "standard_error": 159.94086871919322
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12804.59107765961,
            "upper_bound": 12824.59261077693
          },
          "point_estimate": 12814.120779641084,
          "standard_error": 5.104967765893462
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12804.594172072208,
            "upper_bound": 12824.067053854276
          },
          "point_estimate": 12810.477335836364,
          "standard_error": 5.605370094939519
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6707911062439061,
            "upper_bound": 28.60567319858828
          },
          "point_estimate": 8.917109937899092,
          "standard_error": 7.241160102182049
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12804.56044050408,
            "upper_bound": 12824.090105909654
          },
          "point_estimate": 12813.090672298487,
          "standard_error": 4.924520839178901
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.403509988081436,
            "upper_bound": 22.525263762104903
          },
          "point_estimate": 17.024505098727097,
          "standard_error": 3.8986839172948855
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12374.107866507882,
            "upper_bound": 12403.445851000435
          },
          "point_estimate": 12390.274471002987,
          "standard_error": 7.556314473938744
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12379.376494272428,
            "upper_bound": 12406.512057672677
          },
          "point_estimate": 12395.921004876942,
          "standard_error": 5.503179267483948
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0024932579761774,
            "upper_bound": 37.870029099703935
          },
          "point_estimate": 11.444577684988314,
          "standard_error": 10.286445907359196
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12383.354147619317,
            "upper_bound": 12398.088447611726
          },
          "point_estimate": 12391.35784147802,
          "standard_error": 3.708553150596598
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.936264366800177,
            "upper_bound": 34.755885498693424
          },
          "point_estimate": 25.196895667975788,
          "standard_error": 7.246153811068932
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1675314.437828283,
            "upper_bound": 1679431.4908419913
          },
          "point_estimate": 1677456.5095598844,
          "standard_error": 1055.9046331656475
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1675175.3428030305,
            "upper_bound": 1680261.2102272727
          },
          "point_estimate": 1677770.9808080806,
          "standard_error": 1509.1180606725563
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 523.8430052452064,
            "upper_bound": 6270.189345045475
          },
          "point_estimate": 3520.3965725004327,
          "standard_error": 1322.1359730364293
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1677061.8916354263,
            "upper_bound": 1680142.5805793626
          },
          "point_estimate": 1678888.515820543,
          "standard_error": 781.6048558543898
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1898.6899416064864,
            "upper_bound": 4593.96270485851
          },
          "point_estimate": 3509.6722411028895,
          "standard_error": 721.0217696177863
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2540.50245020013,
            "upper_bound": 2546.011313512287
          },
          "point_estimate": 2542.7377944940886,
          "standard_error": 1.4644899563373193
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2539.846132365261,
            "upper_bound": 2543.3423677045516
          },
          "point_estimate": 2541.3919703527877,
          "standard_error": 0.9095878158630972
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4336998240511343,
            "upper_bound": 4.218256894183659
          },
          "point_estimate": 2.428563067747002,
          "standard_error": 0.9434665190061986
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2540.2014633883045,
            "upper_bound": 2542.544103541639
          },
          "point_estimate": 2541.351154179526,
          "standard_error": 0.6036294117807272
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2048562138157075,
            "upper_bound": 7.354895980239425
          },
          "point_estimate": 4.896535450159597,
          "standard_error": 1.987877129902702
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 730.7229692984032,
            "upper_bound": 731.396289327795
          },
          "point_estimate": 731.0511400511195,
          "standard_error": 0.1724046216741697
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 730.4860298555507,
            "upper_bound": 731.5823930425186
          },
          "point_estimate": 731.0044516758539,
          "standard_error": 0.2768183026686054
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1205329578649952,
            "upper_bound": 0.990036511614963
          },
          "point_estimate": 0.6721660750581567,
          "standard_error": 0.2248066130110136
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 730.5659099057059,
            "upper_bound": 731.2493815604001
          },
          "point_estimate": 730.9305025236704,
          "standard_error": 0.1768977711586991
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.33905848665694466,
            "upper_bound": 0.6957173837315936
          },
          "point_estimate": 0.5744489407594757,
          "standard_error": 0.09099056291098326
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139.2132744425956,
            "upper_bound": 139.35812803154815
          },
          "point_estimate": 139.2818948437802,
          "standard_error": 0.037172666231909936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139.17425251278223,
            "upper_bound": 139.37501102284318
          },
          "point_estimate": 139.27785113027275,
          "standard_error": 0.045580345853721296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0235121914701529,
            "upper_bound": 0.21559116661951447
          },
          "point_estimate": 0.12139954231373878,
          "standard_error": 0.05243032946672136
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139.18770479124652,
            "upper_bound": 139.4294699836708
          },
          "point_estimate": 139.30394561768605,
          "standard_error": 0.06582696908518917
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06014811262251427,
            "upper_bound": 0.15935040841311615
          },
          "point_estimate": 0.12396212451599647,
          "standard_error": 0.026029731608140136
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.60839038668826,
            "upper_bound": 49.699898517599095
          },
          "point_estimate": 49.65874059645705,
          "standard_error": 0.023476554484683033
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.62361565300677,
            "upper_bound": 49.71217322295957
          },
          "point_estimate": 49.66787931860163,
          "standard_error": 0.0205247116032647
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01284855255290232,
            "upper_bound": 0.1109728666759324
          },
          "point_estimate": 0.06028984064362811,
          "standard_error": 0.02513503641176584
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.64515856713421,
            "upper_bound": 49.68917596744146
          },
          "point_estimate": 49.671929144946866,
          "standard_error": 0.011123256417108004
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030819799576324507,
            "upper_bound": 0.11055170519149672
          },
          "point_estimate": 0.07838046228246691,
          "standard_error": 0.0231754891655409
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.77933250675436,
            "upper_bound": 79.87068193519505
          },
          "point_estimate": 79.82190592059735,
          "standard_error": 0.023449063930045727
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.76010220877312,
            "upper_bound": 79.87884418308064
          },
          "point_estimate": 79.79693739025907,
          "standard_error": 0.03401455589522256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005627571546235554,
            "upper_bound": 0.1361379796465353
          },
          "point_estimate": 0.07098662787198527,
          "standard_error": 0.03200962596957091
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.76330292549129,
            "upper_bound": 79.85405492309967
          },
          "point_estimate": 79.79878651572645,
          "standard_error": 0.023292979948370218
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03714818605742227,
            "upper_bound": 0.10176166139921652
          },
          "point_estimate": 0.07819454899212065,
          "standard_error": 0.017193022201720212
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12151.386550898267,
            "upper_bound": 12174.401959884652
          },
          "point_estimate": 12162.846909793583,
          "standard_error": 5.937881928385639
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12145.33032786885,
            "upper_bound": 12180.409735697558
          },
          "point_estimate": 12163.23743308799,
          "standard_error": 8.359640532780633
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7714757687061111,
            "upper_bound": 38.06780660167758
          },
          "point_estimate": 15.111031386682727,
          "standard_error": 8.620748434311903
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12150.636319750944,
            "upper_bound": 12175.307552765053
          },
          "point_estimate": 12161.927265775375,
          "standard_error": 6.216050124696955
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.72138230493664,
            "upper_bound": 24.619621385922464
          },
          "point_estimate": 19.853207446857212,
          "standard_error": 3.3539106650864894
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.78542665533351,
            "upper_bound": 63.93517127302621
          },
          "point_estimate": 63.854538175738675,
          "standard_error": 0.03841096204874998
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.78220675444033,
            "upper_bound": 63.9345512424514
          },
          "point_estimate": 63.82013957044474,
          "standard_error": 0.03870975800263454
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008736141081022867,
            "upper_bound": 0.2105829049257116
          },
          "point_estimate": 0.05770282210180972,
          "standard_error": 0.05348179895448245
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.789837208209654,
            "upper_bound": 63.864050361030024
          },
          "point_estimate": 63.81936677016448,
          "standard_error": 0.019204748149291143
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0490092755676246,
            "upper_bound": 0.17277814460674343
          },
          "point_estimate": 0.12761637634612552,
          "standard_error": 0.03271678627569338
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.32752398582087,
            "upper_bound": 109.52074411126696
          },
          "point_estimate": 109.41450831211657,
          "standard_error": 0.04969332547024732
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.30680347243528,
            "upper_bound": 109.5061756619359
          },
          "point_estimate": 109.3683737473704,
          "standard_error": 0.04748570897596911
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016151902998303394,
            "upper_bound": 0.23684317714847883
          },
          "point_estimate": 0.10322438529386849,
          "standard_error": 0.05956375553571589
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.27686961112482,
            "upper_bound": 109.39884150161576
          },
          "point_estimate": 109.32298954527369,
          "standard_error": 0.03118040459529299
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06488958195877546,
            "upper_bound": 0.23087228544987437
          },
          "point_estimate": 0.16601475672071558,
          "standard_error": 0.04710758019119444
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59840.46979637733,
            "upper_bound": 60266.72521573768
          },
          "point_estimate": 60057.2828349364,
          "standard_error": 109.94008196090569
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59660.894039735096,
            "upper_bound": 60374.84650478292
          },
          "point_estimate": 60196.580229028696,
          "standard_error": 229.6775382558177
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.9106192300051,
            "upper_bound": 527.1976591834222
          },
          "point_estimate": 398.0165838526717,
          "standard_error": 145.41287935938527
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59813.569080693174,
            "upper_bound": 60337.34968630184
          },
          "point_estimate": 60118.97343682807,
          "standard_error": 134.59780865264003
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.12959500666992,
            "upper_bound": 409.82426981383475
          },
          "point_estimate": 365.1170955687931,
          "standard_error": 43.9038149380479
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.332689622050104,
            "upper_bound": 52.38248344602422
          },
          "point_estimate": 52.35791066508889,
          "standard_error": 0.012754843803001488
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.316975598468346,
            "upper_bound": 52.39132333709716
          },
          "point_estimate": 52.3691133009183,
          "standard_error": 0.020384265583234685
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007451136362278896,
            "upper_bound": 0.0719426427565075
          },
          "point_estimate": 0.046578766268728755,
          "standard_error": 0.017044665071332357
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.33240843123811,
            "upper_bound": 52.39331764916575
          },
          "point_estimate": 52.365905447285144,
          "standard_error": 0.01536508165817918
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02610833644073253,
            "upper_bound": 0.05154291987654353
          },
          "point_estimate": 0.04245497773101581,
          "standard_error": 0.00651559790674198
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.01528975143296,
            "upper_bound": 109.22663374804492
          },
          "point_estimate": 109.11148007556332,
          "standard_error": 0.05416237249180622
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.99658940723684,
            "upper_bound": 109.1802666473866
          },
          "point_estimate": 109.09647887677524,
          "standard_error": 0.046624951159451006
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030807942616018047,
            "upper_bound": 0.24443254331084596
          },
          "point_estimate": 0.1268530115241253,
          "standard_error": 0.054184834872393374
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.02978171549292,
            "upper_bound": 109.1555606121573
          },
          "point_estimate": 109.10125349798491,
          "standard_error": 0.03239065780304969
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06351662927743812,
            "upper_bound": 0.2570229359196118
          },
          "point_estimate": 0.18057353835605056,
          "standard_error": 0.05499928813706819
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280.1036377289954,
            "upper_bound": 280.62656519553036
          },
          "point_estimate": 280.3680072963979,
          "standard_error": 0.13360541381460272
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280.01333007770666,
            "upper_bound": 280.75281633125144
          },
          "point_estimate": 280.41538733389876,
          "standard_error": 0.15435178057984955
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026629738134676003,
            "upper_bound": 0.7481643519345513
          },
          "point_estimate": 0.4456130900531563,
          "standard_error": 0.1891236293723779
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280.00551087172346,
            "upper_bound": 280.46972855663824
          },
          "point_estimate": 280.22384449231083,
          "standard_error": 0.12463369615206137
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2389455373024122,
            "upper_bound": 0.569354437965239
          },
          "point_estimate": 0.4458298083893387,
          "standard_error": 0.085332372832007
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482.4724217865708,
            "upper_bound": 483.5437562967163
          },
          "point_estimate": 482.9371364177288,
          "standard_error": 0.2781017290010076
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482.28681596860696,
            "upper_bound": 483.29648680640526
          },
          "point_estimate": 482.6709293149616,
          "standard_error": 0.23161320367038896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08123065637560423,
            "upper_bound": 1.1431240562868776
          },
          "point_estimate": 0.5784488214922325,
          "standard_error": 0.28528939510017154
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482.4968580959653,
            "upper_bound": 483.050306466775
          },
          "point_estimate": 482.7422510430996,
          "standard_error": 0.14038104785792285
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.314083786644656,
            "upper_bound": 1.324251981966564
          },
          "point_estimate": 0.9249495380181648,
          "standard_error": 0.30152219662868396
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.328659849284616,
            "upper_bound": 30.36459223757831
          },
          "point_estimate": 30.34739861960775,
          "standard_error": 0.00923337323992604
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.321032053773,
            "upper_bound": 30.37312230433553
          },
          "point_estimate": 30.355187439439497,
          "standard_error": 0.015019460512307872
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032160832274546244,
            "upper_bound": 0.05313716531927997
          },
          "point_estimate": 0.028543626137449884,
          "standard_error": 0.01424097304371874
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.332210574823996,
            "upper_bound": 30.37050289828023
          },
          "point_estimate": 30.35335321042209,
          "standard_error": 0.009764470641912486
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016593176158899263,
            "upper_bound": 0.03745418807632114
          },
          "point_estimate": 0.030854002155226153,
          "standard_error": 0.005314181802345565
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/bstr_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69332.2637921059,
            "upper_bound": 69435.91593768934
          },
          "point_estimate": 69382.98750393797,
          "standard_error": 26.508652826154844
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69310.826608506,
            "upper_bound": 69466.30963740459
          },
          "point_estimate": 69376.53623833758,
          "standard_error": 37.313608259656974
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.49485546710129,
            "upper_bound": 151.7861237365499
          },
          "point_estimate": 112.21971074942672,
          "standard_error": 34.57614831254527
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69341.78672603094,
            "upper_bound": 69425.9807803625
          },
          "point_estimate": 69377.32703479726,
          "standard_error": 21.62873050662511
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.33760259457811,
            "upper_bound": 108.48076312020866
          },
          "point_estimate": 88.59407291025109,
          "standard_error": 14.436379308129927
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/bstr_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.21586096501915,
            "upper_bound": 144.36216391480806
          },
          "point_estimate": 144.2899052038377,
          "standard_error": 0.03737754063743622
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.21832184395942,
            "upper_bound": 144.3808369481166
          },
          "point_estimate": 144.29283805064756,
          "standard_error": 0.03587468273328214
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014482727864852517,
            "upper_bound": 0.20631304669789413
          },
          "point_estimate": 0.07388220571214357,
          "standard_error": 0.05278559438683792
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.24324270270228,
            "upper_bound": 144.32366939250218
          },
          "point_estimate": 144.28242545715375,
          "standard_error": 0.02072357220371875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05845029099021512,
            "upper_bound": 0.16613239856056783
          },
          "point_estimate": 0.12465816617452942,
          "standard_error": 0.027329291807811167
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/bstr_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 816850.4981176588,
            "upper_bound": 818457.6646884701
          },
          "point_estimate": 817693.4500467373,
          "standard_error": 411.13760670636265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 816936.5875,
            "upper_bound": 818845.8222222222
          },
          "point_estimate": 817687.6150793651,
          "standard_error": 507.69126130594617
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 353.544496223365,
            "upper_bound": 2342.116716752469
          },
          "point_estimate": 1334.043456315977,
          "standard_error": 490.4402041622982
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 817280.252375608,
            "upper_bound": 818312.1078135485
          },
          "point_estimate": 817771.5713708514,
          "standard_error": 268.8897816394443
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 699.8070232478397,
            "upper_bound": 1831.922015753176
          },
          "point_estimate": 1372.994099256612,
          "standard_error": 306.8370132428305
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/bstr_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1884.4043684064832,
            "upper_bound": 1886.575904012927
          },
          "point_estimate": 1885.6007213312544,
          "standard_error": 0.5581772245593257
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1884.7584196484624,
            "upper_bound": 1887.156308851224
          },
          "point_estimate": 1885.8761145404665,
          "standard_error": 0.6463421616428197
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.28954974062243666,
            "upper_bound": 2.819252042087347
          },
          "point_estimate": 1.7261710773267107,
          "standard_error": 0.6253917855209001
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1884.6753827888188,
            "upper_bound": 1886.7673174528136
          },
          "point_estimate": 1885.7241990798489,
          "standard_error": 0.5456344818958001
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8093710092104972,
            "upper_bound": 2.5839008816087032
          },
          "point_estimate": 1.8643882274230132,
          "standard_error": 0.5132473479776268
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.620668053793345,
            "upper_bound": 61.69331729063926
          },
          "point_estimate": 61.65696993898424,
          "standard_error": 0.018575289948741063
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.594132354487755,
            "upper_bound": 61.7155099982246
          },
          "point_estimate": 61.66335211263358,
          "standard_error": 0.02575112115239827
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0036440184718620665,
            "upper_bound": 0.10626923007690402
          },
          "point_estimate": 0.07735400916828239,
          "standard_error": 0.028381467040452538
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.61364152240547,
            "upper_bound": 61.70389333899957
          },
          "point_estimate": 61.65478449710984,
          "standard_error": 0.02303988712043534
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03640423141577257,
            "upper_bound": 0.07672781101056572
          },
          "point_estimate": 0.062028356326940745,
          "standard_error": 0.010271624706391695
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.75044241700045,
            "upper_bound": 64.12475707925613
          },
          "point_estimate": 63.94206781595583,
          "standard_error": 0.09573711264292836
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.75292839264569,
            "upper_bound": 64.20378910812161
          },
          "point_estimate": 63.935787203960146,
          "standard_error": 0.11500698497704888
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0799485135853851,
            "upper_bound": 0.5666227050762503
          },
          "point_estimate": 0.2802391268237034,
          "standard_error": 0.1251905510316495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.86293235288613,
            "upper_bound": 64.09622547964415
          },
          "point_estimate": 63.99182637185126,
          "standard_error": 0.05982079402232254
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.16130907372734932,
            "upper_bound": 0.42066063051477925
          },
          "point_estimate": 0.319166190855935,
          "standard_error": 0.06731877735373396
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.44650442497449,
            "upper_bound": 46.50058469635087
          },
          "point_estimate": 46.471709809314945,
          "standard_error": 0.013944696974063125
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.4402722580348,
            "upper_bound": 46.506871450908335
          },
          "point_estimate": 46.459683022450605,
          "standard_error": 0.01539193411721836
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008234147687955896,
            "upper_bound": 0.07559221029707848
          },
          "point_estimate": 0.033874292011387086,
          "standard_error": 0.016677278302596056
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.44294487822382,
            "upper_bound": 46.51125060971809
          },
          "point_estimate": 46.474956297461155,
          "standard_error": 0.017872338560976267
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01704151780936535,
            "upper_bound": 0.05868460302290859
          },
          "point_estimate": 0.04656938320592779,
          "standard_error": 0.010488672793246136
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.42742972883864,
            "upper_bound": 58.49546463590502
          },
          "point_estimate": 58.45709282607436,
          "standard_error": 0.01767695786563154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.42266501853005,
            "upper_bound": 58.481628656898856
          },
          "point_estimate": 58.43452289431784,
          "standard_error": 0.018003785094836716
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002833566268432388,
            "upper_bound": 0.07457945385229735
          },
          "point_estimate": 0.03025137900841753,
          "standard_error": 0.019442126261328865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.41384230333945,
            "upper_bound": 58.53191033199531
          },
          "point_estimate": 58.46086128227563,
          "standard_error": 0.03342214259126272
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02065185214606133,
            "upper_bound": 0.08446062445522393
          },
          "point_estimate": 0.05892156947776704,
          "standard_error": 0.019187016710522052
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.08611746044647,
            "upper_bound": 52.17562417388901
          },
          "point_estimate": 52.1314145724124,
          "standard_error": 0.02299243817030957
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.05085456322893,
            "upper_bound": 52.20079946377779
          },
          "point_estimate": 52.13699096207138,
          "standard_error": 0.03545396248673163
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020163540556180224,
            "upper_bound": 0.13141285057726657
          },
          "point_estimate": 0.1111541528034945,
          "standard_error": 0.031199732580478184
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.07870333778544,
            "upper_bound": 52.165371688041766
          },
          "point_estimate": 52.12275145860187,
          "standard_error": 0.022195512283438277
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04591667416102808,
            "upper_bound": 0.09153362694592997
          },
          "point_estimate": 0.07658660449593646,
          "standard_error": 0.011456699471478596
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.227553170108,
            "upper_bound": 81.35237449431796
          },
          "point_estimate": 81.28107949525176,
          "standard_error": 0.032652603134512695
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.2177087803686,
            "upper_bound": 81.32834143957275
          },
          "point_estimate": 81.24150042276763,
          "standard_error": 0.028683410435828456
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009806990954483213,
            "upper_bound": 0.1334194276390152
          },
          "point_estimate": 0.047571797287717314,
          "standard_error": 0.03205413611782729
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.24333205859405,
            "upper_bound": 81.433137834517
          },
          "point_estimate": 81.3201483727452,
          "standard_error": 0.05400051179921473
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.032565978973799294,
            "upper_bound": 0.15608753084945914
          },
          "point_estimate": 0.10825594321506488,
          "standard_error": 0.036710802970410766
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.23956105108432,
            "upper_bound": 86.36464633467618
          },
          "point_estimate": 86.29971275120418,
          "standard_error": 0.031869470988348914
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.21884837635976,
            "upper_bound": 86.36433885151015
          },
          "point_estimate": 86.27809455865325,
          "standard_error": 0.036536016678631926
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009759597442203262,
            "upper_bound": 0.1759033025601836
          },
          "point_estimate": 0.08837516176842718,
          "standard_error": 0.03926114079280786
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.24967788683922,
            "upper_bound": 86.35680766442151
          },
          "point_estimate": 86.31248201626727,
          "standard_error": 0.02751600150015352
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05136873729568475,
            "upper_bound": 0.14301718910135974
          },
          "point_estimate": 0.10637889295720052,
          "standard_error": 0.024286005004605617
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.791261213648376,
            "upper_bound": 63.880809498564005
          },
          "point_estimate": 63.83319706743301,
          "standard_error": 0.023025794346979045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.774301595251835,
            "upper_bound": 63.88024841172904
          },
          "point_estimate": 63.8212117650677,
          "standard_error": 0.033920241225047235
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012671890088405422,
            "upper_bound": 0.13259583757840315
          },
          "point_estimate": 0.06706022797739376,
          "standard_error": 0.03063944518371431
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.77862701370295,
            "upper_bound": 63.85377995718773
          },
          "point_estimate": 63.819463568337675,
          "standard_error": 0.01942318990075458
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04004103672803301,
            "upper_bound": 0.10210135413756932
          },
          "point_estimate": 0.07690947815549534,
          "standard_error": 0.01762755441096633
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.346326047167,
            "upper_bound": 109.86619458957736
          },
          "point_estimate": 109.57480135901938,
          "standard_error": 0.134696402626265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.27486558452304,
            "upper_bound": 109.80502975429336
          },
          "point_estimate": 109.42107816958264,
          "standard_error": 0.12708273401409162
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04950549699241909,
            "upper_bound": 0.617435208179943
          },
          "point_estimate": 0.2315763146722699,
          "standard_error": 0.14023393717852412
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.30139729100704,
            "upper_bound": 109.48214184127968
          },
          "point_estimate": 109.37563298525072,
          "standard_error": 0.046164963205382374
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12685131516765916,
            "upper_bound": 0.6176369176783721
          },
          "point_estimate": 0.4482627056692376,
          "standard_error": 0.13441337126563405
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.42575111318557,
            "upper_bound": 77.50014283173445
          },
          "point_estimate": 77.4613380846816,
          "standard_error": 0.019065497441797516
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.40660434954117,
            "upper_bound": 77.50951110743414
          },
          "point_estimate": 77.45795673977919,
          "standard_error": 0.023934281642072088
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006251028994662929,
            "upper_bound": 0.11237777875092812
          },
          "point_estimate": 0.058449584160236615,
          "standard_error": 0.02568093992406084
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.42750583724766,
            "upper_bound": 77.49060985783733
          },
          "point_estimate": 77.46258200273616,
          "standard_error": 0.01605752012823306
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.031965620366812976,
            "upper_bound": 0.07913889230520028
          },
          "point_estimate": 0.06353679999796863,
          "standard_error": 0.01186561442250355
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.42770710789298,
            "upper_bound": 59.51576998759669
          },
          "point_estimate": 59.473539994856026,
          "standard_error": 0.022625997961220164
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.38870761345896,
            "upper_bound": 59.53572235076959
          },
          "point_estimate": 59.49545402313271,
          "standard_error": 0.03803543259581653
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005893018409477685,
            "upper_bound": 0.1193513853744993
          },
          "point_estimate": 0.06305510828399502,
          "standard_error": 0.031548340769542105
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.48226104852409,
            "upper_bound": 59.54237517312002
          },
          "point_estimate": 59.52039607646004,
          "standard_error": 0.015463730598024972
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.038990979530166815,
            "upper_bound": 0.08947636665844003
          },
          "point_estimate": 0.07553171602767622,
          "standard_error": 0.0119494620441087
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.20978826915707,
            "upper_bound": 109.5734617621536
          },
          "point_estimate": 109.35320328262848,
          "standard_error": 0.09863708160191062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.19526622558286,
            "upper_bound": 109.35019016471408
          },
          "point_estimate": 109.27529019765348,
          "standard_error": 0.04254520919829028
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02867665385617917,
            "upper_bound": 0.22777043757186213
          },
          "point_estimate": 0.08334144889696143,
          "standard_error": 0.05435727645373563
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.23495429099682,
            "upper_bound": 109.310956557855
          },
          "point_estimate": 109.27207430210814,
          "standard_error": 0.019291109089851207
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05549660245604916,
            "upper_bound": 0.5017022076230906
          },
          "point_estimate": 0.32940604715775673,
          "standard_error": 0.1461473332228642
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2037527.6256481484,
            "upper_bound": 2041644.7146913584
          },
          "point_estimate": 2039427.4808686064,
          "standard_error": 1060.579088694591
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2036532.9813492063,
            "upper_bound": 2042322.601851852
          },
          "point_estimate": 2038526.6166666667,
          "standard_error": 1324.5440133805646
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 442.78377713908066,
            "upper_bound": 5785.3620028448395
          },
          "point_estimate": 2692.2208031294863,
          "standard_error": 1381.251082992898
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2036737.4279219336,
            "upper_bound": 2039560.8715029128
          },
          "point_estimate": 2037904.1533910537,
          "standard_error": 749.6344383321448
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1421.7486201509398,
            "upper_bound": 4483.558833306633
          },
          "point_estimate": 3537.725196961133,
          "standard_error": 792.2367083531121
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3103103.2761111115,
            "upper_bound": 3110172.193558201
          },
          "point_estimate": 3106470.017566138,
          "standard_error": 1811.8975025749132
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3100470.808333333,
            "upper_bound": 3109991.1904761903
          },
          "point_estimate": 3106246.6481481483,
          "standard_error": 2703.6213034028688
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 824.729182024559,
            "upper_bound": 11219.884175807125
          },
          "point_estimate": 6105.855109654456,
          "standard_error": 2511.212386403877
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3101942.208244909,
            "upper_bound": 3109151.136752137
          },
          "point_estimate": 3105650.0274891774,
          "standard_error": 1894.770504083735
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3344.1656267162116,
            "upper_bound": 7897.3590220111555
          },
          "point_estimate": 6031.456786884085,
          "standard_error": 1275.625332176871
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2628215.579107001,
            "upper_bound": 2631282.220271684
          },
          "point_estimate": 2629832.1003004536,
          "standard_error": 786.7762826718458
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2628055.438492063,
            "upper_bound": 2631767.0714285714
          },
          "point_estimate": 2630414.2933673467,
          "standard_error": 1042.7945611652983
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 511.9470659112214,
            "upper_bound": 4357.828459300208
          },
          "point_estimate": 2364.276711597441,
          "standard_error": 965.5012676263372
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2628341.1258503404,
            "upper_bound": 2631134.8375629135
          },
          "point_estimate": 2629612.7586270873,
          "standard_error": 703.5719894832522
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1286.2811922788037,
            "upper_bound": 3338.1153059405324
          },
          "point_estimate": 2627.8898110585747,
          "standard_error": 522.3258664102906
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 381439.3125581597,
            "upper_bound": 382460.9905079985
          },
          "point_estimate": 381960.6038682209,
          "standard_error": 262.6514308738424
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 381454.9791666667,
            "upper_bound": 382811.44375
          },
          "point_estimate": 381875.9129464286,
          "standard_error": 352.05211838255735
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.34500681236386,
            "upper_bound": 1711.026545404514
          },
          "point_estimate": 653.050815749764,
          "standard_error": 389.90483718797714
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 381666.1710304054,
            "upper_bound": 382822.6476728268
          },
          "point_estimate": 382353.8489718615,
          "standard_error": 293.66224484424896
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482.43523678733527,
            "upper_bound": 1144.4502603493022
          },
          "point_estimate": 873.5594334850533,
          "standard_error": 176.18625183565024
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 608503.1037704919,
            "upper_bound": 609627.0341147543
          },
          "point_estimate": 609046.5883606558,
          "standard_error": 287.99647349527885
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 608216.844262295,
            "upper_bound": 609849.7868852459
          },
          "point_estimate": 608904.1830601093,
          "standard_error": 445.3715147380008
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224.3206166732652,
            "upper_bound": 1645.766987175256
          },
          "point_estimate": 1115.402493804227,
          "standard_error": 353.9886004950195
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 608224.4294795202,
            "upper_bound": 609054.851152766
          },
          "point_estimate": 608568.9364700873,
          "standard_error": 214.42218572799823
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 548.8642954348231,
            "upper_bound": 1183.217946957736
          },
          "point_estimate": 962.788820386592,
          "standard_error": 161.78722835243082
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1256208.5769166667,
            "upper_bound": 1258770.5953534483
          },
          "point_estimate": 1257240.579507389,
          "standard_error": 681.7476437011894
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1255933.871100164,
            "upper_bound": 1257723.8448275863
          },
          "point_estimate": 1256635.0982758622,
          "standard_error": 472.2240022722027
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248.8649955816152,
            "upper_bound": 2036.1806500577536
          },
          "point_estimate": 1209.7208754199644,
          "standard_error": 454.3543793288843
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1256233.8229628203,
            "upper_bound": 1257237.4869257214
          },
          "point_estimate": 1256713.8686968205,
          "standard_error": 254.18094190812116
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 559.9893074130572,
            "upper_bound": 3406.886080055353
          },
          "point_estimate": 2270.4315713176625,
          "standard_error": 921.1122644621896
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 826977.4307809117,
            "upper_bound": 828024.9007494588
          },
          "point_estimate": 827477.8810849567,
          "standard_error": 267.6999668095454
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 826699.0681818181,
            "upper_bound": 827970.6428571428
          },
          "point_estimate": 827577.6742424243,
          "standard_error": 361.6714546554148
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193.7560806511385,
            "upper_bound": 1609.1692245565644
          },
          "point_estimate": 908.6231872778284,
          "standard_error": 349.45253450856586
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 827097.066290979,
            "upper_bound": 828050.1564038825
          },
          "point_estimate": 827511.1563754427,
          "standard_error": 242.62181075173024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481.0602928213573,
            "upper_bound": 1179.8493536488006
          },
          "point_estimate": 892.8027686329297,
          "standard_error": 191.5715970045316
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1782365.1300198417,
            "upper_bound": 1800287.7966190474
          },
          "point_estimate": 1790742.6744444445,
          "standard_error": 4609.623701978085
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1779510.7142857143,
            "upper_bound": 1803526.646031746
          },
          "point_estimate": 1787416.6488095238,
          "standard_error": 4871.039781570063
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2019.177614152374,
            "upper_bound": 24465.398805652505
          },
          "point_estimate": 11172.496868315202,
          "standard_error": 5862.2957305980835
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1777895.8175666698,
            "upper_bound": 1791190.785890069
          },
          "point_estimate": 1783622.8591218304,
          "standard_error": 3394.0967846065137
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6121.521592822371,
            "upper_bound": 19345.342401511945
          },
          "point_estimate": 15345.378487021504,
          "standard_error": 3384.9728465651947
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321155.86892655253,
            "upper_bound": 321826.5831691033
          },
          "point_estimate": 321427.86958368146,
          "standard_error": 179.11318395810684
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321119.63801169593,
            "upper_bound": 321494.0493421053
          },
          "point_estimate": 321293.65109649126,
          "standard_error": 104.79808791662656
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.515398023379417,
            "upper_bound": 536.3799604773407
          },
          "point_estimate": 251.73409592563385,
          "standard_error": 130.97142841246247
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321067.65722284437,
            "upper_bound": 321429.7046990267
          },
          "point_estimate": 321250.4737069948,
          "standard_error": 93.6451020601294
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.78407005205693,
            "upper_bound": 896.2233952244586
          },
          "point_estimate": 598.1170173628215,
          "standard_error": 243.1959019200696
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 256254.17311703553,
            "upper_bound": 256490.58019102112
          },
          "point_estimate": 256381.93026156945,
          "standard_error": 61.042800979256256
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 256198.6237424547,
            "upper_bound": 256526.92253521128
          },
          "point_estimate": 256459.33192488263,
          "standard_error": 78.36324464961
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.061448163182014,
            "upper_bound": 340.09355575790374
          },
          "point_estimate": 125.59066093931824,
          "standard_error": 74.3238381050011
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 256308.52601179265,
            "upper_bound": 256497.65664367436
          },
          "point_estimate": 256425.70182915675,
          "standard_error": 48.26394652872828
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.41034732411929,
            "upper_bound": 265.31344840748824
          },
          "point_estimate": 204.14637702989592,
          "standard_error": 47.57014027053153
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126086.6004188087,
            "upper_bound": 126505.75636089484
          },
          "point_estimate": 126281.60476945678,
          "standard_error": 107.7701135495724
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125951.71764705882,
            "upper_bound": 126489.84004229144
          },
          "point_estimate": 126274.10596885812,
          "standard_error": 159.6519227355287
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.29920820452547,
            "upper_bound": 644.0896871666188
          },
          "point_estimate": 460.400131895474,
          "standard_error": 166.6789975124788
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125979.73351283753,
            "upper_bound": 126440.78208001293
          },
          "point_estimate": 126235.3943917674,
          "standard_error": 121.28482282795336
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.16912141133875,
            "upper_bound": 482.5414873888914
          },
          "point_estimate": 360.3676653005216,
          "standard_error": 82.59733140284061
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167394.09540825686,
            "upper_bound": 167537.82369375273
          },
          "point_estimate": 167459.80138488422,
          "standard_error": 36.91352144629927
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167380.8229357798,
            "upper_bound": 167513.0132699869
          },
          "point_estimate": 167448.49063455657,
          "standard_error": 36.205877851995645
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.530044459447076,
            "upper_bound": 178.83323776545726
          },
          "point_estimate": 88.58591517040219,
          "standard_error": 38.82758033146241
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167413.50207629165,
            "upper_bound": 167493.91332185344
          },
          "point_estimate": 167458.78762063626,
          "standard_error": 20.483311583171485
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.443963535424515,
            "upper_bound": 172.89703383755233
          },
          "point_estimate": 123.557600200948,
          "standard_error": 35.148666731618064
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60893.35043378932,
            "upper_bound": 61020.09766717183
          },
          "point_estimate": 60959.85013825742,
          "standard_error": 32.302774953925486
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60895.33994974874,
            "upper_bound": 61032.156895589054
          },
          "point_estimate": 60974.72053122756,
          "standard_error": 35.76505828841119
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.139168415669698,
            "upper_bound": 181.1697433162579
          },
          "point_estimate": 92.18913147331055,
          "standard_error": 37.438093188240614
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60951.82769718614,
            "upper_bound": 61025.28506467118
          },
          "point_estimate": 60992.91346777176,
          "standard_error": 18.499179911813098
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.9071798112148,
            "upper_bound": 144.41945752215383
          },
          "point_estimate": 107.91794279462434,
          "standard_error": 24.654756888946817
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243211.25009629628,
            "upper_bound": 243625.44586666665
          },
          "point_estimate": 243416.1279740741,
          "standard_error": 106.1556728321698
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243102.2637037037,
            "upper_bound": 243849.70266666668
          },
          "point_estimate": 243367.75666666665,
          "standard_error": 178.24197665048942
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.816115736937777,
            "upper_bound": 611.3748091459303
          },
          "point_estimate": 404.81307786865864,
          "standard_error": 148.2877515084693
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243095.7260178971,
            "upper_bound": 243520.2280385852
          },
          "point_estimate": 243308.79146320347,
          "standard_error": 109.82092179983673
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 215.621477018415,
            "upper_bound": 417.99643074153965
          },
          "point_estimate": 352.45503476213804,
          "standard_error": 51.14380189530837
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 687582.7801644204,
            "upper_bound": 688897.9674646226
          },
          "point_estimate": 688143.2420530099,
          "standard_error": 343.9466549066658
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 687485.5786163522,
            "upper_bound": 688603.0978436659
          },
          "point_estimate": 687798.9740566037,
          "standard_error": 244.2893736633872
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.3194855796493,
            "upper_bound": 1178.9019571835636
          },
          "point_estimate": 386.6730294559384,
          "standard_error": 297.26303588712744
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 687673.1829427694,
            "upper_bound": 688192.2248251007
          },
          "point_estimate": 687900.5391325655,
          "standard_error": 130.1221754844114
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232.36311847422937,
            "upper_bound": 1635.1548019635172
          },
          "point_estimate": 1147.9809785138928,
          "standard_error": 399.4639834176083
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1807.96445113962,
            "upper_bound": 1811.8360074158277
          },
          "point_estimate": 1809.7982794144864,
          "standard_error": 0.9960481647321672
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1806.6832807886703,
            "upper_bound": 1812.624771055397
          },
          "point_estimate": 1809.4377352694237,
          "standard_error": 1.4120428438178214
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.31174688432192293,
            "upper_bound": 5.922040515223589
          },
          "point_estimate": 3.564272863292401,
          "standard_error": 1.488737531072223
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1808.558848314687,
            "upper_bound": 1810.9808236693912
          },
          "point_estimate": 1809.8467457078775,
          "standard_error": 0.6028774370428145
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.61569527927812,
            "upper_bound": 4.09835386294879
          },
          "point_estimate": 3.316301308154378,
          "standard_error": 0.6394023292868843
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89219.74932837792,
            "upper_bound": 89566.35202794393
          },
          "point_estimate": 89376.07277680037,
          "standard_error": 89.18584931433045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89147.45484400657,
            "upper_bound": 89515.71435608726
          },
          "point_estimate": 89338.61689244663,
          "standard_error": 100.37390098230316
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.72788153845217,
            "upper_bound": 443.35916712880856
          },
          "point_estimate": 245.81184994631303,
          "standard_error": 94.04731219644312
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89230.50125201882,
            "upper_bound": 89516.3476555373
          },
          "point_estimate": 89343.91271191863,
          "standard_error": 73.36148175524154
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.5891083008435,
            "upper_bound": 409.0004226079744
          },
          "point_estimate": 296.13925106748,
          "standard_error": 80.90854141410826
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3104253.406435185,
            "upper_bound": 3111327.2770595243
          },
          "point_estimate": 3107572.362771164,
          "standard_error": 1811.581888942989
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3102829.441666667,
            "upper_bound": 3113922.4166666665
          },
          "point_estimate": 3105145.534722222,
          "standard_error": 2765.624455116988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 682.3820816350662,
            "upper_bound": 9644.83448432526
          },
          "point_estimate": 3976.92685578426,
          "standard_error": 2392.689080640728
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3103817.5698134876,
            "upper_bound": 3111044.066911765
          },
          "point_estimate": 3106783.953896104,
          "standard_error": 1840.016127007351
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2462.037322732373,
            "upper_bound": 7489.232232678065
          },
          "point_estimate": 6039.503167629809,
          "standard_error": 1187.4700613213088
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3099910.8248296124,
            "upper_bound": 3107208.9841666664
          },
          "point_estimate": 3103356.552642196,
          "standard_error": 1869.8139515083255
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3098124.479761905,
            "upper_bound": 3108183.25
          },
          "point_estimate": 3102840.6185185183,
          "standard_error": 2233.7956042586447
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1519.0815424751515,
            "upper_bound": 10884.334736764386
          },
          "point_estimate": 5300.024958960939,
          "standard_error": 2497.9608840278233
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3097854.144395761,
            "upper_bound": 3103863.422272048
          },
          "point_estimate": 3100215.2214285713,
          "standard_error": 1533.4892986698649
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2957.973270486879,
            "upper_bound": 7989.552478045613
          },
          "point_estimate": 6237.516930969885,
          "standard_error": 1312.4085414912324
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1410349.2248918272,
            "upper_bound": 1412542.5628418804
          },
          "point_estimate": 1411477.1049252138,
          "standard_error": 563.0946472567999
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1410302.5625,
            "upper_bound": 1413131.51875
          },
          "point_estimate": 1411516.860042735,
          "standard_error": 716.0454493536417
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 485.4873404193648,
            "upper_bound": 3313.7956325786613
          },
          "point_estimate": 1511.6342231630128,
          "standard_error": 696.0037717725625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1411255.7012944738,
            "upper_bound": 1412878.8067811655
          },
          "point_estimate": 1412006.5902097905,
          "standard_error": 408.29985643787614
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 985.9578049023372,
            "upper_bound": 2458.844628545774
          },
          "point_estimate": 1876.660044014749,
          "standard_error": 388.94464033765826
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 894757.7832217866,
            "upper_bound": 896602.3138104916
          },
          "point_estimate": 895601.0472251257,
          "standard_error": 473.4138201090966
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 894584.8447299651,
            "upper_bound": 896314.6036585366
          },
          "point_estimate": 895289.235501355,
          "standard_error": 519.9548200656108
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241.6665077827351,
            "upper_bound": 2263.253648233853
          },
          "point_estimate": 1322.582063104854,
          "standard_error": 521.1599649670702
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 894853.6005463764,
            "upper_bound": 896062.6385465629
          },
          "point_estimate": 895515.0791891036,
          "standard_error": 312.11747887853056
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 661.2863216364881,
            "upper_bound": 2213.6924115984866
          },
          "point_estimate": 1585.9961552250309,
          "standard_error": 447.4618189336283
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12747.257712923889,
            "upper_bound": 12776.825732197123
          },
          "point_estimate": 12762.144520595406,
          "standard_error": 7.575392064534042
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12740.583070359982,
            "upper_bound": 12782.698807854138
          },
          "point_estimate": 12762.98066018834,
          "standard_error": 9.494318838457234
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.535292366443231,
            "upper_bound": 47.08082337522734
          },
          "point_estimate": 21.860508605588,
          "standard_error": 10.449840475243109
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12739.29692589712,
            "upper_bound": 12782.979089377946
          },
          "point_estimate": 12762.48528715324,
          "standard_error": 11.09318678401863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.308309264642205,
            "upper_bound": 31.44109968475687
          },
          "point_estimate": 25.13560951573865,
          "standard_error": 4.436581840181593
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11338.591047539285,
            "upper_bound": 11355.875969569095
          },
          "point_estimate": 11348.821634267708,
          "standard_error": 4.619682166108275
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11346.28980990963,
            "upper_bound": 11357.525267476887
          },
          "point_estimate": 11353.134011980192,
          "standard_error": 3.0456335662062934
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.1335379022904184,
            "upper_bound": 13.514440855429598
          },
          "point_estimate": 6.800465493687545,
          "standard_error": 3.0823127776628407
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11333.27128221965,
            "upper_bound": 11357.259741993868
          },
          "point_estimate": 11348.844842225397,
          "standard_error": 6.457668034295108
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.843857781863193,
            "upper_bound": 23.09107984727966
          },
          "point_estimate": 15.41219095380215,
          "standard_error": 6.224845526506164
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1669511.1528246754,
            "upper_bound": 1678801.247777778
          },
          "point_estimate": 1674452.8828102455,
          "standard_error": 2378.649158894851
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1668494.2784090908,
            "upper_bound": 1681198.6363636365
          },
          "point_estimate": 1675919.9343434344,
          "standard_error": 3276.0539585022507
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 849.8161962762888,
            "upper_bound": 13101.105862408807
          },
          "point_estimate": 7378.114964921372,
          "standard_error": 3332.80716686435
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1666939.7923444975,
            "upper_bound": 1678785.0050249202
          },
          "point_estimate": 1673210.0873671782,
          "standard_error": 2976.6966235896452
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3666.590544437331,
            "upper_bound": 10042.703357150804
          },
          "point_estimate": 7934.878485698801,
          "standard_error": 1658.6292808381263
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 309.154666231808,
            "upper_bound": 309.66178283751685
          },
          "point_estimate": 309.402707166592,
          "standard_error": 0.1294951755959492
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 309.0819466305545,
            "upper_bound": 309.66614540349684
          },
          "point_estimate": 309.427274246577,
          "standard_error": 0.20532038465830843
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03169008000913728,
            "upper_bound": 0.7724689387256725
          },
          "point_estimate": 0.3919473493609482,
          "standard_error": 0.2094414151230618
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 309.14097151443957,
            "upper_bound": 309.6064250157712
          },
          "point_estimate": 309.3742197599057,
          "standard_error": 0.12189103175699424
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24471438925645836,
            "upper_bound": 0.5636594128576797
          },
          "point_estimate": 0.4329121520191019,
          "standard_error": 0.08398598221673013
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.48272198369465,
            "upper_bound": 117.63896066538538
          },
          "point_estimate": 117.55670894606077,
          "standard_error": 0.04003721450777456
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.43705164006305,
            "upper_bound": 117.64621309953836
          },
          "point_estimate": 117.54502259124676,
          "standard_error": 0.05683572053449864
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01999192584951562,
            "upper_bound": 0.22046138031627124
          },
          "point_estimate": 0.1493491249618326,
          "standard_error": 0.055232457891923575
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.49604717978872,
            "upper_bound": 117.66309904514866
          },
          "point_estimate": 117.58533529839984,
          "standard_error": 0.042343256352943066
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0674604246098261,
            "upper_bound": 0.1694089440629611
          },
          "point_estimate": 0.1337439440762672,
          "standard_error": 0.026291632685451715
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.768080432828903,
            "upper_bound": 23.821298556030335
          },
          "point_estimate": 23.794303323970485,
          "standard_error": 0.013641309013441473
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.754351937840312,
            "upper_bound": 23.835329908979336
          },
          "point_estimate": 23.791482228955623,
          "standard_error": 0.017706626290846607
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005773835614952093,
            "upper_bound": 0.08200790042211274
          },
          "point_estimate": 0.060028968939631375,
          "standard_error": 0.021745835155481827
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.772959932171393,
            "upper_bound": 23.818708858060617
          },
          "point_estimate": 23.794925771376978,
          "standard_error": 0.011570091748063924
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026234336367960312,
            "upper_bound": 0.056253680373109786
          },
          "point_estimate": 0.04552708260675398,
          "standard_error": 0.007633129874380305
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.578874359825715,
            "upper_bound": 18.608845260006976
          },
          "point_estimate": 18.594456187362773,
          "standard_error": 0.00763847427505211
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.580275666130333,
            "upper_bound": 18.61412053154768
          },
          "point_estimate": 18.59302919863086,
          "standard_error": 0.00917393232317125
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005485112607207865,
            "upper_bound": 0.04258526671126445
          },
          "point_estimate": 0.023487708980931037,
          "standard_error": 0.009177247623865237
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.586874281638337,
            "upper_bound": 18.616360161111704
          },
          "point_estimate": 18.602519936793097,
          "standard_error": 0.007558817076435685
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012515813910956911,
            "upper_bound": 0.03427560779517168
          },
          "point_estimate": 0.025492281268343596,
          "standard_error": 0.005793095559511188
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.98438410246086,
            "upper_bound": 23.015686836904884
          },
          "point_estimate": 23.00015790936252,
          "standard_error": 0.007998020805173669
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.97890608187334,
            "upper_bound": 23.025247255856776
          },
          "point_estimate": 23.003808243120524,
          "standard_error": 0.01111719952868672
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010946302444559649,
            "upper_bound": 0.04666227188725765
          },
          "point_estimate": 0.034352711664038284,
          "standard_error": 0.011523854452493914
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.974645967892947,
            "upper_bound": 23.005486046653832
          },
          "point_estimate": 22.9894780557848,
          "standard_error": 0.007943164888163419
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01576029829350301,
            "upper_bound": 0.0326896918743341
          },
          "point_estimate": 0.026677128648354018,
          "standard_error": 0.004303731056510122
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12041.241262375328,
            "upper_bound": 12073.028214850918
          },
          "point_estimate": 12056.192260892656,
          "standard_error": 8.141698166162355
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12038.833462147142,
            "upper_bound": 12073.952716131167
          },
          "point_estimate": 12050.821210113723,
          "standard_error": 8.2300309866226
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.250185403178165,
            "upper_bound": 44.93585855963187
          },
          "point_estimate": 19.524359107939524,
          "standard_error": 11.090558784589565
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12032.395526821016,
            "upper_bound": 12056.652520053118
          },
          "point_estimate": 12043.256833130434,
          "standard_error": 6.117258586571924
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.99111610392996,
            "upper_bound": 36.237660919209
          },
          "point_estimate": 27.036183343367313,
          "standard_error": 6.399085964239097
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.31162343553004,
            "upper_bound": 19.34878199004408
          },
          "point_estimate": 19.32917255546255,
          "standard_error": 0.009486856735994502
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.308329464864464,
            "upper_bound": 19.344934884762168
          },
          "point_estimate": 19.324931333569783,
          "standard_error": 0.01015978253870406
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002100808991151268,
            "upper_bound": 0.049434150364346854
          },
          "point_estimate": 0.02771267099815497,
          "standard_error": 0.011683748906037015
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.32670520772149,
            "upper_bound": 19.361154333520055
          },
          "point_estimate": 19.34311059481344,
          "standard_error": 0.008700742773006934
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013243259897274776,
            "upper_bound": 0.04371302872886461
          },
          "point_estimate": 0.03156112488546731,
          "standard_error": 0.008091126203483961
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.68170520222914,
            "upper_bound": 25.722928970023
          },
          "point_estimate": 25.70239073848836,
          "standard_error": 0.010550650213649985
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.683403571032493,
            "upper_bound": 25.725949356805863
          },
          "point_estimate": 25.699423191550235,
          "standard_error": 0.009677133310784515
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004379943892193248,
            "upper_bound": 0.06153511099235017
          },
          "point_estimate": 0.02686686669559304,
          "standard_error": 0.015203530312720485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.68049779865192,
            "upper_bound": 25.709201339051127
          },
          "point_estimate": 25.69292270270126,
          "standard_error": 0.007404121489100604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016058741026267566,
            "upper_bound": 0.04722088128489665
          },
          "point_estimate": 0.03508573233203067,
          "standard_error": 0.007970022342934608
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59686.153170143,
            "upper_bound": 60151.28129676636
          },
          "point_estimate": 59913.81543932834,
          "standard_error": 119.52554985335358
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59562.56,
            "upper_bound": 60248.060495867765
          },
          "point_estimate": 59878.27830578512,
          "standard_error": 191.74082923431965
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.8721898626676,
            "upper_bound": 669.1051049474524
          },
          "point_estimate": 474.8997865853678,
          "standard_error": 145.95939446166938
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59614.6580791148,
            "upper_bound": 60182.8040154025
          },
          "point_estimate": 59980.05231726951,
          "standard_error": 144.95245970119734
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236.40705829081185,
            "upper_bound": 498.6069683194432
          },
          "point_estimate": 398.02477021680534,
          "standard_error": 68.04421517791627
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.85015925792327,
            "upper_bound": 17.877499697152164
          },
          "point_estimate": 17.86440808919611,
          "standard_error": 0.006976225214488363
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.85000177650459,
            "upper_bound": 17.878036472048542
          },
          "point_estimate": 17.868228631719795,
          "standard_error": 0.007036984575640156
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003701205540300214,
            "upper_bound": 0.04005108240824313
          },
          "point_estimate": 0.01513190495761358,
          "standard_error": 0.009241052061597584
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.8595923629024,
            "upper_bound": 17.879392478272454
          },
          "point_estimate": 17.869849083952307,
          "standard_error": 0.004966667419431245
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00934362369858647,
            "upper_bound": 0.03109968874609991
          },
          "point_estimate": 0.02324714521076238,
          "standard_error": 0.005450784170132686
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.179565247662417,
            "upper_bound": 25.242731472918052
          },
          "point_estimate": 25.207348327452856,
          "standard_error": 0.016248376487651664
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.17443968048446,
            "upper_bound": 25.238096046328877
          },
          "point_estimate": 25.18161280020395,
          "standard_error": 0.016029370902401724
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002100708678539361,
            "upper_bound": 0.07511579530190969
          },
          "point_estimate": 0.026318580258320023,
          "standard_error": 0.01960673041159379
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.179802922499693,
            "upper_bound": 25.238864990108834
          },
          "point_estimate": 25.211653533649795,
          "standard_error": 0.01501560535277824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015435875918784514,
            "upper_bound": 0.0736459270911059
          },
          "point_estimate": 0.05412855530040888,
          "standard_error": 0.01572621897720813
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.600451438853305,
            "upper_bound": 53.737569580501365
          },
          "point_estimate": 53.65939849796585,
          "standard_error": 0.03573440577214969
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.5796076152208,
            "upper_bound": 53.700938307152875
          },
          "point_estimate": 53.61380206849825,
          "standard_error": 0.03126351714091769
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010946266554744635,
            "upper_bound": 0.145198452467992
          },
          "point_estimate": 0.05431650199153338,
          "standard_error": 0.03545025390446284
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.59665937361089,
            "upper_bound": 53.65893018842996
          },
          "point_estimate": 53.61937452154041,
          "standard_error": 0.015876732887492584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02871078745782667,
            "upper_bound": 0.17051078014478885
          },
          "point_estimate": 0.11899342775272498,
          "standard_error": 0.039887391261611456
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.5594025532614,
            "upper_bound": 93.69794846765964
          },
          "point_estimate": 93.61990613559011,
          "standard_error": 0.035932753026258084
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.53695082347484,
            "upper_bound": 93.67502512951364
          },
          "point_estimate": 93.58459671272824,
          "standard_error": 0.038068286951524664
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019096131455619763,
            "upper_bound": 0.16294622188094535
          },
          "point_estimate": 0.09009281507107288,
          "standard_error": 0.034382284970427134
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.5317117287508,
            "upper_bound": 93.63190383555988
          },
          "point_estimate": 93.56376766304392,
          "standard_error": 0.025776025645245896
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04549708197550452,
            "upper_bound": 0.1719338076325274
          },
          "point_estimate": 0.11979424893700392,
          "standard_error": 0.038469266897110235
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.92748657799271,
            "upper_bound": 14.959271816238642
          },
          "point_estimate": 14.941443983212944,
          "standard_error": 0.008271007380207617
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.923529230530336,
            "upper_bound": 14.954572837960356
          },
          "point_estimate": 14.933942136399237,
          "standard_error": 0.00626214145426995
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002363938960840838,
            "upper_bound": 0.03440827193464467
          },
          "point_estimate": 0.013125324909412685,
          "standard_error": 0.008324708140545476
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.924590782852835,
            "upper_bound": 14.93858714672576
          },
          "point_estimate": 14.930286901950472,
          "standard_error": 0.003600310836312914
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007107852013934016,
            "upper_bound": 0.03810701697847872
          },
          "point_estimate": 0.02752669237227564,
          "standard_error": 0.008552864887537003
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/bstr_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69218.38537865777,
            "upper_bound": 69296.64528174415
          },
          "point_estimate": 69259.62343715921,
          "standard_error": 20.109722078974144
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69204.58511450382,
            "upper_bound": 69306.1166110051
          },
          "point_estimate": 69284.56167302799,
          "standard_error": 29.5170590395086
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1893809330870282,
            "upper_bound": 109.80328881398422
          },
          "point_estimate": 44.10252409775164,
          "standard_error": 28.949681970316863
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69224.8900740923,
            "upper_bound": 69297.33559548145
          },
          "point_estimate": 69262.28362744125,
          "standard_error": 19.280214670138218
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.500459671161636,
            "upper_bound": 83.36316739446202
          },
          "point_estimate": 66.80136937391343,
          "standard_error": 12.61655139300514
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/bstr_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.085398522881473,
            "upper_bound": 29.117317874497864
          },
          "point_estimate": 29.100569268525646,
          "standard_error": 0.008166580533356801
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.07646933067717,
            "upper_bound": 29.117522833746623
          },
          "point_estimate": 29.09768748013282,
          "standard_error": 0.012286471543124624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004553073126875663,
            "upper_bound": 0.050225188790357714
          },
          "point_estimate": 0.030432961285092108,
          "standard_error": 0.010821248501637924
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.077827196736926,
            "upper_bound": 29.11256046411619
          },
          "point_estimate": 29.095816640932377,
          "standard_error": 0.00899219442709571
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014948899719427911,
            "upper_bound": 0.035770444682740564
          },
          "point_estimate": 0.027273874851824552,
          "standard_error": 0.005792265990031291
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/bstr_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 795944.7321419945,
            "upper_bound": 799541.4310973084
          },
          "point_estimate": 797311.603555038,
          "standard_error": 1006.4185183944976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 795681.8224637681,
            "upper_bound": 797085.6814829193
          },
          "point_estimate": 796423.3966183575,
          "standard_error": 424.2486939191694
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258.8094658399159,
            "upper_bound": 1791.961302425599
          },
          "point_estimate": 982.5259858175904,
          "standard_error": 459.1535167662639
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 796083.9793282804,
            "upper_bound": 796933.1926944691
          },
          "point_estimate": 796462.5360248447,
          "standard_error": 216.34578003545383
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 515.3068733286065,
            "upper_bound": 5144.339039445756
          },
          "point_estimate": 3355.2093492078434,
          "standard_error": 1594.387749353354
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/bstr_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1887.598624064928,
            "upper_bound": 1897.1379012607224
          },
          "point_estimate": 1891.0919984114607,
          "standard_error": 2.7646622458248222
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1887.2205816942496,
            "upper_bound": 1889.726103457239
          },
          "point_estimate": 1887.9751390694048,
          "standard_error": 0.8759929689060763
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07046878221636886,
            "upper_bound": 3.262691984359696
          },
          "point_estimate": 1.163793661861789,
          "standard_error": 1.1262413337221855
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1887.36127706891,
            "upper_bound": 1889.797456151752
          },
          "point_estimate": 1888.443102704418,
          "standard_error": 0.6319470479851733
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6940474125073639,
            "upper_bound": 14.205534913994132
          },
          "point_estimate": 9.249258080793403,
          "standard_error": 4.708496106033596
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.600629433718897,
            "upper_bound": 29.737998969696868
          },
          "point_estimate": 28.59808184898696,
          "standard_error": 0.5552188318063267
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.19757741820022,
            "upper_bound": 30.523065213545458
          },
          "point_estimate": 27.370960517192977,
          "standard_error": 0.9809741116352364
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02682042191501921,
            "upper_bound": 2.8268710391355283
          },
          "point_estimate": 0.2838781999179948,
          "standard_error": 0.9289830547355128
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.364466290792553,
            "upper_bound": 29.720501351197836
          },
          "point_estimate": 28.55646441510995,
          "standard_error": 0.6291032331092922
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.889604762230948,
            "upper_bound": 2.1857733220094695
          },
          "point_estimate": 1.8495276586758391,
          "standard_error": 0.3286997455421246
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.752961775644096,
            "upper_bound": 22.779001105645644
          },
          "point_estimate": 22.7675405378986,
          "standard_error": 0.006723335817662535
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.75516919871942,
            "upper_bound": 22.780798627875832
          },
          "point_estimate": 22.775418735202138,
          "standard_error": 0.006420465981796705
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0029536912042400216,
            "upper_bound": 0.03002237416381978
          },
          "point_estimate": 0.009470143234982518,
          "standard_error": 0.007461429327801264
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.764873929871577,
            "upper_bound": 22.77983278842114
          },
          "point_estimate": 22.773304294369044,
          "standard_error": 0.0037935244922120345
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007491110698971113,
            "upper_bound": 0.03168748903208361
          },
          "point_estimate": 0.02240302186717103,
          "standard_error": 0.00699151789707842
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.749979237843966,
            "upper_bound": 22.8043537497272
          },
          "point_estimate": 22.773440378475406,
          "standard_error": 0.014122484960947588
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.746446418043817,
            "upper_bound": 22.78435951905364
          },
          "point_estimate": 22.76540244744973,
          "standard_error": 0.010488712310853849
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008701155033779101,
            "upper_bound": 0.05311474701283484
          },
          "point_estimate": 0.0254439396872094,
          "standard_error": 0.011304097506077209
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.757028549872967,
            "upper_bound": 22.81430430851749
          },
          "point_estimate": 22.77822822531517,
          "standard_error": 0.01482806118706693
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014195416026013695,
            "upper_bound": 0.06888202540286059
          },
          "point_estimate": 0.04714978730286144,
          "standard_error": 0.016721417682150092
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.96713839760791,
            "upper_bound": 46.03298010327309
          },
          "point_estimate": 46.00172394835182,
          "standard_error": 0.01682791974741628
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.97474178376609,
            "upper_bound": 46.0396767699893
          },
          "point_estimate": 45.9991205391362,
          "standard_error": 0.013927039612851798
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0026441579195557664,
            "upper_bound": 0.09422887281514608
          },
          "point_estimate": 0.03443153838514648,
          "standard_error": 0.026981795742654465
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.98081842171218,
            "upper_bound": 46.021448400434544
          },
          "point_estimate": 46.000539006754806,
          "standard_error": 0.010189486654098414
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024619227077444687,
            "upper_bound": 0.07639894386515228
          },
          "point_estimate": 0.056061451978834054,
          "standard_error": 0.013720525318467318
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.017879288206124,
            "upper_bound": 21.041539166351647
          },
          "point_estimate": 21.02956841971609,
          "standard_error": 0.006055566300830886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.014460686652736,
            "upper_bound": 21.04462161347837
          },
          "point_estimate": 21.029752857165533,
          "standard_error": 0.00622923257510364
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011556962871368927,
            "upper_bound": 0.03795963722192973
          },
          "point_estimate": 0.017345840343437283,
          "standard_error": 0.010721072739282142
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.01663926918291,
            "upper_bound": 21.050408087833663
          },
          "point_estimate": 21.0324542846209,
          "standard_error": 0.008769725438787141
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01077124475112669,
            "upper_bound": 0.02573222570892414
          },
          "point_estimate": 0.02018528304100314,
          "standard_error": 0.0038717345311552022
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.445019669654677,
            "upper_bound": 24.52458712140145
          },
          "point_estimate": 24.48060674762901,
          "standard_error": 0.02052456414108478
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.443163843175544,
            "upper_bound": 24.515810945113053
          },
          "point_estimate": 24.45547334475252,
          "standard_error": 0.018665878441811667
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005599405272955797,
            "upper_bound": 0.10106659922066016
          },
          "point_estimate": 0.028800177864880545,
          "standard_error": 0.025421914422664955
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.449273696124887,
            "upper_bound": 24.47961097310753
          },
          "point_estimate": 24.46321303601755,
          "standard_error": 0.007811115036954199
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021690416976911604,
            "upper_bound": 0.0942782451203255
          },
          "point_estimate": 0.06851230018554838,
          "standard_error": 0.0197018492789852
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.999875847355469,
            "upper_bound": 11.01324860116078
          },
          "point_estimate": 11.00661338967962,
          "standard_error": 0.0034318598327874403
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.993944782223986,
            "upper_bound": 11.016710144458898
          },
          "point_estimate": 11.009520400122495,
          "standard_error": 0.00640760662404775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016264646469013968,
            "upper_bound": 0.01859732196840587
          },
          "point_estimate": 0.016332542011832887,
          "standard_error": 0.00472121003100584
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.003904617114252,
            "upper_bound": 11.01701274382492
          },
          "point_estimate": 11.011461228740478,
          "standard_error": 0.0033025531824685673
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00760310728072117,
            "upper_bound": 0.01328387992166983
          },
          "point_estimate": 0.011467619074936678,
          "standard_error": 0.0014736009933195157
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.32160307463787,
            "upper_bound": 19.36488005064086
          },
          "point_estimate": 19.341531810206163,
          "standard_error": 0.011110479511566572
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.31309124155275,
            "upper_bound": 19.372990456213465
          },
          "point_estimate": 19.333061797231323,
          "standard_error": 0.01350660125505347
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0064143864006961674,
            "upper_bound": 0.05508869512367118
          },
          "point_estimate": 0.02584496529663401,
          "standard_error": 0.013051490239948936
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.32170517352279,
            "upper_bound": 19.34768091762262
          },
          "point_estimate": 19.335301347665293,
          "standard_error": 0.006619809769446863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013992827405606202,
            "upper_bound": 0.046437786909246416
          },
          "point_estimate": 0.03698604653989244,
          "standard_error": 0.00846003141631067
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.679365876330667,
            "upper_bound": 25.74062403815573
          },
          "point_estimate": 25.707343586008346,
          "standard_error": 0.015755079051869195
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.66984995515252,
            "upper_bound": 25.733988271382596
          },
          "point_estimate": 25.696385022117695,
          "standard_error": 0.017906250928597718
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012964455861676842,
            "upper_bound": 0.08056663806416361
          },
          "point_estimate": 0.04552490476604159,
          "standard_error": 0.01732257689570485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.67540918072731,
            "upper_bound": 25.712594128206536
          },
          "point_estimate": 25.693002761658395,
          "standard_error": 0.009558347277336402
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022706217581605695,
            "upper_bound": 0.07088598165049874
          },
          "point_estimate": 0.05249556863923988,
          "standard_error": 0.013387295899514784
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.22366177472223,
            "upper_bound": 24.27890935589255
          },
          "point_estimate": 24.25009746100485,
          "standard_error": 0.014188824158970869
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.22100760724621,
            "upper_bound": 24.295044809243127
          },
          "point_estimate": 24.232981580639823,
          "standard_error": 0.017666034003170662
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007341918901619406,
            "upper_bound": 0.07890790261749817
          },
          "point_estimate": 0.023374889504117497,
          "standard_error": 0.02272517158811072
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.223123261049036,
            "upper_bound": 24.276455022301864
          },
          "point_estimate": 24.24547311949883,
          "standard_error": 0.013684273816617694
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02199248790967436,
            "upper_bound": 0.0594457494258146
          },
          "point_estimate": 0.047172375795276306,
          "standard_error": 0.009149216892036387
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.956280325837604,
            "upper_bound": 24.989422538321083
          },
          "point_estimate": 24.97259141732585,
          "standard_error": 0.008535314888861707
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.942230515660533,
            "upper_bound": 25.004460772151123
          },
          "point_estimate": 24.969140348530217,
          "standard_error": 0.015431949515469367
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024816458127831674,
            "upper_bound": 0.04532909985075981
          },
          "point_estimate": 0.04019982270696016,
          "standard_error": 0.011447233232200966
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.946158026675363,
            "upper_bound": 24.98992477425316
          },
          "point_estimate": 24.96538445998088,
          "standard_error": 0.011486405868120615
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01790195268890486,
            "upper_bound": 0.03333765033851297
          },
          "point_estimate": 0.028592439527649494,
          "standard_error": 0.003895259927900164
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.43090398398451,
            "upper_bound": 25.45210111946369
          },
          "point_estimate": 25.441563565604216,
          "standard_error": 0.005426315743485024
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.42322177453232,
            "upper_bound": 25.459497871625405
          },
          "point_estimate": 25.44137462193346,
          "standard_error": 0.009048561454237849
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003904620043822093,
            "upper_bound": 0.030774149560072232
          },
          "point_estimate": 0.026891470297685943,
          "standard_error": 0.007196711105628273
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.42716370627742,
            "upper_bound": 25.45130744594155
          },
          "point_estimate": 25.437759678430677,
          "standard_error": 0.006136007856458031
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011244518282163694,
            "upper_bound": 0.021303231551140736
          },
          "point_estimate": 0.018062085557029563,
          "standard_error": 0.0025264433801341145
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2037986.1308134925,
            "upper_bound": 2042607.0917548505
          },
          "point_estimate": 2039905.4788381837,
          "standard_error": 1224.9290265504585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2037935.5916666668,
            "upper_bound": 2040915.0297619049
          },
          "point_estimate": 2038254.8993055555,
          "standard_error": 826.5112481412472
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.440380910994,
            "upper_bound": 4243.0157996714115
          },
          "point_estimate": 1470.812127221165,
          "standard_error": 1145.3074970850005
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2038143.9214413543,
            "upper_bound": 2040911.4234107612
          },
          "point_estimate": 2039445.0787878789,
          "standard_error": 774.5070458355124
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 963.7665600377632,
            "upper_bound": 6044.931984324832
          },
          "point_estimate": 4091.050262926936,
          "standard_error": 1564.3815511439525
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3101777.1095746523,
            "upper_bound": 3107364.6382899308
          },
          "point_estimate": 3104387.657106482,
          "standard_error": 1434.3399766648624
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3100954.4583333335,
            "upper_bound": 3107187.717013889
          },
          "point_estimate": 3103486.787962963,
          "standard_error": 1595.166794380566
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1257.6957351714832,
            "upper_bound": 7635.608822496238
          },
          "point_estimate": 4257.83150357497,
          "standard_error": 1697.953905485158
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3101536.217997442,
            "upper_bound": 3104730.346434708
          },
          "point_estimate": 3103206.8824675325,
          "standard_error": 800.5488301779002
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2133.9615286595663,
            "upper_bound": 6238.073676055577
          },
          "point_estimate": 4762.530372701662,
          "standard_error": 1080.6858129245588
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2631657.0506746033,
            "upper_bound": 2636146.3009126983
          },
          "point_estimate": 2633822.2098015873,
          "standard_error": 1143.956156350646
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2630818.475,
            "upper_bound": 2636508.8571428573
          },
          "point_estimate": 2633484.767460318,
          "standard_error": 1329.82545433244
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1065.704039413743,
            "upper_bound": 6306.41136970522
          },
          "point_estimate": 3417.534139326854,
          "standard_error": 1433.446179333159
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2630213.5347984484,
            "upper_bound": 2634882.377508845
          },
          "point_estimate": 2632320.03135436,
          "standard_error": 1202.219096539734
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1886.965362710116,
            "upper_bound": 5054.320518000448
          },
          "point_estimate": 3803.6615653114823,
          "standard_error": 828.1071246081676
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 379534.00759259256,
            "upper_bound": 381368.6345152323
          },
          "point_estimate": 380475.4815286045,
          "standard_error": 471.0154268791473
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378883.0052083333,
            "upper_bound": 381826.57523148146
          },
          "point_estimate": 381090.90277777775,
          "standard_error": 934.6412303576026
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.7853302068169,
            "upper_bound": 2433.0855505540103
          },
          "point_estimate": 1466.773347657494,
          "standard_error": 643.8762389939069
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 380315.17686541146,
            "upper_bound": 381845.8614869157
          },
          "point_estimate": 381383.7893127706,
          "standard_error": 397.3272001074859
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 985.2620896107984,
            "upper_bound": 1779.9538419179137
          },
          "point_estimate": 1565.6461596510114,
          "standard_error": 204.31561283161628
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 565042.2278656685,
            "upper_bound": 565867.7349869793
          },
          "point_estimate": 565468.6712543403,
          "standard_error": 211.90326625113065
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 564884.5361328125,
            "upper_bound": 566128.2953125
          },
          "point_estimate": 565445.6450520833,
          "standard_error": 332.05371917563207
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.85547367353064,
            "upper_bound": 1262.897190079093
          },
          "point_estimate": 908.2609494481172,
          "standard_error": 299.8372310705008
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 565059.3374532914,
            "upper_bound": 565908.271302663
          },
          "point_estimate": 565476.1277191559,
          "standard_error": 215.48351992995097
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 417.93326805889296,
            "upper_bound": 865.6984844259777
          },
          "point_estimate": 702.7824558570203,
          "standard_error": 116.52097308337817
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1257101.5998081898,
            "upper_bound": 1259221.6851532566
          },
          "point_estimate": 1258174.3089176244,
          "standard_error": 543.9757308941635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1256628.1206896552,
            "upper_bound": 1259839.6077586208
          },
          "point_estimate": 1258199.4086206898,
          "standard_error": 802.433665883821
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 416.0931027279,
            "upper_bound": 3019.0038412296767
          },
          "point_estimate": 2380.675321958738,
          "standard_error": 702.5548717497545
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1256817.712368788,
            "upper_bound": 1259016.718693285
          },
          "point_estimate": 1258033.1844155844,
          "standard_error": 551.3803439677071
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1031.2251499783465,
            "upper_bound": 2227.031947318956
          },
          "point_estimate": 1810.9183968618752,
          "standard_error": 299.8698224078827
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 830047.5537327516,
            "upper_bound": 831018.908146645
          },
          "point_estimate": 830564.9823854618,
          "standard_error": 249.4505310173593
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 830152.0359848485,
            "upper_bound": 831207.5806502525
          },
          "point_estimate": 830666.9477272728,
          "standard_error": 332.4424974141152
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.28329531474172,
            "upper_bound": 1357.329395069227
          },
          "point_estimate": 659.5208794274712,
          "standard_error": 311.0515906377744
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 830343.4541322314,
            "upper_bound": 831133.5545743977
          },
          "point_estimate": 830742.8198937427,
          "standard_error": 205.716202937332
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406.2662274068001,
            "upper_bound": 1130.0614310564504
          },
          "point_estimate": 831.7531041444659,
          "standard_error": 207.74296218254423
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1792831.853038549,
            "upper_bound": 1806507.8921944443
          },
          "point_estimate": 1799346.3742970522,
          "standard_error": 3513.998789957562
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1788979.642857143,
            "upper_bound": 1810172.0476190476
          },
          "point_estimate": 1796110.641723356,
          "standard_error": 5594.781614024712
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1723.681319398564,
            "upper_bound": 18686.368333964827
          },
          "point_estimate": 10979.110024129348,
          "standard_error": 4612.93919283857
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1791351.566652848,
            "upper_bound": 1807182.543092881
          },
          "point_estimate": 1799159.6521954236,
          "standard_error": 4226.234514280169
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6376.296578172474,
            "upper_bound": 14298.801055374635
          },
          "point_estimate": 11674.793311817706,
          "standard_error": 2030.2590752937408
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 303084.23873586307,
            "upper_bound": 303676.44818427577
          },
          "point_estimate": 303343.436984127,
          "standard_error": 153.33635785945867
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 302985.82430555555,
            "upper_bound": 303506.1329861111
          },
          "point_estimate": 303240.7572916667,
          "standard_error": 132.55151655956772
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.72509715708699,
            "upper_bound": 651.2114467720382
          },
          "point_estimate": 323.64642633740505,
          "standard_error": 145.34084019034108
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 303153.46684539766,
            "upper_bound": 303481.5814386642
          },
          "point_estimate": 303296.61062770564,
          "standard_error": 85.92534810164588
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 185.14160763447143,
            "upper_bound": 734.6623742690002
          },
          "point_estimate": 510.97110447151255,
          "standard_error": 165.48050466007854
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253045.4167103588,
            "upper_bound": 253439.8470023148
          },
          "point_estimate": 253235.38671764775,
          "standard_error": 100.89510636735828
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253045.31577380953,
            "upper_bound": 253431.97699652775
          },
          "point_estimate": 253177.26871141975,
          "standard_error": 100.72880366365858
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.9973453711135,
            "upper_bound": 555.5365592344896
          },
          "point_estimate": 289.8936966183974,
          "standard_error": 126.18442293086808
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253049.89255213953,
            "upper_bound": 253374.9693872243
          },
          "point_estimate": 253193.0204184704,
          "standard_error": 84.88796386414995
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150.9933541671176,
            "upper_bound": 458.16094571307013
          },
          "point_estimate": 336.497730848994,
          "standard_error": 80.34628541145877
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125651.54968493714,
            "upper_bound": 125982.14026331015
          },
          "point_estimate": 125783.0429038525,
          "standard_error": 88.72604529083476
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125620.625,
            "upper_bound": 125841.30381944444
          },
          "point_estimate": 125671.55515873016,
          "standard_error": 56.5150145735556
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.147641974187035,
            "upper_bound": 256.6330791243857
          },
          "point_estimate": 119.12579250314596,
          "standard_error": 64.60507683311262
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125665.04549862364,
            "upper_bound": 125784.65770731286
          },
          "point_estimate": 125710.69037698412,
          "standard_error": 31.85315730174268
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.27130359610085,
            "upper_bound": 446.1981119684283
          },
          "point_estimate": 295.94683877212174,
          "standard_error": 123.84871842449544
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164150.86406711914,
            "upper_bound": 164562.56174954213
          },
          "point_estimate": 164333.01297098328,
          "standard_error": 106.72136587099996
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164093.2786037492,
            "upper_bound": 164495.31146304676
          },
          "point_estimate": 164211.435520362,
          "standard_error": 105.31075209971937
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.16093122122675,
            "upper_bound": 462.32651434108266
          },
          "point_estimate": 257.48853784947397,
          "standard_error": 108.0438540910534
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164071.30548649584,
            "upper_bound": 164368.82269958715
          },
          "point_estimate": 164189.4537815126,
          "standard_error": 74.9943230637938
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131.6109918047478,
            "upper_bound": 504.8723500827303
          },
          "point_estimate": 356.2533399374707,
          "standard_error": 110.22484171440652
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60718.73247481455,
            "upper_bound": 60976.01802712969
          },
          "point_estimate": 60852.90653525564,
          "standard_error": 65.90389703173165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60685.677345058626,
            "upper_bound": 61024.28475711893
          },
          "point_estimate": 60902.343104410946,
          "standard_error": 90.62153177967508
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.26853879680624,
            "upper_bound": 357.9164499271145
          },
          "point_estimate": 185.1778956404053,
          "standard_error": 83.90890031966289
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60827.85705408764,
            "upper_bound": 60997.03848494172
          },
          "point_estimate": 60931.04692727708,
          "standard_error": 43.60034541853411
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.60007341216696,
            "upper_bound": 276.1836270772699
          },
          "point_estimate": 220.4159369705729,
          "standard_error": 41.57976232140947
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243258.94154484128,
            "upper_bound": 243922.62268511904
          },
          "point_estimate": 243569.7183232804,
          "standard_error": 171.66068079494923
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243172.72296296296,
            "upper_bound": 244081.69333333333
          },
          "point_estimate": 243268.68416666667,
          "standard_error": 253.7941365812145
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.40634021162066,
            "upper_bound": 907.3172958918726
          },
          "point_estimate": 308.0660714974129,
          "standard_error": 251.1211067188545
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243094.38302254723,
            "upper_bound": 243463.6593741497
          },
          "point_estimate": 243224.2241904762,
          "standard_error": 95.31111083107056
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240.725259439809,
            "upper_bound": 706.0317261535921
          },
          "point_estimate": 572.3658964226667,
          "standard_error": 113.0939582441547
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1012823.3845756172,
            "upper_bound": 1015807.3528717208
          },
          "point_estimate": 1014069.2597872576,
          "standard_error": 785.1545413374172
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1012135.1574074074,
            "upper_bound": 1014508.075462963
          },
          "point_estimate": 1013751.843998016,
          "standard_error": 717.534941895328
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.990938392001535,
            "upper_bound": 2962.3097925936977
          },
          "point_estimate": 1659.6013612769598,
          "standard_error": 726.8883090117548
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1012744.2023035232,
            "upper_bound": 1014088.2174523008
          },
          "point_estimate": 1013479.7683982684,
          "standard_error": 348.24644728717334
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 839.9450668505154,
            "upper_bound": 3889.2071624695222
          },
          "point_estimate": 2612.6269530564473,
          "standard_error": 992.8197529048896
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2070.723459752665,
            "upper_bound": 2074.0899721036153
          },
          "point_estimate": 2072.3843998165544,
          "standard_error": 0.8605784668628754
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2069.754461998292,
            "upper_bound": 2074.8937944776544
          },
          "point_estimate": 2072.6027384002277,
          "standard_error": 1.5554146031786302
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2982503277537534,
            "upper_bound": 4.624817209097252
          },
          "point_estimate": 4.074231573270098,
          "standard_error": 1.1227158105388622
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2069.6603745917905,
            "upper_bound": 2073.3279943646166
          },
          "point_estimate": 2070.974567487736,
          "standard_error": 0.9270236864824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8361456304091583,
            "upper_bound": 3.387982259961772
          },
          "point_estimate": 2.867102041779043,
          "standard_error": 0.3965020019701187
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1314808.4261111114,
            "upper_bound": 1317032.611721939
          },
          "point_estimate": 1316029.7566822565,
          "standard_error": 575.2471341436018
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1315075.7833333332,
            "upper_bound": 1317099.8505952382
          },
          "point_estimate": 1316765.3194444445,
          "standard_error": 512.5845924926687
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.57884911953647,
            "upper_bound": 2785.13465055391
          },
          "point_estimate": 653.065915429543,
          "standard_error": 734.8719475161733
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1315886.6493147532,
            "upper_bound": 1317413.414551027
          },
          "point_estimate": 1316888.797680891,
          "standard_error": 393.737141989153
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 542.1025694421957,
            "upper_bound": 2550.9477903835564
          },
          "point_estimate": 1913.9259100863944,
          "standard_error": 524.4964134678016
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1513562.103009259,
            "upper_bound": 1516671.3021990738
          },
          "point_estimate": 1514856.220949074,
          "standard_error": 815.2201583847708
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1513108.1712962964,
            "upper_bound": 1515137.7569444445
          },
          "point_estimate": 1514583.6145833333,
          "standard_error": 546.6833196773721
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 209.73298516566535,
            "upper_bound": 2827.5206345238244
          },
          "point_estimate": 876.1831229864026,
          "standard_error": 668.4074738859548
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1512731.5660743315,
            "upper_bound": 1514832.4212952226
          },
          "point_estimate": 1513621.929112554,
          "standard_error": 542.7535066337064
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 716.4250529693915,
            "upper_bound": 4029.407917864845
          },
          "point_estimate": 2706.428759791206,
          "standard_error": 1040.4393881563556
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415503.8665938645,
            "upper_bound": 1417395.5336570514
          },
          "point_estimate": 1416509.4144001831,
          "standard_error": 486.9125967904889
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415392.4256410256,
            "upper_bound": 1417723.241826923
          },
          "point_estimate": 1416760.4743589745,
          "standard_error": 486.84139430573606
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265.45192221054026,
            "upper_bound": 2753.446655731715
          },
          "point_estimate": 946.8668178052592,
          "standard_error": 623.0222654983415
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1416280.0233657127,
            "upper_bound": 1417694.429415568
          },
          "point_estimate": 1417151.813286713,
          "standard_error": 361.5814489160078
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 657.8178010901687,
            "upper_bound": 2093.5434229602292
          },
          "point_estimate": 1618.0186235495487,
          "standard_error": 370.0072868366884
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 753007.8523056367,
            "upper_bound": 754631.8739475218
          },
          "point_estimate": 753807.1277704891,
          "standard_error": 415.9100924285452
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752502.693877551,
            "upper_bound": 754707.737244898
          },
          "point_estimate": 754094.8755102041,
          "standard_error": 583.0501839678333
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.09369589840232,
            "upper_bound": 2430.662142561438
          },
          "point_estimate": 1523.7939923350011,
          "standard_error": 575.1578012150079
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752699.1818655541,
            "upper_bound": 754663.9380672572
          },
          "point_estimate": 753717.7674529552,
          "standard_error": 513.4724960867256
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 803.6833778829772,
            "upper_bound": 1767.168826029978
          },
          "point_estimate": 1389.1373058841189,
          "standard_error": 251.51508443342257
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266172.2244160584,
            "upper_bound": 266948.0829301356
          },
          "point_estimate": 266492.7540534121,
          "standard_error": 205.6617754894061
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266102.8010948905,
            "upper_bound": 266656.5715444329
          },
          "point_estimate": 266227.11143552314,
          "standard_error": 139.33319204671412
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.31215126104324,
            "upper_bound": 660.4939766924291
          },
          "point_estimate": 208.01914729235344,
          "standard_error": 164.50776997580385
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266082.69797812554,
            "upper_bound": 266550.5360213185
          },
          "point_estimate": 266268.2042658072,
          "standard_error": 120.92926094626868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120.85552766403102,
            "upper_bound": 1008.5242721040707
          },
          "point_estimate": 685.0717874741514,
          "standard_error": 263.08437665403056
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477465.4784468795,
            "upper_bound": 478336.1752597402
          },
          "point_estimate": 477881.57825242216,
          "standard_error": 223.07065570579576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477312.926948052,
            "upper_bound": 478455.19144248613
          },
          "point_estimate": 477761.0571428571,
          "standard_error": 276.20112403625757
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161.65966319058134,
            "upper_bound": 1219.044791669326
          },
          "point_estimate": 579.3547413053744,
          "standard_error": 279.19866504674155
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477641.4534293005,
            "upper_bound": 478512.16700532025
          },
          "point_estimate": 478019.3357733176,
          "standard_error": 220.23515299732264
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 365.8518296590224,
            "upper_bound": 951.3225287900872
          },
          "point_estimate": 741.6572753366321,
          "standard_error": 148.93674531191877
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240933.73343431356,
            "upper_bound": 241292.7796780393
          },
          "point_estimate": 241114.6498278671,
          "standard_error": 92.18534106000044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240807.0529801325,
            "upper_bound": 241425.6539735099
          },
          "point_estimate": 241159.24576894776,
          "standard_error": 186.73760405339073
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.526035704372966,
            "upper_bound": 477.9015314990005
          },
          "point_estimate": 431.5538011960031,
          "standard_error": 126.12226447318146
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240995.47079324748,
            "upper_bound": 241383.90958136632
          },
          "point_estimate": 241203.03552077065,
          "standard_error": 99.84696347501652
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.4751696725952,
            "upper_bound": 350.20625856582217
          },
          "point_estimate": 307.04544050434464,
          "standard_error": 37.435592716479995
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 691787.0783061732,
            "upper_bound": 693272.2716909067
          },
          "point_estimate": 692430.1026422582,
          "standard_error": 385.82505026263584
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 691571.1145702306,
            "upper_bound": 692874.8829177897
          },
          "point_estimate": 692163.6493710692,
          "standard_error": 378.5887205014061
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207.80721109063504,
            "upper_bound": 1594.8577458994075
          },
          "point_estimate": 819.7822090308763,
          "standard_error": 342.34970035087946
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 691595.4936420511,
            "upper_bound": 692593.7616480554
          },
          "point_estimate": 692015.0252879197,
          "standard_error": 255.12200046057376
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 468.8614895140622,
            "upper_bound": 1851.6329848629528
          },
          "point_estimate": 1283.3931652205345,
          "standard_error": 422.7197489398801
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2298.894997348184,
            "upper_bound": 2304.134968422051
          },
          "point_estimate": 2301.465127153213,
          "standard_error": 1.3489496170280215
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2297.391285244658,
            "upper_bound": 2305.8156319804443
          },
          "point_estimate": 2300.8809755236653,
          "standard_error": 2.403834908043236
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5595151997961841,
            "upper_bound": 7.350768500171868
          },
          "point_estimate": 5.282463418514154,
          "standard_error": 1.734856969465829
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2297.816344975489,
            "upper_bound": 2303.9916552313625
          },
          "point_estimate": 2301.265018481329,
          "standard_error": 1.6144075099489277
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8096088708094,
            "upper_bound": 5.351575035808219
          },
          "point_estimate": 4.480695156907672,
          "standard_error": 0.6491214260316401
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.7589258333979,
            "upper_bound": 704.8508149416872
          },
          "point_estimate": 704.3213051446537,
          "standard_error": 0.27983440885251426
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.6003302299049,
            "upper_bound": 705.1346366439751
          },
          "point_estimate": 704.3426200681394,
          "standard_error": 0.3771097619577831
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.20953790647370535,
            "upper_bound": 1.6135815178709243
          },
          "point_estimate": 0.9953297664685162,
          "standard_error": 0.355424485060819
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.6060144490275,
            "upper_bound": 704.6785560126849
          },
          "point_estimate": 704.0837085917902,
          "standard_error": 0.2720165588241272
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5121228911074392,
            "upper_bound": 1.1810976518000271
          },
          "point_estimate": 0.9346125580378524,
          "standard_error": 0.17097239459130897
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.54926210089712,
            "upper_bound": 140.70884423464793
          },
          "point_estimate": 140.6180419893098,
          "standard_error": 0.04146019437145337
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.54738834764154,
            "upper_bound": 140.65365958222137
          },
          "point_estimate": 140.58817599801569,
          "standard_error": 0.028437159571767047
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0027124782387388147,
            "upper_bound": 0.16618199126206357
          },
          "point_estimate": 0.06347465284245028,
          "standard_error": 0.041204564498606815
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.56096047193435,
            "upper_bound": 140.6608355749243
          },
          "point_estimate": 140.61922818533833,
          "standard_error": 0.026106361766671308
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04085702332172503,
            "upper_bound": 0.20136315531261
          },
          "point_estimate": 0.13846571392820348,
          "standard_error": 0.04824567490885969
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.95085147752417,
            "upper_bound": 52.02733510900125
          },
          "point_estimate": 51.9868711078701,
          "standard_error": 0.019625127408834785
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.94384362128649,
            "upper_bound": 52.02524418036008
          },
          "point_estimate": 51.977826040588795,
          "standard_error": 0.022308160375635
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011346584259117404,
            "upper_bound": 0.1077322164632994
          },
          "point_estimate": 0.050112087743527545,
          "standard_error": 0.024172261299979073
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.95192512770825,
            "upper_bound": 52.01694206283893
          },
          "point_estimate": 51.98117607626778,
          "standard_error": 0.01643184212775578
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029104111041207414,
            "upper_bound": 0.08695209278543549
          },
          "point_estimate": 0.06522556238119565,
          "standard_error": 0.015332347505780016
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.48309629117854,
            "upper_bound": 80.58321295252364
          },
          "point_estimate": 80.53149708254001,
          "standard_error": 0.025779262223830892
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.45346663336376,
            "upper_bound": 80.62021743091253
          },
          "point_estimate": 80.50366694812041,
          "standard_error": 0.04542838212872587
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011713894839246465,
            "upper_bound": 0.14950823080714076
          },
          "point_estimate": 0.09204438834067429,
          "standard_error": 0.03432495068894857
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.47648112486547,
            "upper_bound": 80.56359456053906
          },
          "point_estimate": 80.51142391702146,
          "standard_error": 0.02208025649235177
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04931752691031066,
            "upper_bound": 0.10112090122159571
          },
          "point_estimate": 0.08571859175193652,
          "standard_error": 0.01284968206163751
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287259.7322132389,
            "upper_bound": 287852.36371109856
          },
          "point_estimate": 287527.8019350706,
          "standard_error": 152.7652563343936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287160.66841644794,
            "upper_bound": 287777.24015748035
          },
          "point_estimate": 287395.7410761155,
          "standard_error": 173.43869660771853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.75720501760166,
            "upper_bound": 759.2756151226336
          },
          "point_estimate": 410.9000533349879,
          "standard_error": 160.8485126351834
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287237.3057491811,
            "upper_bound": 287578.341308298
          },
          "point_estimate": 287381.5151651498,
          "standard_error": 86.27031655984652
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217.1288663146915,
            "upper_bound": 703.656033454341
          },
          "point_estimate": 509.0948232546827,
          "standard_error": 140.3273404233329
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.87160804088482,
            "upper_bound": 62.93231184654813
          },
          "point_estimate": 62.90152529964059,
          "standard_error": 0.01552166223311756
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.86463854598629,
            "upper_bound": 62.92630961304831
          },
          "point_estimate": 62.90644958254711,
          "standard_error": 0.015438538595051558
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007033794276339664,
            "upper_bound": 0.09085605029072163
          },
          "point_estimate": 0.04124643062997647,
          "standard_error": 0.019486442150103898
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.85240240943699,
            "upper_bound": 62.91552483762947
          },
          "point_estimate": 62.88353040502548,
          "standard_error": 0.01605296844421952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022988940856258605,
            "upper_bound": 0.07024671340201771
          },
          "point_estimate": 0.05180274570429846,
          "standard_error": 0.012074215628934529
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.43864359477554,
            "upper_bound": 105.78320303694524
          },
          "point_estimate": 105.57593039018144,
          "standard_error": 0.09308777954131275
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.41555292162984,
            "upper_bound": 105.6007025704528
          },
          "point_estimate": 105.48203152867072,
          "standard_error": 0.058207828250673666
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02470663160217944,
            "upper_bound": 0.2406180434779425
          },
          "point_estimate": 0.1292444762247573,
          "standard_error": 0.05750602390677901
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.4294668718528,
            "upper_bound": 105.55822776499066
          },
          "point_estimate": 105.48790852217246,
          "standard_error": 0.032852162821814246
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06908519199134094,
            "upper_bound": 0.4689396707402214
          },
          "point_estimate": 0.3095039639619638,
          "standard_error": 0.13222566949413234
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126632.67344615897,
            "upper_bound": 126818.206280073
          },
          "point_estimate": 126725.91733864276,
          "standard_error": 47.414578763237806
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126603.07229965155,
            "upper_bound": 126867.47038327526
          },
          "point_estimate": 126748.50232288038,
          "standard_error": 67.08520023408919
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.33340593186234,
            "upper_bound": 265.7449386967142
          },
          "point_estimate": 186.09393401324803,
          "standard_error": 63.67489282808852
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126640.66473338332,
            "upper_bound": 126777.57456445994
          },
          "point_estimate": 126703.4813430472,
          "standard_error": 34.92139005991141
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.82674272428088,
            "upper_bound": 201.55876484532303
          },
          "point_estimate": 158.29224705799712,
          "standard_error": 28.587238666144
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.55710082353599,
            "upper_bound": 60.64251386272355
          },
          "point_estimate": 60.59834441411501,
          "standard_error": 0.02191503216809369
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.53570961168447,
            "upper_bound": 60.64841403778368
          },
          "point_estimate": 60.605787249151106,
          "standard_error": 0.03036672990866059
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008589193219646709,
            "upper_bound": 0.1290050836075452
          },
          "point_estimate": 0.08992351315849981,
          "standard_error": 0.029866427203517686
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.56418039548099,
            "upper_bound": 60.67592041031454
          },
          "point_estimate": 60.6250199288702,
          "standard_error": 0.02836328890419745
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04083453383453693,
            "upper_bound": 0.09358153747179916
          },
          "point_estimate": 0.07315288071880216,
          "standard_error": 0.013999821775725086
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.45862443216906,
            "upper_bound": 102.93894447593186
          },
          "point_estimate": 102.68134224756254,
          "standard_error": 0.12338243758775068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.344282495721,
            "upper_bound": 102.939811893221
          },
          "point_estimate": 102.62627925862638,
          "standard_error": 0.11994644491546602
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03127707810781018,
            "upper_bound": 0.6848648196842909
          },
          "point_estimate": 0.30910510111364164,
          "standard_error": 0.1860634496617127
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.60550706339308,
            "upper_bound": 102.82720389968256
          },
          "point_estimate": 102.68470539439724,
          "standard_error": 0.056952837693788706
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.17730263162065624,
            "upper_bound": 0.5380489679771826
          },
          "point_estimate": 0.4087282403334013,
          "standard_error": 0.09670473114179466
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.24882079419737,
            "upper_bound": 255.53596301891804
          },
          "point_estimate": 255.38300570741495,
          "standard_error": 0.0736702643076429
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.177432209808,
            "upper_bound": 255.54058510762889
          },
          "point_estimate": 255.3309140257185,
          "standard_error": 0.10835941476933272
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04626997048534703,
            "upper_bound": 0.4398270531851171
          },
          "point_estimate": 0.23707011343338127,
          "standard_error": 0.09595798911544778
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.1950250497983,
            "upper_bound": 255.4961243151563
          },
          "point_estimate": 255.3362291576739,
          "standard_error": 0.08314478480258493
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1255026247027827,
            "upper_bound": 0.31857916879898696
          },
          "point_estimate": 0.24494773850252788,
          "standard_error": 0.05259847709608525
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 522.4022011562928,
            "upper_bound": 524.0096662940789
          },
          "point_estimate": 523.1223422766964,
          "standard_error": 0.4140333977602385
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 522.1496194910237,
            "upper_bound": 523.9264938690922
          },
          "point_estimate": 522.6601647701469,
          "standard_error": 0.4028817456931001
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07524416062954939,
            "upper_bound": 2.0048278235185393
          },
          "point_estimate": 0.8067531485756884,
          "standard_error": 0.460610857524657
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 522.3961592534577,
            "upper_bound": 523.0544213089433
          },
          "point_estimate": 522.6578230473689,
          "standard_error": 0.1720460341076935
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.43708054694900217,
            "upper_bound": 1.8448617175359967
          },
          "point_estimate": 1.378844644876117,
          "standard_error": 0.3761155319071086
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.349529953163795,
            "upper_bound": 31.405660096990644
          },
          "point_estimate": 31.376169501078486,
          "standard_error": 0.01441023052139729
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.333119131028205,
            "upper_bound": 31.40852276572017
          },
          "point_estimate": 31.375842072254244,
          "standard_error": 0.0208242825376139
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007932242310367887,
            "upper_bound": 0.08655305518895323
          },
          "point_estimate": 0.05865661053927034,
          "standard_error": 0.019855489202016473
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.33794177864425,
            "upper_bound": 31.382549938868895
          },
          "point_estimate": 31.35487292804514,
          "standard_error": 0.01130688301465784
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025899284561506405,
            "upper_bound": 0.06217054688608492
          },
          "point_estimate": 0.04787597606614055,
          "standard_error": 0.00991590737923262
        }
      }
    },
    "memrmem/krate/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32438.007950551088,
            "upper_bound": 32524.565731754545
          },
          "point_estimate": 32479.99455193129,
          "standard_error": 21.936438475387195
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32454.72886505809,
            "upper_bound": 32505.601522937148
          },
          "point_estimate": 32472.22018667461,
          "standard_error": 15.052316181129116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.621014848437946,
            "upper_bound": 117.49225713363798
          },
          "point_estimate": 39.42263472235949,
          "standard_error": 23.209198690438058
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32459.558658608625,
            "upper_bound": 32496.464541658075
          },
          "point_estimate": 32474.202418671583,
          "standard_error": 9.449436390767913
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.780070827106524,
            "upper_bound": 103.47639708648572
          },
          "point_estimate": 73.22284169202531,
          "standard_error": 22.1768326239332
        }
      }
    },
    "memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142.00397667135945,
            "upper_bound": 142.25585609394852
          },
          "point_estimate": 142.1157763724857,
          "standard_error": 0.06457058150995698
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142.0123866943813,
            "upper_bound": 142.17259260786167
          },
          "point_estimate": 142.09562568490344,
          "standard_error": 0.04417621176238735
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016602975023388544,
            "upper_bound": 0.27370773684094446
          },
          "point_estimate": 0.11121118032983536,
          "standard_error": 0.05854795108636964
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.98016940260197,
            "upper_bound": 142.14772231632543
          },
          "point_estimate": 142.0591653270157,
          "standard_error": 0.04208305069119948
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05733640342167422,
            "upper_bound": 0.31338027829169157
          },
          "point_estimate": 0.21555001364036036,
          "standard_error": 0.0735566449524662
        }
      }
    },
    "memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1005583.0179150578,
            "upper_bound": 1007068.9884234236
          },
          "point_estimate": 1006316.2178314028,
          "standard_error": 378.3105691629576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1005485.0197876449,
            "upper_bound": 1007637.448198198
          },
          "point_estimate": 1005745.1135135136,
          "standard_error": 732.6810691871734
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.61364892493044,
            "upper_bound": 2053.489118137766
          },
          "point_estimate": 1268.8588590947804,
          "standard_error": 627.2356876035135
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1005073.1557610242,
            "upper_bound": 1006961.0961188496
          },
          "point_estimate": 1005921.3045981046,
          "standard_error": 494.9264826648476
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 804.474321867459,
            "upper_bound": 1503.6504639839038
          },
          "point_estimate": 1263.2356017934108,
          "standard_error": 178.78712930025603
        }
      }
    },
    "memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1984.980152050619,
            "upper_bound": 1987.6191233060424
          },
          "point_estimate": 1986.2970575845743,
          "standard_error": 0.6745263328617216
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1984.267854410318,
            "upper_bound": 1988.3649078223484
          },
          "point_estimate": 1986.4260711553177,
          "standard_error": 1.2228641496524737
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.30253476892124787,
            "upper_bound": 3.722749997666963
          },
          "point_estimate": 2.7808751623103136,
          "standard_error": 0.8401289288452868
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1985.159011258983,
            "upper_bound": 1988.029871301696
          },
          "point_estimate": 1986.638182300812,
          "standard_error": 0.7309518672479545
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.457917854719782,
            "upper_bound": 2.6502415759167244
          },
          "point_estimate": 2.2452621465768385,
          "standard_error": 0.3061038761935141
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.454331170596852,
            "upper_bound": 23.486662702262755
          },
          "point_estimate": 23.47003488306704,
          "standard_error": 0.008313675932489054
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.441239535440893,
            "upper_bound": 23.490288977323903
          },
          "point_estimate": 23.469334786030423,
          "standard_error": 0.01229523370262348
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004765162686798966,
            "upper_bound": 0.04760324876098045
          },
          "point_estimate": 0.036360350622350275,
          "standard_error": 0.01076780509782731
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.462235414521547,
            "upper_bound": 23.503246955225517
          },
          "point_estimate": 23.48394422856038,
          "standard_error": 0.01090549215359316
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01595492686681852,
            "upper_bound": 0.034617633354685935
          },
          "point_estimate": 0.027743596286967553,
          "standard_error": 0.004840194740130964
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.741099903839537,
            "upper_bound": 23.811926169776335
          },
          "point_estimate": 23.773774455024533,
          "standard_error": 0.018157859445379235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.7338058331067,
            "upper_bound": 23.80339568318986
          },
          "point_estimate": 23.76118049007727,
          "standard_error": 0.020903370704466535
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011013718821302525,
            "upper_bound": 0.0921311864960752
          },
          "point_estimate": 0.05360682821748284,
          "standard_error": 0.02030528043690658
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.72732023967532,
            "upper_bound": 23.80037536822359
          },
          "point_estimate": 23.764973515339612,
          "standard_error": 0.01890529196393755
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02759083754299503,
            "upper_bound": 0.08347391501749422
          },
          "point_estimate": 0.060780723900431904,
          "standard_error": 0.015736862543121323
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.570570285853783,
            "upper_bound": 27.95496564654842
          },
          "point_estimate": 27.76138573834666,
          "standard_error": 0.09814522302624848
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.508473959724192,
            "upper_bound": 28.034268826456746
          },
          "point_estimate": 27.722285828431502,
          "standard_error": 0.1100776858155028
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04609939760836736,
            "upper_bound": 0.5634330414499436
          },
          "point_estimate": 0.30521157131135096,
          "standard_error": 0.1504634293864085
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.38541860283244,
            "upper_bound": 27.759215000027492
          },
          "point_estimate": 27.550737507887114,
          "standard_error": 0.09944404749711656
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.16787674610013928,
            "upper_bound": 0.4239283568129909
          },
          "point_estimate": 0.32625712303199667,
          "standard_error": 0.06568486235738709
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.72922407858484,
            "upper_bound": 30.02475559709389
          },
          "point_estimate": 29.886568401724382,
          "standard_error": 0.07598432087494775
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.715918630201656,
            "upper_bound": 30.06354666867679
          },
          "point_estimate": 29.94349759737845,
          "standard_error": 0.09946054644844347
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023687129049333545,
            "upper_bound": 0.3938007697831006
          },
          "point_estimate": 0.1886621237803613,
          "standard_error": 0.10234053101977356
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.697028599652327,
            "upper_bound": 30.100203350281912
          },
          "point_estimate": 29.92128130794028,
          "standard_error": 0.10563423667748796
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11495355578556304,
            "upper_bound": 0.3296376770352539
          },
          "point_estimate": 0.25237088959717235,
          "standard_error": 0.05664497520994341
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.60703824740715,
            "upper_bound": 17.636700523291786
          },
          "point_estimate": 17.621717475143434,
          "standard_error": 0.007600930336261719
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.604314825046472,
            "upper_bound": 17.644856948795137
          },
          "point_estimate": 17.61610832537059,
          "standard_error": 0.009739290495734094
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003324209352482826,
            "upper_bound": 0.04266833706061969
          },
          "point_estimate": 0.02279235456404205,
          "standard_error": 0.01041515368567178
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.609009071528313,
            "upper_bound": 17.631229440436446
          },
          "point_estimate": 17.620257716041454,
          "standard_error": 0.005856753482694153
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013505697164329,
            "upper_bound": 0.03226953123554758
          },
          "point_estimate": 0.02534935060893682,
          "standard_error": 0.004729040366646975
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.352166055640986,
            "upper_bound": 19.4111343584651
          },
          "point_estimate": 19.380181820834274,
          "standard_error": 0.01510759777143863
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.33489276958009,
            "upper_bound": 19.42074835532597
          },
          "point_estimate": 19.363510226068463,
          "standard_error": 0.022734232729374977
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006317606961064486,
            "upper_bound": 0.08277928571780424
          },
          "point_estimate": 0.054798000604097,
          "standard_error": 0.019750620084151772
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.348697668765357,
            "upper_bound": 19.381408214983253
          },
          "point_estimate": 19.361833513529255,
          "standard_error": 0.00824518365939976
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02736560136185492,
            "upper_bound": 0.06271946379197688
          },
          "point_estimate": 0.05029259623607019,
          "standard_error": 0.009158932576079484
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.77291917311648,
            "upper_bound": 33.81721223288174
          },
          "point_estimate": 33.79545486970934,
          "standard_error": 0.0113057229581286
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.761965542592904,
            "upper_bound": 33.82457250847729
          },
          "point_estimate": 33.79711999417531,
          "standard_error": 0.014461451740866242
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009387998565694864,
            "upper_bound": 0.06398246933075666
          },
          "point_estimate": 0.034603366528948847,
          "standard_error": 0.015054000636745548
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.767765566092464,
            "upper_bound": 33.8101677760237
          },
          "point_estimate": 33.78590001531783,
          "standard_error": 0.010595300292558306
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02024990215321286,
            "upper_bound": 0.047587589079920736
          },
          "point_estimate": 0.037609367361862184,
          "standard_error": 0.006927173058865993
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.74695459328565,
            "upper_bound": 23.770928258465425
          },
          "point_estimate": 23.757770853036178,
          "standard_error": 0.006105976299611925
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.74583042793992,
            "upper_bound": 23.762707293187955
          },
          "point_estimate": 23.756799782822036,
          "standard_error": 0.004729548147715095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001280779099117129,
            "upper_bound": 0.026705625318186387
          },
          "point_estimate": 0.008864094977199158,
          "standard_error": 0.006605717599066868
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.754316821846235,
            "upper_bound": 23.774870671835977
          },
          "point_estimate": 23.76302463691495,
          "standard_error": 0.005196666509508699
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005828543792134867,
            "upper_bound": 0.029294755590964127
          },
          "point_estimate": 0.020320792248613165,
          "standard_error": 0.006425315022073594
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.127366921820673,
            "upper_bound": 25.197483468106668
          },
          "point_estimate": 25.15784652297272,
          "standard_error": 0.018121975403509176
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.11489103495976,
            "upper_bound": 25.17245787570115
          },
          "point_estimate": 25.156234496048185,
          "standard_error": 0.016724019507086945
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00419491715740088,
            "upper_bound": 0.0827864126067778
          },
          "point_estimate": 0.032594710650693026,
          "standard_error": 0.01896127485622153
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.118282188960865,
            "upper_bound": 25.1616452842271
          },
          "point_estimate": 25.141148581087315,
          "standard_error": 0.01119448353792466
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022506875873068953,
            "upper_bound": 0.08733153233998425
          },
          "point_estimate": 0.06025219279129717,
          "standard_error": 0.02004231796052473
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.67052166322681,
            "upper_bound": 26.752759694099534
          },
          "point_estimate": 26.703058699418836,
          "standard_error": 0.022257746298161137
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.66481817622572,
            "upper_bound": 26.70095565850284
          },
          "point_estimate": 26.690270710248175,
          "standard_error": 0.011279367398898309
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002366167774558318,
            "upper_bound": 0.05364448028360718
          },
          "point_estimate": 0.01959968850939753,
          "standard_error": 0.013941646227862946
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.672570820135995,
            "upper_bound": 26.69656412340237
          },
          "point_estimate": 26.683462553856028,
          "standard_error": 0.006065758370791973
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013435442195127267,
            "upper_bound": 0.113127425271088
          },
          "point_estimate": 0.07443740606504409,
          "standard_error": 0.032850816042528
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.72907626825615,
            "upper_bound": 22.76386416155931
          },
          "point_estimate": 22.746730259377703,
          "standard_error": 0.008923437173639625
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.71642546250343,
            "upper_bound": 22.77400382232346
          },
          "point_estimate": 22.74749090218336,
          "standard_error": 0.013502843246584014
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007387047604531946,
            "upper_bound": 0.05203546103130643
          },
          "point_estimate": 0.039284861419332816,
          "standard_error": 0.011891574459758272
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.7388226248131,
            "upper_bound": 22.776963927922928
          },
          "point_estimate": 22.760662506102893,
          "standard_error": 0.0098868230050157
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017521149695370403,
            "upper_bound": 0.03579875690182378
          },
          "point_estimate": 0.029746236015123873,
          "standard_error": 0.004575677291745623
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.442144515968867,
            "upper_bound": 24.487683191555707
          },
          "point_estimate": 24.46668880110042,
          "standard_error": 0.011694733042904026
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.446212535206055,
            "upper_bound": 24.49256323629866
          },
          "point_estimate": 24.4752804800399,
          "standard_error": 0.012571309617222156
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009710280354382456,
            "upper_bound": 0.05854502462976857
          },
          "point_estimate": 0.03169595685015635,
          "standard_error": 0.01262124184545264
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.452276564112285,
            "upper_bound": 24.48526226492069
          },
          "point_estimate": 24.46629765444177,
          "standard_error": 0.008317903871610398
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016865152067274108,
            "upper_bound": 0.05323413903652993
          },
          "point_estimate": 0.03889990692933999,
          "standard_error": 0.01022011081908022
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1167665.7217894346,
            "upper_bound": 1169169.5132717015
          },
          "point_estimate": 1168469.4469630455,
          "standard_error": 385.56173611336186
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1167509.4910714286,
            "upper_bound": 1169500.03125
          },
          "point_estimate": 1168733.2729166667,
          "standard_error": 470.7041029026609
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 327.40749418758514,
            "upper_bound": 2104.1465511855213
          },
          "point_estimate": 1163.8996655866044,
          "standard_error": 476.1736262590654
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1167743.4632279116,
            "upper_bound": 1169326.4469290194
          },
          "point_estimate": 1168591.9401785715,
          "standard_error": 401.5385338067464
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 600.5041111728957,
            "upper_bound": 1697.261939128818
          },
          "point_estimate": 1288.895945513833,
          "standard_error": 290.4906299929316
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1576494.0254910714,
            "upper_bound": 1581736.4120834572
          },
          "point_estimate": 1578703.89625496,
          "standard_error": 1374.2743809220522
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1575762.1726190476,
            "upper_bound": 1580077.6333333333
          },
          "point_estimate": 1577254.9444444445,
          "standard_error": 1106.717237971469
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 565.4636299610484,
            "upper_bound": 5002.807580870093
          },
          "point_estimate": 2709.138164403216,
          "standard_error": 1145.9923597509978
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1575910.9500227687,
            "upper_bound": 1578361.4403097313
          },
          "point_estimate": 1576953.041233766,
          "standard_error": 622.0949923510067
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1335.743010621499,
            "upper_bound": 6729.685823716153
          },
          "point_estimate": 4581.543968424885,
          "standard_error": 1657.456300468312
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1438863.6014693985,
            "upper_bound": 1440710.8017246644
          },
          "point_estimate": 1439772.7682631258,
          "standard_error": 473.1939253837331
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1438388.4835164836,
            "upper_bound": 1441240.299145299
          },
          "point_estimate": 1439229.2692307692,
          "standard_error": 824.3652812026231
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.99206879252597,
            "upper_bound": 2355.29696715973
          },
          "point_estimate": 2159.356966471483,
          "standard_error": 672.3732927386229
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1439070.361214087,
            "upper_bound": 1440702.3828727906
          },
          "point_estimate": 1439803.505194805,
          "standard_error": 428.735700370136
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1003.2963443456364,
            "upper_bound": 1891.6630875988276
          },
          "point_estimate": 1576.1850980806462,
          "standard_error": 232.86887870539607
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397767.4113586094,
            "upper_bound": 398706.0972126467
          },
          "point_estimate": 398264.03158687026,
          "standard_error": 240.2277035428133
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397838.57669082127,
            "upper_bound": 398926.0509057971
          },
          "point_estimate": 398248.41226708074,
          "standard_error": 308.2487324992456
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 186.282701040665,
            "upper_bound": 1367.9124819647152
          },
          "point_estimate": 796.2544886897455,
          "standard_error": 292.2296436338495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397244.6124733476,
            "upper_bound": 398831.7286641908
          },
          "point_estimate": 398000.1855166573,
          "standard_error": 426.8990815133883
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409.7343270239667,
            "upper_bound": 1076.2471715659967
          },
          "point_estimate": 801.6616022345951,
          "standard_error": 188.3634418022803
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 556865.9300883838,
            "upper_bound": 557822.629559163
          },
          "point_estimate": 557309.8826352813,
          "standard_error": 244.77824354805472
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 556817.6082702021,
            "upper_bound": 557623.1666666666
          },
          "point_estimate": 557293.0016233766,
          "standard_error": 207.91734341192483
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.95289236972872,
            "upper_bound": 1194.320857394927
          },
          "point_estimate": 539.5484563301283,
          "standard_error": 255.0294793732841
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 556810.144990084,
            "upper_bound": 557442.6132932109
          },
          "point_estimate": 557127.6315230224,
          "standard_error": 160.51827309057225
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292.3491423533327,
            "upper_bound": 1158.3734107762832
          },
          "point_estimate": 816.2329519146236,
          "standard_error": 237.24096450541813
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 427905.04882959847,
            "upper_bound": 429074.8582016806
          },
          "point_estimate": 428396.6894962651,
          "standard_error": 305.7693318360781
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 427766.12176470587,
            "upper_bound": 428607.6196078431
          },
          "point_estimate": 428164.419327731,
          "standard_error": 307.66647677439033
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.87189714345523,
            "upper_bound": 1173.420776226392
          },
          "point_estimate": 594.5632882679115,
          "standard_error": 289.92926216855983
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 427791.6512495416,
            "upper_bound": 428543.2201728149
          },
          "point_estimate": 428247.840091673,
          "standard_error": 194.9110500036901
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 351.7749489516793,
            "upper_bound": 1497.8720769668048
          },
          "point_estimate": 1016.2474658118256,
          "standard_error": 368.0250655050936
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 478056.828623892,
            "upper_bound": 478763.0605464853
          },
          "point_estimate": 478410.35038548766,
          "standard_error": 180.29658388273688
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477945.3311688311,
            "upper_bound": 478847.81904761906
          },
          "point_estimate": 478467.1475984333,
          "standard_error": 190.57937605949695
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.702714241385735,
            "upper_bound": 1115.2117002010518
          },
          "point_estimate": 487.08704135247024,
          "standard_error": 323.52095667386
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 478356.1635256502,
            "upper_bound": 478838.9581298371
          },
          "point_estimate": 478557.96194973856,
          "standard_error": 121.89893476763814
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328.4610847929438,
            "upper_bound": 761.9450423885537
          },
          "point_estimate": 599.0673715820991,
          "standard_error": 113.69283482854158
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 999929.240316924,
            "upper_bound": 1001576.8722522524
          },
          "point_estimate": 1000780.3303281852,
          "standard_error": 420.9437366685926
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1000017.6103603604,
            "upper_bound": 1001788.7815315316
          },
          "point_estimate": 1000795.9266409266,
          "standard_error": 363.6611215974306
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.47909556453514,
            "upper_bound": 2372.2367592356914
          },
          "point_estimate": 604.8806587206868,
          "standard_error": 616.8289653520088
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 999882.046108923,
            "upper_bound": 1001021.6197128316
          },
          "point_estimate": 1000495.5734643736,
          "standard_error": 297.53600537024977
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 587.155501916844,
            "upper_bound": 1912.1900266803484
          },
          "point_estimate": 1405.9293178156863,
          "standard_error": 339.6893636366363
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297619.3568533129,
            "upper_bound": 298134.6752178962
          },
          "point_estimate": 297857.7737044627,
          "standard_error": 132.157004004654
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297500.4289617486,
            "upper_bound": 298107.49948770495
          },
          "point_estimate": 297802.4918032787,
          "standard_error": 152.5413747164175
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.99386648130617,
            "upper_bound": 683.3542783394191
          },
          "point_estimate": 397.07105918011354,
          "standard_error": 163.11638275326058
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297552.4036227787,
            "upper_bound": 297867.08612726955
          },
          "point_estimate": 297721.41102831595,
          "standard_error": 82.15406872756083
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193.6656844006145,
            "upper_bound": 587.6204456997459
          },
          "point_estimate": 441.0265727241426,
          "standard_error": 107.27173255654516
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 808741.2458166667,
            "upper_bound": 810042.4658597882
          },
          "point_estimate": 809430.9830943563,
          "standard_error": 333.0302480910018
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 808807.8888888889,
            "upper_bound": 810164.1226851852
          },
          "point_estimate": 809633.1436507937,
          "standard_error": 328.634495203224
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.99357352688804,
            "upper_bound": 1793.7583551914431
          },
          "point_estimate": 918.6580678571892,
          "standard_error": 409.7213045033077
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 809004.9320086649,
            "upper_bound": 810101.337831218
          },
          "point_estimate": 809557.104011544,
          "standard_error": 291.1169376458892
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 473.0543049923444,
            "upper_bound": 1528.3718380909172
          },
          "point_estimate": 1109.4975012139585,
          "standard_error": 282.68240263008977
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390757.798796723,
            "upper_bound": 391148.79331438814
          },
          "point_estimate": 390931.9184920635,
          "standard_error": 100.91851382181542
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390699.10035842296,
            "upper_bound": 391108.05519713263
          },
          "point_estimate": 390830.40975422424,
          "standard_error": 103.44053477141313
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.833679329319494,
            "upper_bound": 479.70691223616853
          },
          "point_estimate": 200.73989912425264,
          "standard_error": 114.2358123816362
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390836.995721388,
            "upper_bound": 391207.3065045168
          },
          "point_estimate": 390990.40564167016,
          "standard_error": 93.40602518836955
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.1195895302926,
            "upper_bound": 463.3672984199982
          },
          "point_estimate": 336.761524401961,
          "standard_error": 96.51818193028672
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169100.4875097822,
            "upper_bound": 169522.5453179679
          },
          "point_estimate": 169273.31660502029,
          "standard_error": 112.40904120256371
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169026.6353488372,
            "upper_bound": 169342.38217054264
          },
          "point_estimate": 169177.1010465116,
          "standard_error": 76.08009959178851
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.70585030733464,
            "upper_bound": 328.57288672481417
          },
          "point_estimate": 190.28791392403235,
          "standard_error": 79.12182852386817
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169030.91331965185,
            "upper_bound": 169179.9379347919
          },
          "point_estimate": 169083.70512836002,
          "standard_error": 38.05920846433025
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.46380089634712,
            "upper_bound": 562.4976191893455
          },
          "point_estimate": 375.54509477413336,
          "standard_error": 150.0485881550151
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231674.72119570823,
            "upper_bound": 232168.79062481047
          },
          "point_estimate": 231901.4951956324,
          "standard_error": 127.03230231008456
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231578.75902335456,
            "upper_bound": 232217.4872611465
          },
          "point_estimate": 231789.57176220807,
          "standard_error": 166.13727029531756
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.18101905366284,
            "upper_bound": 719.0413136676092
          },
          "point_estimate": 378.39366659427264,
          "standard_error": 151.30403928414057
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231593.5112555623,
            "upper_bound": 231964.16812511897
          },
          "point_estimate": 231764.14285714287,
          "standard_error": 94.79355170129
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202.19656034432575,
            "upper_bound": 564.1297185652403
          },
          "point_estimate": 423.47622038751774,
          "standard_error": 100.40993516318792
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173548.5577650471,
            "upper_bound": 173786.82557833978
          },
          "point_estimate": 173663.50105756818,
          "standard_error": 61.071366292643994
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173481.7541866029,
            "upper_bound": 173853.85693779902
          },
          "point_estimate": 173639.3258904838,
          "standard_error": 99.09714249048945
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.365998553335466,
            "upper_bound": 345.1256870546085
          },
          "point_estimate": 209.54412360043224,
          "standard_error": 84.2886591183425
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173527.07216490695,
            "upper_bound": 173827.2663016555
          },
          "point_estimate": 173652.2044739949,
          "standard_error": 77.90062353721758
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.68339747250494,
            "upper_bound": 245.37526950477957
          },
          "point_estimate": 203.4301634954312,
          "standard_error": 32.72511103429069
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649331.6645804989,
            "upper_bound": 649798.3956510418
          },
          "point_estimate": 649556.1125588152,
          "standard_error": 119.92757150186765
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649245.9285714285,
            "upper_bound": 649884.8484126984
          },
          "point_estimate": 649523.3630952381,
          "standard_error": 153.4147385439733
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.63066158335609,
            "upper_bound": 666.8562594109077
          },
          "point_estimate": 370.6146934203067,
          "standard_error": 162.7732249329141
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649354.306407968,
            "upper_bound": 650025.5799359881
          },
          "point_estimate": 649728.1545454545,
          "standard_error": 172.09326450224765
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194.61347440256344,
            "upper_bound": 504.3836571082086
          },
          "point_estimate": 399.637351653159,
          "standard_error": 77.53698258084134
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1354.5485249056433,
            "upper_bound": 1356.8689069090144
          },
          "point_estimate": 1355.6366589976783,
          "standard_error": 0.5946408006818562
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1354.247977180357,
            "upper_bound": 1356.981127452286
          },
          "point_estimate": 1355.3425332786455,
          "standard_error": 0.6323238949595537
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.27107935534260275,
            "upper_bound": 3.1684431258213013
          },
          "point_estimate": 1.7074500054819246,
          "standard_error": 0.7351640879775555
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1354.077220649668,
            "upper_bound": 1356.1923578106828
          },
          "point_estimate": 1355.0977493664843,
          "standard_error": 0.5314551051262265
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8774436798032721,
            "upper_bound": 2.682478742563873
          },
          "point_estimate": 1.9808258517798976,
          "standard_error": 0.4878300078314794
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1315004.935582483,
            "upper_bound": 1318692.5508386483
          },
          "point_estimate": 1316689.988986678,
          "standard_error": 950.5840127372117
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1314665.4354591838,
            "upper_bound": 1318733.4035714285
          },
          "point_estimate": 1315128.550843254,
          "standard_error": 1061.1683688029066
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154.4260247584985,
            "upper_bound": 4822.149795639777
          },
          "point_estimate": 1314.756797372536,
          "standard_error": 1186.461797607502
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1314888.263411212,
            "upper_bound": 1316359.179766886
          },
          "point_estimate": 1315396.835064935,
          "standard_error": 380.20684070665624
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 838.6467035486949,
            "upper_bound": 4035.6772984382233
          },
          "point_estimate": 3168.8609886809486,
          "standard_error": 791.5594521511663
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1511964.8068894048,
            "upper_bound": 1515924.1643873013
          },
          "point_estimate": 1513732.609220635,
          "standard_error": 1017.9735333245044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1511233.42,
            "upper_bound": 1514811.3953333334
          },
          "point_estimate": 1513476.072,
          "standard_error": 966.9746125500932
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149.9679525375394,
            "upper_bound": 4994.538807529147
          },
          "point_estimate": 1997.2631392081917,
          "standard_error": 1123.9734314276743
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1512002.7380590718,
            "upper_bound": 1514688.6043529229
          },
          "point_estimate": 1513439.7876363636,
          "standard_error": 675.8791924122341
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1366.7136171418472,
            "upper_bound": 4804.973222013973
          },
          "point_estimate": 3382.431909657626,
          "standard_error": 1024.4519016921358
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415975.4664737484,
            "upper_bound": 1418270.802910257
          },
          "point_estimate": 1417093.5643345544,
          "standard_error": 588.9349283435316
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415588.058760684,
            "upper_bound": 1418854.9871794872
          },
          "point_estimate": 1416880.2115384615,
          "standard_error": 838.2645149274865
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 533.4492911704091,
            "upper_bound": 3362.3324076144568
          },
          "point_estimate": 2139.993672263779,
          "standard_error": 706.4125195277438
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415197.9133651818,
            "upper_bound": 1418485.1805354622
          },
          "point_estimate": 1416533.8667332667,
          "standard_error": 846.1803280743885
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1106.8180897002137,
            "upper_bound": 2466.8356191637595
          },
          "point_estimate": 1963.8990628890888,
          "standard_error": 346.6282728029779
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 753154.647966189,
            "upper_bound": 754495.7390255507
          },
          "point_estimate": 753813.640611435,
          "standard_error": 343.5467405683991
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752648.3527696793,
            "upper_bound": 754810.243622449
          },
          "point_estimate": 753939.6306689342,
          "standard_error": 597.169418254975
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.09460295163456,
            "upper_bound": 1943.530766090681
          },
          "point_estimate": 1732.0507451342964,
          "standard_error": 506.7551302316113
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752804.4492088412,
            "upper_bound": 754237.6639091454
          },
          "point_estimate": 753498.4503047973,
          "standard_error": 376.5262464768608
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 716.07113914803,
            "upper_bound": 1365.3494096873962
          },
          "point_estimate": 1140.4853665881917,
          "standard_error": 167.39239660799242
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266082.3840939031,
            "upper_bound": 266597.28002450976
          },
          "point_estimate": 266333.42746119277,
          "standard_error": 131.92027497301427
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265897.0507352941,
            "upper_bound": 266674.44852941175
          },
          "point_estimate": 266366.58624387253,
          "standard_error": 170.16679999776946
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.75786005222196,
            "upper_bound": 774.5895193058967
          },
          "point_estimate": 466.73578059246927,
          "standard_error": 190.17518729711145
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265998.5600854621,
            "upper_bound": 266586.1832075762
          },
          "point_estimate": 266229.6091864018,
          "standard_error": 150.13968235883328
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248.88960027432032,
            "upper_bound": 564.3261625576316
          },
          "point_estimate": 439.4326408346856,
          "standard_error": 84.68278684938278
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477858.400982665,
            "upper_bound": 478991.2848250836
          },
          "point_estimate": 478373.0267705722,
          "standard_error": 291.1917803649721
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477499.7456140351,
            "upper_bound": 478866.95942982455
          },
          "point_estimate": 478178.0643274854,
          "standard_error": 313.2521276693427
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.7310859685898,
            "upper_bound": 1421.937969821428
          },
          "point_estimate": 894.3597390561413,
          "standard_error": 335.69173280817205
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477625.9013690363,
            "upper_bound": 478318.77736213815
          },
          "point_estimate": 477930.2985645933,
          "standard_error": 178.85792464170672
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 420.3556319165696,
            "upper_bound": 1339.4446973681231
          },
          "point_estimate": 971.51429942367,
          "standard_error": 263.29016077038796
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240947.5477274519,
            "upper_bound": 241209.81553949855
          },
          "point_estimate": 241081.91753074745,
          "standard_error": 67.18070288407735
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240892.5894039735,
            "upper_bound": 241255.7412803532
          },
          "point_estimate": 241131.6165562914,
          "standard_error": 93.7881459852358
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.94561476977352,
            "upper_bound": 374.9408602971023
          },
          "point_estimate": 261.70115071811074,
          "standard_error": 85.79650085972106
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240842.90027263213,
            "upper_bound": 241252.2279500448
          },
          "point_estimate": 241056.04197127375,
          "standard_error": 108.00972597332546
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.31640111309207,
            "upper_bound": 276.7022796239941
          },
          "point_estimate": 223.42868156021265,
          "standard_error": 37.81145574108145
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 692551.1710516622,
            "upper_bound": 693636.3874249776
          },
          "point_estimate": 693078.4922192274,
          "standard_error": 278.15371354178217
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 692303.9245283019,
            "upper_bound": 693824.9773584906
          },
          "point_estimate": 693064.501347709,
          "standard_error": 330.18022010418997
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147.67894605737908,
            "upper_bound": 1742.8690003786928
          },
          "point_estimate": 745.7176152515988,
          "standard_error": 398.346387214447
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 692498.3960110787,
            "upper_bound": 693746.3112843776
          },
          "point_estimate": 693058.4781181084,
          "standard_error": 322.47482126485176
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486.47130368349815,
            "upper_bound": 1165.6284724715697
          },
          "point_estimate": 925.3469344753248,
          "standard_error": 173.31567716000288
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356.96398066264226,
            "upper_bound": 357.8563676912255
          },
          "point_estimate": 357.3824406824943,
          "standard_error": 0.2292956902179851
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356.8464253978247,
            "upper_bound": 358.0017099986556
          },
          "point_estimate": 357.12682790604134,
          "standard_error": 0.269813931330603
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09124690611885036,
            "upper_bound": 1.2375174780542866
          },
          "point_estimate": 0.48174156418706154,
          "standard_error": 0.3029632619112966
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.03118632539145,
            "upper_bound": 357.5530361288006
          },
          "point_estimate": 357.22496745805125,
          "standard_error": 0.13382212137238536
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3024415988398866,
            "upper_bound": 0.9531338853685528
          },
          "point_estimate": 0.7630098800622765,
          "standard_error": 0.16056105796658257
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.22611470319342,
            "upper_bound": 169.5221084993905
          },
          "point_estimate": 169.3620574176734,
          "standard_error": 0.07638467714999991
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.1721746016957,
            "upper_bound": 169.53163229059908
          },
          "point_estimate": 169.28298684224148,
          "standard_error": 0.0843195223672665
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05058493528008196,
            "upper_bound": 0.400379607340273
          },
          "point_estimate": 0.1920207704038067,
          "standard_error": 0.08849823913439649
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.24390295170758,
            "upper_bound": 169.53031334602304
          },
          "point_estimate": 169.3632339150647,
          "standard_error": 0.07450351068522942
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09172243135783664,
            "upper_bound": 0.3291667417207484
          },
          "point_estimate": 0.2545965374595991,
          "standard_error": 0.06136801659366777
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.327902160087596,
            "upper_bound": 25.576949557220697
          },
          "point_estimate": 25.45208716554794,
          "standard_error": 0.06360762962981747
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.264628781129947,
            "upper_bound": 25.572542091617347
          },
          "point_estimate": 25.489165065237938,
          "standard_error": 0.08167734646669458
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01257251081704343,
            "upper_bound": 0.3944326959337276
          },
          "point_estimate": 0.15292450285441067,
          "standard_error": 0.09528746596042872
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.425440456278498,
            "upper_bound": 25.54200344754644
          },
          "point_estimate": 25.501182914452283,
          "standard_error": 0.02986607618028547
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1102230593660978,
            "upper_bound": 0.27880121080184117
          },
          "point_estimate": 0.2122363779146266,
          "standard_error": 0.042879256468614145
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.859380877525503,
            "upper_bound": 17.904472278207713
          },
          "point_estimate": 17.880180898603037,
          "standard_error": 0.011583047584377794
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.85039775943144,
            "upper_bound": 17.894809706417483
          },
          "point_estimate": 17.88297856368898,
          "standard_error": 0.013072279142611522
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030958925695925483,
            "upper_bound": 0.06312104590505692
          },
          "point_estimate": 0.027187560985951768,
          "standard_error": 0.014631146180420464
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.84753818945626,
            "upper_bound": 17.876456427819637
          },
          "point_estimate": 17.86021051765542,
          "standard_error": 0.007328729326263504
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01744633826284291,
            "upper_bound": 0.05349239205215713
          },
          "point_estimate": 0.03849325571982843,
          "standard_error": 0.010416823665283455
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.17518139320976,
            "upper_bound": 25.223171881697606
          },
          "point_estimate": 25.19420477810423,
          "standard_error": 0.013010686649273472
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.17016832727479,
            "upper_bound": 25.198245219857213
          },
          "point_estimate": 25.18284048091372,
          "standard_error": 0.007255859168739988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004147310892346843,
            "upper_bound": 0.03221258170726774
          },
          "point_estimate": 0.016431718238322676,
          "standard_error": 0.008038857085376013
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.17205072693725,
            "upper_bound": 25.190449893111232
          },
          "point_estimate": 25.181424348160547,
          "standard_error": 0.004690607028652888
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008369632393544171,
            "upper_bound": 0.0653450369361794
          },
          "point_estimate": 0.043239665849637045,
          "standard_error": 0.01856121392152591
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287120.81187780434,
            "upper_bound": 287820.18435101863
          },
          "point_estimate": 287410.8619100737,
          "standard_error": 185.79539287949487
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287070.7283464567,
            "upper_bound": 287556.900984252
          },
          "point_estimate": 287242.6163526434,
          "standard_error": 100.13487226833496
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.677997811820376,
            "upper_bound": 595.7234078686914
          },
          "point_estimate": 192.37610209641988,
          "standard_error": 160.4827636615265
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287123.6731263279,
            "upper_bound": 287344.53026686143
          },
          "point_estimate": 287218.5705900399,
          "standard_error": 56.08909753341538
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121.57448531629456,
            "upper_bound": 906.6107500408696
          },
          "point_estimate": 617.9940228583254,
          "standard_error": 236.01376656537104
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.727326955472838,
            "upper_bound": 15.793328997799708
          },
          "point_estimate": 15.758310264594405,
          "standard_error": 0.01692593382849247
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.71060118224232,
            "upper_bound": 15.798690959681736
          },
          "point_estimate": 15.748140367149944,
          "standard_error": 0.02252417188347704
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011412166524030471,
            "upper_bound": 0.09353966410131756
          },
          "point_estimate": 0.05906217541145568,
          "standard_error": 0.02149745329024812
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.752353965947048,
            "upper_bound": 15.82464958379134
          },
          "point_estimate": 15.78893936253245,
          "standard_error": 0.01844062601142856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026510787488876187,
            "upper_bound": 0.07346826298239394
          },
          "point_estimate": 0.05674863432031776,
          "standard_error": 0.012261069627649944
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.290065739313953,
            "upper_bound": 22.37669785933929
          },
          "point_estimate": 22.33165615241048,
          "standard_error": 0.022148848614590683
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.251815012544668,
            "upper_bound": 22.38040096841645
          },
          "point_estimate": 22.329339137931527,
          "standard_error": 0.029661808803508227
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006376152508889336,
            "upper_bound": 0.1296973807019397
          },
          "point_estimate": 0.0862797077834925,
          "standard_error": 0.03179218583617225
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.298680756849144,
            "upper_bound": 22.40595201011868
          },
          "point_estimate": 22.35164591571051,
          "standard_error": 0.027059987516405135
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04087903385349292,
            "upper_bound": 0.09537080577805962
          },
          "point_estimate": 0.07413349905968367,
          "standard_error": 0.014744616756810491
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126433.15364404212,
            "upper_bound": 126730.6538052524
          },
          "point_estimate": 126585.33912436618,
          "standard_error": 76.1340345344723
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126379.44791666669,
            "upper_bound": 126818.80783730156
          },
          "point_estimate": 126614.25530478396,
          "standard_error": 146.4632701757438
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.106683304265317,
            "upper_bound": 464.63058721411227
          },
          "point_estimate": 308.5515133299696,
          "standard_error": 125.20130140122843
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126475.97810434373,
            "upper_bound": 126784.21406817144
          },
          "point_estimate": 126635.1576569264,
          "standard_error": 78.87017423668645
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160.5268643497612,
            "upper_bound": 309.46222918530816
          },
          "point_estimate": 253.53897408469757,
          "standard_error": 40.131541851513866
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.961383739728284,
            "upper_bound": 24.003301172081382
          },
          "point_estimate": 23.980970098118977,
          "standard_error": 0.010760396786595938
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.95449490747735,
            "upper_bound": 24.00950325036689
          },
          "point_estimate": 23.969989490136363,
          "standard_error": 0.014578609953613412
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003804284737228085,
            "upper_bound": 0.06052007368875207
          },
          "point_estimate": 0.032641789264121986,
          "standard_error": 0.013870369685552853
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.95724027762642,
            "upper_bound": 23.99407611426351
          },
          "point_estimate": 23.973725928690666,
          "standard_error": 0.009477811635065328
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016992516311979077,
            "upper_bound": 0.04633297669806785
          },
          "point_estimate": 0.035875159253822925,
          "standard_error": 0.007687774579751733
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.671552741820317,
            "upper_bound": 20.94271560129325
          },
          "point_estimate": 20.796029434004105,
          "standard_error": 0.06954514875901771
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.637802823077624,
            "upper_bound": 20.94668808928171
          },
          "point_estimate": 20.72016415957173,
          "standard_error": 0.08267810862304308
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02515265874984551,
            "upper_bound": 0.35391659662161207
          },
          "point_estimate": 0.13478806597830756,
          "standard_error": 0.08665138810293746
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.651623859652652,
            "upper_bound": 20.817638286709425
          },
          "point_estimate": 20.7270563433082,
          "standard_error": 0.04384440320361357
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0881996776540868,
            "upper_bound": 0.3014452159921485
          },
          "point_estimate": 0.23096887795323515,
          "standard_error": 0.05587015015018387
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.83116836347191,
            "upper_bound": 41.92903013358703
          },
          "point_estimate": 41.88128028291269,
          "standard_error": 0.025061700249791156
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.82987562621744,
            "upper_bound": 41.92929977687134
          },
          "point_estimate": 41.899822915588445,
          "standard_error": 0.025790215992339948
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009503209451283709,
            "upper_bound": 0.14145459930135323
          },
          "point_estimate": 0.06222797054107636,
          "standard_error": 0.034275095125565376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.83349337622484,
            "upper_bound": 41.91389989921996
          },
          "point_estimate": 41.875597146943946,
          "standard_error": 0.02105017446233464
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.039206972332732085,
            "upper_bound": 0.11090285886390765
          },
          "point_estimate": 0.08348485683939416,
          "standard_error": 0.01852211812481472
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.16086720424369,
            "upper_bound": 74.29359865089445
          },
          "point_estimate": 74.23020591034913,
          "standard_error": 0.033862216613286464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.16872691830731,
            "upper_bound": 74.31515914080242
          },
          "point_estimate": 74.22760178235248,
          "standard_error": 0.036895972355293696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028941271518018917,
            "upper_bound": 0.18815913769893536
          },
          "point_estimate": 0.09257836510162051,
          "standard_error": 0.0414558287951496
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.09309579327423,
            "upper_bound": 74.2768325131182
          },
          "point_estimate": 74.18328372815148,
          "standard_error": 0.04738315180060748
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05214440374112675,
            "upper_bound": 0.15173889688199174
          },
          "point_estimate": 0.11264849074956666,
          "standard_error": 0.026274704795652415
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.761284389063752,
            "upper_bound": 10.776233743294329
          },
          "point_estimate": 10.76862775620775,
          "standard_error": 0.003814971865730258
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.759077496554038,
            "upper_bound": 10.775511589813378
          },
          "point_estimate": 10.769763882499236,
          "standard_error": 0.004271122673193892
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002607227056787227,
            "upper_bound": 0.022197354690859275
          },
          "point_estimate": 0.011149825467159523,
          "standard_error": 0.004718203651739535
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.757692808652312,
            "upper_bound": 10.77179326635942
          },
          "point_estimate": 10.764397233300818,
          "standard_error": 0.003534174564503298
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006190430841110898,
            "upper_bound": 0.016985870334641166
          },
          "point_estimate": 0.012728039338534985,
          "standard_error": 0.0027668863805874948
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32356.719301601428,
            "upper_bound": 32413.402262487296
          },
          "point_estimate": 32384.18906075242,
          "standard_error": 14.479554433009229
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32343.885824436536,
            "upper_bound": 32411.552046263347
          },
          "point_estimate": 32392.400459667853,
          "standard_error": 19.371721500927077
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.404916042912833,
            "upper_bound": 87.02987154209471
          },
          "point_estimate": 44.67915267653513,
          "standard_error": 20.22812876144153
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32363.959397017643,
            "upper_bound": 32413.681830814185
          },
          "point_estimate": 32387.767728890325,
          "standard_error": 12.67506347080624
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.42136608717094,
            "upper_bound": 64.78291750438295
          },
          "point_estimate": 48.43257221859609,
          "standard_error": 10.556221314664487
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.410091354447275,
            "upper_bound": 30.1874308600664
          },
          "point_estimate": 29.80347672362426,
          "standard_error": 0.1987437416560949
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.37470039507536,
            "upper_bound": 30.164844737010128
          },
          "point_estimate": 29.86148672861149,
          "standard_error": 0.2019130795924133
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09366099209709632,
            "upper_bound": 1.1331094209641714
          },
          "point_estimate": 0.3939355225342869,
          "standard_error": 0.25453809476917866
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.697383001068257,
            "upper_bound": 30.194339917865555
          },
          "point_estimate": 29.942733342486576,
          "standard_error": 0.1291570419135627
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2723395629024923,
            "upper_bound": 0.9003426947994747
          },
          "point_estimate": 0.6622274289096467,
          "standard_error": 0.15687396351058647
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978506.2031195177,
            "upper_bound": 979501.4695689224
          },
          "point_estimate": 979061.2489411028,
          "standard_error": 257.62749595168833
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978675.5657894736,
            "upper_bound": 979714.2293233082
          },
          "point_estimate": 979344.6666666667,
          "standard_error": 258.7468683001436
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.49155689383505,
            "upper_bound": 1206.287835163014
          },
          "point_estimate": 576.9234960731638,
          "standard_error": 274.81511401775856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978486.0749633048,
            "upper_bound": 979654.8022935688
          },
          "point_estimate": 979204.280656186,
          "standard_error": 301.27738172888934
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297.96288836937293,
            "upper_bound": 1205.1515135473826
          },
          "point_estimate": 860.4002789460028,
          "standard_error": 255.97074759018895
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1959.395918825028,
            "upper_bound": 1961.348693776457
          },
          "point_estimate": 1960.2913734640576,
          "standard_error": 0.5049753503467403
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1958.980829505539,
            "upper_bound": 1961.3406376654957
          },
          "point_estimate": 1959.998446365847,
          "standard_error": 0.5478919857132932
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2877536533621159,
            "upper_bound": 2.6345866962690234
          },
          "point_estimate": 1.7493257579191084,
          "standard_error": 0.6061676968208434
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1959.4174462875385,
            "upper_bound": 1961.6214081624416
          },
          "point_estimate": 1960.4877662407844,
          "standard_error": 0.5613300736871362
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7461275783307981,
            "upper_bound": 2.2967176816094192
          },
          "point_estimate": 1.6815561569348072,
          "standard_error": 0.43464720683195895
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.825051631704241,
            "upper_bound": 7.837491540516119
          },
          "point_estimate": 7.830771697504505,
          "standard_error": 0.0032026555908451896
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.823193821839881,
            "upper_bound": 7.835288578619969
          },
          "point_estimate": 7.82968172850933,
          "standard_error": 0.0037008838827315193
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019736141885695517,
            "upper_bound": 0.01584186876159324
          },
          "point_estimate": 0.008404615210780805,
          "standard_error": 0.003492260048277877
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.824614042330483,
            "upper_bound": 7.83313164417228
          },
          "point_estimate": 7.828862355255779,
          "standard_error": 0.0021830371247645573
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004696011247631275,
            "upper_bound": 0.01485366474954134
          },
          "point_estimate": 0.010693991787659228,
          "standard_error": 0.002945653657960073
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.101120620897286,
            "upper_bound": 7.109074623041528
          },
          "point_estimate": 7.105001509773217,
          "standard_error": 0.002029978486850153
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.09893591531597,
            "upper_bound": 7.109997564904413
          },
          "point_estimate": 7.105110613147396,
          "standard_error": 0.0027089889542265574
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018434447947777907,
            "upper_bound": 0.011708954038371033
          },
          "point_estimate": 0.0069748443956449335,
          "standard_error": 0.002558658174266124
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.0993273733047495,
            "upper_bound": 7.105497168116417
          },
          "point_estimate": 7.10237732518947,
          "standard_error": 0.0015851024669959483
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003773405168214637,
            "upper_bound": 0.008679004496427889
          },
          "point_estimate": 0.006772390979553298,
          "standard_error": 0.001286845632410604
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.32288361644342,
            "upper_bound": 8.339396200861042
          },
          "point_estimate": 8.330861811383102,
          "standard_error": 0.004238921218419214
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.318854442121019,
            "upper_bound": 8.34792673246114
          },
          "point_estimate": 8.327859978904751,
          "standard_error": 0.0071863519662768065
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025999832900578165,
            "upper_bound": 0.02343178379442729
          },
          "point_estimate": 0.01439557678664995,
          "standard_error": 0.005711629269189755
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.317719280597505,
            "upper_bound": 8.335366537605223
          },
          "point_estimate": 8.323577204116699,
          "standard_error": 0.004547559842707109
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007922165426964991,
            "upper_bound": 0.016549408465648077
          },
          "point_estimate": 0.014124375330151612,
          "standard_error": 0.002091707591791494
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.245240137267437,
            "upper_bound": 22.264989410824477
          },
          "point_estimate": 22.2553163138053,
          "standard_error": 0.005058440483202266
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.241653445991677,
            "upper_bound": 22.26876901462175
          },
          "point_estimate": 22.2554631181496,
          "standard_error": 0.00593564487616843
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0039368178000049785,
            "upper_bound": 0.03258595363736975
          },
          "point_estimate": 0.01528725827471232,
          "standard_error": 0.007852347615609412
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.25115372183438,
            "upper_bound": 22.270694905755605
          },
          "point_estimate": 22.260370207358807,
          "standard_error": 0.005022613534746558
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00921823362955663,
            "upper_bound": 0.021142851687040182
          },
          "point_estimate": 0.01685376616705606,
          "standard_error": 0.0030887981897030907
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.026791276338264,
            "upper_bound": 14.089360326403416
          },
          "point_estimate": 14.05898070818123,
          "standard_error": 0.01595822831276474
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.030172822532313,
            "upper_bound": 14.090155991038143
          },
          "point_estimate": 14.064086630642375,
          "standard_error": 0.013239939900091108
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0071069331896937505,
            "upper_bound": 0.08893104404790664
          },
          "point_estimate": 0.030472038036363943,
          "standard_error": 0.02153780652067979
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.048398229649436,
            "upper_bound": 14.095134001861034
          },
          "point_estimate": 14.070111947262346,
          "standard_error": 0.01174911661759875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02164655856295624,
            "upper_bound": 0.07270822175855718
          },
          "point_estimate": 0.0534474720838304,
          "standard_error": 0.012947167961308657
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.358015429232115,
            "upper_bound": 18.402473759566885
          },
          "point_estimate": 18.38152792020215,
          "standard_error": 0.011406370160501849
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.356572061618508,
            "upper_bound": 18.415398390461934
          },
          "point_estimate": 18.38277749276874,
          "standard_error": 0.016192616860363707
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030972868867274643,
            "upper_bound": 0.06292637645271296
          },
          "point_estimate": 0.04640793222460894,
          "standard_error": 0.017269932522283614
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.37881217921,
            "upper_bound": 18.408849370049435
          },
          "point_estimate": 18.394897640295643,
          "standard_error": 0.007808862830997348
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018749096526835297,
            "upper_bound": 0.048723988686840365
          },
          "point_estimate": 0.03785621080477392,
          "standard_error": 0.007773249035272879
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.763554365734286,
            "upper_bound": 10.781203329188283
          },
          "point_estimate": 10.772457732328352,
          "standard_error": 0.004521771824991834
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.761739450390742,
            "upper_bound": 10.782815416471742
          },
          "point_estimate": 10.77289452766891,
          "standard_error": 0.0048434309244292
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001822435999205782,
            "upper_bound": 0.026684978168546956
          },
          "point_estimate": 0.012724207805511253,
          "standard_error": 0.006592953561719234
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.771456024461592,
            "upper_bound": 10.786938254950211
          },
          "point_estimate": 10.778617715912038,
          "standard_error": 0.003964495612477628
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00809641540165818,
            "upper_bound": 0.01944910179570771
          },
          "point_estimate": 0.015076890506735458,
          "standard_error": 0.0029499636544455697
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.76538567661577,
            "upper_bound": 15.799022309497602
          },
          "point_estimate": 15.782884505339403,
          "standard_error": 0.008630639073545481
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.753194886791675,
            "upper_bound": 15.805760175071809
          },
          "point_estimate": 15.794394438401746,
          "standard_error": 0.013986349015727475
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00481406997582182,
            "upper_bound": 0.04704768659512756
          },
          "point_estimate": 0.02315161691248912,
          "standard_error": 0.011560596156567314
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.758390854251074,
            "upper_bound": 15.799417816845503
          },
          "point_estimate": 15.778774254309925,
          "standard_error": 0.01055001506033704
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014573332630378268,
            "upper_bound": 0.03394571712785297
          },
          "point_estimate": 0.02878113810740916,
          "standard_error": 0.004679906177223134
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.305609598101924,
            "upper_bound": 22.370904087247265
          },
          "point_estimate": 22.33261485429485,
          "standard_error": 0.017226967948394557
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.29647397416915,
            "upper_bound": 22.345145431331957
          },
          "point_estimate": 22.314521183018073,
          "standard_error": 0.01368237128340094
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00700777127954814,
            "upper_bound": 0.05720801407681037
          },
          "point_estimate": 0.034169634499297614,
          "standard_error": 0.012697919797614796
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.303139703668553,
            "upper_bound": 22.33777516132621
          },
          "point_estimate": 22.318555788691015,
          "standard_error": 0.008974049636315658
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016342063778477377,
            "upper_bound": 0.08561175733005472
          },
          "point_estimate": 0.05750522520973117,
          "standard_error": 0.022136961230453625
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.006982752133514,
            "upper_bound": 11.023299131133513
          },
          "point_estimate": 11.014982365842856,
          "standard_error": 0.004177286198272531
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.003286681936146,
            "upper_bound": 11.027574182490106
          },
          "point_estimate": 11.014966553547712,
          "standard_error": 0.005726789851274433
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032287570443871277,
            "upper_bound": 0.02352467354526585
          },
          "point_estimate": 0.016841418502598342,
          "standard_error": 0.0051680557447401285
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.00556177127748,
            "upper_bound": 11.02757657754204
          },
          "point_estimate": 11.015054736571363,
          "standard_error": 0.005622219471940335
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00789665724695614,
            "upper_bound": 0.017466593904512023
          },
          "point_estimate": 0.013895159026311749,
          "standard_error": 0.0024537922271716736
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.972480078369227,
            "upper_bound": 24.01002156040971
          },
          "point_estimate": 23.99225542736021,
          "standard_error": 0.009620407991685646
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.96930535575418,
            "upper_bound": 24.022907186407483
          },
          "point_estimate": 24.00064502956108,
          "standard_error": 0.014543440084924362
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004077888494827375,
            "upper_bound": 0.05448865805595069
          },
          "point_estimate": 0.033216027567287205,
          "standard_error": 0.012703148904242902
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.98005183701208,
            "upper_bound": 24.00616271146787
          },
          "point_estimate": 23.99489885171306,
          "standard_error": 0.006601132423405108
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017276414320528444,
            "upper_bound": 0.040935198990150405
          },
          "point_estimate": 0.03211941524647505,
          "standard_error": 0.006347490697963274
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.72165811192164,
            "upper_bound": 20.849823818423364
          },
          "point_estimate": 20.782624439846455,
          "standard_error": 0.03282006669862972
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.73103609511319,
            "upper_bound": 20.852370245766096
          },
          "point_estimate": 20.749147000763013,
          "standard_error": 0.032563697449443395
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006450215885945544,
            "upper_bound": 0.18244782299496212
          },
          "point_estimate": 0.033562112992567415,
          "standard_error": 0.05340183651424653
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.67245107213633,
            "upper_bound": 20.819591045990386
          },
          "point_estimate": 20.749839345235937,
          "standard_error": 0.038839922251914655
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.045526331281989525,
            "upper_bound": 0.14435648529419282
          },
          "point_estimate": 0.10934503493092984,
          "standard_error": 0.024878195178200366
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1173675.169969278,
            "upper_bound": 1175213.9559395162
          },
          "point_estimate": 1174426.8900204813,
          "standard_error": 394.8440309755651
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1173392.564516129,
            "upper_bound": 1175683.0
          },
          "point_estimate": 1174146.9128264207,
          "standard_error": 506.7380257223447
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.31898560739015,
            "upper_bound": 2565.695962191743
          },
          "point_estimate": 1363.6955696607388,
          "standard_error": 587.2659268135318
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1173122.660017883,
            "upper_bound": 1174907.752688172
          },
          "point_estimate": 1173974.2585672392,
          "standard_error": 466.90401379776574
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 732.1993682445657,
            "upper_bound": 1645.5599431146036
          },
          "point_estimate": 1318.3308060650245,
          "standard_error": 232.14124155036671
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1576748.2813690477,
            "upper_bound": 1579142.3629794973
          },
          "point_estimate": 1577890.906625331,
          "standard_error": 613.2656942633585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1576563.5625,
            "upper_bound": 1579574.4791666667
          },
          "point_estimate": 1577350.7532407409,
          "standard_error": 719.182072318074
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321.33115085768566,
            "upper_bound": 3407.4574603387173
          },
          "point_estimate": 1392.6517143379322,
          "standard_error": 855.2408486269563
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1577161.9003229977,
            "upper_bound": 1578606.58692296
          },
          "point_estimate": 1577665.4557359307,
          "standard_error": 369.9891864969123
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 976.012391427146,
            "upper_bound": 2661.5439633897577
          },
          "point_estimate": 2042.636063148289,
          "standard_error": 434.0715289434079
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1448868.707591575,
            "upper_bound": 1450920.7548728634
          },
          "point_estimate": 1449831.135804335,
          "standard_error": 528.7202433911165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1448541.264423077,
            "upper_bound": 1451080.8769230768
          },
          "point_estimate": 1449324.5747863248,
          "standard_error": 612.1154372631946
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286.55724875883675,
            "upper_bound": 2894.5483563035896
          },
          "point_estimate": 1270.3803373820992,
          "standard_error": 720.7862986065396
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1448928.17535153,
            "upper_bound": 1451241.0085358506
          },
          "point_estimate": 1449976.522877123,
          "standard_error": 601.0073472450149
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 723.3046969078772,
            "upper_bound": 2199.723633479768
          },
          "point_estimate": 1759.942552860825,
          "standard_error": 361.8718492430783
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397806.4241399241,
            "upper_bound": 398313.0731748619
          },
          "point_estimate": 398042.9679805038,
          "standard_error": 129.62793163112502
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397665.37173913047,
            "upper_bound": 398218.8206521739
          },
          "point_estimate": 398127.4616545894,
          "standard_error": 174.09174361968033
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.957684160877175,
            "upper_bound": 766.6500291066652
          },
          "point_estimate": 361.9630856391004,
          "standard_error": 197.10223076945712
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397720.60792063543,
            "upper_bound": 398219.4572560409
          },
          "point_estimate": 397951.019282891,
          "standard_error": 131.25621110917703
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221.7974036166672,
            "upper_bound": 586.1152942845666
          },
          "point_estimate": 431.2234918923603,
          "standard_error": 106.49100897906436
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 558205.2560431698,
            "upper_bound": 559340.4827741854
          },
          "point_estimate": 558764.5069017557,
          "standard_error": 290.80140906229224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 558073.8272727273,
            "upper_bound": 559325.7269570706
          },
          "point_estimate": 558870.6623737374,
          "standard_error": 329.6989235195598
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135.75146487512995,
            "upper_bound": 1691.9568489391334
          },
          "point_estimate": 772.3901938630455,
          "standard_error": 377.4184470440349
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 558273.6848231446,
            "upper_bound": 559052.1080778196
          },
          "point_estimate": 558671.5812278631,
          "standard_error": 206.37695532553164
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 475.1726871058481,
            "upper_bound": 1300.6438112242552
          },
          "point_estimate": 971.8553460219448,
          "standard_error": 213.06284028996112
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 428295.01812867646,
            "upper_bound": 428822.3348543417
          },
          "point_estimate": 428567.96533473395,
          "standard_error": 135.7608738274714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 428055.6808823529,
            "upper_bound": 428933.8338235294
          },
          "point_estimate": 428713.96456582635,
          "standard_error": 203.4309167325324
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.21957124830371,
            "upper_bound": 751.629057361871
          },
          "point_estimate": 414.5349526405421,
          "standard_error": 187.83864746947137
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 428312.621663942,
            "upper_bound": 428960.5666973039
          },
          "point_estimate": 428695.0645072574,
          "standard_error": 167.19153344732587
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232.14086962734217,
            "upper_bound": 543.075848453021
          },
          "point_estimate": 451.98665307618217,
          "standard_error": 74.25302581959615
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481987.76811642386,
            "upper_bound": 482696.1189803936
          },
          "point_estimate": 482319.9992298454,
          "standard_error": 182.0561169283096
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481909.36842105264,
            "upper_bound": 482741.975877193
          },
          "point_estimate": 482248.4024906015,
          "standard_error": 186.1030171725065
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.30043049901724,
            "upper_bound": 978.2266155277445
          },
          "point_estimate": 558.2591555494503,
          "standard_error": 235.48314900939735
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482129.9971746852,
            "upper_bound": 483169.29627473286
          },
          "point_estimate": 482706.87997265894,
          "standard_error": 275.2726799325323
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269.2427799087669,
            "upper_bound": 823.6102607806536
          },
          "point_estimate": 606.3934032624907,
          "standard_error": 150.08604527134182
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1007869.301334513,
            "upper_bound": 1011333.208386449
          },
          "point_estimate": 1009384.3863845988,
          "standard_error": 896.6760784329999
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1007340.027027027,
            "upper_bound": 1010799.527027027
          },
          "point_estimate": 1008538.5888513512,
          "standard_error": 759.6974351637726
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 426.84969134077016,
            "upper_bound": 3961.983687611155
          },
          "point_estimate": 1760.2328468577712,
          "standard_error": 910.8217818119748
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1007422.1551086552,
            "upper_bound": 1009585.7747983586
          },
          "point_estimate": 1008535.7933309934,
          "standard_error": 567.5447638523035
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 884.4081482970978,
            "upper_bound": 4171.588295176566
          },
          "point_estimate": 2978.7865657906996,
          "standard_error": 924.9084025649204
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297667.2392770923,
            "upper_bound": 298084.5768244693
          },
          "point_estimate": 297868.7070651052,
          "standard_error": 106.2557046836394
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297595.9055555556,
            "upper_bound": 298039.90589430893
          },
          "point_estimate": 297910.00152439025,
          "standard_error": 102.85446407739222
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.410743064652737,
            "upper_bound": 599.031303389464
          },
          "point_estimate": 301.9595261364287,
          "standard_error": 150.76720072176423
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297579.7777052939,
            "upper_bound": 297926.7162651709
          },
          "point_estimate": 297763.0637947418,
          "standard_error": 88.53749326420943
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164.8378065466098,
            "upper_bound": 478.91713277806485
          },
          "point_estimate": 355.2210172609064,
          "standard_error": 82.80068596933457
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 813291.3376773149,
            "upper_bound": 814876.1329320988
          },
          "point_estimate": 814016.4221913579,
          "standard_error": 406.5784533158568
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 812941.4333333333,
            "upper_bound": 814398.1711111111
          },
          "point_estimate": 814133.4886111112,
          "standard_error": 386.3399576263011
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.58473026753508,
            "upper_bound": 2126.8771081849027
          },
          "point_estimate": 670.168134768825,
          "standard_error": 526.7490973262178
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 813537.9519063275,
            "upper_bound": 814446.4963381739
          },
          "point_estimate": 814086.7691197691,
          "standard_error": 230.48576581283027
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 566.4307674827679,
            "upper_bound": 1914.378072671238
          },
          "point_estimate": 1356.750158583652,
          "standard_error": 398.2092253015445
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393446.7547806152,
            "upper_bound": 394183.7928682796
          },
          "point_estimate": 393807.3769747397,
          "standard_error": 187.71821584301216
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393409.88040621264,
            "upper_bound": 394124.3467741936
          },
          "point_estimate": 393786.8785714286,
          "standard_error": 134.88289508021833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.50304612618365,
            "upper_bound": 1061.2094575038593
          },
          "point_estimate": 244.18926394436505,
          "standard_error": 307.3353555597678
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393308.3342952352,
            "upper_bound": 393848.140688907
          },
          "point_estimate": 393617.3761206535,
          "standard_error": 144.3589792285946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.31654960900676,
            "upper_bound": 851.1233351682552
          },
          "point_estimate": 626.0919373836016,
          "standard_error": 150.27794071684974
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168867.33977999262,
            "upper_bound": 169154.59515227025
          },
          "point_estimate": 169012.32838796603,
          "standard_error": 73.37866801644765
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168794.2569767442,
            "upper_bound": 169188.58255813952
          },
          "point_estimate": 169039.01640826874,
          "standard_error": 110.345464225917
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.21070186851899,
            "upper_bound": 406.45909266762646
          },
          "point_estimate": 252.6003648952687,
          "standard_error": 90.76462510115628
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168808.23205257836,
            "upper_bound": 169142.07259661632
          },
          "point_estimate": 169001.19572334643,
          "standard_error": 86.33624696067584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.4449582466286,
            "upper_bound": 305.92932900698025
          },
          "point_estimate": 244.5385955043084,
          "standard_error": 41.53632790277838
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232036.66690474295,
            "upper_bound": 232525.55625667272
          },
          "point_estimate": 232258.085188808,
          "standard_error": 126.09374808586584
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231933.78439490445,
            "upper_bound": 232457.56050955417
          },
          "point_estimate": 232181.10469745225,
          "standard_error": 150.7073360166659
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.72612782023646,
            "upper_bound": 664.6041222614022
          },
          "point_estimate": 386.2476466459236,
          "standard_error": 136.7800388349808
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231938.8249889559,
            "upper_bound": 232317.1182470396
          },
          "point_estimate": 232122.4939531806,
          "standard_error": 99.38380422776696
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.79837798060908,
            "upper_bound": 580.4493698447817
          },
          "point_estimate": 419.3653485925457,
          "standard_error": 114.3329351856313
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174524.6801414711,
            "upper_bound": 174841.62428353078
          },
          "point_estimate": 174688.43167217285,
          "standard_error": 80.84069981718098
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174542.90962918662,
            "upper_bound": 174894.33660287084
          },
          "point_estimate": 174702.3036758563,
          "standard_error": 81.78641953742527
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.97334998969431,
            "upper_bound": 457.1298193364798
          },
          "point_estimate": 189.64688203978687,
          "standard_error": 107.71695934600385
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174580.79784562925,
            "upper_bound": 174825.343742142
          },
          "point_estimate": 174708.1267880445,
          "standard_error": 61.13055721707701
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.14288710115588,
            "upper_bound": 360.79435819606726
          },
          "point_estimate": 269.85770154468133,
          "standard_error": 60.84950723088786
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649227.5587372448,
            "upper_bound": 650770.0720804104
          },
          "point_estimate": 649858.6532759353,
          "standard_error": 408.75916997541594
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649129.5788690476,
            "upper_bound": 650157.5785714285
          },
          "point_estimate": 649426.8314732143,
          "standard_error": 284.32010369139965
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.90295067418624,
            "upper_bound": 1269.515939961636
          },
          "point_estimate": 479.67183523411853,
          "standard_error": 312.62028830805644
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649166.305375437,
            "upper_bound": 649664.4725699936
          },
          "point_estimate": 649383.2192949908,
          "standard_error": 129.29697403435736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 339.3556200370147,
            "upper_bound": 2030.740608130568
          },
          "point_estimate": 1361.132957696283,
          "standard_error": 534.6985423623427
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1312.034604678218,
            "upper_bound": 1313.9904447254055
          },
          "point_estimate": 1312.9637916895074,
          "standard_error": 0.5014620973260328
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1312.0190834480377,
            "upper_bound": 1313.6155004631132
          },
          "point_estimate": 1312.96800789097,
          "standard_error": 0.4127004563390758
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14052848095923665,
            "upper_bound": 2.562793712214788
          },
          "point_estimate": 1.0276590610621532,
          "standard_error": 0.5591249959122808
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1312.1314032862674,
            "upper_bound": 1312.9642285611087
          },
          "point_estimate": 1312.5090910965546,
          "standard_error": 0.211046750506038
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5945055222983578,
            "upper_bound": 2.366007858603386
          },
          "point_estimate": 1.674086086233396,
          "standard_error": 0.4653301906771689
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1314904.46934949,
            "upper_bound": 1316941.315099206
          },
          "point_estimate": 1315880.2457383785,
          "standard_error": 522.5493976110878
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1314548.71875,
            "upper_bound": 1317051.5099206348
          },
          "point_estimate": 1315757.7049319728,
          "standard_error": 655.7634507725428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 422.9193643667379,
            "upper_bound": 3040.2427031676984
          },
          "point_estimate": 1517.0181349425543,
          "standard_error": 627.5187748436093
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1315041.125867979,
            "upper_bound": 1317980.8475580232
          },
          "point_estimate": 1316669.756122449,
          "standard_error": 779.5855959547993
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 916.5686911447248,
            "upper_bound": 2272.6402204440733
          },
          "point_estimate": 1741.383577263122,
          "standard_error": 359.0012256110385
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1512642.9331555555,
            "upper_bound": 1515098.0984086508
          },
          "point_estimate": 1513897.206869841,
          "standard_error": 627.011237619381
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1512366.3022222223,
            "upper_bound": 1515496.365
          },
          "point_estimate": 1514051.691666667,
          "standard_error": 805.7737741133478
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274.0466031347943,
            "upper_bound": 3490.591371029496
          },
          "point_estimate": 1946.827912103538,
          "standard_error": 810.4033148020193
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1513069.137509319,
            "upper_bound": 1514935.492707009
          },
          "point_estimate": 1513845.401142857,
          "standard_error": 474.7626675943285
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1107.5794333179456,
            "upper_bound": 2730.191865683625
          },
          "point_estimate": 2091.64585808212,
          "standard_error": 418.9185870398348
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1416519.431655983,
            "upper_bound": 1420264.9153739316
          },
          "point_estimate": 1418115.5531944442,
          "standard_error": 980.7958732134962
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415685.5512820513,
            "upper_bound": 1419048.1923076925
          },
          "point_estimate": 1417174.4860576922,
          "standard_error": 834.3060217231048
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120.1666286358024,
            "upper_bound": 3834.1765019296336
          },
          "point_estimate": 2267.577973492317,
          "standard_error": 917.2475551030049
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1416054.8380687267,
            "upper_bound": 1417682.4493698392
          },
          "point_estimate": 1416774.223976024,
          "standard_error": 423.35692364924745
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 923.6675317006312,
            "upper_bound": 4756.364168020455
          },
          "point_estimate": 3268.802093326774,
          "standard_error": 1140.4343836030985
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 753147.5629988662,
            "upper_bound": 754291.0915160349
          },
          "point_estimate": 753694.1192670877,
          "standard_error": 293.4432155047226
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752780.693877551,
            "upper_bound": 754538.1749271137
          },
          "point_estimate": 753542.9068027211,
          "standard_error": 481.8311997044699
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.4143999121842,
            "upper_bound": 1593.47240895501
          },
          "point_estimate": 1148.4516924680786,
          "standard_error": 430.9642774453968
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752920.8822115385,
            "upper_bound": 754411.7809707479
          },
          "point_estimate": 753538.6309567983,
          "standard_error": 387.5689241829648
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 513.2346036758225,
            "upper_bound": 1161.7269584454432
          },
          "point_estimate": 974.2420567656668,
          "standard_error": 158.25993500412335
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266245.32900130353,
            "upper_bound": 266501.8768534063
          },
          "point_estimate": 266373.9782864094,
          "standard_error": 65.35071465247884
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266245.34732360096,
            "upper_bound": 266547.4338894682
          },
          "point_estimate": 266328.3729927007,
          "standard_error": 85.86679424606392
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.93423474578884,
            "upper_bound": 393.7592539582956
          },
          "point_estimate": 194.86954967908343,
          "standard_error": 92.63850965680696
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266197.62024139264,
            "upper_bound": 266470.60362348665
          },
          "point_estimate": 266342.34845009004,
          "standard_error": 69.91203643049523
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.17539202880916,
            "upper_bound": 285.6684374843935
          },
          "point_estimate": 218.38329798473225,
          "standard_error": 43.38326334092738
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477928.06701659446,
            "upper_bound": 478879.6782451428
          },
          "point_estimate": 478363.7274989693,
          "standard_error": 243.90903430551847
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477687.32467532466,
            "upper_bound": 478858.6363636363
          },
          "point_estimate": 478146.8741883117,
          "standard_error": 303.2560235476957
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.47471591867333,
            "upper_bound": 1338.6792287336905
          },
          "point_estimate": 769.2568642975228,
          "standard_error": 284.81505390208923
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477749.1156805125,
            "upper_bound": 478607.3332230029
          },
          "point_estimate": 478131.32406813966,
          "standard_error": 226.36796694587147
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.665149777875,
            "upper_bound": 1099.850176724792
          },
          "point_estimate": 811.4788926269888,
          "standard_error": 204.7529155421923
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240810.06664427623,
            "upper_bound": 241214.57528697568
          },
          "point_estimate": 241005.82538552507,
          "standard_error": 103.43714445292612
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240736.1407284768,
            "upper_bound": 241210.86912251657
          },
          "point_estimate": 240991.9017660044,
          "standard_error": 129.8493989852571
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.048335337616905,
            "upper_bound": 603.7986578234683
          },
          "point_estimate": 312.74770835491364,
          "standard_error": 131.9787500881418
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240947.7687675008,
            "upper_bound": 241463.556748513
          },
          "point_estimate": 241216.7165562914,
          "standard_error": 140.19511056239963
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.71323357345193,
            "upper_bound": 457.91618629958833
          },
          "point_estimate": 345.11252529053496,
          "standard_error": 74.55928780006538
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 691639.8569664196,
            "upper_bound": 692387.3933456873
          },
          "point_estimate": 692013.1047648997,
          "standard_error": 191.1841288604608
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 691526.7531446541,
            "upper_bound": 692612.1761006289
          },
          "point_estimate": 691903.4246630727,
          "standard_error": 244.41920748113003
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.1147244978107,
            "upper_bound": 1189.3160564325406
          },
          "point_estimate": 619.8899682400225,
          "standard_error": 304.13459048469633
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 691425.279470233,
            "upper_bound": 692511.4886497641
          },
          "point_estimate": 691953.8371967655,
          "standard_error": 278.1113670105678
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 362.857511160516,
            "upper_bound": 795.7509904718709
          },
          "point_estimate": 636.6336231980326,
          "standard_error": 111.29638490466483
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2296.1087050525884,
            "upper_bound": 2301.1517928929884
          },
          "point_estimate": 2298.3062293973444,
          "standard_error": 1.3062100519698627
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2295.552175679669,
            "upper_bound": 2300.522960879831
          },
          "point_estimate": 2296.4569243920973,
          "standard_error": 1.0715429024163603
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09702805678831236,
            "upper_bound": 5.481246744127069
          },
          "point_estimate": 2.495354206268732,
          "standard_error": 1.3915311188218542
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2296.0736961981715,
            "upper_bound": 2299.9213066308394
          },
          "point_estimate": 2297.597325293754,
          "standard_error": 1.0330074423481816
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0977094601120367,
            "upper_bound": 6.095653455901657
          },
          "point_estimate": 4.352659305350829,
          "standard_error": 1.390682864129232
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.9647651263024,
            "upper_bound": 705.5926250795376
          },
          "point_estimate": 704.7454586544864,
          "standard_error": 0.4170168972317121
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.5507015319872,
            "upper_bound": 706.0676321355116
          },
          "point_estimate": 704.4922047809383,
          "standard_error": 0.6935941603971265
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.21420718797772645,
            "upper_bound": 2.3275719963486257
          },
          "point_estimate": 1.478259734029599,
          "standard_error": 0.5427976220816684
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 704.251250541186,
            "upper_bound": 705.9080889658085
          },
          "point_estimate": 705.0325328984338,
          "standard_error": 0.42002284979445065
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.80877633283925,
            "upper_bound": 1.713274073088822
          },
          "point_estimate": 1.3894582418783672,
          "standard_error": 0.23606226679893225
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.88912413321916,
            "upper_bound": 141.1755841108369
          },
          "point_estimate": 141.0171197110063,
          "standard_error": 0.07367308918966865
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.84173928317273,
            "upper_bound": 141.10644501358047
          },
          "point_estimate": 140.99892286054143,
          "standard_error": 0.07888339282139775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009824889070071096,
            "upper_bound": 0.34143814359574504
          },
          "point_estimate": 0.22486548628064756,
          "standard_error": 0.07944084567553286
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.89278627496327,
            "upper_bound": 141.1561715325866
          },
          "point_estimate": 140.9986061495374,
          "standard_error": 0.06700615625381634
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1000722364694144,
            "upper_bound": 0.3474133291060688
          },
          "point_estimate": 0.2455546775912633,
          "standard_error": 0.07322439958410389
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.48534658988087,
            "upper_bound": 52.58314429623543
          },
          "point_estimate": 52.53339831004447,
          "standard_error": 0.02491885605047922
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.45218829520225,
            "upper_bound": 52.57117747915858
          },
          "point_estimate": 52.54655214426715,
          "standard_error": 0.026666126792410275
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00277855037437057,
            "upper_bound": 0.15084897363364447
          },
          "point_estimate": 0.04439642647718869,
          "standard_error": 0.0413851876110225
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.50623106540364,
            "upper_bound": 52.56264488888207
          },
          "point_estimate": 52.54243597985055,
          "standard_error": 0.014378566871807129
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04154315733970541,
            "upper_bound": 0.110469059111071
          },
          "point_estimate": 0.08306104270089698,
          "standard_error": 0.01839472548276762
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.03194725662419,
            "upper_bound": 81.12585511113271
          },
          "point_estimate": 81.08497938343555,
          "standard_error": 0.024333911603571696
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.05968216996533,
            "upper_bound": 81.13711686631396
          },
          "point_estimate": 81.10295390025988,
          "standard_error": 0.021706894493510848
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004685619785694591,
            "upper_bound": 0.09994950073093932
          },
          "point_estimate": 0.04756082611430929,
          "standard_error": 0.02281045438508321
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.04790510948854,
            "upper_bound": 81.11040887314724
          },
          "point_estimate": 81.08063076090126,
          "standard_error": 0.015638942434303787
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028974696693504567,
            "upper_bound": 0.11631978498447744
          },
          "point_estimate": 0.08105592633579414,
          "standard_error": 0.02626772884475991
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287181.2147498125,
            "upper_bound": 287713.3215198022
          },
          "point_estimate": 287440.55905793025,
          "standard_error": 136.9037620910728
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287074.07716535433,
            "upper_bound": 287724.2707677166
          },
          "point_estimate": 287395.2965879265,
          "standard_error": 170.19308519225635
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.42668618675064,
            "upper_bound": 765.0941598814189
          },
          "point_estimate": 471.53022044754226,
          "standard_error": 174.7177006597598
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287317.6502571842,
            "upper_bound": 287651.4174610602
          },
          "point_estimate": 287516.9096840168,
          "standard_error": 85.00566038509585
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.62571072483755,
            "upper_bound": 600.9171113523802
          },
          "point_estimate": 455.90024792686825,
          "standard_error": 96.17803950439652
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.18404154298144,
            "upper_bound": 63.26556739230777
          },
          "point_estimate": 63.223917318135136,
          "standard_error": 0.020823579097472636
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.17760814131803,
            "upper_bound": 63.25939705760055
          },
          "point_estimate": 63.22964151688722,
          "standard_error": 0.018532786734893635
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006001680391687142,
            "upper_bound": 0.12328286199018426
          },
          "point_estimate": 0.04474958772354484,
          "standard_error": 0.03294085574892785
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.18774487637399,
            "upper_bound": 63.23635853402774
          },
          "point_estimate": 63.21711970299764,
          "standard_error": 0.012336746275115768
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03252752001808661,
            "upper_bound": 0.09323864606933432
          },
          "point_estimate": 0.06938515152170685,
          "standard_error": 0.015801817580222224
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.9718056743342,
            "upper_bound": 106.14947083489548
          },
          "point_estimate": 106.06191157653475,
          "standard_error": 0.04522833118718355
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.98651593753732,
            "upper_bound": 106.16537390514084
          },
          "point_estimate": 106.05104397829028,
          "standard_error": 0.031840641592339264
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0073376934635653165,
            "upper_bound": 0.27528475226662175
          },
          "point_estimate": 0.048664292527700784,
          "standard_error": 0.07428714656291906
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.99290080264323,
            "upper_bound": 106.17331660165443
          },
          "point_estimate": 106.08642363872492,
          "standard_error": 0.048529719672474984
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.060027563139519874,
            "upper_bound": 0.20686977409322949
          },
          "point_estimate": 0.15127445246776827,
          "standard_error": 0.03663508094325613
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126717.692026962,
            "upper_bound": 126931.40690040652
          },
          "point_estimate": 126827.14215861956,
          "standard_error": 54.861269228843405
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126669.59268292684,
            "upper_bound": 126962.85133565622
          },
          "point_estimate": 126892.93402190144,
          "standard_error": 84.89643859916274
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.566937965709773,
            "upper_bound": 310.42337302548486
          },
          "point_estimate": 189.32026785840637,
          "standard_error": 81.87231842728593
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126758.8879088474,
            "upper_bound": 126921.12523269105
          },
          "point_estimate": 126848.36698493145,
          "standard_error": 43.33774693101267
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.29314486186406,
            "upper_bound": 228.38951485958836
          },
          "point_estimate": 182.7409356061376,
          "standard_error": 31.30989135251655
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.04739865457168,
            "upper_bound": 61.10877653643803
          },
          "point_estimate": 61.07676314694049,
          "standard_error": 0.01571596962532179
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.03625444428568,
            "upper_bound": 61.11480367394283
          },
          "point_estimate": 61.057688118912935,
          "standard_error": 0.019627421026809044
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003202288193414436,
            "upper_bound": 0.08756267905365571
          },
          "point_estimate": 0.052793783366724543,
          "standard_error": 0.022988786312034257
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.03245606710315,
            "upper_bound": 61.108341305841144
          },
          "point_estimate": 61.064836428564334,
          "standard_error": 0.01967617158304826
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02615330804912388,
            "upper_bound": 0.06667400250514986
          },
          "point_estimate": 0.052543423484257175,
          "standard_error": 0.010374663642541509
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.25805967155034,
            "upper_bound": 105.24511262312409
          },
          "point_estimate": 104.22669079626472,
          "standard_error": 0.5071136053288002
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.8329929654154,
            "upper_bound": 106.02177443255512
          },
          "point_estimate": 103.34509388167452,
          "standard_error": 1.026701364686459
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2202264296661444,
            "upper_bound": 2.6074126235419324
          },
          "point_estimate": 1.1856948395237812,
          "standard_error": 0.7007058868758204
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.28519453150484,
            "upper_bound": 106.02046092542248
          },
          "point_estimate": 104.87862304946756,
          "standard_error": 0.6972397023326998
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0435008039681768,
            "upper_bound": 1.8762270904868648
          },
          "point_estimate": 1.682674336927223,
          "standard_error": 0.21585001887883232
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.659227318246,
            "upper_bound": 255.08807370393353
          },
          "point_estimate": 254.85927799880005,
          "standard_error": 0.10952636380426714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.6922836803245,
            "upper_bound": 255.0080732359501
          },
          "point_estimate": 254.78824307282724,
          "standard_error": 0.08434895736794101
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0439515967956366,
            "upper_bound": 0.5681650661498999
          },
          "point_estimate": 0.15630885145361556,
          "standard_error": 0.13533672169221497
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.7229111974665,
            "upper_bound": 255.03575588403845
          },
          "point_estimate": 254.86908495854863,
          "standard_error": 0.085345288909245
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12769630237280402,
            "upper_bound": 0.513967387197249
          },
          "point_estimate": 0.3647229081823433,
          "standard_error": 0.10301288459564906
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 523.5524279372856,
            "upper_bound": 524.2312261287091
          },
          "point_estimate": 523.8727678927447,
          "standard_error": 0.1744312867860902
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 523.3647659539423,
            "upper_bound": 524.2774927337802
          },
          "point_estimate": 523.741911075881,
          "standard_error": 0.250990490615982
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11000947055537803,
            "upper_bound": 0.9766997255440713
          },
          "point_estimate": 0.5620216762656435,
          "standard_error": 0.219933600442488
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 523.440720375216,
            "upper_bound": 524.0506349564259
          },
          "point_estimate": 523.7327481425399,
          "standard_error": 0.15375358355769103
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.29550439611371054,
            "upper_bound": 0.7393782500241196
          },
          "point_estimate": 0.5805951940636409,
          "standard_error": 0.11622783918094816
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.83715285078595,
            "upper_bound": 31.87350800860676
          },
          "point_estimate": 31.85501296822514,
          "standard_error": 0.009309983264998808
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.830799913346205,
            "upper_bound": 31.8909718191842
          },
          "point_estimate": 31.847621062188132,
          "standard_error": 0.014794081452889804
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006780923735162886,
            "upper_bound": 0.05136177798593588
          },
          "point_estimate": 0.028646211674289133,
          "standard_error": 0.012355942023825743
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.83869702075044,
            "upper_bound": 31.873721591935787
          },
          "point_estimate": 31.852812515682217,
          "standard_error": 0.008933414007399354
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017816303567969382,
            "upper_bound": 0.03726303703269595
          },
          "point_estimate": 0.03104966639778117,
          "standard_error": 0.0048072081684664035
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32447.155244785696,
            "upper_bound": 32544.550087241405
          },
          "point_estimate": 32498.39413831188,
          "standard_error": 24.89221543931352
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32457.79090098127,
            "upper_bound": 32540.545941123997
          },
          "point_estimate": 32512.535236396077,
          "standard_error": 20.63992546277537
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.323919343749424,
            "upper_bound": 135.85451953279326
          },
          "point_estimate": 42.1898601787203,
          "standard_error": 32.196376502735724
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32466.499762621148,
            "upper_bound": 32536.810301198897
          },
          "point_estimate": 32503.26903622693,
          "standard_error": 18.42985124740623
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.508048753702553,
            "upper_bound": 115.57519620222416
          },
          "point_estimate": 82.89030150507688,
          "standard_error": 22.357528021760743
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142.44253739734623,
            "upper_bound": 142.6860271718946
          },
          "point_estimate": 142.55306203607427,
          "standard_error": 0.06292339983328546
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142.41089066290994,
            "upper_bound": 142.69292935222143
          },
          "point_estimate": 142.52175530249667,
          "standard_error": 0.05596535775040187
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01273261611580459,
            "upper_bound": 0.31003614817032865
          },
          "point_estimate": 0.12338095500476558,
          "standard_error": 0.07649589583873087
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142.38309953239812,
            "upper_bound": 142.5421162814834
          },
          "point_estimate": 142.44646348095046,
          "standard_error": 0.04112665935411669
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06854497160580043,
            "upper_bound": 0.2740566875080013
          },
          "point_estimate": 0.2094076774029687,
          "standard_error": 0.05409896618981068
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1004430.1291689188,
            "upper_bound": 1006859.0513513512
          },
          "point_estimate": 1005754.0401930502,
          "standard_error": 625.8036504694342
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1004384.5445945946,
            "upper_bound": 1007226.3513513514
          },
          "point_estimate": 1006560.5482625484,
          "standard_error": 732.3583955046639
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.70570582323705,
            "upper_bound": 3196.5175994666843
          },
          "point_estimate": 1044.6539660482392,
          "standard_error": 777.7837400898661
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1005606.1427090808,
            "upper_bound": 1007252.3914369816
          },
          "point_estimate": 1006536.116953317,
          "standard_error": 423.0255604341838
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 676.1428023327403,
            "upper_bound": 2701.766442814312
          },
          "point_estimate": 2079.5950594969454,
          "standard_error": 523.5919618581424
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1985.3308115721509,
            "upper_bound": 1988.431725837895
          },
          "point_estimate": 1986.8649897713108,
          "standard_error": 0.7858389686328393
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1984.4964848020993,
            "upper_bound": 1989.2572787479305
          },
          "point_estimate": 1986.638551765192,
          "standard_error": 1.1978239052891364
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2543000993558835,
            "upper_bound": 4.358537546718475
          },
          "point_estimate": 2.8723608718131235,
          "standard_error": 1.0223790499530443
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1986.2259235643005,
            "upper_bound": 1989.0867517747545
          },
          "point_estimate": 1987.5037301098205,
          "standard_error": 0.7360187454949072
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.551810221506717,
            "upper_bound": 3.2421787734975194
          },
          "point_estimate": 2.621246654919891,
          "standard_error": 0.4378748515683321
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.85300722344374,
            "upper_bound": 45.93802911317935
          },
          "point_estimate": 45.88762550774761,
          "standard_error": 0.022743586186282503
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.84851326041388,
            "upper_bound": 45.89737001266325
          },
          "point_estimate": 45.864814114800886,
          "standard_error": 0.012377283089429198
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003057353968665175,
            "upper_bound": 0.06202230075888144
          },
          "point_estimate": 0.03106983507812438,
          "standard_error": 0.01565708987072614
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.85280271603713,
            "upper_bound": 45.879849195557384
          },
          "point_estimate": 45.865211259158606,
          "standard_error": 0.006829569578842196
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017215617254411873,
            "upper_bound": 0.11468323767106148
          },
          "point_estimate": 0.07627694720223237,
          "standard_error": 0.03144579442246495
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.9521537218361,
            "upper_bound": 49.99232371441672
          },
          "point_estimate": 49.97192518845655,
          "standard_error": 0.010312908601162067
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.94551364281961,
            "upper_bound": 50.00055724770944
          },
          "point_estimate": 49.96805067521957,
          "standard_error": 0.013140634226838848
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006777309683411635,
            "upper_bound": 0.05830483383278408
          },
          "point_estimate": 0.030012703579232733,
          "standard_error": 0.013291686635433056
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.941261412711405,
            "upper_bound": 49.98392427196035
          },
          "point_estimate": 49.95909262926274,
          "standard_error": 0.01073782579683959
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0186684595732852,
            "upper_bound": 0.04384941912124975
          },
          "point_estimate": 0.034510151174960986,
          "standard_error": 0.0064388046792336565
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.42303788744128,
            "upper_bound": 36.46535651362769
          },
          "point_estimate": 36.442836648345846,
          "standard_error": 0.010878728505229756
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.4132281091323,
            "upper_bound": 36.46963549709746
          },
          "point_estimate": 36.439291468838306,
          "standard_error": 0.012136326521874803
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0051077840451470265,
            "upper_bound": 0.06338361319010939
          },
          "point_estimate": 0.02787027811465819,
          "standard_error": 0.014512977575166055
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.41973436547426,
            "upper_bound": 36.4576758771477
          },
          "point_estimate": 36.43591682861762,
          "standard_error": 0.00977389070344948
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015677253748568074,
            "upper_bound": 0.04664267798865705
          },
          "point_estimate": 0.03625892190281321,
          "standard_error": 0.008071666397023319
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.890993526108765,
            "upper_bound": 40.9630146517275
          },
          "point_estimate": 40.92816713733472,
          "standard_error": 0.018357388997536216
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.891426439975305,
            "upper_bound": 40.96864341629811
          },
          "point_estimate": 40.93052265562218,
          "standard_error": 0.01546648827932696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001982110031296048,
            "upper_bound": 0.10870796418729942
          },
          "point_estimate": 0.03500579799646046,
          "standard_error": 0.031246583545873816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.8889801601487,
            "upper_bound": 40.942864398505805
          },
          "point_estimate": 40.920326348447986,
          "standard_error": 0.01386965361305614
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028731974086945868,
            "upper_bound": 0.08079152852542687
          },
          "point_estimate": 0.061189598092898215,
          "standard_error": 0.013430664687789282
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.40169306528682,
            "upper_bound": 48.46113352345193
          },
          "point_estimate": 48.429974603493505,
          "standard_error": 0.0152635744513228
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.39124381640956,
            "upper_bound": 48.47764593805633
          },
          "point_estimate": 48.41537395587464,
          "standard_error": 0.021098453133222988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011932183297431837,
            "upper_bound": 0.08077365808953102
          },
          "point_estimate": 0.04363565231424838,
          "standard_error": 0.019829578860597927
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.384477911854766,
            "upper_bound": 48.466832238952115
          },
          "point_estimate": 48.41363692176347,
          "standard_error": 0.021214745695047957
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023392645041944257,
            "upper_bound": 0.06193251957908713
          },
          "point_estimate": 0.05093210085665868,
          "standard_error": 0.009081750376771678
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.68703559185867,
            "upper_bound": 74.75404866649991
          },
          "point_estimate": 74.72262528363828,
          "standard_error": 0.017213149091396755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.68771950932307,
            "upper_bound": 74.77061599380949
          },
          "point_estimate": 74.73338005959391,
          "standard_error": 0.018822748634287092
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011426499300530885,
            "upper_bound": 0.09524721367646656
          },
          "point_estimate": 0.04443233787927876,
          "standard_error": 0.022260375735484207
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.71424516638787,
            "upper_bound": 74.7606768715634
          },
          "point_estimate": 74.74125145065152,
          "standard_error": 0.011700436173473102
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02571502557143711,
            "upper_bound": 0.0748712704063573
          },
          "point_estimate": 0.05722291486687426,
          "standard_error": 0.012907118492661927
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.10790422638081,
            "upper_bound": 89.5094546865202
          },
          "point_estimate": 89.30550624263334,
          "standard_error": 0.10245576020579462
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.02770590710861,
            "upper_bound": 89.52375338751482
          },
          "point_estimate": 89.32930663091523,
          "standard_error": 0.11212441714305751
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07085208823228095,
            "upper_bound": 0.5968743437855759
          },
          "point_estimate": 0.2943014647842617,
          "standard_error": 0.1388214802454505
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.21197463871688,
            "upper_bound": 89.48354900593586
          },
          "point_estimate": 89.35432159761956,
          "standard_error": 0.07048298407239974
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1696170286848551,
            "upper_bound": 0.45053980909346875
          },
          "point_estimate": 0.3423493551406959,
          "standard_error": 0.07138288300079006
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.29185196412859,
            "upper_bound": 63.43180212337023
          },
          "point_estimate": 63.35833551112565,
          "standard_error": 0.0359448421681794
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.2924622147559,
            "upper_bound": 63.44447927892906
          },
          "point_estimate": 63.33242424295014,
          "standard_error": 0.03286616895429033
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015321934780225536,
            "upper_bound": 0.20280583003073727
          },
          "point_estimate": 0.07187795794326696,
          "standard_error": 0.04938318091745647
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.31234000293123,
            "upper_bound": 63.39976442502097
          },
          "point_estimate": 63.34581969468618,
          "standard_error": 0.022385396765046094
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04393410936630971,
            "upper_bound": 0.15661707837242786
          },
          "point_estimate": 0.11976273356623147,
          "standard_error": 0.027537081975546947
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.01415638180532,
            "upper_bound": 106.19290106603668
          },
          "point_estimate": 106.09571916514446,
          "standard_error": 0.046014731452058574
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.98068404748612,
            "upper_bound": 106.17631353101862
          },
          "point_estimate": 106.08944690960269,
          "standard_error": 0.04489850066371807
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011583012451987383,
            "upper_bound": 0.228203354611911
          },
          "point_estimate": 0.12784580413749982,
          "standard_error": 0.06045360226350829
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.99170401099826,
            "upper_bound": 106.10440766500228
          },
          "point_estimate": 106.0401364878027,
          "standard_error": 0.02902277398640027
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06344964048234679,
            "upper_bound": 0.20862837130361764
          },
          "point_estimate": 0.15264091328089138,
          "standard_error": 0.04073683436960965
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.4883713102696,
            "upper_bound": 64.56057857367054
          },
          "point_estimate": 64.5227755529326,
          "standard_error": 0.018571170131879262
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.47494796239133,
            "upper_bound": 64.56761399725703
          },
          "point_estimate": 64.5067061845695,
          "standard_error": 0.02435089094587624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012878363766764638,
            "upper_bound": 0.09873507469282727
          },
          "point_estimate": 0.04941979501955739,
          "standard_error": 0.02333443397161623
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.48289813491157,
            "upper_bound": 64.5614372637018
          },
          "point_estimate": 64.51704767047684,
          "standard_error": 0.02024849209551711
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030438138470375107,
            "upper_bound": 0.07810420095105242
          },
          "point_estimate": 0.06175109238434596,
          "standard_error": 0.0119760508639019
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.026315984850015,
            "upper_bound": 61.100805142985394
          },
          "point_estimate": 61.06204318653414,
          "standard_error": 0.019172637241406235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.00801694932361,
            "upper_bound": 61.112153183128726
          },
          "point_estimate": 61.04752797131825,
          "standard_error": 0.030767728859568116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012907659684250236,
            "upper_bound": 0.10895836588296456
          },
          "point_estimate": 0.0686784399116968,
          "standard_error": 0.024139924000661124
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.01561061222884,
            "upper_bound": 61.0923733343894
          },
          "point_estimate": 61.04982446020634,
          "standard_error": 0.019892727229374605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.036488410701870394,
            "upper_bound": 0.07944890737594121
          },
          "point_estimate": 0.06377266624915652,
          "standard_error": 0.011235032414547666
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.812196467419,
            "upper_bound": 103.03991690164128
          },
          "point_estimate": 102.91968380637506,
          "standard_error": 0.05846447011675467
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.77715664187036,
            "upper_bound": 103.07283126697368
          },
          "point_estimate": 102.84840323532885,
          "standard_error": 0.08279388126868077
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019677492601412497,
            "upper_bound": 0.32585824811496555
          },
          "point_estimate": 0.15045937114390226,
          "standard_error": 0.0785768883425895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.77178052749514,
            "upper_bound": 102.9861514869859
          },
          "point_estimate": 102.8497900235082,
          "standard_error": 0.05490534664510066
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09497898438611332,
            "upper_bound": 0.24869267482870944
          },
          "point_estimate": 0.19496141069280332,
          "standard_error": 0.03970000258910188
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1189680.252985151,
            "upper_bound": 1191329.2194137224
          },
          "point_estimate": 1190512.1144674856,
          "standard_error": 423.1693748651693
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1189167.7193548386,
            "upper_bound": 1191689.2217741937
          },
          "point_estimate": 1190650.3082437275,
          "standard_error": 684.8238129200944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 421.0225231703781,
            "upper_bound": 2436.2258128774997
          },
          "point_estimate": 1688.6249089994624,
          "standard_error": 517.1858937225367
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1189318.2442065722,
            "upper_bound": 1191113.9000807109
          },
          "point_estimate": 1190108.5824046922,
          "standard_error": 462.1099218274573
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 871.4374685585838,
            "upper_bound": 1694.9446171610405
          },
          "point_estimate": 1409.503493789232,
          "standard_error": 208.6852496164722
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523716.2432621524,
            "upper_bound": 1526457.8817013886
          },
          "point_estimate": 1525135.2994791665,
          "standard_error": 701.1326651945047
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523714.1109375,
            "upper_bound": 1527053.7065972222
          },
          "point_estimate": 1525064.486111111,
          "standard_error": 1030.6654164462727
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.88502462699608,
            "upper_bound": 4090.7929361239994
          },
          "point_estimate": 2850.2124791905717,
          "standard_error": 939.2617631914528
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523750.8719122254,
            "upper_bound": 1526146.4715136054
          },
          "point_estimate": 1524766.3017316018,
          "standard_error": 621.3500555046562
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1289.4018287150832,
            "upper_bound": 3034.152657432375
          },
          "point_estimate": 2340.484266906336,
          "standard_error": 465.29014105259375
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1470790.4434442858,
            "upper_bound": 1473597.6972933332
          },
          "point_estimate": 1472076.2131492062,
          "standard_error": 721.7931557359309
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1469878.496,
            "upper_bound": 1473261.7200000002
          },
          "point_estimate": 1471643.2917460315,
          "standard_error": 881.324252781058
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246.12345643030815,
            "upper_bound": 3825.444270484983
          },
          "point_estimate": 2505.863788712069,
          "standard_error": 863.6412661679874
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1470371.2909247754,
            "upper_bound": 1472426.61728958
          },
          "point_estimate": 1471270.2716883116,
          "standard_error": 531.1008539411474
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1103.4480205932923,
            "upper_bound": 3243.8776577782533
          },
          "point_estimate": 2402.580224291748,
          "standard_error": 599.3453075147636
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 656300.8590968365,
            "upper_bound": 657881.8170627572
          },
          "point_estimate": 657110.0425360082,
          "standard_error": 403.8850743855679
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 656380.1851851852,
            "upper_bound": 658122.7777777778
          },
          "point_estimate": 657016.8956018519,
          "standard_error": 436.11545005512977
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.63270607260625,
            "upper_bound": 2429.2492087241253
          },
          "point_estimate": 1260.9246228456202,
          "standard_error": 581.2656419589816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 656803.898989899,
            "upper_bound": 658522.3283160008
          },
          "point_estimate": 657846.3630591631,
          "standard_error": 451.294644597618
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.986248371602,
            "upper_bound": 1771.310234571866
          },
          "point_estimate": 1344.8101693690387,
          "standard_error": 281.205396094885
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1428995.3696050823,
            "upper_bound": 1433884.0419347528
          },
          "point_estimate": 1431212.1669322343,
          "standard_error": 1260.8139605159397
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1428130.3846153845,
            "upper_bound": 1434671.3397435895
          },
          "point_estimate": 1429865.4567307692,
          "standard_error": 1537.2940054396186
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297.09877895623276,
            "upper_bound": 5377.722739141571
          },
          "point_estimate": 2390.6863479414674,
          "standard_error": 1567.475730259905
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1428775.1600224592,
            "upper_bound": 1431205.4217214503
          },
          "point_estimate": 1429750.042957043,
          "standard_error": 620.1451265739598
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1367.5973593823717,
            "upper_bound": 5272.799335104347
          },
          "point_estimate": 4206.128372054525,
          "standard_error": 1047.348632339922
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 448482.77465234225,
            "upper_bound": 449017.0629109321
          },
          "point_estimate": 448733.4833996322,
          "standard_error": 136.3867131608729
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 448350.3493902439,
            "upper_bound": 448950.85365853657
          },
          "point_estimate": 448781.3135670732,
          "standard_error": 186.89397933019885
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.31166198267783,
            "upper_bound": 805.8864262109923
          },
          "point_estimate": 420.6847291167449,
          "standard_error": 189.79894387523777
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 448359.2334045763,
            "upper_bound": 448893.4878048781
          },
          "point_estimate": 448615.7595818815,
          "standard_error": 136.0929069174934
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 235.80388485663255,
            "upper_bound": 615.7063857987542
          },
          "point_estimate": 455.41896413626847,
          "standard_error": 107.900853595943
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 581761.6728665911,
            "upper_bound": 583712.1082251512
          },
          "point_estimate": 582546.0133427817,
          "standard_error": 522.5584080017593
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 581624.9322089948,
            "upper_bound": 582774.9301587301
          },
          "point_estimate": 582041.8612244898,
          "standard_error": 334.1999545173836
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174.8149121741717,
            "upper_bound": 1378.8714157978957
          },
          "point_estimate": 776.8431639860337,
          "standard_error": 342.30947752265064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 581817.5080073159,
            "upper_bound": 582378.1315846043
          },
          "point_estimate": 582014.0211502783,
          "standard_error": 144.59452461081483
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 407.59857380329976,
            "upper_bound": 2621.861152938718
          },
          "point_estimate": 1738.3173229646234,
          "standard_error": 726.3453568211363
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1030360.70103836,
            "upper_bound": 1031493.5113359786
          },
          "point_estimate": 1030916.9259457672,
          "standard_error": 289.83122780659346
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1030000.0092592592,
            "upper_bound": 1031737.2118055556
          },
          "point_estimate": 1030900.3986111112,
          "standard_error": 516.6410983133567
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 235.3352902664622,
            "upper_bound": 1602.6908657131985
          },
          "point_estimate": 1188.3526125135493,
          "standard_error": 374.2850693997792
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1030307.120306384,
            "upper_bound": 1031616.7828361742
          },
          "point_estimate": 1030973.1981962482,
          "standard_error": 332.0320000874934
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 608.6120474005744,
            "upper_bound": 1153.652460352234
          },
          "point_estimate": 962.9860366762368,
          "standard_error": 139.9126568932886
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 716460.6576309523,
            "upper_bound": 717530.1636634921
          },
          "point_estimate": 716995.9737801587,
          "standard_error": 271.9156698376147
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 716549.64,
            "upper_bound": 717655.8866666667
          },
          "point_estimate": 716861.2088888888,
          "standard_error": 259.20636792430247
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.3802458530944,
            "upper_bound": 1607.6937287078022
          },
          "point_estimate": 473.60379475839926,
          "standard_error": 411.9638619271825
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 716811.34825,
            "upper_bound": 717742.9773538462
          },
          "point_estimate": 717277.3979220779,
          "standard_error": 243.6642201722962
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 402.52012850715585,
            "upper_bound": 1227.3495015125247
          },
          "point_estimate": 905.75462263642,
          "standard_error": 213.0129199792779
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 799694.8201920288,
            "upper_bound": 801114.0761798008
          },
          "point_estimate": 800396.2498731883,
          "standard_error": 361.66559010918706
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 799488.4376811595,
            "upper_bound": 801347.8583333333
          },
          "point_estimate": 800374.3899456521,
          "standard_error": 348.52960838120725
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.80932315659814,
            "upper_bound": 2503.7740103316814
          },
          "point_estimate": 665.6121838351754,
          "standard_error": 657.3715146957554
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 800156.9294584743,
            "upper_bound": 801142.047314578
          },
          "point_estimate": 800626.7150197629,
          "standard_error": 244.5425156549641
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 621.301206001026,
            "upper_bound": 1546.0052353012306
          },
          "point_estimate": 1204.9186033369883,
          "standard_error": 238.09479334387188
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 443370.44462833914,
            "upper_bound": 444409.3683072009
          },
          "point_estimate": 443844.5879006969,
          "standard_error": 267.7439664230545
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 443216.3577235773,
            "upper_bound": 444397.4131097561
          },
          "point_estimate": 443466.76114982576,
          "standard_error": 306.0641245207656
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.9151578047235,
            "upper_bound": 1403.7976756873577
          },
          "point_estimate": 402.1687601770994,
          "standard_error": 320.1736168687854
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 443226.97232782847,
            "upper_bound": 443932.2616916472
          },
          "point_estimate": 443482.3241685144,
          "standard_error": 181.06932429831343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241.32595453212195,
            "upper_bound": 1136.9343638614876
          },
          "point_estimate": 889.8737842834037,
          "standard_error": 218.24090802675937
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322363.27914502163,
            "upper_bound": 322711.06087626267
          },
          "point_estimate": 322531.5363250361,
          "standard_error": 89.21394040321702
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322298.9090909091,
            "upper_bound": 322726.4818181818
          },
          "point_estimate": 322532.09918831167,
          "standard_error": 114.87843992507302
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.84211589308235,
            "upper_bound": 513.2296860399446
          },
          "point_estimate": 310.64962175759536,
          "standard_error": 111.06357357910834
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322421.1607441095,
            "upper_bound": 322896.04225637025
          },
          "point_estimate": 322692.6126800472,
          "standard_error": 124.2828518763942
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159.55791643605508,
            "upper_bound": 382.5241016758759
          },
          "point_estimate": 297.8935943530634,
          "standard_error": 57.56289265365845
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 249401.1951826484,
            "upper_bound": 249755.4363470319
          },
          "point_estimate": 249564.0997969667,
          "standard_error": 91.32254389349448
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 249339.0308219178,
            "upper_bound": 249833.7808219178
          },
          "point_estimate": 249401.66552511416,
          "standard_error": 133.88122480191234
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.17243268351623,
            "upper_bound": 505.6540250125612
          },
          "point_estimate": 128.57471698105047,
          "standard_error": 133.65325966426977
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 249392.78265033665,
            "upper_bound": 249687.95133582037
          },
          "point_estimate": 249496.32193559865,
          "standard_error": 75.44231906085508
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119.38781991097748,
            "upper_bound": 392.0907242046082
          },
          "point_estimate": 304.7566990583457,
          "standard_error": 67.9118641236293
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175196.7755603537,
            "upper_bound": 175562.97709984548
          },
          "point_estimate": 175355.8459283425,
          "standard_error": 94.98175670635788
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175189.44375,
            "upper_bound": 175428.78966346153
          },
          "point_estimate": 175298.50532280217,
          "standard_error": 65.9215137265237
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.54651128900358,
            "upper_bound": 394.9040459698422
          },
          "point_estimate": 173.23562942446284,
          "standard_error": 84.58729187565619
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175237.81517623176,
            "upper_bound": 175368.78078457408
          },
          "point_estimate": 175304.2168206793,
          "standard_error": 32.95513070658406
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.90776531695028,
            "upper_bound": 459.5076438334766
          },
          "point_estimate": 317.0376882722101,
          "standard_error": 107.38937551197084
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-repeated-rare-huge_common-m"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3177110.1716269846,
            "upper_bound": 3181640.5015257937
          },
          "point_estimate": 3179367.463313492,
          "standard_error": 1167.7990882672725
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3175468.597222222,
            "upper_bound": 3184009.4166666665
          },
          "point_estimate": 3179524.196428572,
          "standard_error": 2089.102690843784
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.00060777871303,
            "upper_bound": 6698.301372747263
          },
          "point_estimate": 5826.2678382295435,
          "standard_error": 1754.3336355025394
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3176879.005769231,
            "upper_bound": 3180579.7698220066
          },
          "point_estimate": 3178713.453679654,
          "standard_error": 939.3083772520567
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2435.737403429516,
            "upper_bound": 4539.90880210705
          },
          "point_estimate": 3888.838714959824,
          "standard_error": 523.5542190302526
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-repeated-rare-small_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6405.787105969444,
            "upper_bound": 6415.29234804628
          },
          "point_estimate": 6410.5606247946,
          "standard_error": 2.43537389658377
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6404.502583613519,
            "upper_bound": 6418.019878487583
          },
          "point_estimate": 6410.425022119978,
          "standard_error": 3.8537821067437457
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.099062209238782,
            "upper_bound": 13.337424499543156
          },
          "point_estimate": 9.789211111819432,
          "standard_error": 2.976391260049834
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6404.947775017408,
            "upper_bound": 6416.17513493187
          },
          "point_estimate": 6409.301400280838,
          "standard_error": 2.846917436648848
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.886513274240873,
            "upper_bound": 9.978185201170955
          },
          "point_estimate": 8.114014945760864,
          "standard_error": 1.3061290812654789
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1313310.862797619,
            "upper_bound": 1315393.9502380951
          },
          "point_estimate": 1314333.989702381,
          "standard_error": 534.3820927545476
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1312784.9761904762,
            "upper_bound": 1316146.0714285714
          },
          "point_estimate": 1313678.0401785714,
          "standard_error": 945.1942652098112
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291.81626981911876,
            "upper_bound": 2874.934318959759
          },
          "point_estimate": 2167.6119052671056,
          "standard_error": 697.610205482686
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1313042.0996510673,
            "upper_bound": 1315667.6790608934
          },
          "point_estimate": 1314393.799257885,
          "standard_error": 679.0733432410369
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1121.9365613017198,
            "upper_bound": 2081.1285718945537
          },
          "point_estimate": 1785.7156426015104,
          "standard_error": 245.5630445123095
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1513220.2096190476,
            "upper_bound": 1518211.476739682
          },
          "point_estimate": 1515251.071531746,
          "standard_error": 1339.459076078606
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1512575.88,
            "upper_bound": 1516241.7433333332
          },
          "point_estimate": 1513679.8975,
          "standard_error": 888.6411181519397
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395.9304409707034,
            "upper_bound": 3798.1925650684184
          },
          "point_estimate": 1978.1752528803677,
          "standard_error": 968.0528461172746
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1512807.3776551725,
            "upper_bound": 1515214.055093405
          },
          "point_estimate": 1513878.3696623377,
          "standard_error": 615.9172932715801
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 944.4764346040522,
            "upper_bound": 6680.081257317843
          },
          "point_estimate": 4469.01798457416,
          "standard_error": 1805.9174633644652
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1416094.337362065,
            "upper_bound": 1418898.8156642627
          },
          "point_estimate": 1417525.1701144688,
          "standard_error": 711.4125527785441
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1416608.1263736263,
            "upper_bound": 1418849.752403846
          },
          "point_estimate": 1417311.9525641026,
          "standard_error": 615.2111303700095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 264.86838606697074,
            "upper_bound": 3888.314787218301
          },
          "point_estimate": 1169.4634546225473,
          "standard_error": 948.4741876554637
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1417306.909836047,
            "upper_bound": 1419083.5012777036
          },
          "point_estimate": 1418088.0463536463,
          "standard_error": 455.2770325671123
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 903.9115259990848,
            "upper_bound": 3296.913241491183
          },
          "point_estimate": 2372.3245140941335,
          "standard_error": 602.657526585388
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752554.1585807216,
            "upper_bound": 754211.6906190476
          },
          "point_estimate": 753254.375813897,
          "standard_error": 433.37171033456895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752310.9982993198,
            "upper_bound": 753537.8969387754
          },
          "point_estimate": 753002.4743075802,
          "standard_error": 308.38898549080216
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 211.6660003237692,
            "upper_bound": 1519.2060730286105
          },
          "point_estimate": 707.2697788720295,
          "standard_error": 340.93796566407985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752590.8624003235,
            "upper_bound": 753534.9189012577
          },
          "point_estimate": 753152.8053538299,
          "standard_error": 241.50095898513223
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 435.7769355323275,
            "upper_bound": 2135.885715491251
          },
          "point_estimate": 1450.9043417643245,
          "standard_error": 533.7564151101933
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266157.78943691344,
            "upper_bound": 266521.0070757444
          },
          "point_estimate": 266321.4975715444,
          "standard_error": 93.52511638145822
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266098.5961678832,
            "upper_bound": 266449.09326845093
          },
          "point_estimate": 266262.2582116788,
          "standard_error": 85.16937003965737
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.35177252390595,
            "upper_bound": 434.78786455892543
          },
          "point_estimate": 228.87652124075015,
          "standard_error": 109.3673890304892
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266090.5938097072,
            "upper_bound": 266406.70915978967
          },
          "point_estimate": 266242.3530382027,
          "standard_error": 80.15359574964046
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123.88616131204752,
            "upper_bound": 431.72349499264106
          },
          "point_estimate": 311.63249357385496,
          "standard_error": 87.50235115216795
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477258.15149425383,
            "upper_bound": 478228.1412600495
          },
          "point_estimate": 477764.3727788085,
          "standard_error": 246.98631165799256
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477472.75227272726,
            "upper_bound": 478238.86270871985
          },
          "point_estimate": 477689.1487012987,
          "standard_error": 222.2336103128727
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.26180106791221,
            "upper_bound": 1263.481913174789
          },
          "point_estimate": 612.101989133042,
          "standard_error": 295.9371031216309
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477526.9914664447,
            "upper_bound": 478176.8040517434
          },
          "point_estimate": 477845.5301737224,
          "standard_error": 166.24071630649976
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307.62337925603214,
            "upper_bound": 1155.0938139205505
          },
          "point_estimate": 824.6095539164708,
          "standard_error": 223.8088555110586
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240953.27241169973,
            "upper_bound": 241586.80750930303
          },
          "point_estimate": 241258.84321665097,
          "standard_error": 161.63588070788407
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240807.2831125828,
            "upper_bound": 241477.27270577103
          },
          "point_estimate": 241312.98973509937,
          "standard_error": 164.8963177129895
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.05062108520883,
            "upper_bound": 961.934100140816
          },
          "point_estimate": 334.1169487040008,
          "standard_error": 234.00129303176612
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240972.7855960265,
            "upper_bound": 241429.8306104294
          },
          "point_estimate": 241260.39769502025,
          "standard_error": 117.1332085893126
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201.50337787842977,
            "upper_bound": 735.4206695535728
          },
          "point_estimate": 538.9316776305515,
          "standard_error": 132.61908785643692
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 692735.6029072326,
            "upper_bound": 694566.9634553759
          },
          "point_estimate": 693555.5497132374,
          "standard_error": 468.2226976632395
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 692673.8786163522,
            "upper_bound": 693963.2688679246
          },
          "point_estimate": 693402.6797506739,
          "standard_error": 374.32548956033384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.09037236845997,
            "upper_bound": 1957.2433212267936
          },
          "point_estimate": 764.3984748254759,
          "standard_error": 489.1312372008439
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 692982.5982532752,
            "upper_bound": 693780.1677512992
          },
          "point_estimate": 693365.8358735604,
          "standard_error": 206.3070744829609
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464.09162826339326,
            "upper_bound": 2249.3707008525193
          },
          "point_estimate": 1562.9061190609602,
          "standard_error": 501.3636359740696
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.05361385033075,
            "upper_bound": 357.3750962380076
          },
          "point_estimate": 357.21229707426113,
          "standard_error": 0.08200753703959421
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.06816798472624,
            "upper_bound": 357.39116108727717
          },
          "point_estimate": 357.173881236699,
          "standard_error": 0.08733908845178084
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04215148338245949,
            "upper_bound": 0.4850099610925882
          },
          "point_estimate": 0.17842093739496193,
          "standard_error": 0.10916139104602592
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.13406478668895,
            "upper_bound": 357.3583173432332
          },
          "point_estimate": 357.22737716488604,
          "standard_error": 0.05669447136959887
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12808425330729614,
            "upper_bound": 0.36570247944079265
          },
          "point_estimate": 0.27279753874831536,
          "standard_error": 0.06149777821792338
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.23927246911177,
            "upper_bound": 169.51103533288813
          },
          "point_estimate": 169.37011537035949,
          "standard_error": 0.06962084393902271
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.20927381330296,
            "upper_bound": 169.5306404678319
          },
          "point_estimate": 169.3225290371975,
          "standard_error": 0.07288097288012746
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014482420964320248,
            "upper_bound": 0.4148085280311856
          },
          "point_estimate": 0.1609973061435656,
          "standard_error": 0.10348981852293146
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.3254983778465,
            "upper_bound": 169.52816803983714
          },
          "point_estimate": 169.41455908615586,
          "standard_error": 0.05122824924208292
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11001465123925892,
            "upper_bound": 0.2994926000182882
          },
          "point_estimate": 0.23256111350993777,
          "standard_error": 0.04754697846931723
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.30908722456953,
            "upper_bound": 25.64385451386532
          },
          "point_estimate": 25.482360058928347,
          "standard_error": 0.08579395992605478
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.32675180494054,
            "upper_bound": 25.665431210485757
          },
          "point_estimate": 25.496215387454505,
          "standard_error": 0.08148325509986286
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04868811976074501,
            "upper_bound": 0.47519713912609446
          },
          "point_estimate": 0.20301776261488227,
          "standard_error": 0.1060347198981487
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.34550141107655,
            "upper_bound": 25.58958221421824
          },
          "point_estimate": 25.445179234853757,
          "standard_error": 0.06199469410150386
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12785365203902394,
            "upper_bound": 0.38937886179006514
          },
          "point_estimate": 0.28557535609947,
          "standard_error": 0.06859662251869898
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.85435720950708,
            "upper_bound": 17.875638414879187
          },
          "point_estimate": 17.8647339765204,
          "standard_error": 0.005461966380731952
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.85033081613369,
            "upper_bound": 17.881083948434725
          },
          "point_estimate": 17.858939888690166,
          "standard_error": 0.009905478209889886
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017453011417327554,
            "upper_bound": 0.028306855071613124
          },
          "point_estimate": 0.020204979710926937,
          "standard_error": 0.007527384284204774
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.854018447894322,
            "upper_bound": 17.878069932211417
          },
          "point_estimate": 17.868036187535992,
          "standard_error": 0.006166025348646443
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010993721105435766,
            "upper_bound": 0.021905749660787708
          },
          "point_estimate": 0.01818876791292423,
          "standard_error": 0.0027788197757940996
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.17267914501195,
            "upper_bound": 25.197645435008106
          },
          "point_estimate": 25.185714662177,
          "standard_error": 0.006405384860682529
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.164760359109803,
            "upper_bound": 25.20153091794022
          },
          "point_estimate": 25.19123490577212,
          "standard_error": 0.007669609696847186
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001325913007108218,
            "upper_bound": 0.03493670559116663
          },
          "point_estimate": 0.018227360450698324,
          "standard_error": 0.010685260281719611
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.173056318720032,
            "upper_bound": 25.20152216548979
          },
          "point_estimate": 25.18911404835387,
          "standard_error": 0.007219407915158307
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01045604731395589,
            "upper_bound": 0.027009749728090553
          },
          "point_estimate": 0.0213648076476624,
          "standard_error": 0.004181869383771651
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286983.87107236596,
            "upper_bound": 287681.0029547245
          },
          "point_estimate": 287303.9081702287,
          "standard_error": 179.78962895650918
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286913.99606299214,
            "upper_bound": 287655.255511811
          },
          "point_estimate": 287068.48087739036,
          "standard_error": 176.21383480606855
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.01076247709141,
            "upper_bound": 943.5167003358296
          },
          "point_estimate": 321.190108077268,
          "standard_error": 241.30850112969216
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286980.3110408775,
            "upper_bound": 287340.6023258631
          },
          "point_estimate": 287133.7190305757,
          "standard_error": 90.91688973872532
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199.18971404410917,
            "upper_bound": 785.1558564122229
          },
          "point_estimate": 599.4030382525879,
          "standard_error": 150.38020781298715
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.753955913001096,
            "upper_bound": 15.83097597935326
          },
          "point_estimate": 15.790567847223643,
          "standard_error": 0.01974629636796576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.73977085351918,
            "upper_bound": 15.836069298001144
          },
          "point_estimate": 15.776137793844384,
          "standard_error": 0.022853201059516052
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007928368156637548,
            "upper_bound": 0.10722682520700554
          },
          "point_estimate": 0.064601727416264,
          "standard_error": 0.029243267554587855
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.767376311922238,
            "upper_bound": 15.805897577329
          },
          "point_estimate": 15.784451273830774,
          "standard_error": 0.009746858878566937
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.031195143621296197,
            "upper_bound": 0.08291940493691352
          },
          "point_estimate": 0.06587180611434393,
          "standard_error": 0.012829894125026406
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.30612579961458,
            "upper_bound": 22.34062550569206
          },
          "point_estimate": 22.322395411602884,
          "standard_error": 0.00886343272002624
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.29915320703901,
            "upper_bound": 22.34395856443993
          },
          "point_estimate": 22.31170707058938,
          "standard_error": 0.013149913214702684
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0044669492400530066,
            "upper_bound": 0.0522315880660954
          },
          "point_estimate": 0.032534930237581444,
          "standard_error": 0.012093637800166975
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.299525436807457,
            "upper_bound": 22.33398155563684
          },
          "point_estimate": 22.31506883529173,
          "standard_error": 0.008857570408711117
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01571684427595732,
            "upper_bound": 0.03804266599758568
          },
          "point_estimate": 0.02946342157160423,
          "standard_error": 0.0059874603002660065
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126422.468543872,
            "upper_bound": 126691.07969823988
          },
          "point_estimate": 126554.09115259112,
          "standard_error": 68.98133291238305
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126383.00447984072,
            "upper_bound": 126768.712543554
          },
          "point_estimate": 126538.51149825784,
          "standard_error": 106.6391793559545
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.8555618233726,
            "upper_bound": 370.6103885423037
          },
          "point_estimate": 228.42986284351537,
          "standard_error": 97.42818189985996
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126439.51322261602,
            "upper_bound": 126742.3211556579
          },
          "point_estimate": 126592.72256663196,
          "standard_error": 76.8620097817271
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.3751726452726,
            "upper_bound": 285.2342076529169
          },
          "point_estimate": 229.53077098719095,
          "standard_error": 39.012677286224736
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.960693515509796,
            "upper_bound": 23.999586771319404
          },
          "point_estimate": 23.97672763857565,
          "standard_error": 0.010269042896213212
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.95683660984305,
            "upper_bound": 23.985330640324555
          },
          "point_estimate": 23.966231704184473,
          "standard_error": 0.0060813893056773476
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019944414079073654,
            "upper_bound": 0.03366655234918053
          },
          "point_estimate": 0.013365935630841545,
          "standard_error": 0.008286949162825458
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.955543893195905,
            "upper_bound": 23.97094524310578
          },
          "point_estimate": 23.9617035991688,
          "standard_error": 0.0039358548832925805
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0068854682480901834,
            "upper_bound": 0.05015076323555644
          },
          "point_estimate": 0.03414537149746908,
          "standard_error": 0.01297995413253693
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.57578504722639,
            "upper_bound": 20.74694192240555
          },
          "point_estimate": 20.66215534833676,
          "standard_error": 0.04384244723952824
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.53269921286185,
            "upper_bound": 20.809289266142414
          },
          "point_estimate": 20.648627072968655,
          "standard_error": 0.07317752454931391
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.033387489152217426,
            "upper_bound": 0.2496492314441198
          },
          "point_estimate": 0.18890414984453965,
          "standard_error": 0.0568806628196116
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.641734972488777,
            "upper_bound": 20.80427361889711
          },
          "point_estimate": 20.727917577373045,
          "standard_error": 0.041831824734518165
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09130904083952691,
            "upper_bound": 0.17664444824127143
          },
          "point_estimate": 0.14593214015841838,
          "standard_error": 0.02180091329761962
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-freq_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.86110910171579,
            "upper_bound": 42.020846870166224
          },
          "point_estimate": 41.930185477657766,
          "standard_error": 0.04162564112167508
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.84639545299456,
            "upper_bound": 41.973315611302674
          },
          "point_estimate": 41.9009754403566,
          "standard_error": 0.031940736524776026
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023834517977504185,
            "upper_bound": 0.16369000965004957
          },
          "point_estimate": 0.07346151608666221,
          "standard_error": 0.03552259504095957
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.85663691932725,
            "upper_bound": 41.92218964991881
          },
          "point_estimate": 41.887558424434935,
          "standard_error": 0.016464652066264303
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.045354021165380294,
            "upper_bound": 0.20089507456783093
          },
          "point_estimate": 0.13849657614972463,
          "standard_error": 0.04727840221034154
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-repeated_"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.02567476619653,
            "upper_bound": 74.20483804147536
          },
          "point_estimate": 74.11584786974545,
          "standard_error": 0.04595899853380874
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.00552449744907,
            "upper_bound": 74.25205926782493
          },
          "point_estimate": 74.10450364316338,
          "standard_error": 0.06506602160825194
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03958060058722689,
            "upper_bound": 0.26314693929026556
          },
          "point_estimate": 0.1558860707645022,
          "standard_error": 0.05908023590708333
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.01586776724035,
            "upper_bound": 74.20646795574233
          },
          "point_estimate": 74.10250328261867,
          "standard_error": 0.05072223802720556
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08852550364572788,
            "upper_bound": 0.19454967347388613
          },
          "point_estimate": 0.1533563942203123,
          "standard_error": 0.027326878549818072
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector_rare-alph"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.758154270815291,
            "upper_bound": 10.782281640731943
          },
          "point_estimate": 10.769254268108408,
          "standard_error": 0.0062217568340775025
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.752587263503989,
            "upper_bound": 10.781767188230068
          },
          "point_estimate": 10.766638934141008,
          "standard_error": 0.005418828754370955
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006004066010658221,
            "upper_bound": 0.03206237997508256
          },
          "point_estimate": 0.011849220755037757,
          "standard_error": 0.009080599963938191
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.756251450256126,
            "upper_bound": 10.767370796281584
          },
          "point_estimate": 10.76254896645156,
          "standard_error": 0.002851254808284126
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007749214776670512,
            "upper_bound": 0.027611762997008356
          },
          "point_estimate": 0.020707638419088084,
          "standard_error": 0.005330933525023085
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32330.167300722336,
            "upper_bound": 32410.43046395738
          },
          "point_estimate": 32368.647024233174,
          "standard_error": 20.47944313549884
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32315.01093035079,
            "upper_bound": 32413.48591340451
          },
          "point_estimate": 32358.046100237247,
          "standard_error": 22.87963200342984
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.469418301595065,
            "upper_bound": 115.93529487234248
          },
          "point_estimate": 63.74105812103835,
          "standard_error": 24.764825327118327
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32335.419899859306,
            "upper_bound": 32385.30455949564
          },
          "point_estimate": 32360.392274807044,
          "standard_error": 12.459156330415452
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.53194756352311,
            "upper_bound": 89.91893742188954
          },
          "point_estimate": 68.0358253444279,
          "standard_error": 14.849689195959392
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.214669067764564,
            "upper_bound": 29.593958649370343
          },
          "point_estimate": 29.39413823698851,
          "standard_error": 0.09724306412885184
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.1361017482336,
            "upper_bound": 29.598851270386092
          },
          "point_estimate": 29.35713410646465,
          "standard_error": 0.1406536900007673
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04354303710497628,
            "upper_bound": 0.56187394358
          },
          "point_estimate": 0.31073187757945203,
          "standard_error": 0.1232010568401038
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.15383658817414,
            "upper_bound": 29.409076602875164
          },
          "point_estimate": 29.252907834858643,
          "standard_error": 0.06597339617750748
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.17202876396958,
            "upper_bound": 0.4232236243130192
          },
          "point_estimate": 0.324506824945199,
          "standard_error": 0.06841206430068152
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-repeated-rare-huge_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978677.755138889,
            "upper_bound": 979915.6102604166
          },
          "point_estimate": 979289.5720687133,
          "standard_error": 316.2616454615248
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978817.1956140352,
            "upper_bound": 979911.2580409356
          },
          "point_estimate": 979045.3601973684,
          "standard_error": 304.52504866944366
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.1590819614486,
            "upper_bound": 1865.045740572977
          },
          "point_estimate": 418.7352190571771,
          "standard_error": 482.05378031860135
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978896.2077702198,
            "upper_bound": 979857.510288066
          },
          "point_estimate": 979268.6618591936,
          "standard_error": 245.85562612952495
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 465.8921860143937,
            "upper_bound": 1421.3700230613565
          },
          "point_estimate": 1055.5916164297405,
          "standard_error": 245.0456264963114
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-repeated-rare-small_never-tric"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1957.14594104426,
            "upper_bound": 1959.383693664565
          },
          "point_estimate": 1958.389507822744,
          "standard_error": 0.5773705238980205
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1957.345799428787,
            "upper_bound": 1959.646711932604
          },
          "point_estimate": 1958.9641415638305,
          "standard_error": 0.5538513882703213
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.23090873411278728,
            "upper_bound": 2.734331826220227
          },
          "point_estimate": 1.0887506502293942,
          "standard_error": 0.6215302910960726
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1958.2287091742296,
            "upper_bound": 1959.3842388341527
          },
          "point_estimate": 1958.9029641910604,
          "standard_error": 0.2959855516949343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6175246747445311,
            "upper_bound": 2.633614772472302
          },
          "point_estimate": 1.9178639314315584,
          "standard_error": 0.5555689387563632
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.816878142977167,
            "upper_bound": 7.825184691393039
          },
          "point_estimate": 7.821080310978959,
          "standard_error": 0.002117936029010903
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.8170313354806815,
            "upper_bound": 7.825106104405204
          },
          "point_estimate": 7.821144319092667,
          "standard_error": 0.0019733635746790787
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016253715588648429,
            "upper_bound": 0.011608777177553504
          },
          "point_estimate": 0.004319400681461796,
          "standard_error": 0.002487349821936311
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.820709271433734,
            "upper_bound": 7.825290859676554
          },
          "point_estimate": 7.822967905838085,
          "standard_error": 0.0011526905806900124
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002885759328544845,
            "upper_bound": 0.009612052199108186
          },
          "point_estimate": 0.007061731760960505,
          "standard_error": 0.0017244903548428495
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.091323396491452,
            "upper_bound": 7.100306715390057
          },
          "point_estimate": 7.09612841363482,
          "standard_error": 0.002314453206526273
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.090499375416975,
            "upper_bound": 7.101480468141709
          },
          "point_estimate": 7.098341857759795,
          "standard_error": 0.0021484261095189787
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000512164492366579,
            "upper_bound": 0.01222948599987151
          },
          "point_estimate": 0.004045764601968278,
          "standard_error": 0.003106471129543202
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.096034408740758,
            "upper_bound": 7.0997978412832
          },
          "point_estimate": 7.097960456376039,
          "standard_error": 0.0009509064631665156
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028269672261501707,
            "upper_bound": 0.009932343869704431
          },
          "point_estimate": 0.007720487866475641,
          "standard_error": 0.00183104209127885
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.312307755593107,
            "upper_bound": 8.329395753765311
          },
          "point_estimate": 8.320639697357123,
          "standard_error": 0.004386566484006344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.311161387574383,
            "upper_bound": 8.333615265009476
          },
          "point_estimate": 8.317493269149704,
          "standard_error": 0.005165456330358842
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008280584328142639,
            "upper_bound": 0.025619591534327215
          },
          "point_estimate": 0.01223352479264757,
          "standard_error": 0.007337245394709458
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.310739401997074,
            "upper_bound": 8.326137177643137
          },
          "point_estimate": 8.316994153033114,
          "standard_error": 0.003980345159008418
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007419635652907109,
            "upper_bound": 0.018233147420722292
          },
          "point_estimate": 0.01461049866558702,
          "standard_error": 0.0026783048917913234
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.2540484631161,
            "upper_bound": 22.285010268204395
          },
          "point_estimate": 22.26898777820748,
          "standard_error": 0.007932257666525914
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.24859058457003,
            "upper_bound": 22.293822223991604
          },
          "point_estimate": 22.257153176314223,
          "standard_error": 0.013860433263599864
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014479597237270893,
            "upper_bound": 0.04132115605480924
          },
          "point_estimate": 0.02106187874882472,
          "standard_error": 0.01124059846988491
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.2508908501816,
            "upper_bound": 22.290436890525807
          },
          "point_estimate": 22.27362406872424,
          "standard_error": 0.01014000917853831
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014555087996389052,
            "upper_bound": 0.03182283747687995
          },
          "point_estimate": 0.026465005669014836,
          "standard_error": 0.004319408958618034
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.032426379407925,
            "upper_bound": 14.07492878926369
          },
          "point_estimate": 14.0542252062405,
          "standard_error": 0.010915670757865457
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.017350975331386,
            "upper_bound": 14.087474496914734
          },
          "point_estimate": 14.059962549158332,
          "standard_error": 0.019406258759646043
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006257058183775406,
            "upper_bound": 0.05656269837564519
          },
          "point_estimate": 0.041192915267131466,
          "standard_error": 0.015033890464940865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.03367463333024,
            "upper_bound": 14.084944066164532
          },
          "point_estimate": 14.05617230153132,
          "standard_error": 0.013271609189728788
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021743088862048305,
            "upper_bound": 0.043136278718006335
          },
          "point_estimate": 0.03630558005853483,
          "standard_error": 0.005412729072313118
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.349597956474,
            "upper_bound": 18.39074441233167
          },
          "point_estimate": 18.369424338461485,
          "standard_error": 0.010581606871207795
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.344545388032927,
            "upper_bound": 18.39495109379849
          },
          "point_estimate": 18.357028358032217,
          "standard_error": 0.016280209376549858
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002865227262261822,
            "upper_bound": 0.06373983903496981
          },
          "point_estimate": 0.038715559810276315,
          "standard_error": 0.01589626208207659
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.351200301120052,
            "upper_bound": 18.3836406129054
          },
          "point_estimate": 18.36478476956211,
          "standard_error": 0.008321841991072583
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019305960862943297,
            "upper_bound": 0.04582504040695355
          },
          "point_estimate": 0.03530411717288315,
          "standard_error": 0.007074051491954004
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.768678847824308,
            "upper_bound": 10.782648112626768
          },
          "point_estimate": 10.775726673855504,
          "standard_error": 0.0035764544157897535
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.768102725515831,
            "upper_bound": 10.787421271560026
          },
          "point_estimate": 10.77280272851576,
          "standard_error": 0.0045371441804417005
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000202926172052596,
            "upper_bound": 0.0232110251134331
          },
          "point_estimate": 0.011316213829534832,
          "standard_error": 0.0062357726672849865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.76972232093329,
            "upper_bound": 10.786537479316356
          },
          "point_estimate": 10.778734810467958,
          "standard_error": 0.004333158059872344
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006790565457800504,
            "upper_bound": 0.015161820345234568
          },
          "point_estimate": 0.01190887763289022,
          "standard_error": 0.0022115058313089834
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.811736399598336,
            "upper_bound": 15.873785207675011
          },
          "point_estimate": 15.84249426437366,
          "standard_error": 0.01590790297578564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.79447100868119,
            "upper_bound": 15.875976182428513
          },
          "point_estimate": 15.852567517595972,
          "standard_error": 0.02444232204444424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004220365380521833,
            "upper_bound": 0.09729077342156528
          },
          "point_estimate": 0.04787238075580713,
          "standard_error": 0.023176252078985184
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.800665861340187,
            "upper_bound": 15.871598449769891
          },
          "point_estimate": 15.84190501008698,
          "standard_error": 0.018758176121901927
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03137414193693416,
            "upper_bound": 0.06714332948801548
          },
          "point_estimate": 0.05288087177186963,
          "standard_error": 0.00934285765624984
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.29578852900328,
            "upper_bound": 22.5345521218038
          },
          "point_estimate": 22.39421615913247,
          "standard_error": 0.06320127603660389
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.271868585738396,
            "upper_bound": 22.429916050133397
          },
          "point_estimate": 22.3178777443741,
          "standard_error": 0.040888855166414224
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009575786152206013,
            "upper_bound": 0.207621557024707
          },
          "point_estimate": 0.06889445233727812,
          "standard_error": 0.05273641192438547
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.289430185332975,
            "upper_bound": 22.338735726146098
          },
          "point_estimate": 22.311008715063306,
          "standard_error": 0.01256761353025393
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04006852252190687,
            "upper_bound": 0.31199293745606893
          },
          "point_estimate": 0.21127232270705865,
          "standard_error": 0.08156030015273857
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.99992919386494,
            "upper_bound": 11.012809017498268
          },
          "point_estimate": 11.005967332252784,
          "standard_error": 0.00331481572330393
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.9981706008443,
            "upper_bound": 11.014938297079675
          },
          "point_estimate": 11.001503217496468,
          "standard_error": 0.004131624716324405
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003236641599661326,
            "upper_bound": 0.01803304582799778
          },
          "point_estimate": 0.008244160203371645,
          "standard_error": 0.004583698447909783
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.998229143618856,
            "upper_bound": 11.010866253294786
          },
          "point_estimate": 11.004551832258208,
          "standard_error": 0.003273063221283529
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004642208757517497,
            "upper_bound": 0.013645408381577791
          },
          "point_estimate": 0.011060112738495389,
          "standard_error": 0.002218141421239274
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.973642145406597,
            "upper_bound": 24.010491831221323
          },
          "point_estimate": 23.991612469217852,
          "standard_error": 0.009457877151665772
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.96244931600101,
            "upper_bound": 24.013961401005663
          },
          "point_estimate": 23.989385411231623,
          "standard_error": 0.013177784322349348
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00992681578189212,
            "upper_bound": 0.053407587116817985
          },
          "point_estimate": 0.036368446459925,
          "standard_error": 0.011378968617717462
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.976612705509226,
            "upper_bound": 24.01124412963072
          },
          "point_estimate": 23.99692381528226,
          "standard_error": 0.008910221664396908
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017915920469476824,
            "upper_bound": 0.040261727319558696
          },
          "point_estimate": 0.031559365544539156,
          "standard_error": 0.005873002492969164
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.563642469485128,
            "upper_bound": 20.664872300396
          },
          "point_estimate": 20.61224733715432,
          "standard_error": 0.025921680723588133
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.52710673965416,
            "upper_bound": 20.68856935603653
          },
          "point_estimate": 20.605059832387845,
          "standard_error": 0.03492033868536943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013222639446049051,
            "upper_bound": 0.15897389848001509
          },
          "point_estimate": 0.09088209349037887,
          "standard_error": 0.03762435500769165
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.58034512892454,
            "upper_bound": 20.699326378489957
          },
          "point_estimate": 20.637457388590796,
          "standard_error": 0.030208053565150945
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04399245345265922,
            "upper_bound": 0.1065249577440061
          },
          "point_estimate": 0.08615961113273948,
          "standard_error": 0.015969535780600168
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1174243.9139580133,
            "upper_bound": 1175674.320668971
          },
          "point_estimate": 1174931.9243881206,
          "standard_error": 366.20512971092654
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1173746.2838709678,
            "upper_bound": 1175780.6505376345
          },
          "point_estimate": 1174836.6460573478,
          "standard_error": 451.2366676302916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.5221518142406,
            "upper_bound": 2111.783404765996
          },
          "point_estimate": 1278.7233469754906,
          "standard_error": 488.0884305626966
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1173961.2301158584,
            "upper_bound": 1174975.8091658838
          },
          "point_estimate": 1174427.9894428153,
          "standard_error": 262.28710378064926
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648.5227426562373,
            "upper_bound": 1592.0718948915642
          },
          "point_estimate": 1222.0858217567234,
          "standard_error": 250.5057977282032
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1576803.926027199,
            "upper_bound": 1579966.6097702135
          },
          "point_estimate": 1578296.1107457008,
          "standard_error": 814.002848832791
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1576320.0083333333,
            "upper_bound": 1580465.1726190476
          },
          "point_estimate": 1577238.2627314816,
          "standard_error": 1148.0849725583216
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328.00362292702476,
            "upper_bound": 4408.445814789857
          },
          "point_estimate": 1955.615001218451,
          "standard_error": 1091.6908070024551
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1576521.372342687,
            "upper_bound": 1578856.9097139533
          },
          "point_estimate": 1577591.1805194805,
          "standard_error": 597.3856073323799
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1258.737597857674,
            "upper_bound": 3455.278940147169
          },
          "point_estimate": 2713.834974581445,
          "standard_error": 565.9275683897029
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1447945.8081775412,
            "upper_bound": 1449519.8999893162
          },
          "point_estimate": 1448719.9855845545,
          "standard_error": 402.61382253346386
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1447498.6057692308,
            "upper_bound": 1449994.0495192308
          },
          "point_estimate": 1448448.9404761903,
          "standard_error": 889.6915731864437
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.23635286546387,
            "upper_bound": 1916.9677105184333
          },
          "point_estimate": 1582.5658382497973,
          "standard_error": 626.620180274716
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1448647.8810566657,
            "upper_bound": 1449885.7649691028
          },
          "point_estimate": 1449451.3346653346,
          "standard_error": 317.2369077281376
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 904.527698019804,
            "upper_bound": 1524.6399244218703
          },
          "point_estimate": 1342.709981764452,
          "standard_error": 157.818580888465
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396287.3983287289,
            "upper_bound": 396912.1849767081
          },
          "point_estimate": 396616.9276453589,
          "standard_error": 160.3735834961535
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396310.3931482919,
            "upper_bound": 397003.6671195652
          },
          "point_estimate": 396718.9698067633,
          "standard_error": 151.70605641208428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.87799598516192,
            "upper_bound": 893.3627277990562
          },
          "point_estimate": 345.112208873068,
          "standard_error": 236.8773737553715
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396326.7715038584,
            "upper_bound": 397107.8181013997
          },
          "point_estimate": 396752.8999435347,
          "standard_error": 203.38621274676223
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246.3746721704098,
            "upper_bound": 707.6143780000286
          },
          "point_estimate": 536.1606378795831,
          "standard_error": 121.3277520591146
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 558471.5864740862,
            "upper_bound": 559392.3638206772
          },
          "point_estimate": 558894.8511820587,
          "standard_error": 235.60710176325392
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 558309.7575757576,
            "upper_bound": 559505.9541847042
          },
          "point_estimate": 558693.4321969696,
          "standard_error": 290.8147176125669
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.89947113122238,
            "upper_bound": 1263.6775785753605
          },
          "point_estimate": 544.5310155851818,
          "standard_error": 303.5321551982012
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 558420.4092512599,
            "upper_bound": 558973.1100873601
          },
          "point_estimate": 558696.2888626525,
          "standard_error": 142.70423459633116
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306.9482850363298,
            "upper_bound": 1011.554595112844
          },
          "point_estimate": 786.5612346306395,
          "standard_error": 184.9755514226333
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 427539.793205392,
            "upper_bound": 428231.6418169934
          },
          "point_estimate": 427896.2965788982,
          "standard_error": 176.11825501693468
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 427576.3586134454,
            "upper_bound": 428269.99852941174
          },
          "point_estimate": 427984.5309803921,
          "standard_error": 155.02833854568607
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.429795843736809,
            "upper_bound": 968.9232657662224
          },
          "point_estimate": 425.2388883916185,
          "standard_error": 250.76434891721445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 427632.9663817664,
            "upper_bound": 428416.8876344029
          },
          "point_estimate": 428039.45032849506,
          "standard_error": 202.92894638882856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.93539608653288,
            "upper_bound": 797.7350097782469
          },
          "point_estimate": 585.9979786043508,
          "standard_error": 138.6338743022241
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481650.1137828947,
            "upper_bound": 482522.2324564144
          },
          "point_estimate": 482080.8855539891,
          "standard_error": 222.46270678928076
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481620.31834795326,
            "upper_bound": 482634.4226973684
          },
          "point_estimate": 481944.37734962406,
          "standard_error": 297.07961338973354
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.608463399797934,
            "upper_bound": 1276.9947555563022
          },
          "point_estimate": 530.9474453983386,
          "standard_error": 312.5452025746884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481735.17175781494,
            "upper_bound": 482693.4035543627
          },
          "point_estimate": 482258.4554340397,
          "standard_error": 250.62277787077775
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395.5221276391647,
            "upper_bound": 957.7088001737094
          },
          "point_estimate": 740.380746888491,
          "standard_error": 143.26735725803593
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1008188.5178172952,
            "upper_bound": 1010754.896672206
          },
          "point_estimate": 1009370.9654993388,
          "standard_error": 656.9524085905289
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1007673.0264550264,
            "upper_bound": 1010845.4944444444
          },
          "point_estimate": 1008800.4703703704,
          "standard_error": 754.2715712305867
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287.81057787645835,
            "upper_bound": 3510.5547855919567
          },
          "point_estimate": 1673.7240984104285,
          "standard_error": 803.8643518121654
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1007776.1548836424,
            "upper_bound": 1009082.1832895634
          },
          "point_estimate": 1008414.2168831168,
          "standard_error": 334.23128880813647
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 856.5533849351157,
            "upper_bound": 2800.051222844305
          },
          "point_estimate": 2192.2867138404467,
          "standard_error": 499.19412397635136
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297732.1706228858,
            "upper_bound": 298211.8471333674
          },
          "point_estimate": 297962.8312428441,
          "standard_error": 122.44558438219944
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297635.8581967213,
            "upper_bound": 298282.8226320583
          },
          "point_estimate": 297915.67657103826,
          "standard_error": 185.97666765912183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.243005584713906,
            "upper_bound": 688.7700722390658
          },
          "point_estimate": 353.2865458181004,
          "standard_error": 172.18754380027997
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297590.33653317194,
            "upper_bound": 298231.62119737256
          },
          "point_estimate": 297946.2355758995,
          "standard_error": 168.18853027643394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221.77195218221584,
            "upper_bound": 518.2467869950357
          },
          "point_estimate": 408.77387347899753,
          "standard_error": 77.18981985688472
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 813533.93532791,
            "upper_bound": 815134.9917208333
          },
          "point_estimate": 814224.6536499118,
          "standard_error": 416.02853246968385
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 813479.8111111111,
            "upper_bound": 814666.0266666666
          },
          "point_estimate": 813918.9149691358,
          "standard_error": 304.4318303850258
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.36705188148613,
            "upper_bound": 1677.1122855031851
          },
          "point_estimate": 738.0851701370966,
          "standard_error": 396.0382870108916
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 813297.1442283865,
            "upper_bound": 814350.9972565158
          },
          "point_estimate": 813896.9386435787,
          "standard_error": 271.884261271361
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 425.900950512922,
            "upper_bound": 2000.0744729824696
          },
          "point_estimate": 1380.7258581110752,
          "standard_error": 472.1853798357003
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393230.88765927416,
            "upper_bound": 393783.8551925776
          },
          "point_estimate": 393500.0043032087,
          "standard_error": 141.86480899731703
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393068.26758832566,
            "upper_bound": 393908.1642771804
          },
          "point_estimate": 393436.0295698925,
          "standard_error": 238.66195668570916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.61668788808315,
            "upper_bound": 782.0505623254377
          },
          "point_estimate": 515.9194830179894,
          "standard_error": 194.083590517498
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393173.12999193213,
            "upper_bound": 393750.4727075866
          },
          "point_estimate": 393461.1376344086,
          "standard_error": 146.1689598679357
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282.00766949263163,
            "upper_bound": 570.3297783814268
          },
          "point_estimate": 474.5241472733545,
          "standard_error": 73.08390321718299
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169092.43502057614,
            "upper_bound": 169436.77553452016
          },
          "point_estimate": 169266.3277316652,
          "standard_error": 88.08670560035114
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169034.2898148148,
            "upper_bound": 169496.21267361112
          },
          "point_estimate": 169305.51028806585,
          "standard_error": 110.93597746328952
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.23586967516376,
            "upper_bound": 539.3424053359973
          },
          "point_estimate": 286.71302299625904,
          "standard_error": 126.94453565661664
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169060.14303778947,
            "upper_bound": 169536.79013815182
          },
          "point_estimate": 169290.54728234728,
          "standard_error": 126.54076974538026
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167.15998172295423,
            "upper_bound": 365.41420228068586
          },
          "point_estimate": 292.75399491666127,
          "standard_error": 51.25752205881841
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232167.21217697905,
            "upper_bound": 232459.0692763687
          },
          "point_estimate": 232300.3227749975,
          "standard_error": 75.1841658527166
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232095.8917197452,
            "upper_bound": 232452.25141037308
          },
          "point_estimate": 232240.403529724,
          "standard_error": 96.18113515257409
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.436741108908745,
            "upper_bound": 408.065489528205
          },
          "point_estimate": 221.56862464917376,
          "standard_error": 95.3845914508673
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232154.44148452173,
            "upper_bound": 232370.12167478455
          },
          "point_estimate": 232259.05939283647,
          "standard_error": 55.81349610894835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.18893021070711,
            "upper_bound": 337.78792765317473
          },
          "point_estimate": 250.2430512701196,
          "standard_error": 63.70489510670122
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174431.71429226472,
            "upper_bound": 174645.85574162676
          },
          "point_estimate": 174522.70893939392,
          "standard_error": 55.936968362821915
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174385.57894736843,
            "upper_bound": 174558.87559808613
          },
          "point_estimate": 174509.79629186605,
          "standard_error": 49.65826479040358
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.352707812810852,
            "upper_bound": 218.56024169394624
          },
          "point_estimate": 127.18378649800968,
          "standard_error": 52.38655746759567
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174422.03031981867,
            "upper_bound": 174539.51666586116
          },
          "point_estimate": 174475.23182750263,
          "standard_error": 29.932174487598264
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.28056339427913,
            "upper_bound": 276.96599307525514
          },
          "point_estimate": 188.04106825643936,
          "standard_error": 67.59870705345733
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-repeated-rare-huge_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648596.8148680032,
            "upper_bound": 649620.5980750486
          },
          "point_estimate": 649087.1786347814,
          "standard_error": 262.2919772525233
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648351.4807017543,
            "upper_bound": 649640.7041910331
          },
          "point_estimate": 649105.5084064327,
          "standard_error": 411.28948024625447
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121.35427591571496,
            "upper_bound": 1641.225495891652
          },
          "point_estimate": 1001.0606059117792,
          "standard_error": 365.37686536022585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648726.0241364809,
            "upper_bound": 649469.4625061573
          },
          "point_estimate": 649164.2037366143,
          "standard_error": 190.1431670174458
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499.1693051866154,
            "upper_bound": 1130.2674756195372
          },
          "point_estimate": 873.795535552824,
          "standard_error": 171.96612236054096
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-repeated-rare-small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1312.1143482365314,
            "upper_bound": 1313.9786049878933
          },
          "point_estimate": 1313.111971467374,
          "standard_error": 0.4787414318695586
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1312.1664268433597,
            "upper_bound": 1314.365362984762
          },
          "point_estimate": 1313.6070127504554,
          "standard_error": 0.5425535234737733
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.19397537028333656,
            "upper_bound": 2.559397643351135
          },
          "point_estimate": 1.54752127271733,
          "standard_error": 0.627717645675481
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1312.8317686141402,
            "upper_bound": 1314.3102799643725
          },
          "point_estimate": 1313.698236666201,
          "standard_error": 0.371839209837416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7234915758054979,
            "upper_bound": 2.1172199417744073
          },
          "point_estimate": 1.5946850655844034,
          "standard_error": 0.375647116473417
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1225313.0225291003,
            "upper_bound": 1227534.265584127
          },
          "point_estimate": 1226378.6893862437,
          "standard_error": 571.9822100196604
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1225019.01,
            "upper_bound": 1228342.433333333
          },
          "point_estimate": 1225331.942857143,
          "standard_error": 1012.6066574719227
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.98573826052213,
            "upper_bound": 3026.73827937596
          },
          "point_estimate": 1118.6786308063463,
          "standard_error": 882.1521566139311
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1225093.9076009502,
            "upper_bound": 1226379.2826839827
          },
          "point_estimate": 1225479.887012987,
          "standard_error": 338.51535821330623
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1080.3118231434703,
            "upper_bound": 2232.49296010793
          },
          "point_estimate": 1898.2971163007903,
          "standard_error": 297.7280320048029
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1365048.4847433865,
            "upper_bound": 1368904.756250661
          },
          "point_estimate": 1366917.9129276895,
          "standard_error": 981.964731346683
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1363881.351851852,
            "upper_bound": 1368747.0
          },
          "point_estimate": 1367205.8209876544,
          "standard_error": 1170.464085873071
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 624.9171963128698,
            "upper_bound": 6445.756679083569
          },
          "point_estimate": 2453.041735153414,
          "standard_error": 1436.8899196773132
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1364232.9848171154,
            "upper_bound": 1368377.0831467484
          },
          "point_estimate": 1366524.1896103895,
          "standard_error": 1144.7748710233843
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1746.1658336569726,
            "upper_bound": 4321.534961676098
          },
          "point_estimate": 3268.2600584559355,
          "standard_error": 702.3710780680074
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1264699.798849035,
            "upper_bound": 1269288.0923850574
          },
          "point_estimate": 1266713.2404282975,
          "standard_error": 1187.753582219309
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1264840.70545977,
            "upper_bound": 1268166.5560344828
          },
          "point_estimate": 1265665.078817734,
          "standard_error": 905.3820108681742
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246.36904217785116,
            "upper_bound": 5254.923603688943
          },
          "point_estimate": 1271.1204162262688,
          "standard_error": 1353.8133128105303
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1265450.5443641816,
            "upper_bound": 1267263.901773088
          },
          "point_estimate": 1266302.5801164352,
          "standard_error": 458.5203824809519
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1282.8416631256723,
            "upper_bound": 5633.221501527338
          },
          "point_estimate": 3950.5664599302577,
          "standard_error": 1277.172605841624
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332263.02276686503,
            "upper_bound": 332652.5085335497
          },
          "point_estimate": 332449.9095793651,
          "standard_error": 99.88842438681084
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332177.53863636364,
            "upper_bound": 332781.8642857143
          },
          "point_estimate": 332333.8135353535,
          "standard_error": 163.4894511168195
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.97357826479195,
            "upper_bound": 544.122614203521
          },
          "point_estimate": 360.06273203184907,
          "standard_error": 135.07893887431376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332238.1070122715,
            "upper_bound": 332524.79595687333
          },
          "point_estimate": 332353.2757733176,
          "standard_error": 72.9222265392333
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 181.16790044812257,
            "upper_bound": 398.70479686759256
          },
          "point_estimate": 332.3978482197115,
          "standard_error": 54.48473583881043
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 260052.75689503967,
            "upper_bound": 260487.62514172337
          },
          "point_estimate": 260271.09484410431,
          "standard_error": 110.7584652311905
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 259974.44988095236,
            "upper_bound": 260487.55229591837
          },
          "point_estimate": 260388.4673809524,
          "standard_error": 162.4970040538485
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.400872423951697,
            "upper_bound": 656.1672725173551
          },
          "point_estimate": 332.774027020661,
          "standard_error": 180.96136891502064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 260041.6598449248,
            "upper_bound": 260416.89268512433
          },
          "point_estimate": 260205.85547309832,
          "standard_error": 95.08556965298784
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206.3135961381029,
            "upper_bound": 476.2366033986661
          },
          "point_estimate": 370.59196903138553,
          "standard_error": 70.74068954028027
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 458017.62750390626,
            "upper_bound": 459428.02627083333
          },
          "point_estimate": 458673.4998854167,
          "standard_error": 362.2873957219171
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 457748.62875,
            "upper_bound": 459469.5125
          },
          "point_estimate": 458301.09958333336,
          "standard_error": 485.3352465904702
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189.2270145155369,
            "upper_bound": 1873.5035182386123
          },
          "point_estimate": 911.8437706865428,
          "standard_error": 459.80717205042777
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 457724.0938016529,
            "upper_bound": 459205.9527152843
          },
          "point_estimate": 458317.6166233766,
          "standard_error": 390.4873412898343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 504.5939205751409,
            "upper_bound": 1523.3885720260123
          },
          "point_estimate": 1201.6235601979463,
          "standard_error": 254.7446619442713
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 242681.28283492065,
            "upper_bound": 243281.6252457937
          },
          "point_estimate": 242940.95479126985,
          "standard_error": 156.07033131243924
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 242606.67777777775,
            "upper_bound": 243152.00208333333
          },
          "point_estimate": 242715.08309523808,
          "standard_error": 156.71520591732454
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.07822808339222,
            "upper_bound": 670.534666428971
          },
          "point_estimate": 296.98270712746626,
          "standard_error": 161.79786451910334
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 242687.05664397785,
            "upper_bound": 243078.92503654657
          },
          "point_estimate": 242902.97021645025,
          "standard_error": 98.18563479704513
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176.660758484529,
            "upper_bound": 748.7658624875831
          },
          "point_estimate": 520.6003968575365,
          "standard_error": 169.8302261840084
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 620391.662219532,
            "upper_bound": 621151.85914812
          },
          "point_estimate": 620768.5272713209,
          "standard_error": 194.73913052782143
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 620177.093220339,
            "upper_bound": 621311.6398305085
          },
          "point_estimate": 620733.301200565,
          "standard_error": 339.80229021237733
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.4183596643661,
            "upper_bound": 1058.4782591743583
          },
          "point_estimate": 841.0393871871883,
          "standard_error": 240.4284211868024
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 620357.6455302235,
            "upper_bound": 621140.7629945861
          },
          "point_estimate": 620723.8199867929,
          "standard_error": 207.33771651083936
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 408.1842938007284,
            "upper_bound": 791.5180198683387
          },
          "point_estimate": 648.9533183018137,
          "standard_error": 98.40379413539522
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115042.35490315135,
            "upper_bound": 115229.5762511302
          },
          "point_estimate": 115130.29226906267,
          "standard_error": 48.05898346189205
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115037.03607594936,
            "upper_bound": 115237.38429588608
          },
          "point_estimate": 115109.0084915612,
          "standard_error": 41.27001383425239
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.68390157917639,
            "upper_bound": 276.78405552912506
          },
          "point_estimate": 84.51898957544549,
          "standard_error": 66.50463783344546
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115059.93439063904,
            "upper_bound": 115153.26805715382
          },
          "point_estimate": 115103.96975176722,
          "standard_error": 24.3757952082202
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.51772244173687,
            "upper_bound": 213.06209576405453
          },
          "point_estimate": 160.62869324498044,
          "standard_error": 38.37676599111452
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123256.68819949556,
            "upper_bound": 123448.7226669626
          },
          "point_estimate": 123353.48591955876,
          "standard_error": 49.089781862350904
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123203.76525423728,
            "upper_bound": 123480.6572881356
          },
          "point_estimate": 123375.18851224106,
          "standard_error": 63.12625734274942
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.94432072585661,
            "upper_bound": 303.09237597498225
          },
          "point_estimate": 179.1037308033259,
          "standard_error": 76.45621428643008
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123213.3722307824,
            "upper_bound": 123407.2454770356
          },
          "point_estimate": 123315.9870922298,
          "standard_error": 49.74726349941938
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.82335954867132,
            "upper_bound": 203.5304888954701
          },
          "point_estimate": 164.50600856103523,
          "standard_error": 27.98299659964127
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205684.28382970137,
            "upper_bound": 206036.84003130885
          },
          "point_estimate": 205834.61201708368,
          "standard_error": 91.95945818892858
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205621.45233050847,
            "upper_bound": 205948.19915254237
          },
          "point_estimate": 205742.44152542372,
          "standard_error": 92.37929642766824
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.172825399793304,
            "upper_bound": 384.2752696607744
          },
          "point_estimate": 226.67445868758617,
          "standard_error": 85.88068402190522
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205644.64978270317,
            "upper_bound": 205895.51695927748
          },
          "point_estimate": 205736.4410008071,
          "standard_error": 64.2762244534357
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.12583115900232,
            "upper_bound": 444.0994229552531
          },
          "point_estimate": 306.25800139720366,
          "standard_error": 103.9295499047764
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 281066.8190769231,
            "upper_bound": 281646.3534632555
          },
          "point_estimate": 281325.6107591575,
          "standard_error": 149.82140169133132
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280916.2938461538,
            "upper_bound": 281536.2631868132
          },
          "point_estimate": 281243.25128205126,
          "standard_error": 155.8003192694066
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.722289426411194,
            "upper_bound": 700.8757309416586
          },
          "point_estimate": 438.93398636120247,
          "standard_error": 167.44198425703615
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 281005.83035117056,
            "upper_bound": 281407.45915659564
          },
          "point_estimate": 281172.33626373624,
          "standard_error": 101.95899118947158
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 196.01079982560967,
            "upper_bound": 694.2153923986241
          },
          "point_estimate": 498.8552865073129,
          "standard_error": 142.64505392096314
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 305239.3723730159,
            "upper_bound": 305924.41303042334
          },
          "point_estimate": 305555.52497751324,
          "standard_error": 175.9936517130515
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 305054.88333333336,
            "upper_bound": 305869.32879629626
          },
          "point_estimate": 305423.9420634921,
          "standard_error": 191.0190178574769
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.04852341367298,
            "upper_bound": 919.2916440126252
          },
          "point_estimate": 551.1663947981905,
          "standard_error": 209.04934631947785
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 305132.5009848485,
            "upper_bound": 305793.2316666667
          },
          "point_estimate": 305436.23991341994,
          "standard_error": 174.84181936574143
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 250.69678655429047,
            "upper_bound": 784.6393175397495
          },
          "point_estimate": 584.8389188915947,
          "standard_error": 145.09906435214177
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325557.462474038,
            "upper_bound": 326503.015904195
          },
          "point_estimate": 326042.2769391298,
          "standard_error": 241.03309909822235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325603.43973214284,
            "upper_bound": 326540.56098533166
          },
          "point_estimate": 326113.721875,
          "standard_error": 207.9846546376779
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.76266005424853,
            "upper_bound": 1330.258689664425
          },
          "point_estimate": 486.0558401207974,
          "standard_error": 331.5489467459965
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326064.7196830768,
            "upper_bound": 326903.0848901099
          },
          "point_estimate": 326470.84758812614,
          "standard_error": 223.21213304556423
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 350.8680203978103,
            "upper_bound": 1079.2725519162564
          },
          "point_estimate": 801.1593158422047,
          "standard_error": 186.13067694727968
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406896.3539880952,
            "upper_bound": 407644.72729365074
          },
          "point_estimate": 407237.2993769841,
          "standard_error": 192.38640968522415
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406771.3486111111,
            "upper_bound": 407748.99652777775
          },
          "point_estimate": 407079.8148148148,
          "standard_error": 206.29310458853817
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.49350927687482,
            "upper_bound": 896.6352807482294
          },
          "point_estimate": 389.63923586031615,
          "standard_error": 197.86182021402743
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406899.75749034056,
            "upper_bound": 407218.4985735736
          },
          "point_estimate": 407080.3712265512,
          "standard_error": 81.55522930267887
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199.6026924284728,
            "upper_bound": 805.507974576019
          },
          "point_estimate": 640.5414178770059,
          "standard_error": 159.84249890595123
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279623.9755580967,
            "upper_bound": 280391.7645869963
          },
          "point_estimate": 279952.4825528083,
          "standard_error": 200.2190535399409
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279530.2813034188,
            "upper_bound": 280210.1561538462
          },
          "point_estimate": 279651.11875,
          "standard_error": 203.5894262321765
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.96239481026172,
            "upper_bound": 841.6424209425043
          },
          "point_estimate": 267.30721550436897,
          "standard_error": 215.6396328793309
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279562.90898552444,
            "upper_bound": 280012.5369462499
          },
          "point_estimate": 279759.0872927073,
          "standard_error": 129.47236349570557
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206.24238030402697,
            "upper_bound": 965.0495583460582
          },
          "point_estimate": 668.0868952786049,
          "standard_error": 228.4218752658068
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126036.58382455925,
            "upper_bound": 126279.78119823078
          },
          "point_estimate": 126163.88134028122,
          "standard_error": 62.50447648652377
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125979.0351787774,
            "upper_bound": 126333.5782391388
          },
          "point_estimate": 126229.2865793376,
          "standard_error": 105.16283417960494
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.36064374385907,
            "upper_bound": 338.48849378300986
          },
          "point_estimate": 163.54297941487312,
          "standard_error": 98.04279439722625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126002.03707482478,
            "upper_bound": 126251.26141126532
          },
          "point_estimate": 126133.7625129196,
          "standard_error": 63.42502983382409
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.3914661854334,
            "upper_bound": 247.63340809508344
          },
          "point_estimate": 207.67814782422815,
          "standard_error": 34.81581309365766
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207592.13443238093,
            "upper_bound": 207874.26260663263
          },
          "point_estimate": 207715.8482752834,
          "standard_error": 73.07790791801283
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207586.1,
            "upper_bound": 207794.33697959184
          },
          "point_estimate": 207629.6151111111,
          "standard_error": 68.24464574717413
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.798299088361173,
            "upper_bound": 295.4991053062382
          },
          "point_estimate": 134.61075841017757,
          "standard_error": 79.56311818792783
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207628.1762722256,
            "upper_bound": 207922.3031746288
          },
          "point_estimate": 207746.04065306124,
          "standard_error": 74.57555989086654
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.62651113073521,
            "upper_bound": 353.38144415211747
          },
          "point_estimate": 244.7302713665096,
          "standard_error": 81.97322210673356
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154688.01446858747,
            "upper_bound": 154910.4745587639
          },
          "point_estimate": 154802.38626933467,
          "standard_error": 56.85814704703798
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154685.2096926714,
            "upper_bound": 154934.06382978722
          },
          "point_estimate": 154804.85939716312,
          "standard_error": 55.38368749711656
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.935927288120023,
            "upper_bound": 324.8211370878671
          },
          "point_estimate": 152.52483814319646,
          "standard_error": 81.75317433434259
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154792.91420589248,
            "upper_bound": 154986.48489880827
          },
          "point_estimate": 154898.68184581376,
          "standard_error": 49.55734911738412
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.12024816598074,
            "upper_bound": 248.13895275207693
          },
          "point_estimate": 189.45028550572337,
          "standard_error": 40.52178389179863
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44348.94277479675,
            "upper_bound": 44461.274168132986
          },
          "point_estimate": 44395.903917247386,
          "standard_error": 29.43580362201578
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44338.74918699187,
            "upper_bound": 44424.6906097561
          },
          "point_estimate": 44365.2475029036,
          "standard_error": 20.187117239911395
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.849212572443681,
            "upper_bound": 102.32757361015275
          },
          "point_estimate": 39.533653139602436,
          "standard_error": 25.228320046286143
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44344.56458685224,
            "upper_bound": 44379.26293100989
          },
          "point_estimate": 44361.79368070953,
          "standard_error": 8.800603364499116
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.773343316614746,
            "upper_bound": 141.558588440045
          },
          "point_estimate": 97.83637256395744,
          "standard_error": 35.501945937890355
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1225761.881750265,
            "upper_bound": 1228412.340142857
          },
          "point_estimate": 1226975.0199947092,
          "standard_error": 683.0666616438828
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1225313.85,
            "upper_bound": 1228465.1037037035
          },
          "point_estimate": 1225883.741269841,
          "standard_error": 937.3305706666176
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.52391411870371,
            "upper_bound": 3893.7385830945104
          },
          "point_estimate": 1423.292915398314,
          "standard_error": 934.8705573948558
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1226199.5835581296,
            "upper_bound": 1228271.243908046
          },
          "point_estimate": 1227465.2096103895,
          "standard_error": 529.6475255159984
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1007.8740822904714,
            "upper_bound": 3039.3361205934243
          },
          "point_estimate": 2268.5783019382447,
          "standard_error": 558.4962294038041
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103854.0370277891,
            "upper_bound": 104071.2470547619
          },
          "point_estimate": 103964.60634829932,
          "standard_error": 55.56640555720372
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103839.49912244898,
            "upper_bound": 104100.20857142858
          },
          "point_estimate": 103976.8792857143,
          "standard_error": 67.68113630103885
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.1353240566548,
            "upper_bound": 310.40076134642675
          },
          "point_estimate": 190.8049686125369,
          "standard_error": 69.58872370875442
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103856.44659778092,
            "upper_bound": 104111.69241153447
          },
          "point_estimate": 103981.23988868274,
          "standard_error": 66.02653842570743
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.1198707222531,
            "upper_bound": 234.91050687573576
          },
          "point_estimate": 184.66124380445177,
          "standard_error": 34.53525837106281
        }
      }
    },
    "memrmem/stud/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/stud_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27122.406596381184,
            "upper_bound": 27165.3312754679
          },
          "point_estimate": 27143.0802754679,
          "standard_error": 10.964785238820095
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27117.216268656717,
            "upper_bound": 27165.234048507464
          },
          "point_estimate": 27140.407483416253,
          "standard_error": 10.771881591455182
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.917803031072125,
            "upper_bound": 61.33942061996375
          },
          "point_estimate": 21.628937464267867,
          "standard_error": 13.73227155749322
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27118.105975478877,
            "upper_bound": 27150.098280910275
          },
          "point_estimate": 27133.279780965306,
          "standard_error": 8.554648802959514
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.24437191725368,
            "upper_bound": 49.527899382140525
          },
          "point_estimate": 36.58885949797117,
          "standard_error": 8.600540784940758
        }
      }
    },
    "memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/stud_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28304.96913128772,
            "upper_bound": 28340.799167714624
          },
          "point_estimate": 28321.22619923243,
          "standard_error": 9.204920014184207
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28302.24769170579,
            "upper_bound": 28341.31701319025
          },
          "point_estimate": 28309.53922143975,
          "standard_error": 9.946013757251135
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5358473613780937,
            "upper_bound": 47.35580541904755
          },
          "point_estimate": 21.540391072980093,
          "standard_error": 11.813388927867232
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28298.56804903495,
            "upper_bound": 28326.11150225917
          },
          "point_estimate": 28312.402554722274,
          "standard_error": 7.007767913228283
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.61061044260933,
            "upper_bound": 42.24282901575682
          },
          "point_estimate": 30.733213779024915,
          "standard_error": 8.33837924046477
        }
      }
    },
    "memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/stud_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 564732.1568634616,
            "upper_bound": 565766.9498571429
          },
          "point_estimate": 565231.9390054945,
          "standard_error": 264.74450171019174
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 564491.973076923,
            "upper_bound": 566206.6681318681
          },
          "point_estimate": 564966.9176923077,
          "standard_error": 434.54606237574177
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164.2112524693528,
            "upper_bound": 1392.032828978887
          },
          "point_estimate": 930.5324898259144,
          "standard_error": 335.4217784342885
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 564648.479380484,
            "upper_bound": 565566.8214556744
          },
          "point_estimate": 565095.8111888112,
          "standard_error": 230.9773641180157
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498.5849317079139,
            "upper_bound": 1057.0561444507446
          },
          "point_estimate": 883.0710836381281,
          "standard_error": 138.60465273168103
        }
      }
    },
    "memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/stud_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1129.2122028768915,
            "upper_bound": 1130.9929139312342
          },
          "point_estimate": 1130.0772284686163,
          "standard_error": 0.4572727522438531
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128.6432432432432,
            "upper_bound": 1131.0158537848192
          },
          "point_estimate": 1130.2071443364548,
          "standard_error": 0.5820050368295925
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.15129981092050385,
            "upper_bound": 2.69115231165103
          },
          "point_estimate": 1.6406150090838332,
          "standard_error": 0.6514293363230942
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128.7691656828854,
            "upper_bound": 1130.5365077361844
          },
          "point_estimate": 1129.6472627217454,
          "standard_error": 0.47131423384627785
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8550895418742038,
            "upper_bound": 1.946726412817458
          },
          "point_estimate": 1.5217741462994578,
          "standard_error": 0.2886083858411263
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.45564385296542,
            "upper_bound": 34.51309362272997
          },
          "point_estimate": 34.48397676849755,
          "standard_error": 0.014718770387664204
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.443629958435565,
            "upper_bound": 34.52299118825772
          },
          "point_estimate": 34.48449588361072,
          "standard_error": 0.01768655499685911
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00834171943588774,
            "upper_bound": 0.09439784897710982
          },
          "point_estimate": 0.03890963762233583,
          "standard_error": 0.021540863672261143
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.45253190623792,
            "upper_bound": 34.519582934790975
          },
          "point_estimate": 34.48226654299552,
          "standard_error": 0.017004874967504757
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026967944406157008,
            "upper_bound": 0.06191565367517068
          },
          "point_estimate": 0.04911666435241838,
          "standard_error": 0.009010865452800856
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.89856996087535,
            "upper_bound": 38.01004055370729
          },
          "point_estimate": 37.945583844256554,
          "standard_error": 0.02926100849911718
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.901159353182095,
            "upper_bound": 37.96822508807667
          },
          "point_estimate": 37.91513842595298,
          "standard_error": 0.01845175361641776
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004674260717370291,
            "upper_bound": 0.1041592484450906
          },
          "point_estimate": 0.030420507236555473,
          "standard_error": 0.026659705287510636
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.90151642998891,
            "upper_bound": 37.92804570663988
          },
          "point_estimate": 37.91132539860834,
          "standard_error": 0.00666256417939665
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024925111263933132,
            "upper_bound": 0.14350145150719565
          },
          "point_estimate": 0.09721247098990056,
          "standard_error": 0.03659086699170306
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.39119949264061,
            "upper_bound": 26.41966081818255
          },
          "point_estimate": 26.40598616868005,
          "standard_error": 0.007262945370932014
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.386371901286047,
            "upper_bound": 26.429594719889725
          },
          "point_estimate": 26.408616430911685,
          "standard_error": 0.00926464766865978
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004506764870257695,
            "upper_bound": 0.04416692532908096
          },
          "point_estimate": 0.02511326127809239,
          "standard_error": 0.010522334660173186
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.401015845809347,
            "upper_bound": 26.420554278362665
          },
          "point_estimate": 26.411347903803257,
          "standard_error": 0.004980901168616639
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012704933193633128,
            "upper_bound": 0.030364437341709596
          },
          "point_estimate": 0.024200931693763608,
          "standard_error": 0.004553377101992936
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.55748569128823,
            "upper_bound": 32.5933735338155
          },
          "point_estimate": 32.577493317567985,
          "standard_error": 0.009286654179160823
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.561514248462984,
            "upper_bound": 32.60133167248025
          },
          "point_estimate": 32.58543695764884,
          "standard_error": 0.008565591530476155
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003468818278924417,
            "upper_bound": 0.04133856129662249
          },
          "point_estimate": 0.020087080690101424,
          "standard_error": 0.010162409763225142
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.57800214124371,
            "upper_bound": 32.60073975680092
          },
          "point_estimate": 32.59192426898877,
          "standard_error": 0.0058691019761879
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01061639241880915,
            "upper_bound": 0.0433208673865025
          },
          "point_estimate": 0.03095424333222376,
          "standard_error": 0.009328766913761086
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.3392150468761,
            "upper_bound": 40.37696369526623
          },
          "point_estimate": 40.35762788902315,
          "standard_error": 0.009659696924706718
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.329054582737086,
            "upper_bound": 40.38232963908263
          },
          "point_estimate": 40.35343691397175,
          "standard_error": 0.01804762334080356
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00546482360161644,
            "upper_bound": 0.057199200632838336
          },
          "point_estimate": 0.038753247822217105,
          "standard_error": 0.01458411700765
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.34262408376408,
            "upper_bound": 40.37854265332451
          },
          "point_estimate": 40.36173522436156,
          "standard_error": 0.0092420311997644
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02019025495192876,
            "upper_bound": 0.039311943655643375
          },
          "point_estimate": 0.03218511657150755,
          "standard_error": 0.005067811159083797
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.94726204548513,
            "upper_bound": 73.0509919223604
          },
          "point_estimate": 72.99527522784514,
          "standard_error": 0.026760138073938653
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.92847179966401,
            "upper_bound": 73.08399486704728
          },
          "point_estimate": 72.95234786971848,
          "standard_error": 0.03872939815700355
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0069872815597874305,
            "upper_bound": 0.13370497791580016
          },
          "point_estimate": 0.049176963781660886,
          "standard_error": 0.03468238610688769
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.93654086891783,
            "upper_bound": 73.01137104444517
          },
          "point_estimate": 72.96243581624869,
          "standard_error": 0.019316596337992875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02720942315455407,
            "upper_bound": 0.11197287020019275
          },
          "point_estimate": 0.08906580200691117,
          "standard_error": 0.019140863926733132
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.931063779929,
            "upper_bound": 66.06039846244931
          },
          "point_estimate": 65.99300889782427,
          "standard_error": 0.03328126141450817
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.89938573214604,
            "upper_bound": 66.11258266478586
          },
          "point_estimate": 65.96609880631422,
          "standard_error": 0.05045109506641327
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02572560371154009,
            "upper_bound": 0.18060909932095223
          },
          "point_estimate": 0.1074603356099603,
          "standard_error": 0.0425508427149183
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.90619214016459,
            "upper_bound": 66.0283823644196
          },
          "point_estimate": 65.95762595711712,
          "standard_error": 0.03146935065920187
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05397075045996296,
            "upper_bound": 0.13479137181132617
          },
          "point_estimate": 0.11113629163584818,
          "standard_error": 0.019080510831203337
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.566751139305595,
            "upper_bound": 56.88385956079361
          },
          "point_estimate": 56.6970380010204,
          "standard_error": 0.08399305234308924
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.5494881164998,
            "upper_bound": 56.76580389465651
          },
          "point_estimate": 56.61466529777532,
          "standard_error": 0.04959354219013565
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019858907395708277,
            "upper_bound": 0.28498077757130574
          },
          "point_estimate": 0.092016991323476,
          "standard_error": 0.06573147394952042
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.531431293236224,
            "upper_bound": 56.660421274625875
          },
          "point_estimate": 56.58724005176612,
          "standard_error": 0.03242325149451301
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.054464211264862485,
            "upper_bound": 0.4132643127063161
          },
          "point_estimate": 0.27876320056273574,
          "standard_error": 0.10883858419923044
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.5405281672979,
            "upper_bound": 108.70466790991586
          },
          "point_estimate": 108.62197587410374,
          "standard_error": 0.04200312474552222
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.46007499424066,
            "upper_bound": 108.7173613476258
          },
          "point_estimate": 108.64876215685454,
          "standard_error": 0.057051414934822346
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0110434067804759,
            "upper_bound": 0.28053257954493066
          },
          "point_estimate": 0.10708210917168368,
          "standard_error": 0.06863632244169941
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.5313629871273,
            "upper_bound": 108.6896382733669
          },
          "point_estimate": 108.61554553858495,
          "standard_error": 0.041415475356265
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07816414061696558,
            "upper_bound": 0.17782485329268455
          },
          "point_estimate": 0.14012770510706957,
          "standard_error": 0.025852188800413813
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.73765557246841,
            "upper_bound": 45.809932332524255
          },
          "point_estimate": 45.77631259032755,
          "standard_error": 0.018557016325453257
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.7360563977396,
            "upper_bound": 45.82050910087406
          },
          "point_estimate": 45.7881052303084,
          "standard_error": 0.021160575864670968
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014506078948873045,
            "upper_bound": 0.09790078158634594
          },
          "point_estimate": 0.04986373208577092,
          "standard_error": 0.0214393559513385
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.74963798576178,
            "upper_bound": 45.81071511814623
          },
          "point_estimate": 45.78061714174737,
          "standard_error": 0.01556796637270064
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028316516775347474,
            "upper_bound": 0.08412689930535809
          },
          "point_estimate": 0.0618450206785358,
          "standard_error": 0.015482172680164693
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.907588138548675,
            "upper_bound": 50.006190258434515
          },
          "point_estimate": 49.949452537616914,
          "standard_error": 0.025686564673898852
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.89984016276997,
            "upper_bound": 49.96794423653105
          },
          "point_estimate": 49.93116935734641,
          "standard_error": 0.017637978629324313
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009279097451517173,
            "upper_bound": 0.09056839263383436
          },
          "point_estimate": 0.05048554898279412,
          "standard_error": 0.019845426573040687
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.90258856786001,
            "upper_bound": 49.95910217435289
          },
          "point_estimate": 49.93173131619415,
          "standard_error": 0.014629161219362984
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024527786821593324,
            "upper_bound": 0.12648921518364284
          },
          "point_estimate": 0.08603056042119668,
          "standard_error": 0.03158460935880657
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.64974420364685,
            "upper_bound": 90.78193801635862
          },
          "point_estimate": 90.71093724605336,
          "standard_error": 0.03387533448036337
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.6374744645347,
            "upper_bound": 90.74976657446048
          },
          "point_estimate": 90.71166213118556,
          "standard_error": 0.03056622851247931
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01556574715514976,
            "upper_bound": 0.1670741196489601
          },
          "point_estimate": 0.07650719326167554,
          "standard_error": 0.03667878129397049
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.67713000010284,
            "upper_bound": 90.74123743153106
          },
          "point_estimate": 90.7111163768698,
          "standard_error": 0.016302320493309184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.043183035286683054,
            "upper_bound": 0.15931958576378685
          },
          "point_estimate": 0.11277591310229242,
          "standard_error": 0.031991668588631554
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 940955.3402584454,
            "upper_bound": 941854.092153948
          },
          "point_estimate": 941426.707083842,
          "standard_error": 230.58991377188391
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 940871.817948718,
            "upper_bound": 941991.1666666667
          },
          "point_estimate": 941592.1908831908,
          "standard_error": 317.54632352405747
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157.55222438238107,
            "upper_bound": 1354.4139998005248
          },
          "point_estimate": 711.9014232586554,
          "standard_error": 281.3156023234085
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 940905.043108467,
            "upper_bound": 941725.3533073209
          },
          "point_estimate": 941279.9025641026,
          "standard_error": 210.74983549675255
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396.9186339991639,
            "upper_bound": 1013.2614824207552
          },
          "point_estimate": 770.3276138440086,
          "standard_error": 167.7290638713429
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1202245.4931210957,
            "upper_bound": 1203185.555949821
          },
          "point_estimate": 1202720.1185266257,
          "standard_error": 239.4634029240866
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1202284.3360215053,
            "upper_bound": 1203283.5004480286
          },
          "point_estimate": 1202595.260080645,
          "standard_error": 254.43215703594973
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.65190057911433,
            "upper_bound": 1406.891544062864
          },
          "point_estimate": 689.4514995877065,
          "standard_error": 314.0120779082835
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1202424.686542848,
            "upper_bound": 1203017.7902215903
          },
          "point_estimate": 1202772.2790950984,
          "standard_error": 150.8134913824289
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 375.76206906237167,
            "upper_bound": 1067.330311074188
          },
          "point_estimate": 796.9050770049164,
          "standard_error": 175.83965663695778
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1172084.6616315923,
            "upper_bound": 1175137.808106951
          },
          "point_estimate": 1173476.559000256,
          "standard_error": 785.7564770610879
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1171513.8894009218,
            "upper_bound": 1175766.8919354838
          },
          "point_estimate": 1172771.1592741937,
          "standard_error": 939.389985682338
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 331.37544185862487,
            "upper_bound": 3696.399959106878
          },
          "point_estimate": 1672.7693259475452,
          "standard_error": 859.2424026094718
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1171502.443526171,
            "upper_bound": 1173063.5243977134
          },
          "point_estimate": 1172270.6594888982,
          "standard_error": 408.76095934959056
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 931.8099732329296,
            "upper_bound": 3261.7357085231392
          },
          "point_estimate": 2610.9122557840547,
          "standard_error": 610.7952388305513
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1538347.3774059603,
            "upper_bound": 1541809.3116517856
          },
          "point_estimate": 1539813.1322536375,
          "standard_error": 905.621357371754
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1537922.615740741,
            "upper_bound": 1540769.8348214286
          },
          "point_estimate": 1538657.1104166666,
          "standard_error": 849.8044627578387
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 411.93800032557857,
            "upper_bound": 3559.679294303221
          },
          "point_estimate": 1727.8521246367857,
          "standard_error": 811.5454201596914
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1538254.3526331938,
            "upper_bound": 1540509.5445285528
          },
          "point_estimate": 1539076.4385281384,
          "standard_error": 575.5184154985211
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 943.002455753272,
            "upper_bound": 4416.898348892847
          },
          "point_estimate": 3016.3170171262163,
          "standard_error": 1073.1880271966045
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1495723.3800564285,
            "upper_bound": 1497890.4188333335
          },
          "point_estimate": 1496686.3021031744,
          "standard_error": 559.3709702751596
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1495595.28,
            "upper_bound": 1497339.5733333332
          },
          "point_estimate": 1496172.6819047618,
          "standard_error": 468.76634735912626
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.63984315080992,
            "upper_bound": 2477.9314640076714
          },
          "point_estimate": 859.3096144440116,
          "standard_error": 605.7049034256153
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1495700.537329942,
            "upper_bound": 1499110.0303845592
          },
          "point_estimate": 1497076.3313246754,
          "standard_error": 967.2486560223924
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 604.3076589576721,
            "upper_bound": 2638.999329054108
          },
          "point_estimate": 1866.1052849761604,
          "standard_error": 583.6981861053492
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 320276.87362956,
            "upper_bound": 320939.9059899749
          },
          "point_estimate": 320616.83485206065,
          "standard_error": 170.14900052781098
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 320248.00615253416,
            "upper_bound": 321034.83201754384
          },
          "point_estimate": 320665.0285087719,
          "standard_error": 200.94602460447183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134.22081603815076,
            "upper_bound": 958.9560671857216
          },
          "point_estimate": 487.99843159945874,
          "standard_error": 207.3478189624864
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 320047.8971477319,
            "upper_bound": 320693.431985007
          },
          "point_estimate": 320321.42200956936,
          "standard_error": 161.30921047600518
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 293.5348512817109,
            "upper_bound": 737.9272091898989
          },
          "point_estimate": 566.7125911258339,
          "standard_error": 114.72860060455696
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387284.78540070925,
            "upper_bound": 387773.88023640664
          },
          "point_estimate": 387543.4159515367,
          "standard_error": 125.23001997891366
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387127.3457446809,
            "upper_bound": 387823.1170212766
          },
          "point_estimate": 387670.57872340427,
          "standard_error": 167.36191393289442
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.7312736028782,
            "upper_bound": 680.063087607251
          },
          "point_estimate": 250.5189132119621,
          "standard_error": 167.98522344534615
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387625.2585496268,
            "upper_bound": 387803.73052921746
          },
          "point_estimate": 387718.4756009948,
          "standard_error": 46.218456858124405
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184.01882781358583,
            "upper_bound": 528.8198846404249
          },
          "point_estimate": 416.8564387341285,
          "standard_error": 85.42767473844113
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 841265.0456324855,
            "upper_bound": 843334.1217514429
          },
          "point_estimate": 842226.9630393218,
          "standard_error": 528.6384268456975
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 841250.7642045454,
            "upper_bound": 842923.2045454546
          },
          "point_estimate": 841982.2247474748,
          "standard_error": 445.8574616015172
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 284.5094373731571,
            "upper_bound": 2581.5080081462793
          },
          "point_estimate": 1149.8803644339982,
          "standard_error": 568.451279232343
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 841735.3034603517,
            "upper_bound": 842772.5434892159
          },
          "point_estimate": 842279.977863046,
          "standard_error": 275.2092579245735
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 636.921806115631,
            "upper_bound": 2497.7475009768254
          },
          "point_estimate": 1767.1696381057443,
          "standard_error": 502.8458174719972
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 922199.6918035714,
            "upper_bound": 923629.6675711308
          },
          "point_estimate": 922856.8148035712,
          "standard_error": 366.7519549039653
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 922157.5392857144,
            "upper_bound": 923327.0375
          },
          "point_estimate": 922609.37375,
          "standard_error": 297.29359458823075
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.5728520975242,
            "upper_bound": 1773.54168601341
          },
          "point_estimate": 774.0947011321232,
          "standard_error": 386.7825839798091
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 922249.5120122104,
            "upper_bound": 922927.0302083332
          },
          "point_estimate": 922567.4873376624,
          "standard_error": 171.67123003723233
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 440.5707810799604,
            "upper_bound": 1734.023503595538
          },
          "point_estimate": 1228.7731841859995,
          "standard_error": 361.7536531862016
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 686040.3205489668,
            "upper_bound": 687666.8379323898
          },
          "point_estimate": 686835.0228451632,
          "standard_error": 415.6800036004908
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 685979.3122641509,
            "upper_bound": 687913.9559748428
          },
          "point_estimate": 686794.9272237197,
          "standard_error": 373.3848517753847
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.9973069865202,
            "upper_bound": 2565.78145350481
          },
          "point_estimate": 716.690181552938,
          "standard_error": 661.9396638498197
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 686256.2976311528,
            "upper_bound": 687080.5486150141
          },
          "point_estimate": 686612.884587111,
          "standard_error": 208.74023172966903
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 621.6118232331393,
            "upper_bound": 1819.0881063797676
          },
          "point_estimate": 1385.9785302836142,
          "standard_error": 300.40969714196024
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 312060.3914031339,
            "upper_bound": 312572.92492320406
          },
          "point_estimate": 312307.696498779,
          "standard_error": 131.442143705838
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 311949.3076923077,
            "upper_bound": 312601.1991249491
          },
          "point_estimate": 312305.3423076923,
          "standard_error": 162.18035745051102
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107.6490074905983,
            "upper_bound": 733.1872323679663
          },
          "point_estimate": 420.0721045080449,
          "standard_error": 160.59651021717136
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 312097.67834757833,
            "upper_bound": 312566.8663234913
          },
          "point_estimate": 312313.343012543,
          "standard_error": 117.81568978572125
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 234.31197484896123,
            "upper_bound": 563.4568265933922
          },
          "point_estimate": 435.8593739678139,
          "standard_error": 86.41914662171243
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648274.7630311891,
            "upper_bound": 650162.0182247285
          },
          "point_estimate": 649096.1677332219,
          "standard_error": 488.8422327564875
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 647885.6929824562,
            "upper_bound": 649687.0384990254
          },
          "point_estimate": 648849.6025062656,
          "standard_error": 386.8015240105088
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174.24906453803632,
            "upper_bound": 1959.6442890690028
          },
          "point_estimate": 993.9328857751514,
          "standard_error": 513.5568507613865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648514.7381072547,
            "upper_bound": 649289.0588648361
          },
          "point_estimate": 648900.0384597859,
          "standard_error": 193.59212827505405
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 539.3451421942383,
            "upper_bound": 2351.052703796939
          },
          "point_estimate": 1635.5118294558386,
          "standard_error": 541.9311738032098
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187196.6247766323,
            "upper_bound": 187509.3477387089
          },
          "point_estimate": 187328.37125245456,
          "standard_error": 81.94102432185161
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187172.88453608248,
            "upper_bound": 187397.49502945508
          },
          "point_estimate": 187252.61383161513,
          "standard_error": 66.10109597918414
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.510935945183167,
            "upper_bound": 294.9679774952639
          },
          "point_estimate": 144.96350510678565,
          "standard_error": 66.56748052357375
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187166.2331753858,
            "upper_bound": 187379.88542642925
          },
          "point_estimate": 187290.6098808408,
          "standard_error": 55.169676435871715
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.1024027419185,
            "upper_bound": 399.808632511902
          },
          "point_estimate": 272.5006448750423,
          "standard_error": 98.54578646050943
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/stud_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142268.2850613839,
            "upper_bound": 142583.64958031062
          },
          "point_estimate": 142419.80109002977,
          "standard_error": 80.76677954487968
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142197.60546875,
            "upper_bound": 142677.4296875
          },
          "point_estimate": 142349.76627604166,
          "standard_error": 125.3448666472495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.36479150642134,
            "upper_bound": 436.6584412711686
          },
          "point_estimate": 298.44998083426617,
          "standard_error": 98.87637707327336
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142183.01126889573,
            "upper_bound": 142557.62236588786
          },
          "point_estimate": 142337.67781047078,
          "standard_error": 96.68654171666743
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153.31623999408396,
            "upper_bound": 332.9627848778299
          },
          "point_estimate": 270.2315497876481,
          "standard_error": 45.960515048713845
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/stud_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 625158.0735977603,
            "upper_bound": 625876.0496990517
          },
          "point_estimate": 625529.1995486951,
          "standard_error": 183.33477687304293
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 625090.9790254238,
            "upper_bound": 625951.6854990583
          },
          "point_estimate": 625681.3958837772,
          "standard_error": 238.4525499236064
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.49728532629123,
            "upper_bound": 1015.8333171347972
          },
          "point_estimate": 438.8294891583516,
          "standard_error": 247.2571648914056
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 625162.5842339125,
            "upper_bound": 625845.9995798392
          },
          "point_estimate": 625494.3396434075,
          "standard_error": 172.82041505739844
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 315.2046631364478,
            "upper_bound": 786.772632141252
          },
          "point_estimate": 611.9431153430523,
          "standard_error": 120.65363872834924
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/stud_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1298.141625383544,
            "upper_bound": 1299.7436564212326
          },
          "point_estimate": 1298.9374520123076,
          "standard_error": 0.4096602319915081
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1297.9019881284416,
            "upper_bound": 1299.978134162912
          },
          "point_estimate": 1298.8320979897358,
          "standard_error": 0.5471161839910603
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2311548182697054,
            "upper_bound": 2.3319266870381616
          },
          "point_estimate": 1.4060576947687242,
          "standard_error": 0.5324799714050445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1297.9750660151672,
            "upper_bound": 1299.7130174276367
          },
          "point_estimate": 1298.8446407557972,
          "standard_error": 0.4554839264141951
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7382323922260894,
            "upper_bound": 1.7642125084347235
          },
          "point_estimate": 1.3675613393793902,
          "standard_error": 0.2619480323255711
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104313.7524579123,
            "upper_bound": 1107953.968386243
          },
          "point_estimate": 1106238.4748424725,
          "standard_error": 937.099932748774
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1103539.121212121,
            "upper_bound": 1108586.7794612795
          },
          "point_estimate": 1107454.896969697,
          "standard_error": 1258.6721425262103
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 493.3539233372914,
            "upper_bound": 5153.345822146346
          },
          "point_estimate": 2816.2061378811277,
          "standard_error": 1228.7775161111365
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1106677.853958968,
            "upper_bound": 1108562.9675371426
          },
          "point_estimate": 1107823.4994096812,
          "standard_error": 477.7364579604394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1422.3636692645607,
            "upper_bound": 3895.8286899135624
          },
          "point_estimate": 3134.076509995866,
          "standard_error": 621.7824365234984
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1232361.3340833334,
            "upper_bound": 1234189.407358135
          },
          "point_estimate": 1233311.9728822752,
          "standard_error": 468.3704967039991
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231989.3458333334,
            "upper_bound": 1234521.2483333335
          },
          "point_estimate": 1233754.984920635,
          "standard_error": 624.2370331343775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286.12943992023304,
            "upper_bound": 2573.5365703105836
          },
          "point_estimate": 1470.5340808928404,
          "standard_error": 618.8024705541485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1232769.3846560067,
            "upper_bound": 1234125.927759627
          },
          "point_estimate": 1233582.155064935,
          "standard_error": 353.615095137967
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 803.298276398298,
            "upper_bound": 1933.3858563600215
          },
          "point_estimate": 1558.68486611275,
          "standard_error": 281.1454689933089
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1136179.4325744049,
            "upper_bound": 1138323.1217410716
          },
          "point_estimate": 1137268.9286160716,
          "standard_error": 544.1127489759856
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1136259.353125,
            "upper_bound": 1138216.265625
          },
          "point_estimate": 1137183.0212053573,
          "standard_error": 488.73272324086656
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.8627347331381,
            "upper_bound": 3189.118874631822
          },
          "point_estimate": 1382.270977335051,
          "standard_error": 638.5740946222085
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1136609.8663219514,
            "upper_bound": 1137711.8700339147
          },
          "point_estimate": 1137116.827353896,
          "standard_error": 290.4874740758402
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 685.9994292225435,
            "upper_bound": 2507.4361200439957
          },
          "point_estimate": 1816.9826067647407,
          "standard_error": 477.82694930148176
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 689244.2147484276,
            "upper_bound": 691243.0345290132
          },
          "point_estimate": 690252.6672289607,
          "standard_error": 511.8957337734285
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 688890.5094339623,
            "upper_bound": 691679.2339622641
          },
          "point_estimate": 690427.0948450135,
          "standard_error": 699.9564008846666
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.95564894826185,
            "upper_bound": 2886.0524200831073
          },
          "point_estimate": 1731.9689100532237,
          "standard_error": 695.3292936009437
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 689594.889463255,
            "upper_bound": 691014.295565503
          },
          "point_estimate": 690497.6384219554,
          "standard_error": 364.1175465312828
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 951.4368162150956,
            "upper_bound": 2160.3333729617752
          },
          "point_estimate": 1703.2817050037283,
          "standard_error": 308.31298150650133
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237595.99397520485,
            "upper_bound": 237942.49637754177
          },
          "point_estimate": 237761.3238730678,
          "standard_error": 89.16046125601636
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237505.01307189543,
            "upper_bound": 238003.822167756
          },
          "point_estimate": 237706.1462599855,
          "standard_error": 125.29843985874513
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.62941541276786,
            "upper_bound": 512.8511381172774
          },
          "point_estimate": 324.80546124880345,
          "standard_error": 113.00746246024312
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237628.16013681056,
            "upper_bound": 237845.8645228758
          },
          "point_estimate": 237728.27462863937,
          "standard_error": 54.64562473176127
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148.15089512335817,
            "upper_bound": 362.9906854791487
          },
          "point_estimate": 297.17747613784724,
          "standard_error": 53.342526727447414
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 410626.6544850187,
            "upper_bound": 411470.5900749064
          },
          "point_estimate": 411031.05477528094,
          "standard_error": 215.8270140359708
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 410474.61329588015,
            "upper_bound": 411525.3970037453
          },
          "point_estimate": 410940.10955056176,
          "standard_error": 253.40645834468876
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159.11573874813783,
            "upper_bound": 1223.0047698603462
          },
          "point_estimate": 675.8726278885108,
          "standard_error": 275.33016682473783
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 410629.6695163654,
            "upper_bound": 411072.32174808066
          },
          "point_estimate": 410832.28799066105,
          "standard_error": 109.87073730611635
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 363.81538854133976,
            "upper_bound": 944.20391843049
          },
          "point_estimate": 717.1020293352193,
          "standard_error": 153.47652090759908
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 215389.8248832108,
            "upper_bound": 216031.62748933825
          },
          "point_estimate": 215698.98843627452,
          "standard_error": 163.71053239893385
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 215355.36205882352,
            "upper_bound": 216074.6149509804
          },
          "point_estimate": 215650.45830882352,
          "standard_error": 192.15355401428909
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.72930591071719,
            "upper_bound": 943.6801159522188
          },
          "point_estimate": 409.6590519770747,
          "standard_error": 220.17394182200695
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 215440.9272884262,
            "upper_bound": 216325.35006836185
          },
          "point_estimate": 215819.63601222308,
          "standard_error": 238.7244786888128
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 244.49927637949463,
            "upper_bound": 723.8687865100319
          },
          "point_estimate": 545.3359205908544,
          "standard_error": 121.26444377028872
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 566761.6463704117,
            "upper_bound": 567448.924898019
          },
          "point_estimate": 567078.7878825645,
          "standard_error": 176.9406436932559
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 566575.9375,
            "upper_bound": 567397.41796875
          },
          "point_estimate": 567067.3571428572,
          "standard_error": 205.54455625480267
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.0809779460714,
            "upper_bound": 919.588425080292
          },
          "point_estimate": 600.883391936417,
          "standard_error": 227.40172588278855
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 566586.261153199,
            "upper_bound": 567308.0514597736
          },
          "point_estimate": 566840.5421266233,
          "standard_error": 185.29667558964465
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 270.9551533042594,
            "upper_bound": 797.5003370652968
          },
          "point_estimate": 589.4723875800009,
          "standard_error": 149.20396534614358
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1969.870512594322,
            "upper_bound": 1972.8683137715716
          },
          "point_estimate": 1971.4854992339665,
          "standard_error": 0.7708355779830346
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1969.83563595896,
            "upper_bound": 1973.623860449125
          },
          "point_estimate": 1972.2230911731176,
          "standard_error": 1.2130074792083465
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10135557752125884,
            "upper_bound": 4.736150766576223
          },
          "point_estimate": 2.2083543199944944,
          "standard_error": 1.228382759723882
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1971.4750235953056,
            "upper_bound": 1973.4399669855488
          },
          "point_estimate": 1972.7444563904692,
          "standard_error": 0.5028549528595347
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2781495569831738,
            "upper_bound": 3.400031260954186
          },
          "point_estimate": 2.562825655883983,
          "standard_error": 0.5959664528469311
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 610.1229516025012,
            "upper_bound": 610.7687693305246
          },
          "point_estimate": 610.4423709085786,
          "standard_error": 0.1652739405591464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 610.0027277252086,
            "upper_bound": 610.9471050600631
          },
          "point_estimate": 610.3084896028071,
          "standard_error": 0.25078523046009643
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09621861616973108,
            "upper_bound": 0.9047398604560009
          },
          "point_estimate": 0.7203759740013931,
          "standard_error": 0.2156634741134846
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 610.0637511810755,
            "upper_bound": 610.6942688248547
          },
          "point_estimate": 610.3867725690858,
          "standard_error": 0.16236431360115608
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3282954971120209,
            "upper_bound": 0.6745346604412251
          },
          "point_estimate": 0.5498195914056989,
          "standard_error": 0.08886801426893016
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.26825063414732,
            "upper_bound": 113.4243336945387
          },
          "point_estimate": 113.35006484550578,
          "standard_error": 0.04023516894382274
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.22213654187168,
            "upper_bound": 113.46277458842708
          },
          "point_estimate": 113.37684890044709,
          "standard_error": 0.06128639634338906
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025429741383575375,
            "upper_bound": 0.21734966899148664
          },
          "point_estimate": 0.13681039818052176,
          "standard_error": 0.05254374988337558
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.37536627044604,
            "upper_bound": 113.45151234898914
          },
          "point_estimate": 113.41904244822337,
          "standard_error": 0.01946768731591252
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06514644295912968,
            "upper_bound": 0.16082371056684758
          },
          "point_estimate": 0.1342290396422293,
          "standard_error": 0.0224795237574343
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.18124080194336,
            "upper_bound": 37.23395118009465
          },
          "point_estimate": 37.20691340279505,
          "standard_error": 0.01350779861431016
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.167858688353185,
            "upper_bound": 37.24890908526548
          },
          "point_estimate": 37.196803823633545,
          "standard_error": 0.027316555560019263
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0033509463408031917,
            "upper_bound": 0.07045506438654475
          },
          "point_estimate": 0.04971025194226478,
          "standard_error": 0.02071817069228372
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.173023339769934,
            "upper_bound": 37.22248862838144
          },
          "point_estimate": 37.19604677247933,
          "standard_error": 0.012621928058119848
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02938707480587283,
            "upper_bound": 0.05146686767743587
          },
          "point_estimate": 0.04494744003066803,
          "standard_error": 0.005719606610889498
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.952246836736435,
            "upper_bound": 62.02661175417085
          },
          "point_estimate": 61.98804377621862,
          "standard_error": 0.019053260786498456
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.929940098090974,
            "upper_bound": 62.041973399635566
          },
          "point_estimate": 61.9735141601812,
          "standard_error": 0.031070054524044342
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0144046281991646,
            "upper_bound": 0.1068064808953124
          },
          "point_estimate": 0.06915568106648191,
          "standard_error": 0.024024080576381692
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.957661435310314,
            "upper_bound": 62.067929965309816
          },
          "point_estimate": 62.02185040551655,
          "standard_error": 0.028547584477915357
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03672096691777395,
            "upper_bound": 0.07900418119541847
          },
          "point_estimate": 0.06331100682786577,
          "standard_error": 0.011170333356329222
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 264407.579101987,
            "upper_bound": 265187.3410165972
          },
          "point_estimate": 264827.6479619975,
          "standard_error": 199.1898210431197
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 264491.95681265206,
            "upper_bound": 265258.7708029197
          },
          "point_estimate": 264923.86073456146,
          "standard_error": 174.3037386681997
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.69640683726037,
            "upper_bound": 1022.7438757599654
          },
          "point_estimate": 458.3885283582995,
          "standard_error": 230.8244415616652
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 264738.9256314448,
            "upper_bound": 265156.63511759933
          },
          "point_estimate": 264900.1593515973,
          "standard_error": 106.74405627646009
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 268.5217734601023,
            "upper_bound": 913.2182346092889
          },
          "point_estimate": 662.7849363686678,
          "standard_error": 175.41267535831705
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.92708288245427,
            "upper_bound": 48.02478356899439
          },
          "point_estimate": 47.971980047637025,
          "standard_error": 0.025269593897683348
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.915857942426136,
            "upper_bound": 48.028915398232655
          },
          "point_estimate": 47.94269247816993,
          "standard_error": 0.028004562255057493
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013329264962028868,
            "upper_bound": 0.13141548102171155
          },
          "point_estimate": 0.05190174121588743,
          "standard_error": 0.03043408587530809
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.93173377312837,
            "upper_bound": 48.04489640288915
          },
          "point_estimate": 47.98669035331054,
          "standard_error": 0.03142519286321577
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029692176149660125,
            "upper_bound": 0.1071739751517378
          },
          "point_estimate": 0.08416667409309564,
          "standard_error": 0.019582647313035693
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.75329654192743,
            "upper_bound": 85.00920668934054
          },
          "point_estimate": 84.88449036432148,
          "standard_error": 0.06569143558459122
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.71478480130268,
            "upper_bound": 85.05274823507915
          },
          "point_estimate": 84.93779948138518,
          "standard_error": 0.08852138691154347
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.039990349950207275,
            "upper_bound": 0.3601356030954278
          },
          "point_estimate": 0.25704188389161486,
          "standard_error": 0.08588976959170158
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.67914304731225,
            "upper_bound": 85.0019499364086
          },
          "point_estimate": 84.8521120078757,
          "standard_error": 0.08333730694174214
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12127335810266376,
            "upper_bound": 0.2770658744539826
          },
          "point_estimate": 0.21940434221322616,
          "standard_error": 0.04005060329725865
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107747.14641444114,
            "upper_bound": 108282.83632572948
          },
          "point_estimate": 108005.68717147096,
          "standard_error": 137.29210323813768
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107639.37889465876,
            "upper_bound": 108286.09198813056
          },
          "point_estimate": 107995.68879821958,
          "standard_error": 167.43864438656
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139.8344007815479,
            "upper_bound": 788.8884711576479
          },
          "point_estimate": 464.3614359695871,
          "standard_error": 173.50523300767443
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107645.93217178604,
            "upper_bound": 108549.13069759968
          },
          "point_estimate": 108163.28416509306,
          "standard_error": 237.52480956719305
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243.73625441243337,
            "upper_bound": 595.338085660277
          },
          "point_estimate": 458.2872081553265,
          "standard_error": 93.18593209055103
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.16964598140055,
            "upper_bound": 47.21463995221537
          },
          "point_estimate": 47.19287188826737,
          "standard_error": 0.011500402558003725
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.15931826975261,
            "upper_bound": 47.22584292770361
          },
          "point_estimate": 47.20578160015165,
          "standard_error": 0.018898168768767196
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004859695487093145,
            "upper_bound": 0.06075551622614175
          },
          "point_estimate": 0.04799185768204771,
          "standard_error": 0.015409107684507966
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.17475690694531,
            "upper_bound": 47.22084549918241
          },
          "point_estimate": 47.200608050976136,
          "standard_error": 0.011660488275947274
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022505891938620695,
            "upper_bound": 0.04717536367392851
          },
          "point_estimate": 0.03817259325888418,
          "standard_error": 0.006440072714610315
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.12412356091309,
            "upper_bound": 79.20766452823892
          },
          "point_estimate": 79.16577971162417,
          "standard_error": 0.021407307869430077
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.09899606226287,
            "upper_bound": 79.23120705698278
          },
          "point_estimate": 79.16007307302228,
          "standard_error": 0.03766314470690779
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011846949732248596,
            "upper_bound": 0.11867024140673488
          },
          "point_estimate": 0.08928874246480734,
          "standard_error": 0.026134163019588187
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.15042775755711,
            "upper_bound": 79.23148365169699
          },
          "point_estimate": 79.19900904947397,
          "standard_error": 0.020584564420819403
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.045436106987286326,
            "upper_bound": 0.08413194508354697
          },
          "point_estimate": 0.07117165774823343,
          "standard_error": 0.009836119862949248
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 210.97320042793748,
            "upper_bound": 211.83000975171103
          },
          "point_estimate": 211.3938295107473,
          "standard_error": 0.2190750128465068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 210.7283948637506,
            "upper_bound": 212.15247803040464
          },
          "point_estimate": 211.30934716139805,
          "standard_error": 0.3355236899234241
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1424413548335239,
            "upper_bound": 1.2763248152584878
          },
          "point_estimate": 0.9325405386173972,
          "standard_error": 0.2931015670696582
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 210.6809225433854,
            "upper_bound": 211.45241673620745
          },
          "point_estimate": 211.0696553651903,
          "standard_error": 0.20156757730463284
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4304130665638393,
            "upper_bound": 0.8743152247416603
          },
          "point_estimate": 0.7305056642504235,
          "standard_error": 0.11102482490504822
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 485.3181347271894,
            "upper_bound": 486.23661439984335
          },
          "point_estimate": 485.7448492824989,
          "standard_error": 0.23590600161145445
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 485.1446790149817,
            "upper_bound": 486.20441309917794
          },
          "point_estimate": 485.636356524234,
          "standard_error": 0.30122420431886
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.20223447649928653,
            "upper_bound": 1.3009645278534208
          },
          "point_estimate": 0.7855808626678639,
          "standard_error": 0.26835212587928053
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 485.247758777763,
            "upper_bound": 485.9836259042898
          },
          "point_estimate": 485.6218424657439,
          "standard_error": 0.19465840352589953
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3792732844459131,
            "upper_bound": 1.0510461626900245
          },
          "point_estimate": 0.7846949636807784,
          "standard_error": 0.18759144256456783
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.065658283996527,
            "upper_bound": 19.101662584539
          },
          "point_estimate": 19.083084539247807,
          "standard_error": 0.009216461072653672
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.057682780357407,
            "upper_bound": 19.101640775436746
          },
          "point_estimate": 19.08369221241224,
          "standard_error": 0.011506725162162276
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007382350734421623,
            "upper_bound": 0.0528969343315944
          },
          "point_estimate": 0.032131334259987196,
          "standard_error": 0.011336607547996643
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.068047772994372,
            "upper_bound": 19.09311554844256
          },
          "point_estimate": 19.08130734252665,
          "standard_error": 0.006542337356778224
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015705160512973677,
            "upper_bound": 0.04056225948537487
          },
          "point_estimate": 0.03059611441092488,
          "standard_error": 0.0066189831365847385
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/twoway_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28931.872282337314,
            "upper_bound": 29006.78091804211
          },
          "point_estimate": 28965.910306266993,
          "standard_error": 19.28193401039623
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28919.1015936255,
            "upper_bound": 28999.087029659146
          },
          "point_estimate": 28957.526752988048,
          "standard_error": 20.542396232801995
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.246768456583194,
            "upper_bound": 93.9030050261172
          },
          "point_estimate": 58.2798958046362,
          "standard_error": 19.604556239994693
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28938.401875230003,
            "upper_bound": 28982.883322956957
          },
          "point_estimate": 28963.48460909608,
          "standard_error": 11.259937820329936
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.563126462339596,
            "upper_bound": 89.58129858103621
          },
          "point_estimate": 64.32913433809908,
          "standard_error": 18.178029588059868
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/twoway_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.5565658668432,
            "upper_bound": 114.8602803004706
          },
          "point_estimate": 114.69264630214984,
          "standard_error": 0.07853673783019312
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.49257296774988,
            "upper_bound": 114.8581130538302
          },
          "point_estimate": 114.62507109327332,
          "standard_error": 0.07244575694225742
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03231560775928574,
            "upper_bound": 0.3555241201987769
          },
          "point_estimate": 0.1592924998008231,
          "standard_error": 0.0895361347097952
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.53243369843604,
            "upper_bound": 114.79600737509465
          },
          "point_estimate": 114.65259428380487,
          "standard_error": 0.06641383770245132
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08492585918084973,
            "upper_bound": 0.34776287322372396
          },
          "point_estimate": 0.2615429879038586,
          "standard_error": 0.07054309825068417
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/twoway_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 967584.2748809524,
            "upper_bound": 968474.0308312448
          },
          "point_estimate": 967942.7143838764,
          "standard_error": 239.55108702930752
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 967563.9105263158,
            "upper_bound": 967994.5210526315
          },
          "point_estimate": 967752.105263158,
          "standard_error": 105.8384961184722
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.990522263045115,
            "upper_bound": 634.4997768055965
          },
          "point_estimate": 212.7365145126256,
          "standard_error": 157.72941364505965
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 967431.6522594364,
            "upper_bound": 968523.1001740934
          },
          "point_estimate": 967797.3053315106,
          "standard_error": 286.1476992016108
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135.05604862152094,
            "upper_bound": 1212.7219505827566
          },
          "point_estimate": 802.2859889133312,
          "standard_error": 343.275928931126
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/twoway_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1972.061186021016,
            "upper_bound": 1975.377775755051
          },
          "point_estimate": 1973.6547812824003,
          "standard_error": 0.8517746884810732
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1971.4522093009216,
            "upper_bound": 1976.1029480491252
          },
          "point_estimate": 1972.9190511124568,
          "standard_error": 1.0242731203924864
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4412279320645123,
            "upper_bound": 5.1203523893896286
          },
          "point_estimate": 2.7174551253781782,
          "standard_error": 1.2214220253462174
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1972.3173338609583,
            "upper_bound": 1974.554457565401
          },
          "point_estimate": 1973.2989557882613,
          "standard_error": 0.5664617572999576
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4105102173020247,
            "upper_bound": 3.558256810054527
          },
          "point_estimate": 2.8378931660343985,
          "standard_error": 0.5412987728587253
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.6278344529892,
            "upper_bound": 30.673733082650045
          },
          "point_estimate": 30.650835957381354,
          "standard_error": 0.011728447566426933
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.62430998007126,
            "upper_bound": 30.67871375095267
          },
          "point_estimate": 30.652848026645444,
          "standard_error": 0.018126524309739805
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012526208991486163,
            "upper_bound": 0.06660765590475205
          },
          "point_estimate": 0.036076699244556906,
          "standard_error": 0.01816463277406717
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.609393645911872,
            "upper_bound": 30.6707810761854
          },
          "point_estimate": 30.636404337086727,
          "standard_error": 0.015529505318374996
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021945558594335425,
            "upper_bound": 0.05064799828462965
          },
          "point_estimate": 0.03913418180595836,
          "standard_error": 0.007457247602486837
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.7418581075302,
            "upper_bound": 33.79066946266563
          },
          "point_estimate": 33.765456821546834,
          "standard_error": 0.01256489288761983
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.72709947020405,
            "upper_bound": 33.81220237901588
          },
          "point_estimate": 33.75320382627997,
          "standard_error": 0.020048573118207354
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008125323073414894,
            "upper_bound": 0.06812459573632439
          },
          "point_estimate": 0.04087708644382579,
          "standard_error": 0.016096538396386927
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.736027414436506,
            "upper_bound": 33.77235616373667
          },
          "point_estimate": 33.754727921919184,
          "standard_error": 0.009080573066722835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022199257383302803,
            "upper_bound": 0.04926337324170785
          },
          "point_estimate": 0.04164985441140397,
          "standard_error": 0.006525578146160876
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.717864952156344,
            "upper_bound": 23.752769113263874
          },
          "point_estimate": 23.73404985265255,
          "standard_error": 0.008970623442281594
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.71186234232462,
            "upper_bound": 23.753232886246515
          },
          "point_estimate": 23.728970408810177,
          "standard_error": 0.011392417808175915
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025929205758699,
            "upper_bound": 0.04407725054433574
          },
          "point_estimate": 0.02258944532055412,
          "standard_error": 0.012003420769604506
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.7138998129106,
            "upper_bound": 23.737774744051865
          },
          "point_estimate": 23.727508391659203,
          "standard_error": 0.006148084907179247
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013191131159400906,
            "upper_bound": 0.03969850576516138
          },
          "point_estimate": 0.029985029981024352,
          "standard_error": 0.007200820664737257
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.387818263399595,
            "upper_bound": 27.415388390449497
          },
          "point_estimate": 27.401398468982496,
          "standard_error": 0.0070814285770738
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.382839528830715,
            "upper_bound": 27.42762319640404
          },
          "point_estimate": 27.39511266773122,
          "standard_error": 0.010241782688428257
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002412809880129489,
            "upper_bound": 0.040711815365846525
          },
          "point_estimate": 0.025654934261484564,
          "standard_error": 0.010586466287803064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.382544593873945,
            "upper_bound": 27.42548747833493
          },
          "point_estimate": 27.408093543998245,
          "standard_error": 0.011125544369093672
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013610642118337644,
            "upper_bound": 0.028625804709311843
          },
          "point_estimate": 0.023589490645584014,
          "standard_error": 0.003778485529659581
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.966019767467124,
            "upper_bound": 35.001950037292
          },
          "point_estimate": 34.98364427564907,
          "standard_error": 0.009211750636757804
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.954381069001826,
            "upper_bound": 35.01902214538965
          },
          "point_estimate": 34.978768484504094,
          "standard_error": 0.014927392464940504
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007036675041341535,
            "upper_bound": 0.05101646756491428
          },
          "point_estimate": 0.03671739123829344,
          "standard_error": 0.012220346659191084
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.965628958589114,
            "upper_bound": 34.99413305969092
          },
          "point_estimate": 34.981163807740195,
          "standard_error": 0.007156643353156492
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018241906531530985,
            "upper_bound": 0.03669709582253881
          },
          "point_estimate": 0.03071691807871079,
          "standard_error": 0.004592535465243616
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.20962419322194,
            "upper_bound": 57.2885173940081
          },
          "point_estimate": 57.24750678639562,
          "standard_error": 0.02022249229342607
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.191409696036054,
            "upper_bound": 57.29541779224886
          },
          "point_estimate": 57.244216723280104,
          "standard_error": 0.027399040796055717
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01967595485499754,
            "upper_bound": 0.11591415980882903
          },
          "point_estimate": 0.06907067662989977,
          "standard_error": 0.02407069405594804
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.21184374887087,
            "upper_bound": 57.29249433497232
          },
          "point_estimate": 57.26230038477579,
          "standard_error": 0.02061650977185464
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03709437125002706,
            "upper_bound": 0.08568266199328275
          },
          "point_estimate": 0.06705703841285,
          "standard_error": 0.01277434295949207
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.40101566841199,
            "upper_bound": 66.45450995803434
          },
          "point_estimate": 66.42866937047427,
          "standard_error": 0.013713453465913688
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.39070185287991,
            "upper_bound": 66.46727542793249
          },
          "point_estimate": 66.4377947202932,
          "standard_error": 0.021498134009526514
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014690806932514058,
            "upper_bound": 0.07528892958575206
          },
          "point_estimate": 0.05469380965161916,
          "standard_error": 0.016360596321200762
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.37209822836626,
            "upper_bound": 66.4356971474677
          },
          "point_estimate": 66.39703080230356,
          "standard_error": 0.016348372155118678
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02677066700538451,
            "upper_bound": 0.056521003341179855
          },
          "point_estimate": 0.04571844460328597,
          "standard_error": 0.007698924381628776
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.877155783088746,
            "upper_bound": 47.95108017767438
          },
          "point_estimate": 47.91646543557171,
          "standard_error": 0.018940851798719727
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.88143591549388,
            "upper_bound": 47.96912089955568
          },
          "point_estimate": 47.91595234892336,
          "standard_error": 0.02452658389904439
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012368344805283833,
            "upper_bound": 0.10555767833583923
          },
          "point_estimate": 0.06547722484484814,
          "standard_error": 0.02315367903252975
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.90559352642883,
            "upper_bound": 47.95582968492709
          },
          "point_estimate": 47.928756760113814,
          "standard_error": 0.01305373906333639
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03198142593243069,
            "upper_bound": 0.08506941590249252
          },
          "point_estimate": 0.06329029988852944,
          "standard_error": 0.01487552344992113
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.83624596979651,
            "upper_bound": 85.1381878232216
          },
          "point_estimate": 84.98489457987924,
          "standard_error": 0.07685021205138262
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.85043130772792,
            "upper_bound": 85.16514531568014
          },
          "point_estimate": 84.94022283923582,
          "standard_error": 0.08372909738501415
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.039603873132572735,
            "upper_bound": 0.4491757025605321
          },
          "point_estimate": 0.18582702368230397,
          "standard_error": 0.09922388754148516
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.89909719715617,
            "upper_bound": 85.16881672160497
          },
          "point_estimate": 85.04963557662947,
          "standard_error": 0.0686596134723691
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11893962432763008,
            "upper_bound": 0.3433110593220009
          },
          "point_estimate": 0.256056178249907,
          "standard_error": 0.05821866180268254
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.260205920671964,
            "upper_bound": 45.35951714226234
          },
          "point_estimate": 45.304379591480526,
          "standard_error": 0.025575513641546622
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.24716487668681,
            "upper_bound": 45.34875353497262
          },
          "point_estimate": 45.28598895918945,
          "standard_error": 0.023686090303592727
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010762813317619238,
            "upper_bound": 0.11833146498967148
          },
          "point_estimate": 0.06018927652751267,
          "standard_error": 0.02819218651088168
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.24237557140812,
            "upper_bound": 45.318032239283134
          },
          "point_estimate": 45.280832743008666,
          "standard_error": 0.019234506501824546
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03193043895985547,
            "upper_bound": 0.11934118532924132
          },
          "point_estimate": 0.08511099066235811,
          "standard_error": 0.025144655040267017
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.18545179656681,
            "upper_bound": 47.26804860101383
          },
          "point_estimate": 47.226680339113024,
          "standard_error": 0.021095723050110333
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.18138288388872,
            "upper_bound": 47.2858218743563
          },
          "point_estimate": 47.212538979129775,
          "standard_error": 0.03183127951822279
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00879070960163412,
            "upper_bound": 0.11969890464177677
          },
          "point_estimate": 0.06465383940967008,
          "standard_error": 0.030291868751890497
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.20920085236328,
            "upper_bound": 47.281869012869336
          },
          "point_estimate": 47.25489850361819,
          "standard_error": 0.01868761308744295
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04049802196433647,
            "upper_bound": 0.08970507558761669
          },
          "point_estimate": 0.0704078077655101,
          "standard_error": 0.012670503385631497
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.16301641012582,
            "upper_bound": 79.24475988109478
          },
          "point_estimate": 79.20396583299876,
          "standard_error": 0.02091352755644199
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.15064954011775,
            "upper_bound": 79.25945017119774
          },
          "point_estimate": 79.20642954818064,
          "standard_error": 0.027272543850150183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02018701419583565,
            "upper_bound": 0.11713582099062264
          },
          "point_estimate": 0.06364306302065396,
          "standard_error": 0.025789552852753708
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.18976497842819,
            "upper_bound": 79.28576752278966
          },
          "point_estimate": 79.24919743445072,
          "standard_error": 0.024462703467690992
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.038394875361186825,
            "upper_bound": 0.08934301443840349
          },
          "point_estimate": 0.06963261039386262,
          "standard_error": 0.013055397625485653
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 953922.4644078144,
            "upper_bound": 954954.411645299
          },
          "point_estimate": 954403.10752442,
          "standard_error": 265.5080607810805
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 953583.05982906,
            "upper_bound": 954832.4903846154
          },
          "point_estimate": 954446.2868589744,
          "standard_error": 336.9867893429257
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.36941572119414,
            "upper_bound": 1433.5173610883812
          },
          "point_estimate": 974.8665057695496,
          "standard_error": 356.52935041542764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 954070.8317537286,
            "upper_bound": 955179.3259210098
          },
          "point_estimate": 954671.9214119214,
          "standard_error": 277.87235245234444
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 444.6604609270723,
            "upper_bound": 1189.7384047116116
          },
          "point_estimate": 887.633490284538,
          "standard_error": 212.13204739303436
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1223531.0255,
            "upper_bound": 1234075.3251019844
          },
          "point_estimate": 1228652.675177249,
          "standard_error": 2704.1693327072444
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1220570.9,
            "upper_bound": 1237400.0904761904
          },
          "point_estimate": 1227100.4244444445,
          "standard_error": 4823.525519006831
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1288.7524981201293,
            "upper_bound": 15016.157930965455
          },
          "point_estimate": 10063.368887673008,
          "standard_error": 3751.9110187681695
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1226188.936938438,
            "upper_bound": 1238712.959573282
          },
          "point_estimate": 1233399.8957575758,
          "standard_error": 3183.8580059273663
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5433.901318557574,
            "upper_bound": 10931.159205489925
          },
          "point_estimate": 9036.416057187793,
          "standard_error": 1419.1724680471318
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1237264.9512698413,
            "upper_bound": 1239398.2452962962
          },
          "point_estimate": 1238338.0406402117,
          "standard_error": 547.2846806822552
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1236766.286111111,
            "upper_bound": 1240304.3666666667
          },
          "point_estimate": 1238193.331904762,
          "standard_error": 945.2657120883428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332.5581563180094,
            "upper_bound": 3069.673825502407
          },
          "point_estimate": 2479.0965993206705,
          "standard_error": 720.250544355869
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1237884.6971767307,
            "upper_bound": 1240022.950442512
          },
          "point_estimate": 1238991.565021645,
          "standard_error": 548.3484243329818
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1169.1490395871117,
            "upper_bound": 2170.953186741902
          },
          "point_estimate": 1829.8546242747352,
          "standard_error": 256.8470047192457
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 367061.4616161616,
            "upper_bound": 367668.1297193363
          },
          "point_estimate": 367373.5422623056,
          "standard_error": 155.43638035847732
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 366937.1212121212,
            "upper_bound": 367883.8585858586
          },
          "point_estimate": 367478.12727272726,
          "standard_error": 261.8626245402655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.16810215671107,
            "upper_bound": 849.8719809723715
          },
          "point_estimate": 645.0599940024679,
          "standard_error": 195.94589342093093
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 367286.92412874626,
            "upper_bound": 367834.1945130316
          },
          "point_estimate": 367600.4950019677,
          "standard_error": 139.93287975363046
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 314.2268363079247,
            "upper_bound": 630.9105502944584
          },
          "point_estimate": 519.943707492257,
          "standard_error": 81.59313905625788
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 552700.5397627766,
            "upper_bound": 553737.4307924483
          },
          "point_estimate": 553183.6011333574,
          "standard_error": 266.90275627757575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 552430.9487012987,
            "upper_bound": 553644.0871212122
          },
          "point_estimate": 553211.5957912458,
          "standard_error": 354.1889629483205
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.0486515374606,
            "upper_bound": 1539.0827806303162
          },
          "point_estimate": 865.3618346367687,
          "standard_error": 339.9012218261458
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 552426.124040404,
            "upper_bound": 553248.9331657366
          },
          "point_estimate": 552752.7361275088,
          "standard_error": 208.77810050135875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 452.55600461270154,
            "upper_bound": 1200.2374967885985
          },
          "point_estimate": 888.913191961212,
          "standard_error": 217.37562573562784
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356540.47521377995,
            "upper_bound": 356959.1174183007
          },
          "point_estimate": 356760.1916938998,
          "standard_error": 107.96689566430726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356471.36225490196,
            "upper_bound": 357046.3333333333
          },
          "point_estimate": 356821.9673202614,
          "standard_error": 131.81345654562713
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.611149939882495,
            "upper_bound": 619.3212542988892
          },
          "point_estimate": 351.30351729250134,
          "standard_error": 139.6574358223493
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356414.7454693999,
            "upper_bound": 357090.41429845494
          },
          "point_estimate": 356790.69378660555,
          "standard_error": 178.7088155245651
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166.94315482355896,
            "upper_bound": 448.48283002225634
          },
          "point_estimate": 359.3842652065905,
          "standard_error": 70.03339002124277
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-you": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 458143.3500347221,
            "upper_bound": 459165.002790625
          },
          "point_estimate": 458665.80613888893,
          "standard_error": 261.7858448581458
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 457922.7937499999,
            "upper_bound": 459384.01375
          },
          "point_estimate": 458850.6423611111,
          "standard_error": 439.7307662785106
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.3480627909182,
            "upper_bound": 1426.247790095728
          },
          "point_estimate": 971.100202884544,
          "standard_error": 331.2686137257759
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 458312.3095659164,
            "upper_bound": 459149.77208083833
          },
          "point_estimate": 458760.9406168831,
          "standard_error": 215.91247957006703
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 529.1910569907043,
            "upper_bound": 1044.584956753003
          },
          "point_estimate": 872.1856831416169,
          "standard_error": 130.8673022782708
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 980230.7961873432,
            "upper_bound": 981928.0832368422
          },
          "point_estimate": 981047.314389098,
          "standard_error": 435.1095118426578
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 979920.3201754388,
            "upper_bound": 982166.3289473684
          },
          "point_estimate": 980857.817669173,
          "standard_error": 544.6243070954133
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223.636062806032,
            "upper_bound": 2384.036712346057
          },
          "point_estimate": 1494.8075495145713,
          "standard_error": 572.6580301273199
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 979628.62970382,
            "upper_bound": 981035.7742932346
          },
          "point_estimate": 980201.457826384,
          "standard_error": 356.7843795328232
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 719.3711765970053,
            "upper_bound": 1828.2699942382703
          },
          "point_estimate": 1450.7727298158186,
          "standard_error": 275.26940440104323
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295360.293762421,
            "upper_bound": 296046.78757258994
          },
          "point_estimate": 295633.29592947476,
          "standard_error": 186.08157169833493
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295306.8617886179,
            "upper_bound": 295692.8398825655
          },
          "point_estimate": 295443.3148664344,
          "standard_error": 116.61857601356604
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.240290363090814,
            "upper_bound": 473.4750746022931
          },
          "point_estimate": 263.31685359077807,
          "standard_error": 109.33337206280518
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295287.9250033722,
            "upper_bound": 295562.71569567226
          },
          "point_estimate": 295410.6692218351,
          "standard_error": 71.05268493539273
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135.50860988396354,
            "upper_bound": 945.002927115732
          },
          "point_estimate": 623.806611501759,
          "standard_error": 269.7377448791745
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 738877.7871578572,
            "upper_bound": 740139.6432571429
          },
          "point_estimate": 739512.5209238095,
          "standard_error": 323.2294835259018
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 738723.79,
            "upper_bound": 740365.797
          },
          "point_estimate": 739557.0442857143,
          "standard_error": 437.6559194497012
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321.35848629477977,
            "upper_bound": 1823.6108168242984
          },
          "point_estimate": 1180.0309710502383,
          "standard_error": 414.61931076887754
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 739206.843945946,
            "upper_bound": 740543.265695474
          },
          "point_estimate": 739907.9833766234,
          "standard_error": 349.44497690088406
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 610.8221207532108,
            "upper_bound": 1364.9472230199715
          },
          "point_estimate": 1074.8403333274853,
          "standard_error": 194.42000562983492
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 352246.3296162737,
            "upper_bound": 352944.64146093384
          },
          "point_estimate": 352572.59173062106,
          "standard_error": 179.1174313981545
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 352090.0760517799,
            "upper_bound": 352927.3042841732
          },
          "point_estimate": 352526.161407767,
          "standard_error": 162.74918807448802
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.119230420879894,
            "upper_bound": 1042.0355897525903
          },
          "point_estimate": 322.12999005772775,
          "standard_error": 266.2622314384457
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 352373.5333034074,
            "upper_bound": 353026.9135884928
          },
          "point_estimate": 352655.8379523389,
          "standard_error": 173.99027879989427
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253.0782615389455,
            "upper_bound": 789.9127704448886
          },
          "point_estimate": 596.4755185715329,
          "standard_error": 142.19040921198354
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158526.90315389924,
            "upper_bound": 158775.80954187372
          },
          "point_estimate": 158652.90455762594,
          "standard_error": 63.68821905469604
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158416.9108695652,
            "upper_bound": 158795.05120772947
          },
          "point_estimate": 158709.53203933747,
          "standard_error": 93.34713789991918
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.829179998321074,
            "upper_bound": 411.349511392744
          },
          "point_estimate": 142.71926342274352,
          "standard_error": 99.15231757820796
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158665.50528332012,
            "upper_bound": 158805.25703753982
          },
          "point_estimate": 158751.84877470357,
          "standard_error": 35.57457016180486
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120.07911168148247,
            "upper_bound": 265.8052143891401
          },
          "point_estimate": 211.9905246497626,
          "standard_error": 37.07665986792689
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194450.4238971384,
            "upper_bound": 194724.80423780877
          },
          "point_estimate": 194590.09893663524,
          "standard_error": 70.00794604990733
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194372.46310160423,
            "upper_bound": 194773.46527777775
          },
          "point_estimate": 194619.18649732615,
          "standard_error": 118.39936940193344
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.61324557009544,
            "upper_bound": 367.5309852085562
          },
          "point_estimate": 239.2716166878967,
          "standard_error": 99.20595516172094
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194503.62015698716,
            "upper_bound": 194792.06638737168
          },
          "point_estimate": 194654.8978956872,
          "standard_error": 75.71958884423992
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.684572007748,
            "upper_bound": 284.13199021589736
          },
          "point_estimate": 233.5454455602073,
          "standard_error": 35.81749098842874
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146361.59465053765,
            "upper_bound": 146727.13016129032
          },
          "point_estimate": 146544.12735215054,
          "standard_error": 93.94717859491566
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146276.9233870968,
            "upper_bound": 146873.24193548388
          },
          "point_estimate": 146497.26048387098,
          "standard_error": 176.19744095095402
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.3489773915228,
            "upper_bound": 501.7530808501714
          },
          "point_estimate": 443.4041034586486,
          "standard_error": 128.6121565082356
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146398.08962736375,
            "upper_bound": 146714.65084367245
          },
          "point_estimate": 146532.97965018853,
          "standard_error": 80.12913870327516
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.50492560260685,
            "upper_bound": 362.9823360085176
          },
          "point_estimate": 311.7331738760475,
          "standard_error": 40.71896422606706
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2115855.4002006175,
            "upper_bound": 2118873.8969444446
          },
          "point_estimate": 2117425.8833443564,
          "standard_error": 770.6548169197384
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2115916.2813051147,
            "upper_bound": 2119907.3506944445
          },
          "point_estimate": 2117010.638888889,
          "standard_error": 1319.3023508360395
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120.63009952533716,
            "upper_bound": 5250.411741369653
          },
          "point_estimate": 2792.232810381568,
          "standard_error": 1295.4258918257708
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2115384.293002137,
            "upper_bound": 2118484.110899067
          },
          "point_estimate": 2116739.7624819623,
          "standard_error": 782.0415450131771
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1532.0710884319308,
            "upper_bound": 3297.361532580523
          },
          "point_estimate": 2567.6661185155504,
          "standard_error": 488.1104322333964
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-04-30",
      "fullname": "2021-04-30/memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4271.4527507553685,
            "upper_bound": 4279.277437534749
          },
          "point_estimate": 4274.95504403971,
          "standard_error": 2.0179962236688564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4269.501642713932,
            "upper_bound": 4277.900134973988
          },
          "point_estimate": 4273.702549435861,
          "standard_error": 2.1377050726477407
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6155594379053372,
            "upper_bound": 9.673427147522254
          },
          "point_estimate": 6.014773929069957,
          "standard_error": 2.349070918305718
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4271.446929838544,
            "upper_bound": 4276.3842299035805
          },
          "point_estimate": 4273.666917349663,
          "standard_error": 1.2631703212990044
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4873882990176672,
            "upper_bound": 9.222181453283246
          },
          "point_estimate": 6.714482097589733,
          "standard_error": 1.8660200246582408
        }
      }
    }
  }
}
