{
  "name": "2022-04-29_wasm-changes",
  "benchmarks": {
    "memchr1/fallback/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/fallback/empty/never",
        "directory_name": "memchr1/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.26791731074116,
            "upper_bound": 2.312809926217896
          },
          "point_estimate": 2.2896674978854747,
          "standard_error": 0.011537363818032376
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.2568565187858973,
            "upper_bound": 2.3219668354607874
          },
          "point_estimate": 2.286416107188047,
          "standard_error": 0.016766395033951994
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008858726648956207,
            "upper_bound": 0.06652370813167605
          },
          "point_estimate": 0.04042081620194297,
          "standard_error": 0.015504175893776308
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.2633001589386925,
            "upper_bound": 2.3003702064056313
          },
          "point_estimate": 2.2812364109301795,
          "standard_error": 0.009714926396556326
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021021779117867588,
            "upper_bound": 0.047598930896493315
          },
          "point_estimate": 0.03858779123145184,
          "standard_error": 0.006714941092604705
        }
      }
    },
    "memchr1/fallback/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/common",
        "directory_name": "memchr1/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 665001.5321515151,
            "upper_bound": 665831.2779049603
          },
          "point_estimate": 665401.815549062,
          "standard_error": 212.71703638502248
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 664809.1878787879,
            "upper_bound": 666023.9311868687
          },
          "point_estimate": 665293.0466666666,
          "standard_error": 285.8633757087488
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206.43362615338685,
            "upper_bound": 1305.5889290940868
          },
          "point_estimate": 700.1183657522002,
          "standard_error": 276.0245491840204
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 665297.2476889865,
            "upper_bound": 666038.1640112465
          },
          "point_estimate": 665629.348051948,
          "standard_error": 190.34829664499136
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 377.2060314112653,
            "upper_bound": 874.1676345613174
          },
          "point_estimate": 709.5617129850202,
          "standard_error": 124.4286272560776
        }
      }
    },
    "memchr1/fallback/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/never",
        "directory_name": "memchr1/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42006.546227530795,
            "upper_bound": 42030.23365461413
          },
          "point_estimate": 42017.19633852963,
          "standard_error": 6.0950271734757715
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42002.53825057737,
            "upper_bound": 42027.53498845265
          },
          "point_estimate": 42011.16920708238,
          "standard_error": 6.552454589459323
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.478532796644727,
            "upper_bound": 30.392272254890884
          },
          "point_estimate": 13.776060156706276,
          "standard_error": 7.231188390042068
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42006.132461288005,
            "upper_bound": 42019.67404701975
          },
          "point_estimate": 42011.71983743739,
          "standard_error": 3.3874236836551868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.0683669343765105,
            "upper_bound": 27.062865764166588
          },
          "point_estimate": 20.246634993617903,
          "standard_error": 5.385185364905811
        }
      }
    },
    "memchr1/fallback/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/rare",
        "directory_name": "memchr1/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44479.64421764029,
            "upper_bound": 44504.074993596456
          },
          "point_estimate": 44489.6380752998,
          "standard_error": 6.538295778919106
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44477.781228169755,
            "upper_bound": 44494.039323553385
          },
          "point_estimate": 44483.74867563162,
          "standard_error": 3.4880216668008206
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9997279162405592,
            "upper_bound": 19.041380664393504
          },
          "point_estimate": 5.8113379163890775,
          "standard_error": 4.93981791848453
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44479.16358390388,
            "upper_bound": 44486.29352862247
          },
          "point_estimate": 44483.08224049789,
          "standard_error": 1.847083350512996
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.99391336483059,
            "upper_bound": 32.30253455506431
          },
          "point_estimate": 21.805184948057036,
          "standard_error": 8.653259021909799
        }
      }
    },
    "memchr1/fallback/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/uncommon",
        "directory_name": "memchr1/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163964.3650850851,
            "upper_bound": 164257.7759207422
          },
          "point_estimate": 164078.24168901044,
          "standard_error": 80.70772060997926
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163934.65565565566,
            "upper_bound": 164093.05105105106
          },
          "point_estimate": 163997.2150900901,
          "standard_error": 46.55428029627547
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.181953633358432,
            "upper_bound": 183.39784730708936
          },
          "point_estimate": 97.61558251996792,
          "standard_error": 47.67748801875364
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163949.4565101292,
            "upper_bound": 164028.48521195614
          },
          "point_estimate": 163980.0493974494,
          "standard_error": 19.964229771230308
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.19606918988008,
            "upper_bound": 409.0562139710601
          },
          "point_estimate": 268.4638431550406,
          "standard_error": 121.50356512058984
        }
      }
    },
    "memchr1/fallback/huge/verycommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/verycommon",
        "directory_name": "memchr1/fallback_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1196510.829314196,
            "upper_bound": 1202012.4762519202
          },
          "point_estimate": 1199090.1559920637,
          "standard_error": 1411.458876358963
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1195586.6221198156,
            "upper_bound": 1204409.7096774194
          },
          "point_estimate": 1197353.2437275986,
          "standard_error": 2044.2574933867356
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 701.0639657794521,
            "upper_bound": 7261.832745270364
          },
          "point_estimate": 3186.3354649367466,
          "standard_error": 1684.7141512439368
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1195839.9173387096,
            "upper_bound": 1200343.92726801
          },
          "point_estimate": 1197748.3514872224,
          "standard_error": 1135.3767014569985
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1643.5049633036765,
            "upper_bound": 5729.795870350694
          },
          "point_estimate": 4697.87076488433,
          "standard_error": 903.2012104939216
        }
      }
    },
    "memchr1/fallback/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/common",
        "directory_name": "memchr1/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274.1508983790551,
            "upper_bound": 274.77716016390184
          },
          "point_estimate": 274.46404676318434,
          "standard_error": 0.1604699175883206
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 273.9773006736623,
            "upper_bound": 274.9825511658959
          },
          "point_estimate": 274.46545132284314,
          "standard_error": 0.23501875672021763
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.15808218444599548,
            "upper_bound": 0.926022421115886
          },
          "point_estimate": 0.7451921766629934,
          "standard_error": 0.20771158496082484
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274.20013120494485,
            "upper_bound": 274.8780604300015
          },
          "point_estimate": 274.58515525015423,
          "standard_error": 0.1742415703856956
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3230976460675538,
            "upper_bound": 0.6491553444271834
          },
          "point_estimate": 0.534743181455041,
          "standard_error": 0.08294141086270712
        }
      }
    },
    "memchr1/fallback/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/never",
        "directory_name": "memchr1/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.80360868792211,
            "upper_bound": 56.91661371432603
          },
          "point_estimate": 56.866484950192046,
          "standard_error": 0.029332441104666488
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.815404491060576,
            "upper_bound": 56.93049053817965
          },
          "point_estimate": 56.8931441490618,
          "standard_error": 0.02599621100678513
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003780768022481591,
            "upper_bound": 0.1284298013591897
          },
          "point_estimate": 0.06697944635130172,
          "standard_error": 0.03477553701188629
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.76070849830277,
            "upper_bound": 56.895600550607554
          },
          "point_estimate": 56.834186354218446,
          "standard_error": 0.03375107676176997
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03280105477664829,
            "upper_bound": 0.13736591121969016
          },
          "point_estimate": 0.09781222679388868,
          "standard_error": 0.02955620699017421
        }
      }
    },
    "memchr1/fallback/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/rare",
        "directory_name": "memchr1/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.836354586255126,
            "upper_bound": 63.969999929745235
          },
          "point_estimate": 63.467934945435594,
          "standard_error": 0.2928127856910909
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.08243787950453,
            "upper_bound": 64.11711778529872
          },
          "point_estimate": 63.58916410001409,
          "standard_error": 0.29379898761684986
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12042850314056774,
            "upper_bound": 1.280105765659796
          },
          "point_estimate": 0.7517715198336562,
          "standard_error": 0.27581636791599445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.19355575810109,
            "upper_bound": 63.925717490824205
          },
          "point_estimate": 63.464840049511345,
          "standard_error": 0.1863958179017623
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3712554606345142,
            "upper_bound": 1.400741323129877
          },
          "point_estimate": 0.9782297890694608,
          "standard_error": 0.31151999048638856
        }
      }
    },
    "memchr1/fallback/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/uncommon",
        "directory_name": "memchr1/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.33868678037622,
            "upper_bound": 94.42124310020932
          },
          "point_estimate": 94.38143435007215,
          "standard_error": 0.021114778335359585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.3237441494015,
            "upper_bound": 94.43627903233762
          },
          "point_estimate": 94.38626017572204,
          "standard_error": 0.026952837704102537
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01358551670986094,
            "upper_bound": 0.11529060233980364
          },
          "point_estimate": 0.08342210723950524,
          "standard_error": 0.02618995500916045
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.34034138478604,
            "upper_bound": 94.4220522867268
          },
          "point_estimate": 94.38562309407054,
          "standard_error": 0.020928470189988654
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.037423577151911906,
            "upper_bound": 0.09113911006866778
          },
          "point_estimate": 0.0705485483390155,
          "standard_error": 0.014016985063674332
        }
      }
    },
    "memchr1/fallback/small/verycommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/verycommon",
        "directory_name": "memchr1/fallback_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 542.7535796232393,
            "upper_bound": 543.2624517158001
          },
          "point_estimate": 543.0140189391155,
          "standard_error": 0.13043474267637234
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 542.6733926082461,
            "upper_bound": 543.366177330156
          },
          "point_estimate": 543.045561916847,
          "standard_error": 0.17537875212864398
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1190652160268578,
            "upper_bound": 0.7658053134528457
          },
          "point_estimate": 0.4121532046561562,
          "standard_error": 0.16101660699892917
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 542.6542054973256,
            "upper_bound": 543.0841022453144
          },
          "point_estimate": 542.8699280276998,
          "standard_error": 0.10728699982814828
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24133922396552163,
            "upper_bound": 0.5430951512107104
          },
          "point_estimate": 0.43515830797829574,
          "standard_error": 0.07685694887555376
        }
      }
    },
    "memchr1/fallback/tiny/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/common",
        "directory_name": "memchr1/fallback_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.68197964324119,
            "upper_bound": 54.77148817862553
          },
          "point_estimate": 54.72354942752573,
          "standard_error": 0.02311398748553936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.660231859011894,
            "upper_bound": 54.78097512656504
          },
          "point_estimate": 54.70871343345274,
          "standard_error": 0.024154660099854463
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002210836607178449,
            "upper_bound": 0.1171852205957273
          },
          "point_estimate": 0.06137022885763648,
          "standard_error": 0.02950559552663308
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.669273024077654,
            "upper_bound": 54.798246264273125
          },
          "point_estimate": 54.73052614428082,
          "standard_error": 0.03553511997595202
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02856860282693776,
            "upper_bound": 0.09667324338726958
          },
          "point_estimate": 0.07688096044040237,
          "standard_error": 0.01757350967651259
        }
      }
    },
    "memchr1/fallback/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/never",
        "directory_name": "memchr1/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.970462913599623,
            "upper_bound": 7.993214816534026
          },
          "point_estimate": 7.981719673262163,
          "standard_error": 0.005843947718160353
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.965035358568367,
            "upper_bound": 8.001585652360587
          },
          "point_estimate": 7.975820176759096,
          "standard_error": 0.01042968978998047
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010073390352654278,
            "upper_bound": 0.02977044088245833
          },
          "point_estimate": 0.027561229597011995,
          "standard_error": 0.008226078887075824
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.9642006263210705,
            "upper_bound": 7.9888110651858675
          },
          "point_estimate": 7.973812255508991,
          "standard_error": 0.006302838037592959
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012734240013558792,
            "upper_bound": 0.02273366929678719
          },
          "point_estimate": 0.019498594158842253,
          "standard_error": 0.0025832495127626936
        }
      }
    },
    "memchr1/fallback/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/rare",
        "directory_name": "memchr1/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.288592055886106,
            "upper_bound": 11.294662726050294
          },
          "point_estimate": 11.291219866173352,
          "standard_error": 0.0015811168011859562
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.287550919739246,
            "upper_bound": 11.292793703689208
          },
          "point_estimate": 11.290518919613575,
          "standard_error": 0.0013783764494376175
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00021487362328528664,
            "upper_bound": 0.006315789422620557
          },
          "point_estimate": 0.004091451030434562,
          "standard_error": 0.001633920278799987
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.288250701238706,
            "upper_bound": 11.292997458361835
          },
          "point_estimate": 11.290957145505956,
          "standard_error": 0.0012156694038318555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001721308923728031,
            "upper_bound": 0.007603056584326871
          },
          "point_estimate": 0.005280337827405429,
          "standard_error": 0.0017599113669873236
        }
      }
    },
    "memchr1/fallback/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/uncommon",
        "directory_name": "memchr1/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.209072086919605,
            "upper_bound": 40.26118924489642
          },
          "point_estimate": 40.234894677737365,
          "standard_error": 0.013307479960426995
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.19960053115693,
            "upper_bound": 40.26147813314554
          },
          "point_estimate": 40.24138091083104,
          "standard_error": 0.01765766497652946
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015050876377935116,
            "upper_bound": 0.08062688644503267
          },
          "point_estimate": 0.03539067325275314,
          "standard_error": 0.018907599408417835
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.212236633648025,
            "upper_bound": 40.25377716269633
          },
          "point_estimate": 40.235833630505326,
          "standard_error": 0.010720465897298576
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023701996959703812,
            "upper_bound": 0.05797145120081177
          },
          "point_estimate": 0.04430550381007745,
          "standard_error": 0.008843470111860353
        }
      }
    },
    "memchr1/krate/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/krate/empty/never",
        "directory_name": "memchr1/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030676758161837507,
            "upper_bound": 0.03071654654693557
          },
          "point_estimate": 0.030692382497526857,
          "standard_error": 0.000010882436565803744
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03067318186516544,
            "upper_bound": 0.030694557426899513
          },
          "point_estimate": 0.03067800198987181,
          "standard_error": 6.099723074126326e-6
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0146946255648711e-6,
            "upper_bound": 0.000025783922838795537
          },
          "point_estimate": 7.752206450838814e-6,
          "standard_error": 7.005838816544298e-6
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03067664380905481,
            "upper_bound": 0.030689398136474658
          },
          "point_estimate": 0.030682132453167105,
          "standard_error": 3.219630657401066e-6
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.426957759751668e-6,
            "upper_bound": 0.00005480090230222375
          },
          "point_estimate": 0.00003619940138332912,
          "standard_error": 0.00001582931603659163
        }
      }
    },
    "memchr1/krate/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/common",
        "directory_name": "memchr1/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220565.30737373736,
            "upper_bound": 221059.50116041367
          },
          "point_estimate": 220817.25333213084,
          "standard_error": 126.66865752469576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220435.2383838384,
            "upper_bound": 221182.01897546896
          },
          "point_estimate": 220841.665993266,
          "standard_error": 177.66241048017807
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.9835406969364,
            "upper_bound": 767.299648700949
          },
          "point_estimate": 492.03973823424366,
          "standard_error": 168.51846184842572
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220523.6179657079,
            "upper_bound": 221066.54958348424
          },
          "point_estimate": 220797.2464856356,
          "standard_error": 137.70021415633678
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 242.98266859178776,
            "upper_bound": 514.8025479438746
          },
          "point_estimate": 422.0511409713467,
          "standard_error": 68.36602054824873
        }
      }
    },
    "memchr1/krate/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/never",
        "directory_name": "memchr1/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8517.904620736714,
            "upper_bound": 8530.630603040372
          },
          "point_estimate": 8524.253998232603,
          "standard_error": 3.243830847391242
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8515.085427922637,
            "upper_bound": 8533.070378200611
          },
          "point_estimate": 8525.01229739253,
          "standard_error": 4.015915755265328
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9942161486625314,
            "upper_bound": 18.480206416094735
          },
          "point_estimate": 13.33224340436775,
          "standard_error": 4.499691501196793
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8517.488397950166,
            "upper_bound": 8534.172773299262
          },
          "point_estimate": 8525.747970798287,
          "standard_error": 4.292408654422305
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.209798390877132,
            "upper_bound": 13.640065934886447
          },
          "point_estimate": 10.827263034414525,
          "standard_error": 1.9144962608088032
        }
      }
    },
    "memchr1/krate/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/rare",
        "directory_name": "memchr1/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9480.15000159351,
            "upper_bound": 9494.139504563233
          },
          "point_estimate": 9486.578188479129,
          "standard_error": 3.599713434313782
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9478.14762603216,
            "upper_bound": 9492.473585397653
          },
          "point_estimate": 9485.023756751722,
          "standard_error": 3.964566229503064
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6215279899642363,
            "upper_bound": 18.277059379341377
          },
          "point_estimate": 10.615695020573504,
          "standard_error": 3.812162111141302
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9478.161342238282,
            "upper_bound": 9488.001466742064
          },
          "point_estimate": 9482.993646353649,
          "standard_error": 2.584090589746219
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.356448659111941,
            "upper_bound": 16.515446150678542
          },
          "point_estimate": 11.93409930253754,
          "standard_error": 3.2048928221670363
        }
      }
    },
    "memchr1/krate/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/uncommon",
        "directory_name": "memchr1/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77087.03180706585,
            "upper_bound": 77229.64211452937
          },
          "point_estimate": 77158.85229913358,
          "standard_error": 36.48517125973455
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77078.94797441365,
            "upper_bound": 77289.07036247334
          },
          "point_estimate": 77146.39413646056,
          "standard_error": 50.13365242462749
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.309379884680872,
            "upper_bound": 227.6670561819883
          },
          "point_estimate": 101.51459665546604,
          "standard_error": 53.21984209033631
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77041.0329000435,
            "upper_bound": 77234.29840345311
          },
          "point_estimate": 77124.66013346994,
          "standard_error": 48.214546728981134
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.00615347931942,
            "upper_bound": 155.28220663572347
          },
          "point_estimate": 121.75540326616836,
          "standard_error": 22.469310807386385
        }
      }
    },
    "memchr1/krate/huge/verycommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/verycommon",
        "directory_name": "memchr1/krate_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 458883.93043981894,
            "upper_bound": 459579.42853125
          },
          "point_estimate": 459221.6638834326,
          "standard_error": 178.1070554800623
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 458703.2375868056,
            "upper_bound": 459705.22
          },
          "point_estimate": 459193.92276785715,
          "standard_error": 262.97839186880566
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179.15933702761834,
            "upper_bound": 1049.1163380515234
          },
          "point_estimate": 714.3199105057831,
          "standard_error": 211.1378208192061
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 458722.38183411217,
            "upper_bound": 459514.56929335586
          },
          "point_estimate": 459122.3187987013,
          "standard_error": 208.56824274856365
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 341.4607807672768,
            "upper_bound": 749.1678220076968
          },
          "point_estimate": 592.517634204801,
          "standard_error": 106.94185323513976
        }
      }
    },
    "memchr1/krate/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/common",
        "directory_name": "memchr1/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.55943796416727,
            "upper_bound": 204.81349823043345
          },
          "point_estimate": 204.65745703521912,
          "standard_error": 0.07008741171592461
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.5597469501215,
            "upper_bound": 204.64142170102937
          },
          "point_estimate": 204.60724066118752,
          "standard_error": 0.024800400628463247
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005342910907200406,
            "upper_bound": 0.13491972334072103
          },
          "point_estimate": 0.05408383316466552,
          "standard_error": 0.03408110242019716
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.5687679179774,
            "upper_bound": 204.63396783731653
          },
          "point_estimate": 204.6033397243344,
          "standard_error": 0.017167612275849866
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02849111523091785,
            "upper_bound": 0.358733770395039
          },
          "point_estimate": 0.23426294402382597,
          "standard_error": 0.10974892653483063
        }
      }
    },
    "memchr1/krate/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/never",
        "directory_name": "memchr1/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.810382807108447,
            "upper_bound": 6.826982446400905
          },
          "point_estimate": 6.818638058670652,
          "standard_error": 0.004262071429772716
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.804130811964756,
            "upper_bound": 6.828797857160163
          },
          "point_estimate": 6.82070463855688,
          "standard_error": 0.00661233961080574
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003230783817468463,
            "upper_bound": 0.02527264485559244
          },
          "point_estimate": 0.01327158030295924,
          "standard_error": 0.005679607936298764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.803825442600189,
            "upper_bound": 6.8253515806083875
          },
          "point_estimate": 6.813203145474624,
          "standard_error": 0.005615714917156127
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00853606870690257,
            "upper_bound": 0.017606539078605286
          },
          "point_estimate": 0.014159066420476605,
          "standard_error": 0.002345389848137486
        }
      }
    },
    "memchr1/krate/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/rare",
        "directory_name": "memchr1/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.115895183416434,
            "upper_bound": 11.136465283348414
          },
          "point_estimate": 11.124684129305892,
          "standard_error": 0.005407033888956555
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.113767129399491,
            "upper_bound": 11.129968871681402
          },
          "point_estimate": 11.118414188516194,
          "standard_error": 0.004687799198782522
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001612667409229454,
            "upper_bound": 0.020401034941661636
          },
          "point_estimate": 0.008287558064850183,
          "standard_error": 0.004956948708866942
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.11337958534709,
            "upper_bound": 11.122817792956612
          },
          "point_estimate": 11.116757009056652,
          "standard_error": 0.00242324218325895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00543962179668643,
            "upper_bound": 0.02641364581329721
          },
          "point_estimate": 0.01813201540100351,
          "standard_error": 0.006370713616283138
        }
      }
    },
    "memchr1/krate/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/uncommon",
        "directory_name": "memchr1/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.61634441699761,
            "upper_bound": 45.69236896350096
          },
          "point_estimate": 45.65325153573388,
          "standard_error": 0.01954166944949702
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.5952049951839,
            "upper_bound": 45.734239036930155
          },
          "point_estimate": 45.631117567266216,
          "standard_error": 0.03707543567972309
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0054175456128354536,
            "upper_bound": 0.10347235762636704
          },
          "point_estimate": 0.06731782395249988,
          "standard_error": 0.027670172273954977
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.60059781758964,
            "upper_bound": 45.67739348679017
          },
          "point_estimate": 45.62724653916875,
          "standard_error": 0.019778504591948715
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04004900043119981,
            "upper_bound": 0.07444726414937211
          },
          "point_estimate": 0.06512928393971701,
          "standard_error": 0.008878424967177697
        }
      }
    },
    "memchr1/krate/small/verycommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/verycommon",
        "directory_name": "memchr1/krate_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 490.84473694885367,
            "upper_bound": 491.9685483878262
          },
          "point_estimate": 491.3173831908833,
          "standard_error": 0.2959062730486813
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 490.72306335639666,
            "upper_bound": 491.5806923529146
          },
          "point_estimate": 490.86078584995255,
          "standard_error": 0.24657746468205372
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04390542182254451,
            "upper_bound": 1.1110699945195637
          },
          "point_estimate": 0.24809278714344815,
          "standard_error": 0.2904001265220923
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 490.7498608302711,
            "upper_bound": 491.0939049483409
          },
          "point_estimate": 490.8616303978209,
          "standard_error": 0.09002020368674905
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1759726787844208,
            "upper_bound": 1.43207776154262
          },
          "point_estimate": 0.984591255002352,
          "standard_error": 0.35711427759886133
        }
      }
    },
    "memchr1/krate/tiny/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/common",
        "directory_name": "memchr1/krate_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.428484009438165,
            "upper_bound": 50.48912512177608
          },
          "point_estimate": 50.4574135953179,
          "standard_error": 0.015231838537478028
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.42958545039927,
            "upper_bound": 50.47946456437337
          },
          "point_estimate": 50.45390078792074,
          "standard_error": 0.012456900450611664
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018516210327826144,
            "upper_bound": 0.07573576288675318
          },
          "point_estimate": 0.02105976426941581,
          "standard_error": 0.018276576591693194
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.40284052195439,
            "upper_bound": 50.472759109476144
          },
          "point_estimate": 50.44136137205311,
          "standard_error": 0.018484685670731135
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017451582147953306,
            "upper_bound": 0.07111574319539685
          },
          "point_estimate": 0.05074070543795922,
          "standard_error": 0.013789824265651215
        }
      }
    },
    "memchr1/krate/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/never",
        "directory_name": "memchr1/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.633026172343681,
            "upper_bound": 3.657392395894886
          },
          "point_estimate": 3.644942324097987,
          "standard_error": 0.0062144104806246495
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6335531690999927,
            "upper_bound": 3.658540772459144
          },
          "point_estimate": 3.6428530719436103,
          "standard_error": 0.006015038437783245
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003948174310755228,
            "upper_bound": 0.03624355814005269
          },
          "point_estimate": 0.01595001994323629,
          "standard_error": 0.007808360655580513
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.63656360989789,
            "upper_bound": 3.651234207019948
          },
          "point_estimate": 3.644386717643101,
          "standard_error": 0.0036931231181511702
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00902154773313052,
            "upper_bound": 0.02811784571760833
          },
          "point_estimate": 0.020663865153208946,
          "standard_error": 0.004939575966469026
        }
      }
    },
    "memchr1/krate/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/rare",
        "directory_name": "memchr1/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.880564014645914,
            "upper_bound": 5.912039677946695
          },
          "point_estimate": 5.8968882414075585,
          "standard_error": 0.008074912639526562
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.874258967444546,
            "upper_bound": 5.919221611112002
          },
          "point_estimate": 5.9008096234103675,
          "standard_error": 0.010351089631976833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006625824237337351,
            "upper_bound": 0.04756273358633223
          },
          "point_estimate": 0.028864091028404863,
          "standard_error": 0.010586557532165586
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.877226886761963,
            "upper_bound": 5.913005728669749
          },
          "point_estimate": 5.898345304033051,
          "standard_error": 0.009157215789874992
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013794694380753895,
            "upper_bound": 0.033256165157522624
          },
          "point_estimate": 0.02679930708705949,
          "standard_error": 0.004856409676402578
        }
      }
    },
    "memchr1/krate/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/uncommon",
        "directory_name": "memchr1/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.60518740810874,
            "upper_bound": 17.616600378168442
          },
          "point_estimate": 17.610580100674788,
          "standard_error": 0.002926680909732955
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.60313004074509,
            "upper_bound": 17.616641640101683
          },
          "point_estimate": 17.608089146659548,
          "standard_error": 0.0038364413459853783
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022852301917144933,
            "upper_bound": 0.016583094690973844
          },
          "point_estimate": 0.008865780799833481,
          "standard_error": 0.0036012001171917183
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.605179926652365,
            "upper_bound": 17.612089661937386
          },
          "point_estimate": 17.60884863522833,
          "standard_error": 0.0017487167711191143
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0048935529194445,
            "upper_bound": 0.012963693013157977
          },
          "point_estimate": 0.009767037509957067,
          "standard_error": 0.002194811457747833
        }
      }
    },
    "memchr1/libc/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/libc/empty/never",
        "directory_name": "memchr1/libc_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.430967945612017,
            "upper_bound": 2.4916189283716013
          },
          "point_estimate": 2.463153220090734,
          "standard_error": 0.015579198581999338
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.428814331689221,
            "upper_bound": 2.5023040106729844
          },
          "point_estimate": 2.4715165681278046,
          "standard_error": 0.01880436332192035
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008063719616648791,
            "upper_bound": 0.08513663783178256
          },
          "point_estimate": 0.04749019913062221,
          "standard_error": 0.019057217163827852
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4662417949926683,
            "upper_bound": 2.5047184407133485
          },
          "point_estimate": 2.4895801155658868,
          "standard_error": 0.009725591716854848
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02374193980729769,
            "upper_bound": 0.06854254394219486
          },
          "point_estimate": 0.05196867327569837,
          "standard_error": 0.01185600412400354
        }
      }
    },
    "memchr1/libc/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/common",
        "directory_name": "memchr1/libc_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255969.3185545843,
            "upper_bound": 256595.45245726497
          },
          "point_estimate": 256255.53453185703,
          "standard_error": 161.37434856737812
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255890.46503496505,
            "upper_bound": 256690.6744755245
          },
          "point_estimate": 256133.23776223775,
          "standard_error": 150.37843337849856
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.044714254122496,
            "upper_bound": 787.5168442705321
          },
          "point_estimate": 255.82560620995864,
          "standard_error": 179.34851082377918
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255848.76119473745,
            "upper_bound": 256244.49037345633
          },
          "point_estimate": 256044.8112251385,
          "standard_error": 103.90157198194022
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153.1914506592989,
            "upper_bound": 685.9983981742764
          },
          "point_estimate": 539.1807595410489,
          "standard_error": 137.2197283067664
        }
      }
    },
    "memchr1/libc/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/never",
        "directory_name": "memchr1/libc_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9136.138695233309,
            "upper_bound": 9159.489719136578
          },
          "point_estimate": 9146.638450685545,
          "standard_error": 5.987935868644629
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9132.671958336825,
            "upper_bound": 9162.408645388288
          },
          "point_estimate": 9139.739001633576,
          "standard_error": 5.905479154377737
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6363563668671604,
            "upper_bound": 27.528951584646947
          },
          "point_estimate": 9.316742629895632,
          "standard_error": 6.061738149342437
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9135.679591802183,
            "upper_bound": 9142.57628753706
          },
          "point_estimate": 9138.9438526289,
          "standard_error": 1.7273433172711774
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.429183545696546,
            "upper_bound": 25.218427595081636
          },
          "point_estimate": 19.902440950476155,
          "standard_error": 5.23186565330593
        }
      }
    },
    "memchr1/libc/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/rare",
        "directory_name": "memchr1/libc_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9583.09353662738,
            "upper_bound": 9597.08184397252
          },
          "point_estimate": 9589.719342079388,
          "standard_error": 3.5741515441866967
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9579.656299420138,
            "upper_bound": 9597.409374450888
          },
          "point_estimate": 9588.131114918291,
          "standard_error": 5.2561089768091716
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3863010470740407,
            "upper_bound": 21.796441602580128
          },
          "point_estimate": 13.16035428665239,
          "standard_error": 4.51906492839855
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9583.211868194565,
            "upper_bound": 9595.332099224028
          },
          "point_estimate": 9589.946751192929,
          "standard_error": 3.096370906975628
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.411324504357338,
            "upper_bound": 15.566822711548252
          },
          "point_estimate": 11.919491773180107,
          "standard_error": 2.5314921919122892
        }
      }
    },
    "memchr1/libc/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/uncommon",
        "directory_name": "memchr1/libc_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70743.20204362177,
            "upper_bound": 70910.67329713171
          },
          "point_estimate": 70825.6117021721,
          "standard_error": 42.8786874363485
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70697.66081871346,
            "upper_bound": 70932.43794671865
          },
          "point_estimate": 70819.89990253412,
          "standard_error": 52.54070728043278
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.74135471703096,
            "upper_bound": 247.31578664240232
          },
          "point_estimate": 153.95268995684992,
          "standard_error": 59.34990296038411
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70770.55673227037,
            "upper_bound": 70874.33407247983
          },
          "point_estimate": 70825.4043340675,
          "standard_error": 26.12609062977027
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.17408528049023,
            "upper_bound": 182.1115910023888
          },
          "point_estimate": 143.6491849815188,
          "standard_error": 26.51420763324103
        }
      }
    },
    "memchr1/libc/huge/verycommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/verycommon",
        "directory_name": "memchr1/libc_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 532799.2344841561,
            "upper_bound": 533657.9324183007
          },
          "point_estimate": 533201.2650303454,
          "standard_error": 219.9068069947841
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 532622.125,
            "upper_bound": 533809.5114379085
          },
          "point_estimate": 532951.2047794118,
          "standard_error": 308.145164356733
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.2514495945944,
            "upper_bound": 1201.5271202371698
          },
          "point_estimate": 517.2988313308842,
          "standard_error": 282.4370633673233
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 532814.7942534982,
            "upper_bound": 533647.4639346213
          },
          "point_estimate": 533208.6489304813,
          "standard_error": 213.25538388653527
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218.47656363729783,
            "upper_bound": 881.4956351496422
          },
          "point_estimate": 733.9391278612667,
          "standard_error": 146.50044654985456
        }
      }
    },
    "memchr1/libc/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/common",
        "directory_name": "memchr1/libc_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 222.0047675490071,
            "upper_bound": 222.21366482124685
          },
          "point_estimate": 222.12047877839936,
          "standard_error": 0.05390321575936831
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 222.0351706836336,
            "upper_bound": 222.2157354168237
          },
          "point_estimate": 222.16459729800744,
          "standard_error": 0.0398291110931094
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02295372445607727,
            "upper_bound": 0.27255028204174114
          },
          "point_estimate": 0.068415008225869,
          "standard_error": 0.0620887119993176
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221.9274363929423,
            "upper_bound": 222.20230505455507
          },
          "point_estimate": 222.0972294525714,
          "standard_error": 0.07547408580624144
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04613351154630315,
            "upper_bound": 0.2490664894630329
          },
          "point_estimate": 0.17924258202928312,
          "standard_error": 0.053890657023642055
        }
      }
    },
    "memchr1/libc/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/never",
        "directory_name": "memchr1/libc_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.923223916776793,
            "upper_bound": 6.935733955153817
          },
          "point_estimate": 6.929415967697831,
          "standard_error": 0.003189089054992107
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.9199399010565825,
            "upper_bound": 6.9372624414929955
          },
          "point_estimate": 6.930341418973853,
          "standard_error": 0.004152999051092879
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002421540243970841,
            "upper_bound": 0.019156265874188557
          },
          "point_estimate": 0.010723269418325972,
          "standard_error": 0.004424421300573707
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.9213253906187315,
            "upper_bound": 6.936095669155462
          },
          "point_estimate": 6.929386750036064,
          "standard_error": 0.0038456255911029752
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006131202923308914,
            "upper_bound": 0.013450157679283328
          },
          "point_estimate": 0.010623051236443864,
          "standard_error": 0.0019146990762534063
        }
      }
    },
    "memchr1/libc/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/rare",
        "directory_name": "memchr1/libc_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.212828921586375,
            "upper_bound": 11.219498661618628
          },
          "point_estimate": 11.2158067740671,
          "standard_error": 0.0017175849153887588
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.211573749747162,
            "upper_bound": 11.2185605565395
          },
          "point_estimate": 11.214330577605216,
          "standard_error": 0.0017725279411578943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001189055230355569,
            "upper_bound": 0.007909853293727292
          },
          "point_estimate": 0.004446324719469606,
          "standard_error": 0.00183158435479632
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.21218061867526,
            "upper_bound": 11.216994792122414
          },
          "point_estimate": 11.214587250573,
          "standard_error": 0.0012825926071931566
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002178234471492144,
            "upper_bound": 0.007938972298570707
          },
          "point_estimate": 0.005716285513324676,
          "standard_error": 0.0016255149846445172
        }
      }
    },
    "memchr1/libc/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/uncommon",
        "directory_name": "memchr1/libc_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.578338631885835,
            "upper_bound": 47.61907072417064
          },
          "point_estimate": 47.59684474589984,
          "standard_error": 0.010490564151319258
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.572624192613645,
            "upper_bound": 47.626819341752274
          },
          "point_estimate": 47.58027103462817,
          "standard_error": 0.01378930741601838
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002756990081708526,
            "upper_bound": 0.05573520434977553
          },
          "point_estimate": 0.014305421178338664,
          "standard_error": 0.013560813961951
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.57398248994877,
            "upper_bound": 47.60425548320783
          },
          "point_estimate": 47.58510348566568,
          "standard_error": 0.007819557356996487
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00815426057681703,
            "upper_bound": 0.04479609278232178
          },
          "point_estimate": 0.03473920782598913,
          "standard_error": 0.008287657781503673
        }
      }
    },
    "memchr1/libc/small/verycommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/verycommon",
        "directory_name": "memchr1/libc_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579.0093914495552,
            "upper_bound": 579.6121046364482
          },
          "point_estimate": 579.2963844324456,
          "standard_error": 0.1542243192853111
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578.9279104644298,
            "upper_bound": 579.6504523045996
          },
          "point_estimate": 579.2093154412113,
          "standard_error": 0.18317189200846337
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12025238760749132,
            "upper_bound": 0.8386265325993878
          },
          "point_estimate": 0.4489190037140157,
          "standard_error": 0.20197809020261304
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579.0395619919461,
            "upper_bound": 579.5867805060851
          },
          "point_estimate": 579.2996118545059,
          "standard_error": 0.1407727804527236
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2384479817324542,
            "upper_bound": 0.6546290231046716
          },
          "point_estimate": 0.514086205397921,
          "standard_error": 0.1033397986673902
        }
      }
    },
    "memchr1/libc/tiny/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/common",
        "directory_name": "memchr1/libc_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.47361750807754,
            "upper_bound": 51.5806099004195
          },
          "point_estimate": 51.53847711159042,
          "standard_error": 0.029092358681685256
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.5433884342973,
            "upper_bound": 51.58048156853428
          },
          "point_estimate": 51.557805727551056,
          "standard_error": 0.01092167949855248
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0043369245672574405,
            "upper_bound": 0.06382669307300132
          },
          "point_estimate": 0.020171799060918837,
          "standard_error": 0.016488254133836353
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.37535606090529,
            "upper_bound": 51.57221864304342
          },
          "point_estimate": 51.484456310137894,
          "standard_error": 0.0586334256127228
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013648603142710556,
            "upper_bound": 0.14796057273098082
          },
          "point_estimate": 0.09690709442739842,
          "standard_error": 0.04381741422045279
        }
      }
    },
    "memchr1/libc/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/never",
        "directory_name": "memchr1/libc_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.188767793173086,
            "upper_bound": 3.1923770045502913
          },
          "point_estimate": 3.190546894619081,
          "standard_error": 0.0008879940551321311
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.1898457085959344,
            "upper_bound": 3.191204392326459
          },
          "point_estimate": 3.190338772562664,
          "standard_error": 0.00039784548794772814
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00013920385274863162,
            "upper_bound": 0.00460274610682342
          },
          "point_estimate": 0.0007741625696394842,
          "standard_error": 0.0009217134992847796
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.1895024732489135,
            "upper_bound": 3.190882876074029
          },
          "point_estimate": 3.190342307657123,
          "standard_error": 0.00034986242894024605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005068376424258973,
            "upper_bound": 0.004207841072771652
          },
          "point_estimate": 0.002951437555101842,
          "standard_error": 0.0009830857727053583
        }
      }
    },
    "memchr1/libc/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/rare",
        "directory_name": "memchr1/libc_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.18106744938566,
            "upper_bound": 5.194946663939285
          },
          "point_estimate": 5.186535482805974,
          "standard_error": 0.003782733705137472
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.17989164275132,
            "upper_bound": 5.186598143829455
          },
          "point_estimate": 5.183887382811438,
          "standard_error": 0.001972267050505926
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009915551856382555,
            "upper_bound": 0.008496429822754153
          },
          "point_estimate": 0.004877555664027644,
          "standard_error": 0.002027403975091393
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.180367261907968,
            "upper_bound": 5.192775413154135
          },
          "point_estimate": 5.184034314489787,
          "standard_error": 0.0033363402192190133
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024903003862470708,
            "upper_bound": 0.019228633561442183
          },
          "point_estimate": 0.012635675645221774,
          "standard_error": 0.005599991269170813
        }
      }
    },
    "memchr1/libc/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/libc/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/uncommon",
        "directory_name": "memchr1/libc_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.292885511095115,
            "upper_bound": 19.345484364133185
          },
          "point_estimate": 19.319778976526784,
          "standard_error": 0.013498361749168844
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.27560539661365,
            "upper_bound": 19.355943029147284
          },
          "point_estimate": 19.32955543883923,
          "standard_error": 0.01986719355399871
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0120875301534834,
            "upper_bound": 0.07429357433674891
          },
          "point_estimate": 0.044096223809204166,
          "standard_error": 0.016687666730927062
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.306562937520937,
            "upper_bound": 19.364750684254325
          },
          "point_estimate": 19.33958079065758,
          "standard_error": 0.01499322483961651
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02560547597088175,
            "upper_bound": 0.05603469716588698
          },
          "point_estimate": 0.045167463156483206,
          "standard_error": 0.007703381778044656
        }
      }
    },
    "memchr1/naive/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/naive/empty/never",
        "directory_name": "memchr1/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4908247746822975,
            "upper_bound": 0.49296941174657744
          },
          "point_estimate": 0.4917175440672069,
          "standard_error": 0.0005569022075499368
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4907412271505591,
            "upper_bound": 0.492122896808933
          },
          "point_estimate": 0.4908220837249521,
          "standard_error": 0.000402703175197427
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000012072303912228402,
            "upper_bound": 0.0018785527269694923
          },
          "point_estimate": 0.0001338473096143768,
          "standard_error": 0.0005185068993933784
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4908083116827081,
            "upper_bound": 0.492481877223216
          },
          "point_estimate": 0.4914793484916716,
          "standard_error": 0.0004597221495231874
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00009900182696436891,
            "upper_bound": 0.002689394334245566
          },
          "point_estimate": 0.0018517063130596245,
          "standard_error": 0.0007047228400841946
        }
      }
    },
    "memchr1/naive/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/common",
        "directory_name": "memchr1/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 453956.6778718402,
            "upper_bound": 454427.57079218107
          },
          "point_estimate": 454184.7163198119,
          "standard_error": 120.95531788249158
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 453831.5074074074,
            "upper_bound": 454547.1712962963
          },
          "point_estimate": 454070.9400352734,
          "standard_error": 202.27615130188664
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.79204217817704,
            "upper_bound": 645.1849524345167
          },
          "point_estimate": 510.2572418670338,
          "standard_error": 147.1528088730621
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 453922.5459770115,
            "upper_bound": 454317.194727739
          },
          "point_estimate": 454097.8090748757,
          "standard_error": 101.7390024093246
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241.4449409283786,
            "upper_bound": 488.6608558326203
          },
          "point_estimate": 403.1565497445334,
          "standard_error": 63.08932660550126
        }
      }
    },
    "memchr1/naive/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/never",
        "directory_name": "memchr1/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146046.57893988656,
            "upper_bound": 146255.9688627526
          },
          "point_estimate": 146137.58957671956,
          "standard_error": 54.41515105865368
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146026.9935742972,
            "upper_bound": 146224.12918340028
          },
          "point_estimate": 146063.9851851852,
          "standard_error": 51.30514680330287
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.586948784160196,
            "upper_bound": 237.8833146321474
          },
          "point_estimate": 63.66514374924011,
          "standard_error": 60.23141092363264
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146047.33121527205,
            "upper_bound": 146127.328117178
          },
          "point_estimate": 146075.1905387785,
          "standard_error": 20.67762276327533
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.68391287239139,
            "upper_bound": 257.69822725619724
          },
          "point_estimate": 181.71024692673615,
          "standard_error": 57.90338799956868
        }
      }
    },
    "memchr1/naive/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/rare",
        "directory_name": "memchr1/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147419.94500623757,
            "upper_bound": 147533.76000899685
          },
          "point_estimate": 147469.90304527344,
          "standard_error": 29.367568676523124
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147402.0693319838,
            "upper_bound": 147510.4392037787
          },
          "point_estimate": 147435.19329091962,
          "standard_error": 30.592542654960265
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.5101103397329,
            "upper_bound": 131.67995566829703
          },
          "point_estimate": 78.79984560508463,
          "standard_error": 29.44774405345635
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147424.2586946978,
            "upper_bound": 147494.53603138984
          },
          "point_estimate": 147466.13042746726,
          "standard_error": 17.925896508347297
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.174704920813426,
            "upper_bound": 139.4617200120173
          },
          "point_estimate": 98.01460322652368,
          "standard_error": 30.54757007200081
        }
      }
    },
    "memchr1/naive/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/uncommon",
        "directory_name": "memchr1/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198753.81696426403,
            "upper_bound": 199079.17274438377
          },
          "point_estimate": 198893.819043499,
          "standard_error": 84.74270594111528
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198723.8232240437,
            "upper_bound": 198999.19307832423
          },
          "point_estimate": 198751.42820279297,
          "standard_error": 79.31556949668953
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.154459546515223,
            "upper_bound": 359.6485171395629
          },
          "point_estimate": 71.57219866556468,
          "standard_error": 97.00574090775888
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198727.8792499725,
            "upper_bound": 198884.3465876061
          },
          "point_estimate": 198784.90163934423,
          "standard_error": 42.10302365426483
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.21320556959998,
            "upper_bound": 397.6590096882618
          },
          "point_estimate": 281.04359373838804,
          "standard_error": 92.3760677747735
        }
      }
    },
    "memchr1/naive/huge/verycommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/verycommon",
        "directory_name": "memchr1/naive_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 918528.3886702382,
            "upper_bound": 920575.1417738094
          },
          "point_estimate": 919493.2983065476,
          "standard_error": 525.2140812227781
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 918171.8625,
            "upper_bound": 921135.8928571428
          },
          "point_estimate": 918981.0838541668,
          "standard_error": 715.616498564741
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.53482975063176,
            "upper_bound": 3038.2457948103374
          },
          "point_estimate": 1385.54913383908,
          "standard_error": 701.4624103511146
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 918585.9762786828,
            "upper_bound": 919928.215235188
          },
          "point_estimate": 919144.7889610388,
          "standard_error": 343.2825469419079
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648.4258234095606,
            "upper_bound": 2106.8803809324254
          },
          "point_estimate": 1755.9199475139335,
          "standard_error": 326.0623136866721
        }
      }
    },
    "memchr1/naive/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/common",
        "directory_name": "memchr1/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241.5635553978309,
            "upper_bound": 244.06351125995016
          },
          "point_estimate": 242.8715818836819,
          "standard_error": 0.6400916231933802
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241.77090813388375,
            "upper_bound": 244.79296166687857
          },
          "point_estimate": 243.01788756331425,
          "standard_error": 0.6910099121195353
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.27102377994011023,
            "upper_bound": 3.5411074632446544
          },
          "point_estimate": 2.2178327103863795,
          "standard_error": 0.8340106612791829
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241.91729543527083,
            "upper_bound": 243.77851889066284
          },
          "point_estimate": 242.751954508393,
          "standard_error": 0.47145998050304194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0468307239640613,
            "upper_bound": 2.85369877309492
          },
          "point_estimate": 2.1369192565394353,
          "standard_error": 0.49115855407827547
        }
      }
    },
    "memchr1/naive/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/never",
        "directory_name": "memchr1/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.75830136871477,
            "upper_bound": 172.9368850824428
          },
          "point_estimate": 172.83119653983795,
          "standard_error": 0.04742960566159091
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.73594477711245,
            "upper_bound": 172.85539635015684
          },
          "point_estimate": 172.78948966013553,
          "standard_error": 0.036070013747900705
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005423196288754933,
            "upper_bound": 0.14977912450320297
          },
          "point_estimate": 0.0803401669829169,
          "standard_error": 0.034523968810517454
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.75327763117753,
            "upper_bound": 172.82444510564792
          },
          "point_estimate": 172.78700662990656,
          "standard_error": 0.018108121443814774
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.041733023980905865,
            "upper_bound": 0.23620709124240888
          },
          "point_estimate": 0.15789495219122252,
          "standard_error": 0.0629961868199685
        }
      }
    },
    "memchr1/naive/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/rare",
        "directory_name": "memchr1/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 181.33147471675863,
            "upper_bound": 182.9135258243252
          },
          "point_estimate": 182.11031157347847,
          "standard_error": 0.4051509827415498
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.7997633030169,
            "upper_bound": 183.45058938638036
          },
          "point_estimate": 181.87312428414907,
          "standard_error": 0.9158932608687912
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05156660054347316,
            "upper_bound": 1.9285775926125752
          },
          "point_estimate": 1.6451918223988726,
          "standard_error": 0.613533933439484
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 181.0516892940486,
            "upper_bound": 183.12813133855195
          },
          "point_estimate": 181.72884337953337,
          "standard_error": 0.500159900846193
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9217871572008888,
            "upper_bound": 1.501350829614127
          },
          "point_estimate": 1.3538483247981596,
          "standard_error": 0.14631025801035016
        }
      }
    },
    "memchr1/naive/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/uncommon",
        "directory_name": "memchr1/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207.51309473783832,
            "upper_bound": 211.08292214958357
          },
          "point_estimate": 209.23521185406017,
          "standard_error": 0.9172592849319868
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206.92165341661936,
            "upper_bound": 211.40293299261785
          },
          "point_estimate": 208.7813833049404,
          "standard_error": 1.0697926860114255
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7566530209790052,
            "upper_bound": 5.133264569854411
          },
          "point_estimate": 3.0266206433138736,
          "standard_error": 1.2167123314903874
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207.10243757274145,
            "upper_bound": 210.7604470629452
          },
          "point_estimate": 208.83712301894585,
          "standard_error": 0.9203094606012068
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.563222820018086,
            "upper_bound": 3.922061757656486
          },
          "point_estimate": 3.052360182998223,
          "standard_error": 0.605251790985832
        }
      }
    },
    "memchr1/naive/small/verycommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/verycommon",
        "directory_name": "memchr1/naive_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 327.2271094304912,
            "upper_bound": 334.68043672422266
          },
          "point_estimate": 330.872130933868,
          "standard_error": 1.9015234949606763
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326.19891722129404,
            "upper_bound": 335.37610474425406
          },
          "point_estimate": 331.00464586015175,
          "standard_error": 2.43720483287483
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.60906078072926,
            "upper_bound": 10.507451812092098
          },
          "point_estimate": 6.125739916307344,
          "standard_error": 2.53189017170841
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328.5519725683491,
            "upper_bound": 338.36835832038616
          },
          "point_estimate": 333.28351543465345,
          "standard_error": 2.560664824954181
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.2200650571259932,
            "upper_bound": 8.26167966034504
          },
          "point_estimate": 6.344407289229651,
          "standard_error": 1.280990356964435
        }
      }
    },
    "memchr1/naive/tiny/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/common",
        "directory_name": "memchr1/naive_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.32174577578492,
            "upper_bound": 36.34013339563968
          },
          "point_estimate": 36.32953259520873,
          "standard_error": 0.004798806225913366
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.31901641055008,
            "upper_bound": 36.33478934915568
          },
          "point_estimate": 36.32602005224938,
          "standard_error": 0.003668922318290896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022676421614139844,
            "upper_bound": 0.017876351840257397
          },
          "point_estimate": 0.0075691546309306305,
          "standard_error": 0.004311610478929127
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.32027215702996,
            "upper_bound": 36.331242531299736
          },
          "point_estimate": 36.32533043947935,
          "standard_error": 0.002762762090802634
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004762803632282924,
            "upper_bound": 0.023195153960751173
          },
          "point_estimate": 0.01594394058608344,
          "standard_error": 0.0056073069324882934
        }
      }
    },
    "memchr1/naive/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/never",
        "directory_name": "memchr1/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.676998226722507,
            "upper_bound": 27.810472753917985
          },
          "point_estimate": 27.7404191719614,
          "standard_error": 0.034119436993382164
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.649684490776757,
            "upper_bound": 27.841415408319325
          },
          "point_estimate": 27.67910668136222,
          "standard_error": 0.06043618688301664
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01112417977495683,
            "upper_bound": 0.17758511145519615
          },
          "point_estimate": 0.07063891925529572,
          "standard_error": 0.05038368846783695
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.654826535211413,
            "upper_bound": 27.82613415572575
          },
          "point_estimate": 27.72352269998101,
          "standard_error": 0.04329446460099703
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06332609048580577,
            "upper_bound": 0.13785780906142503
          },
          "point_estimate": 0.11383339873568774,
          "standard_error": 0.019176440589833418
        }
      }
    },
    "memchr1/naive/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/rare",
        "directory_name": "memchr1/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.189259359644566,
            "upper_bound": 28.271865532985483
          },
          "point_estimate": 28.232682683083887,
          "standard_error": 0.021162849022911087
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.18355313588512,
            "upper_bound": 28.289535451753075
          },
          "point_estimate": 28.246377493366648,
          "standard_error": 0.024392381873616385
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010769934279650364,
            "upper_bound": 0.11492525139339292
          },
          "point_estimate": 0.06896097689381643,
          "standard_error": 0.028650020152571804
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.16674359734406,
            "upper_bound": 28.26800200490438
          },
          "point_estimate": 28.212714841416567,
          "standard_error": 0.025342818819145213
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03389202104787195,
            "upper_bound": 0.09068337023169556
          },
          "point_estimate": 0.07050910391626612,
          "standard_error": 0.014358247009691228
        }
      }
    },
    "memchr1/naive/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr1/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/uncommon",
        "directory_name": "memchr1/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.23488156364634,
            "upper_bound": 25.5339003357764
          },
          "point_estimate": 25.379907639279235,
          "standard_error": 0.07633732482091873
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.188768728362614,
            "upper_bound": 25.585253222091573
          },
          "point_estimate": 25.302251383404663,
          "standard_error": 0.10279486009099552
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013478939303734111,
            "upper_bound": 0.40668765845226074
          },
          "point_estimate": 0.30204156877625576,
          "standard_error": 0.11157488231344276
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.275991432844,
            "upper_bound": 25.452182427427463
          },
          "point_estimate": 25.34122832317317,
          "standard_error": 0.0448477604953024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13702978857044373,
            "upper_bound": 0.32773004664618555
          },
          "point_estimate": 0.2549244439816459,
          "standard_error": 0.0492436575294338
        }
      }
    },
    "memchr2/fallback/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/fallback/empty/never",
        "directory_name": "memchr2/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.699171135668025,
            "upper_bound": 2.7016447918153608
          },
          "point_estimate": 2.7000724515153136,
          "standard_error": 0.0007207101900166143
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6989900463070224,
            "upper_bound": 2.699779454383856
          },
          "point_estimate": 2.699311486344927,
          "standard_error": 0.0002595023705470395
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000040944644809829086,
            "upper_bound": 0.0008278982775981603
          },
          "point_estimate": 0.000464534127505101,
          "standard_error": 0.0002802047961840092
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6991482527081736,
            "upper_bound": 2.69971898160915
          },
          "point_estimate": 2.6994122543113637,
          "standard_error": 0.00014608225874031952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00021421084878727131,
            "upper_bound": 0.0036854340857126753
          },
          "point_estimate": 0.002399576342462387,
          "standard_error": 0.0012270806813068074
        }
      }
    },
    "memchr2/fallback/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/common",
        "directory_name": "memchr2/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1241494.682542361,
            "upper_bound": 1243494.8496031747
          },
          "point_estimate": 1242272.8496071429,
          "standard_error": 548.7712080186371
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1241247.4362499998,
            "upper_bound": 1242194.3833333333
          },
          "point_estimate": 1242009.514166667,
          "standard_error": 317.86241310738035
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.61593272856402,
            "upper_bound": 1292.7742270487188
          },
          "point_estimate": 689.8156731700863,
          "standard_error": 342.3741139033949
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1241225.4003278688,
            "upper_bound": 1241872.9396313364
          },
          "point_estimate": 1241439.5033766234,
          "standard_error": 165.48012164338823
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 375.5408752760662,
            "upper_bound": 2795.73576446835
          },
          "point_estimate": 1830.81607442122,
          "standard_error": 833.4211374564258
        }
      }
    },
    "memchr2/fallback/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/never",
        "directory_name": "memchr2/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83900.15114146734,
            "upper_bound": 83926.01370356046
          },
          "point_estimate": 83912.16968217396,
          "standard_error": 6.640208905084361
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83895.72145417306,
            "upper_bound": 83926.16203094141
          },
          "point_estimate": 83906.68778801843,
          "standard_error": 7.963508989021934
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.099032131512644,
            "upper_bound": 35.06631988437297
          },
          "point_estimate": 17.14809442674027,
          "standard_error": 7.807142453059478
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83896.10976276774,
            "upper_bound": 83912.3772024939
          },
          "point_estimate": 83902.49206415704,
          "standard_error": 4.142460776249525
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.835837697931066,
            "upper_bound": 29.214697052267912
          },
          "point_estimate": 22.107076916384877,
          "standard_error": 5.178528359627691
        }
      }
    },
    "memchr2/fallback/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/rare",
        "directory_name": "memchr2/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89974.67110836084,
            "upper_bound": 90014.02997701555
          },
          "point_estimate": 89991.96436507937,
          "standard_error": 10.160749371318412
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89971.06160616061,
            "upper_bound": 90003.95905528053
          },
          "point_estimate": 89982.92288425271,
          "standard_error": 9.059981230451532
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.499632138339542,
            "upper_bound": 41.986295455094194
          },
          "point_estimate": 23.04218381122401,
          "standard_error": 9.327158488333628
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89973.68708226658,
            "upper_bound": 89993.41196049217
          },
          "point_estimate": 89981.90012215507,
          "standard_error": 5.0820755172200665
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.187576147228944,
            "upper_bound": 48.2894447813628
          },
          "point_estimate": 33.718995090514476,
          "standard_error": 10.80577186963542
        }
      }
    },
    "memchr2/fallback/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/uncommon",
        "directory_name": "memchr2/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325845.5200201956,
            "upper_bound": 326304.9856150794
          },
          "point_estimate": 326033.8352483702,
          "standard_error": 122.08263081995116
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325811.5734126984,
            "upper_bound": 326105.1361607143
          },
          "point_estimate": 325861.1315369898,
          "standard_error": 85.12765807201606
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.717158072066615,
            "upper_bound": 379.59247607343985
          },
          "point_estimate": 87.89368906459359,
          "standard_error": 100.3633336562373
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325829.81847492786,
            "upper_bound": 325984.70459793584
          },
          "point_estimate": 325901.83935528755,
          "standard_error": 39.685237932709974
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.16110375841236,
            "upper_bound": 603.6105352862905
          },
          "point_estimate": 407.33220324426543,
          "standard_error": 161.45216652164277
        }
      }
    },
    "memchr2/fallback/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/common",
        "directory_name": "memchr2/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 465.2350018572697,
            "upper_bound": 465.82359848767817
          },
          "point_estimate": 465.54764654789585,
          "standard_error": 0.1507279283988652
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 465.1367540545469,
            "upper_bound": 465.8955112383321
          },
          "point_estimate": 465.7968647425377,
          "standard_error": 0.21090627121230832
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0453527278721477,
            "upper_bound": 0.8076706420530726
          },
          "point_estimate": 0.3196025313489403,
          "standard_error": 0.2086728260715349
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 465.58933928301127,
            "upper_bound": 465.9093107929161
          },
          "point_estimate": 465.7727286048652,
          "standard_error": 0.08315966739924795
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.20323926408350443,
            "upper_bound": 0.6112033068282181
          },
          "point_estimate": 0.5015120955024311,
          "standard_error": 0.10063118325499
        }
      }
    },
    "memchr2/fallback/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/never",
        "directory_name": "memchr2/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.48813902473783,
            "upper_bound": 103.55449431783134
          },
          "point_estimate": 103.51622533428142,
          "standard_error": 0.01747294990261821
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.48125406292854,
            "upper_bound": 103.53256558812224
          },
          "point_estimate": 103.49981111633063,
          "standard_error": 0.01310514590937232
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005829954515071637,
            "upper_bound": 0.06105423777852744
          },
          "point_estimate": 0.03291730678414028,
          "standard_error": 0.014809263593139166
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.49543653332742,
            "upper_bound": 103.52522193910234
          },
          "point_estimate": 103.51098986111184,
          "standard_error": 0.007469195994668289
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016724440464019608,
            "upper_bound": 0.08576949950024668
          },
          "point_estimate": 0.058473467132680285,
          "standard_error": 0.02128493703673952
        }
      }
    },
    "memchr2/fallback/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/rare",
        "directory_name": "memchr2/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.64954888554284,
            "upper_bound": 118.23692127218484
          },
          "point_estimate": 117.9551308509775,
          "standard_error": 0.1499752612929442
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.5480262352351,
            "upper_bound": 118.35795590114002
          },
          "point_estimate": 118.0281719239353,
          "standard_error": 0.2138382924627752
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11649412890862636,
            "upper_bound": 0.8409756816763042
          },
          "point_estimate": 0.4851643431258912,
          "standard_error": 0.18426242719185495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.85149148482512,
            "upper_bound": 118.36488840054918
          },
          "point_estimate": 118.11599984953948,
          "standard_error": 0.13120291589480115
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2692235751303989,
            "upper_bound": 0.6301341316968883
          },
          "point_estimate": 0.4986590617627219,
          "standard_error": 0.09264580808064649
        }
      }
    },
    "memchr2/fallback/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/uncommon",
        "directory_name": "memchr2/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.3510415757315,
            "upper_bound": 177.74182994034945
          },
          "point_estimate": 177.51688655340698,
          "standard_error": 0.10170042986647584
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.30310165489817,
            "upper_bound": 177.59967561526705
          },
          "point_estimate": 177.45165294894474,
          "standard_error": 0.08682952061543833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0605370846281119,
            "upper_bound": 0.3932113502817124
          },
          "point_estimate": 0.2091871023585288,
          "standard_error": 0.08126243082462116
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.34546047109032,
            "upper_bound": 177.54042453068274
          },
          "point_estimate": 177.45742615400087,
          "standard_error": 0.04994985502430064
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11123294774140584,
            "upper_bound": 0.49869210313307794
          },
          "point_estimate": 0.3387683837200663,
          "standard_error": 0.12092055547016034
        }
      }
    },
    "memchr2/fallback/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/never",
        "directory_name": "memchr2/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.497879449902188,
            "upper_bound": 13.515137719424317
          },
          "point_estimate": 13.504236683825344,
          "standard_error": 0.004965717692613957
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.496798762881545,
            "upper_bound": 13.502336030369417
          },
          "point_estimate": 13.499515791222397,
          "standard_error": 0.0020953517804610925
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00021875255348461297,
            "upper_bound": 0.00738811559541259
          },
          "point_estimate": 0.003918889869888801,
          "standard_error": 0.00204907368157491
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.496732854861357,
            "upper_bound": 13.501446889550293
          },
          "point_estimate": 13.499092502282004,
          "standard_error": 0.0012352813313157426
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002052792069339925,
            "upper_bound": 0.025418697329688825
          },
          "point_estimate": 0.016523912302098404,
          "standard_error": 0.008304445288122058
        }
      }
    },
    "memchr2/fallback/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/rare",
        "directory_name": "memchr2/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.839263444132975,
            "upper_bound": 21.862160295106776
          },
          "point_estimate": 21.847984733784372,
          "standard_error": 0.006403300444297223
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.838008335264153,
            "upper_bound": 21.84592633127317
          },
          "point_estimate": 21.84332567247074,
          "standard_error": 0.0024509045175368296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001151149313213603,
            "upper_bound": 0.011858383090133349
          },
          "point_estimate": 0.0041433713077529384,
          "standard_error": 0.0031562181333231977
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.839019922842187,
            "upper_bound": 21.843984726901475
          },
          "point_estimate": 21.84137549746479,
          "standard_error": 0.0012531302780183571
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002735415375933133,
            "upper_bound": 0.032751751552025574
          },
          "point_estimate": 0.02135226917326103,
          "standard_error": 0.01019511840028983
        }
      }
    },
    "memchr2/fallback/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/uncommon",
        "directory_name": "memchr2/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.317407438878504,
            "upper_bound": 60.51266962682738
          },
          "point_estimate": 60.411507289367,
          "standard_error": 0.050220308100400346
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.29584008110656,
            "upper_bound": 60.53109930072283
          },
          "point_estimate": 60.3815632507939,
          "standard_error": 0.07645993595078711
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005495543623006823,
            "upper_bound": 0.2875512634651985
          },
          "point_estimate": 0.13815661219773126,
          "standard_error": 0.07586281655138649
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.34273567646653,
            "upper_bound": 60.62556365050133
          },
          "point_estimate": 60.502312306203024,
          "standard_error": 0.0740162840557579
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0894090275032591,
            "upper_bound": 0.2164073665179715
          },
          "point_estimate": 0.16752728405003622,
          "standard_error": 0.03340410637274475
        }
      }
    },
    "memchr2/krate/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/krate/empty/never",
        "directory_name": "memchr2/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03067673010328812,
            "upper_bound": 0.03070251651641318
          },
          "point_estimate": 0.030687174013384805,
          "standard_error": 6.8909312767738035e-6
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030675131405348083,
            "upper_bound": 0.03069253552504316
          },
          "point_estimate": 0.03067844059795563,
          "standard_error": 4.689722741676807e-6
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0926653375498591e-6,
            "upper_bound": 0.00002007411773901147
          },
          "point_estimate": 8.482444424714472e-6,
          "standard_error": 5.41657371995154e-6
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03067569341028203,
            "upper_bound": 0.03068279232681062
          },
          "point_estimate": 0.030678650876594327,
          "standard_error": 1.7883565203620125e-6
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.636352584034834e-6,
            "upper_bound": 0.000034104305884673735
          },
          "point_estimate": 0.000022835248678415134,
          "standard_error": 9.192758471396467e-6
        }
      }
    },
    "memchr2/krate/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/common",
        "directory_name": "memchr2/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414380.7567564935,
            "upper_bound": 415328.7424178165
          },
          "point_estimate": 414858.0012256493,
          "standard_error": 243.1212063737733
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413936.1306818182,
            "upper_bound": 415569.0321969697
          },
          "point_estimate": 414927.9303030303,
          "standard_error": 365.0396185218586
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160.7652227140325,
            "upper_bound": 1462.7238677814605
          },
          "point_estimate": 1084.308716886036,
          "standard_error": 354.3879132538913
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414232.1383309659,
            "upper_bound": 415090.4072059457
          },
          "point_estimate": 414645.08084415586,
          "standard_error": 225.93486754666816
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 485.77266821882625,
            "upper_bound": 976.79896724762
          },
          "point_estimate": 809.1708896232803,
          "standard_error": 124.41838306925608
        }
      }
    },
    "memchr2/krate/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/never",
        "directory_name": "memchr2/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11583.941516612633,
            "upper_bound": 11596.835929522367
          },
          "point_estimate": 11590.405667529662,
          "standard_error": 3.2953115763696452
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11581.192211415815,
            "upper_bound": 11599.02232142857
          },
          "point_estimate": 11589.976782677357,
          "standard_error": 4.116255087273769
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.022856519198127,
            "upper_bound": 19.089558087432607
          },
          "point_estimate": 10.325144757167555,
          "standard_error": 4.413277150079133
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11583.683012613785,
            "upper_bound": 11591.231478357471
          },
          "point_estimate": 11587.25111068778,
          "standard_error": 1.9023300003499777
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.19589065922546,
            "upper_bound": 13.871110082256807
          },
          "point_estimate": 10.984090306577995,
          "standard_error": 1.9701170995604536
        }
      }
    },
    "memchr2/krate/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/rare",
        "directory_name": "memchr2/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15907.76762858954,
            "upper_bound": 15928.764528643169
          },
          "point_estimate": 15918.269657291905,
          "standard_error": 5.3956139944596435
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15904.38355638438,
            "upper_bound": 15936.604839215195
          },
          "point_estimate": 15915.9821802935,
          "standard_error": 9.056710830833214
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.462681217392846,
            "upper_bound": 30.450945283579262
          },
          "point_estimate": 21.65125616453756,
          "standard_error": 6.975377142023235
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15910.055591104236,
            "upper_bound": 15934.844871840794
          },
          "point_estimate": 15921.090290227545,
          "standard_error": 6.420389573270338
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.198517912671418,
            "upper_bound": 21.9980446556335
          },
          "point_estimate": 18.017367693956796,
          "standard_error": 2.7529920530772363
        }
      }
    },
    "memchr2/krate/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/uncommon",
        "directory_name": "memchr2/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162148.53692247736,
            "upper_bound": 162519.83642609126
          },
          "point_estimate": 162343.85017502832,
          "standard_error": 95.3654944147958
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162094.76964285714,
            "upper_bound": 162620.86160714287
          },
          "point_estimate": 162388.98071676586,
          "standard_error": 135.27523764742713
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.08702792826592,
            "upper_bound": 513.1316645508186
          },
          "point_estimate": 361.0176899448495,
          "standard_error": 124.5806876005601
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162084.31403362198,
            "upper_bound": 162489.8036190343
          },
          "point_estimate": 162326.76389146567,
          "standard_error": 103.33832203733188
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161.67322701090586,
            "upper_bound": 403.6890984643317
          },
          "point_estimate": 317.7843220292887,
          "standard_error": 62.64560642026282
        }
      }
    },
    "memchr2/krate/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/common",
        "directory_name": "memchr2/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 412.411578454469,
            "upper_bound": 413.5722740436775
          },
          "point_estimate": 413.0111251567265,
          "standard_error": 0.29832542211220703
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 412.0870863787753,
            "upper_bound": 413.77667882948936
          },
          "point_estimate": 413.2246919554503,
          "standard_error": 0.48380144439026457
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13943188776364354,
            "upper_bound": 1.632950392445488
          },
          "point_estimate": 0.8635735113944766,
          "standard_error": 0.39747749918151104
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 412.02473043946446,
            "upper_bound": 413.265666929382
          },
          "point_estimate": 412.6140837485409,
          "standard_error": 0.32473978470661563
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5689742579828259,
            "upper_bound": 1.1979657690329997
          },
          "point_estimate": 0.9916932007449574,
          "standard_error": 0.15759992643546417
        }
      }
    },
    "memchr2/krate/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/never",
        "directory_name": "memchr2/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.014923906635929,
            "upper_bound": 12.021487021048472
          },
          "point_estimate": 12.01773501874046,
          "standard_error": 0.0017038850421351082
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.01472184881699,
            "upper_bound": 12.018775638671686
          },
          "point_estimate": 12.016828067074822,
          "standard_error": 0.0009096460608650764
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00027193716128983433,
            "upper_bound": 0.006489813443653194
          },
          "point_estimate": 0.002017438929643542,
          "standard_error": 0.0017273714536306663
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.015076097293504,
            "upper_bound": 12.017604719001495
          },
          "point_estimate": 12.01656996873498,
          "standard_error": 0.0006425916307943467
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015518043893648456,
            "upper_bound": 0.008343104463092466
          },
          "point_estimate": 0.005693259851596,
          "standard_error": 0.002040798885767512
        }
      }
    },
    "memchr2/krate/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/rare",
        "directory_name": "memchr2/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.28245211178064,
            "upper_bound": 23.29843403001104
          },
          "point_estimate": 23.289840832915015,
          "standard_error": 0.004108948785801122
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.277783010214357,
            "upper_bound": 23.298112734875
          },
          "point_estimate": 23.287761520383757,
          "standard_error": 0.004830871526526646
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0020235159249294716,
            "upper_bound": 0.022332501765902436
          },
          "point_estimate": 0.015063977099281051,
          "standard_error": 0.005100808149689222
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.281087340534594,
            "upper_bound": 23.29220003360983
          },
          "point_estimate": 23.28623473078831,
          "standard_error": 0.00279688131883808
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006029077723084873,
            "upper_bound": 0.01834759741680561
          },
          "point_estimate": 0.013711436040027903,
          "standard_error": 0.0033641993715942315
        }
      }
    },
    "memchr2/krate/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/uncommon",
        "directory_name": "memchr2/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.69747773024253,
            "upper_bound": 115.3619699882016
          },
          "point_estimate": 115.02311627775524,
          "standard_error": 0.17039911168518304
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.52900081594444,
            "upper_bound": 115.53976034907603
          },
          "point_estimate": 115.06472943992196,
          "standard_error": 0.26448744229546395
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030964865214747873,
            "upper_bound": 1.0065145953636505
          },
          "point_estimate": 0.742360692290876,
          "standard_error": 0.26950084707289756
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.51157725129096,
            "upper_bound": 115.35825577808146
          },
          "point_estimate": 114.9063070706231,
          "standard_error": 0.2188528547126462
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3257779878504596,
            "upper_bound": 0.6971363456755308
          },
          "point_estimate": 0.5679629764594791,
          "standard_error": 0.09500163361189816
        }
      }
    },
    "memchr2/krate/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/never",
        "directory_name": "memchr2/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.654237044120251,
            "upper_bound": 4.657939816819079
          },
          "point_estimate": 4.656027404822009,
          "standard_error": 0.0009525627026119468
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.653104637559554,
            "upper_bound": 4.658953002410209
          },
          "point_estimate": 4.655945392965343,
          "standard_error": 0.0015015109475973692
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00039582670213414857,
            "upper_bound": 0.0056402698538991795
          },
          "point_estimate": 0.003966531655072392,
          "standard_error": 0.0014472328072288494
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.6546479641997935,
            "upper_bound": 4.659061981052839
          },
          "point_estimate": 4.657126069216628,
          "standard_error": 0.0011168360957793918
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018186180240604552,
            "upper_bound": 0.003863404684207377
          },
          "point_estimate": 0.0031784940000204936,
          "standard_error": 0.0005238787800307611
        }
      }
    },
    "memchr2/krate/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/rare",
        "directory_name": "memchr2/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.678464609545593,
            "upper_bound": 10.686231556486652
          },
          "point_estimate": 10.68197811083602,
          "standard_error": 0.001993914352781178
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.67741961077708,
            "upper_bound": 10.687298458210511
          },
          "point_estimate": 10.679415732182754,
          "standard_error": 0.0020306462953306797
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004714490311209795,
            "upper_bound": 0.009037192578473303
          },
          "point_estimate": 0.0033019792542704125,
          "standard_error": 0.001966389280697131
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.67818398707884,
            "upper_bound": 10.681546963594542
          },
          "point_estimate": 10.679809007423312,
          "standard_error": 0.0008601786225638367
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001772438491615239,
            "upper_bound": 0.008321901781762045
          },
          "point_estimate": 0.006635100434435682,
          "standard_error": 0.0017102708868511922
        }
      }
    },
    "memchr2/krate/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/uncommon",
        "directory_name": "memchr2/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.606984562003525,
            "upper_bound": 55.68632323013524
          },
          "point_estimate": 55.645840725266375,
          "standard_error": 0.02034418448720299
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.5874213495607,
            "upper_bound": 55.71398874622048
          },
          "point_estimate": 55.64277697785644,
          "standard_error": 0.032508476941879944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013699182849871904,
            "upper_bound": 0.11503528142410809
          },
          "point_estimate": 0.08279643528176635,
          "standard_error": 0.026598861774826257
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.59542283730403,
            "upper_bound": 55.66795531579797
          },
          "point_estimate": 55.630843829794664,
          "standard_error": 0.018462562761571863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04045833330758306,
            "upper_bound": 0.08176728541876774
          },
          "point_estimate": 0.06796310481960281,
          "standard_error": 0.010316464946813007
        }
      }
    },
    "memchr2/naive/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/naive/empty/never",
        "directory_name": "memchr2/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6584656559252462,
            "upper_bound": 0.6704840350581085
          },
          "point_estimate": 0.6640496323219038,
          "standard_error": 0.0030923823236715577
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.655420174498139,
            "upper_bound": 0.6745170287735592
          },
          "point_estimate": 0.6603329196379498,
          "standard_error": 0.004668876015589216
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009838300352874303,
            "upper_bound": 0.01649083449831001
          },
          "point_estimate": 0.008005795730923658,
          "standard_error": 0.004089019297409128
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6566786485480988,
            "upper_bound": 0.6747242558232285
          },
          "point_estimate": 0.6641556501318631,
          "standard_error": 0.004906852717241105
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004449980743494696,
            "upper_bound": 0.012866387901470627
          },
          "point_estimate": 0.010311377602109968,
          "standard_error": 0.002019054890341301
        }
      }
    },
    "memchr2/naive/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/common",
        "directory_name": "memchr2/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 857501.6467428943,
            "upper_bound": 861886.8812403101
          },
          "point_estimate": 859649.3322637506,
          "standard_error": 1118.4380573351432
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 856470.0481727575,
            "upper_bound": 862567.2911821706
          },
          "point_estimate": 859887.1779069768,
          "standard_error": 1535.7853007204217
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396.49636563522506,
            "upper_bound": 6515.861200076179
          },
          "point_estimate": 4041.193430347511,
          "standard_error": 1523.872654790442
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 856101.7651142513,
            "upper_bound": 860939.5323200879
          },
          "point_estimate": 858414.166052552,
          "standard_error": 1230.208442058248
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2082.763806662719,
            "upper_bound": 4719.451207751397
          },
          "point_estimate": 3736.9050363892234,
          "standard_error": 679.1043528731385
        }
      }
    },
    "memchr2/naive/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/never",
        "directory_name": "memchr2/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219036.49278327596,
            "upper_bound": 219235.1292211704
          },
          "point_estimate": 219117.71116011665,
          "standard_error": 52.757055153698886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219022.35548862116,
            "upper_bound": 219132.60708907057
          },
          "point_estimate": 219076.01782128515,
          "standard_error": 28.757083920294487
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.859814217886252,
            "upper_bound": 150.44426457306565
          },
          "point_estimate": 63.23085806417654,
          "standard_error": 37.14064602882075
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219051.3766057098,
            "upper_bound": 219120.60163374693
          },
          "point_estimate": 219082.4717258645,
          "standard_error": 17.694345471201974
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.39288788661792,
            "upper_bound": 264.48698315156884
          },
          "point_estimate": 176.0286349256115,
          "standard_error": 71.7795185898603
        }
      }
    },
    "memchr2/naive/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/rare",
        "directory_name": "memchr2/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226900.34843511903,
            "upper_bound": 228036.14823958333
          },
          "point_estimate": 227460.80728050595,
          "standard_error": 290.6728694492793
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226690.98729166668,
            "upper_bound": 228199.896875
          },
          "point_estimate": 227430.65984375,
          "standard_error": 371.694457238814
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232.73252080567096,
            "upper_bound": 1671.0161251460167
          },
          "point_estimate": 1113.5421205431835,
          "standard_error": 373.9212460685289
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226493.32348918577,
            "upper_bound": 227483.43414408868
          },
          "point_estimate": 226963.8825,
          "standard_error": 253.72246531399588
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 552.0421350683863,
            "upper_bound": 1227.8409165726305
          },
          "point_estimate": 970.2125274402192,
          "standard_error": 175.16517014803566
        }
      }
    },
    "memchr2/naive/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/uncommon",
        "directory_name": "memchr2/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357016.3878839869,
            "upper_bound": 357308.5561437909
          },
          "point_estimate": 357148.5747654062,
          "standard_error": 74.89147340792672
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357011.11855742295,
            "upper_bound": 357241.5945669935
          },
          "point_estimate": 357116.87377450976,
          "standard_error": 63.56451452168258
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.418220051723548,
            "upper_bound": 342.1155590488805
          },
          "point_estimate": 121.9230139138988,
          "standard_error": 76.8867474008393
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357013.42781672254,
            "upper_bound": 357207.4612556561
          },
          "point_estimate": 357106.5707410237,
          "standard_error": 50.63995543170186
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.73661918071022,
            "upper_bound": 356.44906895388254
          },
          "point_estimate": 250.6315682657198,
          "standard_error": 76.57634009112358
        }
      }
    },
    "memchr2/naive/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/common",
        "directory_name": "memchr2/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.7670157377049,
            "upper_bound": 335.95220899458684
          },
          "point_estimate": 335.8398703635291,
          "standard_error": 0.05036447615953465
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.744477026091,
            "upper_bound": 335.8475733087047
          },
          "point_estimate": 335.8002621924626,
          "standard_error": 0.02628776566024275
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011925765046524777,
            "upper_bound": 0.11711147679410552
          },
          "point_estimate": 0.07642527294472017,
          "standard_error": 0.028703211809652893
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.7614364754388,
            "upper_bound": 335.8411227084439
          },
          "point_estimate": 335.80655728109343,
          "standard_error": 0.02060450188049308
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03399386275604487,
            "upper_bound": 0.25619313311868963
          },
          "point_estimate": 0.16846232442650624,
          "standard_error": 0.0745506934739051
        }
      }
    },
    "memchr2/naive/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/never",
        "directory_name": "memchr2/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279.7816866375761,
            "upper_bound": 279.9370233554035
          },
          "point_estimate": 279.84920100542905,
          "standard_error": 0.04031022586770332
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279.75146799652146,
            "upper_bound": 279.9131242993507
          },
          "point_estimate": 279.8205723454852,
          "standard_error": 0.03612421060183113
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006393702957427758,
            "upper_bound": 0.1448932958214969
          },
          "point_estimate": 0.09389434714283389,
          "standard_error": 0.04405531398026089
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279.77102036636126,
            "upper_bound": 279.84093331676075
          },
          "point_estimate": 279.8023026389106,
          "standard_error": 0.017764620974483
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0375947104945584,
            "upper_bound": 0.1892892360678194
          },
          "point_estimate": 0.13471562637883763,
          "standard_error": 0.04296662638865946
        }
      }
    },
    "memchr2/naive/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/rare",
        "directory_name": "memchr2/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296.57716035869834,
            "upper_bound": 296.68257614920253
          },
          "point_estimate": 296.6302625731962,
          "standard_error": 0.026985261837707577
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296.53800874702176,
            "upper_bound": 296.70138521709805
          },
          "point_estimate": 296.6486736448825,
          "standard_error": 0.0471421780073456
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02455195662862993,
            "upper_bound": 0.14467401369072158
          },
          "point_estimate": 0.12949401929113974,
          "standard_error": 0.035129388090921934
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296.556378489444,
            "upper_bound": 296.6681869169153
          },
          "point_estimate": 296.6035024116195,
          "standard_error": 0.02832228929807914
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.057942826394460856,
            "upper_bound": 0.10649211066239438
          },
          "point_estimate": 0.0899326968149437,
          "standard_error": 0.01253337375027326
        }
      }
    },
    "memchr2/naive/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/uncommon",
        "directory_name": "memchr2/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 373.34660481530045,
            "upper_bound": 373.5580303382735
          },
          "point_estimate": 373.4441940958202,
          "standard_error": 0.054127655864307025
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 373.28930401125245,
            "upper_bound": 373.5526760505539
          },
          "point_estimate": 373.39976617283185,
          "standard_error": 0.06954874306338862
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014765806728197656,
            "upper_bound": 0.28836161077775924
          },
          "point_estimate": 0.1656709392632181,
          "standard_error": 0.06960724916836125
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 373.3362060673148,
            "upper_bound": 373.494454640028
          },
          "point_estimate": 373.4116775248447,
          "standard_error": 0.040119417455406536
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.075618759070502,
            "upper_bound": 0.23813219753112055
          },
          "point_estimate": 0.18038575082061267,
          "standard_error": 0.04295860337061614
        }
      }
    },
    "memchr2/naive/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/never",
        "directory_name": "memchr2/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.07624879217944,
            "upper_bound": 36.098860894743645
          },
          "point_estimate": 36.08664260528073,
          "standard_error": 0.005818003343593062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.069855390124246,
            "upper_bound": 36.09999511071025
          },
          "point_estimate": 36.083396250819135,
          "standard_error": 0.007394229118183834
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011460165552038336,
            "upper_bound": 0.030981970228558888
          },
          "point_estimate": 0.01666169424211695,
          "standard_error": 0.008255557497476777
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.07450185965012,
            "upper_bound": 36.09520949622354
          },
          "point_estimate": 36.08464502097054,
          "standard_error": 0.00520049067961062
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008213126852356515,
            "upper_bound": 0.025255252609781256
          },
          "point_estimate": 0.019373373635469,
          "standard_error": 0.004544784990080736
        }
      }
    },
    "memchr2/naive/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/rare",
        "directory_name": "memchr2/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.2737183915821,
            "upper_bound": 39.31734400792896
          },
          "point_estimate": 39.29418748079872,
          "standard_error": 0.011156732926106723
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.26564576459072,
            "upper_bound": 39.31898435963758
          },
          "point_estimate": 39.28761334138411,
          "standard_error": 0.014183914429566171
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00962981575549621,
            "upper_bound": 0.062104797859277895
          },
          "point_estimate": 0.0378691211185267,
          "standard_error": 0.012726437241252908
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.27749872373058,
            "upper_bound": 39.308569667261864
          },
          "point_estimate": 39.29599440744917,
          "standard_error": 0.007810360106322298
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018325457045043455,
            "upper_bound": 0.04989479693880711
          },
          "point_estimate": 0.03719436606538477,
          "standard_error": 0.008759946998807427
        }
      }
    },
    "memchr2/naive/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr2/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/uncommon",
        "directory_name": "memchr2/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.872218154362365,
            "upper_bound": 35.91747666651155
          },
          "point_estimate": 35.89174453775568,
          "standard_error": 0.011767066186148234
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.86709203616694,
            "upper_bound": 35.90713826482937
          },
          "point_estimate": 35.878228952797414,
          "standard_error": 0.00929611515433195
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004385416367539966,
            "upper_bound": 0.0494924585840586
          },
          "point_estimate": 0.019684259365964088,
          "standard_error": 0.01129552395252769
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.869552316147306,
            "upper_bound": 35.88315668727018
          },
          "point_estimate": 35.87642041833083,
          "standard_error": 0.0034672902595815226
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008865592637594332,
            "upper_bound": 0.05533622633906974
          },
          "point_estimate": 0.039223118444195855,
          "standard_error": 0.0129234827884258
        }
      }
    },
    "memchr3/fallback/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/fallback/empty/never",
        "directory_name": "memchr3/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.709265263983981,
            "upper_bound": 2.71061294213532
          },
          "point_estimate": 2.7099341566466943,
          "standard_error": 0.0003433829059295711
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7093830959839855,
            "upper_bound": 2.71084748905591
          },
          "point_estimate": 2.7096124422511747,
          "standard_error": 0.0003812797370909726
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00006685903073883648,
            "upper_bound": 0.0018965189028509047
          },
          "point_estimate": 0.000866026265349003,
          "standard_error": 0.0005165298345290409
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7092856770240283,
            "upper_bound": 2.711070038724301
          },
          "point_estimate": 2.7101459200817706,
          "standard_error": 0.00046724631158701714
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005184068493633791,
            "upper_bound": 0.0015258514397061393
          },
          "point_estimate": 0.0011435818628949366,
          "standard_error": 0.00024996156800209625
        }
      }
    },
    "memchr3/fallback/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/common",
        "directory_name": "memchr3/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2015219.2641404548,
            "upper_bound": 2016453.4691207183
          },
          "point_estimate": 2015757.3180304929,
          "standard_error": 322.0862291834202
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2015056.7536549708,
            "upper_bound": 2016121.8665413535
          },
          "point_estimate": 2015440.1298245615,
          "standard_error": 287.41093078620935
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.73554184997778,
            "upper_bound": 1323.4984640035505
          },
          "point_estimate": 726.737343676844,
          "standard_error": 295.24951830034166
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2015281.415938632,
            "upper_bound": 2015989.8711995955
          },
          "point_estimate": 2015622.2822966508,
          "standard_error": 179.84338906269227
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 370.0317873202434,
            "upper_bound": 1546.2071530580506
          },
          "point_estimate": 1077.463330802437,
          "standard_error": 351.8629196270812
        }
      }
    },
    "memchr3/fallback/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/never",
        "directory_name": "memchr3/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109692.54700301203,
            "upper_bound": 109767.43855565116
          },
          "point_estimate": 109726.73918495409,
          "standard_error": 19.24558632773359
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109677.38058483935,
            "upper_bound": 109756.20080321284
          },
          "point_estimate": 109719.94563253011,
          "standard_error": 23.002205892591412
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.809978664056828,
            "upper_bound": 98.55111240697312
          },
          "point_estimate": 56.81900658160576,
          "standard_error": 21.357609548933414
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109693.01350659988,
            "upper_bound": 109744.15607758136
          },
          "point_estimate": 109719.51489594745,
          "standard_error": 12.952687012303675
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.903502455964773,
            "upper_bound": 87.98691857405069
          },
          "point_estimate": 63.915750170779326,
          "standard_error": 17.11385212438396
        }
      }
    },
    "memchr3/fallback/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/rare",
        "directory_name": "memchr3/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117846.35887155189,
            "upper_bound": 118131.2893275158
          },
          "point_estimate": 117947.90752311602,
          "standard_error": 84.7116092670462
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117838.07591693636,
            "upper_bound": 117888.56920943136
          },
          "point_estimate": 117870.81272923408,
          "standard_error": 25.618273178735265
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.934868661719063,
            "upper_bound": 68.58542264160627
          },
          "point_estimate": 37.58786773073608,
          "standard_error": 27.950437827569573
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117855.66517258884,
            "upper_bound": 117886.25637169198
          },
          "point_estimate": 117872.41507165974,
          "standard_error": 7.839444122073914
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.965007123273793,
            "upper_bound": 433.52332094552014
          },
          "point_estimate": 282.0873780300703,
          "standard_error": 149.8821732834028
        }
      }
    },
    "memchr3/fallback/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/uncommon",
        "directory_name": "memchr3/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498919.7333769026,
            "upper_bound": 499297.13358447497
          },
          "point_estimate": 499080.6768531202,
          "standard_error": 98.6233994243776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498881.6803652968,
            "upper_bound": 499152.40958904114
          },
          "point_estimate": 499028.88099315064,
          "standard_error": 61.82244192002721
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.984710027030314,
            "upper_bound": 359.1799839657919
          },
          "point_estimate": 159.0985478604041,
          "standard_error": 90.64511957396662
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498957.5023509429,
            "upper_bound": 499114.222100456
          },
          "point_estimate": 499041.0000711617,
          "standard_error": 39.28897610045864
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.51544183982612,
            "upper_bound": 483.0473703827409
          },
          "point_estimate": 329.02180114248193,
          "standard_error": 118.76226545097263
        }
      }
    },
    "memchr3/fallback/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/common",
        "directory_name": "memchr3/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 700.6547496568074,
            "upper_bound": 701.0893099696156
          },
          "point_estimate": 700.8501314522866,
          "standard_error": 0.1117275842790103
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 700.6710744518791,
            "upper_bound": 701.045305540022
          },
          "point_estimate": 700.7632254981992,
          "standard_error": 0.08681043866158579
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029921626058844013,
            "upper_bound": 0.5314356513634516
          },
          "point_estimate": 0.15781001346035092,
          "standard_error": 0.13479956254477288
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 700.6723512082671,
            "upper_bound": 700.8502209683064
          },
          "point_estimate": 700.739717110056,
          "standard_error": 0.04537691512947347
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12802191024768847,
            "upper_bound": 0.5282421499265385
          },
          "point_estimate": 0.3722752146448063,
          "standard_error": 0.11411304450772174
        }
      }
    },
    "memchr3/fallback/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/never",
        "directory_name": "memchr3/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.7505241895405,
            "upper_bound": 132.82714819653123
          },
          "point_estimate": 132.78538611255766,
          "standard_error": 0.01972824035374693
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.7359699188799,
            "upper_bound": 132.81529459517702
          },
          "point_estimate": 132.77860745643406,
          "standard_error": 0.020034863450313183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01538455085299571,
            "upper_bound": 0.09790720044007958
          },
          "point_estimate": 0.05183925865143633,
          "standard_error": 0.02192801632954362
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.75905960117188,
            "upper_bound": 132.79367797431792
          },
          "point_estimate": 132.77704785224785,
          "standard_error": 0.0086739528750806
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028520078006403275,
            "upper_bound": 0.09083268180075443
          },
          "point_estimate": 0.06556116527072461,
          "standard_error": 0.0180223566969181
        }
      }
    },
    "memchr3/fallback/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/rare",
        "directory_name": "memchr3/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.54524095443847,
            "upper_bound": 157.0186688734287
          },
          "point_estimate": 156.80677945442127,
          "standard_error": 0.12120553534406678
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.549571208627,
            "upper_bound": 157.02532668447884
          },
          "point_estimate": 156.97974781762483,
          "standard_error": 0.11844869898791362
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012357027359747743,
            "upper_bound": 0.5937956666709471
          },
          "point_estimate": 0.0814653122917109,
          "standard_error": 0.14182178222970715
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.24211598656217,
            "upper_bound": 157.00947662683382
          },
          "point_estimate": 156.59701485806897,
          "standard_error": 0.2061717768083905
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.089958014613137,
            "upper_bound": 0.5098468179000193
          },
          "point_estimate": 0.40373058249704086,
          "standard_error": 0.10822848532158176
        }
      }
    },
    "memchr3/fallback/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/uncommon",
        "directory_name": "memchr3/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263.0351870816725,
            "upper_bound": 263.21423382933136
          },
          "point_estimate": 263.1119991709823,
          "standard_error": 0.04658462892603471
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263.0164874720536,
            "upper_bound": 263.1654149573237
          },
          "point_estimate": 263.0669318883033,
          "standard_error": 0.037878723268070034
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013919216120161424,
            "upper_bound": 0.18277333807743895
          },
          "point_estimate": 0.0804115171623872,
          "standard_error": 0.04278290998424406
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262.9961469600828,
            "upper_bound": 263.08399143874755
          },
          "point_estimate": 263.0320829426342,
          "standard_error": 0.022819321927190852
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04469145339913259,
            "upper_bound": 0.2239379337509515
          },
          "point_estimate": 0.15543247996592852,
          "standard_error": 0.052659452895882794
        }
      }
    },
    "memchr3/fallback/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/never",
        "directory_name": "memchr3/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.435826719183083,
            "upper_bound": 17.44649936712089
          },
          "point_estimate": 17.440347576917674,
          "standard_error": 0.0028024039824642846
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.435452253732862,
            "upper_bound": 17.44241455104218
          },
          "point_estimate": 17.437639611193745,
          "standard_error": 0.002122411481320773
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010272566825465436,
            "upper_bound": 0.009627677468813888
          },
          "point_estimate": 0.004654589436933874,
          "standard_error": 0.0022792816792785585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.43677878768505,
            "upper_bound": 17.44147525305613
          },
          "point_estimate": 17.43937988719124,
          "standard_error": 0.001191267063227249
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002699497913169948,
            "upper_bound": 0.01378988452116551
          },
          "point_estimate": 0.00934393136670252,
          "standard_error": 0.0034927030175543943
        }
      }
    },
    "memchr3/fallback/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/rare",
        "directory_name": "memchr3/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.037287042097496,
            "upper_bound": 31.247640790009022
          },
          "point_estimate": 31.116474607704312,
          "standard_error": 0.059301548396419615
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.03338741778978,
            "upper_bound": 31.10057766145891
          },
          "point_estimate": 31.04560775525064,
          "standard_error": 0.022698855975151297
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005911898212568653,
            "upper_bound": 0.09894859785377888
          },
          "point_estimate": 0.020674035986072523,
          "standard_error": 0.029579015802315735
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.038780565534218,
            "upper_bound": 31.100183039319685
          },
          "point_estimate": 31.06638694198047,
          "standard_error": 0.01589693640914633
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018711498174191495,
            "upper_bound": 0.3038806838866514
          },
          "point_estimate": 0.19856960180584585,
          "standard_error": 0.09559593238168594
        }
      }
    },
    "memchr3/fallback/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/uncommon",
        "directory_name": "memchr3/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.13588095650894,
            "upper_bound": 88.39606568655324
          },
          "point_estimate": 88.27022614506804,
          "standard_error": 0.06604006822463407
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.19006685305663,
            "upper_bound": 88.38835015167425
          },
          "point_estimate": 88.24821367517222,
          "standard_error": 0.06224753431310457
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01910080683214505,
            "upper_bound": 0.34580484883058477
          },
          "point_estimate": 0.1576515581908999,
          "standard_error": 0.08060140190279225
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.20539861939946,
            "upper_bound": 88.51278919242905
          },
          "point_estimate": 88.34416486018122,
          "standard_error": 0.08242999420341117
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07936680995606762,
            "upper_bound": 0.3098555180898923
          },
          "point_estimate": 0.22077041591825797,
          "standard_error": 0.060948135512544376
        }
      }
    },
    "memchr3/krate/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/krate/empty/never",
        "directory_name": "memchr3/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030675194916344968,
            "upper_bound": 0.030686893746620512
          },
          "point_estimate": 0.030680639944603284,
          "standard_error": 2.9943632179503807e-6
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030672660655941947,
            "upper_bound": 0.030688636667459045
          },
          "point_estimate": 0.030676507558015,
          "standard_error": 4.082325874510241e-6
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.978924716313395e-7,
            "upper_bound": 0.000016372647180833686
          },
          "point_estimate": 5.897812090331535e-6,
          "standard_error": 4.089487800984292e-6
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03067396544251134,
            "upper_bound": 0.030683708933836727
          },
          "point_estimate": 0.030678083739904317,
          "standard_error": 2.4711850371480245e-6
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.7587024603623007e-6,
            "upper_bound": 0.00001232007401446763
          },
          "point_estimate": 0.000010016544030924302,
          "standard_error": 2.1485923029137337e-6
        }
      }
    },
    "memchr3/krate/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/common",
        "directory_name": "memchr3/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 633070.9427715517,
            "upper_bound": 634419.0802959771
          },
          "point_estimate": 633710.3328263547,
          "standard_error": 346.7392951954052
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632758.3344827587,
            "upper_bound": 634702.6674876848
          },
          "point_estimate": 633235.2385775861,
          "standard_error": 550.2043019492597
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 186.07396531718783,
            "upper_bound": 1915.9628504675684
          },
          "point_estimate": 977.124021488709,
          "standard_error": 470.0824189667281
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632890.9528213167,
            "upper_bound": 634050.7425082101
          },
          "point_estimate": 633385.3619346172,
          "standard_error": 293.00664643357936
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 618.8101911450904,
            "upper_bound": 1457.2967596411768
          },
          "point_estimate": 1157.253240622788,
          "standard_error": 222.5794750332998
        }
      }
    },
    "memchr3/krate/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/never",
        "directory_name": "memchr3/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15086.493648013566,
            "upper_bound": 15128.481420756869
          },
          "point_estimate": 15103.426734166536,
          "standard_error": 11.324746844924332
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15083.048885658916,
            "upper_bound": 15109.90652520698
          },
          "point_estimate": 15089.660299003324,
          "standard_error": 6.536806435246904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6143079655249832,
            "upper_bound": 31.730014270567533
          },
          "point_estimate": 13.187169901540557,
          "standard_error": 7.905462912908881
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15087.06123710629,
            "upper_bound": 15104.699128116772
          },
          "point_estimate": 15094.647123225612,
          "standard_error": 4.492650866462688
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.766702573540363,
            "upper_bound": 56.55772640319968
          },
          "point_estimate": 37.753656640290416,
          "standard_error": 15.601412689752138
        }
      }
    },
    "memchr3/krate/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/rare",
        "directory_name": "memchr3/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20368.311013639763,
            "upper_bound": 20404.895311098655
          },
          "point_estimate": 20386.53596568261,
          "standard_error": 9.370266085622168
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20356.00182174888,
            "upper_bound": 20414.599635650226
          },
          "point_estimate": 20386.57567264574,
          "standard_error": 16.181120135025484
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.640660022240754,
            "upper_bound": 51.89217640496367
          },
          "point_estimate": 40.33305606926056,
          "standard_error": 11.448893686153127
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20359.035346582896,
            "upper_bound": 20401.062709983595
          },
          "point_estimate": 20375.837978277326,
          "standard_error": 10.7612831612266
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.92297467704389,
            "upper_bound": 37.41947780619771
          },
          "point_estimate": 31.290764661163916,
          "standard_error": 4.490718379053503
        }
      }
    },
    "memchr3/krate/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/uncommon",
        "directory_name": "memchr3/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218665.63462589108,
            "upper_bound": 219224.5011610113
          },
          "point_estimate": 218931.9151423344,
          "standard_error": 143.44873302562075
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218551.01423819028,
            "upper_bound": 219290.4775449102
          },
          "point_estimate": 218830.47133233532,
          "standard_error": 214.32965334811985
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.88570488519765,
            "upper_bound": 797.913382490862
          },
          "point_estimate": 424.97600019467694,
          "standard_error": 191.0022105289055
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218531.3324687403,
            "upper_bound": 218948.50379973996
          },
          "point_estimate": 218697.2572517303,
          "standard_error": 106.97026279725624
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.74238686500811,
            "upper_bound": 610.9787656552915
          },
          "point_estimate": 477.481293325963,
          "standard_error": 94.05216165349394
        }
      }
    },
    "memchr3/krate/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/common",
        "directory_name": "memchr3/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629.7172572925595,
            "upper_bound": 630.2437128277387
          },
          "point_estimate": 629.9278625972041,
          "standard_error": 0.14278757713990792
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629.6513420415895,
            "upper_bound": 629.978126569998
          },
          "point_estimate": 629.8149504654,
          "standard_error": 0.09781631100336775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018407393606274608,
            "upper_bound": 0.3801395042490464
          },
          "point_estimate": 0.23336214803199165,
          "standard_error": 0.09886828359088222
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629.7555001469489,
            "upper_bound": 629.9743712360784
          },
          "point_estimate": 629.8869582473034,
          "standard_error": 0.05616339815700963
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10840355951118612,
            "upper_bound": 0.7188861142669387
          },
          "point_estimate": 0.47544315042855106,
          "standard_error": 0.2024243682027808
        }
      }
    },
    "memchr3/krate/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/never",
        "directory_name": "memchr3/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.903013071276176,
            "upper_bound": 14.95206279168254
          },
          "point_estimate": 14.924965156349302,
          "standard_error": 0.012574397412939349
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.89722848739068,
            "upper_bound": 14.95691972009426
          },
          "point_estimate": 14.910627787477717,
          "standard_error": 0.012407175529336983
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004982511123732206,
            "upper_bound": 0.05546106092598083
          },
          "point_estimate": 0.019666870413165612,
          "standard_error": 0.01237874217871447
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.894759200562596,
            "upper_bound": 14.916856315926278
          },
          "point_estimate": 14.902177519240414,
          "standard_error": 0.0057055375941090615
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010954841459398906,
            "upper_bound": 0.05343324920566783
          },
          "point_estimate": 0.04191994141482631,
          "standard_error": 0.011232889230718834
        }
      }
    },
    "memchr3/krate/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/rare",
        "directory_name": "memchr3/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.36861112911089,
            "upper_bound": 33.40624712506098
          },
          "point_estimate": 33.38551816663875,
          "standard_error": 0.009744249949307362
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.366041398282405,
            "upper_bound": 33.40352347029003
          },
          "point_estimate": 33.37744217820746,
          "standard_error": 0.007348263538745865
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002527656396949003,
            "upper_bound": 0.04834291755032077
          },
          "point_estimate": 0.013371700529948988,
          "standard_error": 0.011367290834272238
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.36799376661063,
            "upper_bound": 33.381029256389255
          },
          "point_estimate": 33.37471350598957,
          "standard_error": 0.0033245669591708475
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008686057638807086,
            "upper_bound": 0.04405276760431133
          },
          "point_estimate": 0.03247766138523242,
          "standard_error": 0.0092909633163565
        }
      }
    },
    "memchr3/krate/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/uncommon",
        "directory_name": "memchr3/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.50122162915312,
            "upper_bound": 173.14118822634111
          },
          "point_estimate": 172.81385719286624,
          "standard_error": 0.16413510613490026
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.3236361240653,
            "upper_bound": 173.35341892508742
          },
          "point_estimate": 172.59040624122554,
          "standard_error": 0.3090749822832753
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.036592488232771385,
            "upper_bound": 0.8256969323357115
          },
          "point_estimate": 0.5686553131734196,
          "standard_error": 0.20714475071957897
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.49577855074068,
            "upper_bound": 173.41847721126322
          },
          "point_estimate": 173.08915492956885,
          "standard_error": 0.23264153191953027
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.33997099007266457,
            "upper_bound": 0.642214872100545
          },
          "point_estimate": 0.54793911900373,
          "standard_error": 0.0757280524590877
        }
      }
    },
    "memchr3/krate/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/never",
        "directory_name": "memchr3/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.153726424645768,
            "upper_bound": 5.1591141889710475
          },
          "point_estimate": 5.155811499521108,
          "standard_error": 0.0015012937857975825
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.1534250463968885,
            "upper_bound": 5.1560795641649975
          },
          "point_estimate": 5.154059203577704,
          "standard_error": 0.0006486998573665617
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00015402610753103332,
            "upper_bound": 0.0028293516130836085
          },
          "point_estimate": 0.0008567809541496627,
          "standard_error": 0.0007751328855539726
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.1538061462999805,
            "upper_bound": 5.1548996708321
          },
          "point_estimate": 5.154192403005432,
          "standard_error": 0.00028710160765319995
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005159893521977782,
            "upper_bound": 0.007580113902183033
          },
          "point_estimate": 0.004995378031046996,
          "standard_error": 0.0022852968181835665
        }
      }
    },
    "memchr3/krate/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/rare",
        "directory_name": "memchr3/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.05769187630718,
            "upper_bound": 20.075671001705437
          },
          "point_estimate": 20.06728250596243,
          "standard_error": 0.004601811773113961
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.059413270768367,
            "upper_bound": 20.07740010745494
          },
          "point_estimate": 20.068297811360225,
          "standard_error": 0.003950246978589868
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016215593418767715,
            "upper_bound": 0.02432429668603316
          },
          "point_estimate": 0.009875473506771615,
          "standard_error": 0.005732058582935287
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.05735563767893,
            "upper_bound": 20.07294685532307
          },
          "point_estimate": 20.066475959355476,
          "standard_error": 0.003970052002652463
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00574621692231579,
            "upper_bound": 0.021166812176505956
          },
          "point_estimate": 0.015323912966799826,
          "standard_error": 0.0040685821416216825
        }
      }
    },
    "memchr3/krate/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/uncommon",
        "directory_name": "memchr3/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.97884190036551,
            "upper_bound": 82.12098225269949
          },
          "point_estimate": 82.05621111446125,
          "standard_error": 0.03646371456375414
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.01937756265247,
            "upper_bound": 82.1286684123204
          },
          "point_estimate": 82.06921272521379,
          "standard_error": 0.02444311407555154
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015998846907305162,
            "upper_bound": 0.16691600380857452
          },
          "point_estimate": 0.05056891746657385,
          "standard_error": 0.03989329580372251
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.87378404619602,
            "upper_bound": 82.07467091657894
          },
          "point_estimate": 81.98793779261675,
          "standard_error": 0.06035296186600048
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03868156290062624,
            "upper_bound": 0.1727927376106896
          },
          "point_estimate": 0.12144681872404169,
          "standard_error": 0.03691241474071492
        }
      }
    },
    "memchr3/naive/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/naive/empty/never",
        "directory_name": "memchr3/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7200347150127485,
            "upper_bound": 0.743632396079611
          },
          "point_estimate": 0.732779365294186,
          "standard_error": 0.006016859034400108
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7248218359495059,
            "upper_bound": 0.7421754363811949
          },
          "point_estimate": 0.7368162729228529,
          "standard_error": 0.004032645659420476
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008897058551147454,
            "upper_bound": 0.0293531881161288
          },
          "point_estimate": 0.010689985589990196,
          "standard_error": 0.0070003155339833775
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7315804798794829,
            "upper_bound": 0.7432495045195355
          },
          "point_estimate": 0.7377633372716625,
          "standard_error": 0.003059377862622691
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006065613586645391,
            "upper_bound": 0.028834530806491145
          },
          "point_estimate": 0.020061611567417103,
          "standard_error": 0.006141350050921077
        }
      }
    },
    "memchr3/naive/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/common",
        "directory_name": "memchr3/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1159925.8216060267,
            "upper_bound": 1160463.8498958333
          },
          "point_estimate": 1160157.2508779762,
          "standard_error": 140.11419214859652
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1159825.8973214286,
            "upper_bound": 1160351.0833333335
          },
          "point_estimate": 1160021.6822916665,
          "standard_error": 110.26092497708709
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.57464308291674,
            "upper_bound": 563.4652087463778
          },
          "point_estimate": 249.97529087450215,
          "standard_error": 141.16186392579638
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1159846.0697745115,
            "upper_bound": 1160138.166462677
          },
          "point_estimate": 1159968.8582792208,
          "standard_error": 75.13118697396871
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135.08452660940128,
            "upper_bound": 672.4228410402021
          },
          "point_estimate": 467.8496379839082,
          "standard_error": 157.13772859422863
        }
      }
    },
    "memchr3/naive/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/never",
        "directory_name": "memchr3/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290661.08687139687,
            "upper_bound": 291878.49867570633
          },
          "point_estimate": 291268.0809292064,
          "standard_error": 312.03516092419113
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290248.9774666667,
            "upper_bound": 292082.3617777778
          },
          "point_estimate": 291394.8568571429,
          "standard_error": 621.5808393987494
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.22831399420081,
            "upper_bound": 1680.467766165678
          },
          "point_estimate": 1263.298134531961,
          "standard_error": 458.25556926038985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290733.9696243655,
            "upper_bound": 291927.636868217
          },
          "point_estimate": 291533.526774026,
          "standard_error": 301.30188989445674
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 690.2585118830403,
            "upper_bound": 1238.8053963488917
          },
          "point_estimate": 1042.120032947038,
          "standard_error": 141.53913088521276
        }
      }
    },
    "memchr3/naive/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/rare",
        "directory_name": "memchr3/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296307.93746757647,
            "upper_bound": 296515.65805313597
          },
          "point_estimate": 296404.58300167765,
          "standard_error": 53.588920046606056
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296255.60298102984,
            "upper_bound": 296525.743902439
          },
          "point_estimate": 296351.15514905145,
          "standard_error": 75.04428981651097
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.884689573133958,
            "upper_bound": 288.2120107368588
          },
          "point_estimate": 142.897841365408,
          "standard_error": 74.11226355773627
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296266.1045497862,
            "upper_bound": 296415.38492654235
          },
          "point_estimate": 296342.32872980676,
          "standard_error": 38.81255871822745
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.33160603772488,
            "upper_bound": 230.05758215446733
          },
          "point_estimate": 178.32963527367542,
          "standard_error": 39.238867410144906
        }
      }
    },
    "memchr3/naive/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/uncommon",
        "directory_name": "memchr3/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464460.758046514,
            "upper_bound": 464729.345566079
          },
          "point_estimate": 464581.7547202129,
          "standard_error": 69.17300972383367
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464429.2163150492,
            "upper_bound": 464731.4028028933
          },
          "point_estimate": 464517.5181962025,
          "standard_error": 61.788726424599474
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.927018256109385,
            "upper_bound": 332.36105224285933
          },
          "point_estimate": 120.11749736116803,
          "standard_error": 76.31375676116407
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464430.5929152483,
            "upper_bound": 464548.9782155425
          },
          "point_estimate": 464484.7949367089,
          "standard_error": 30.14438867283774
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.38046330721676,
            "upper_bound": 308.26824613588275
          },
          "point_estimate": 231.4714998530368,
          "standard_error": 62.86547241480388
        }
      }
    },
    "memchr3/naive/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/common",
        "directory_name": "memchr3/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414.9201976311476,
            "upper_bound": 415.61917927600587
          },
          "point_estimate": 415.25381301690015,
          "standard_error": 0.17954935511343925
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414.7389047207663,
            "upper_bound": 415.73830882694006
          },
          "point_estimate": 415.2421756795858,
          "standard_error": 0.23630892642344276
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.031164009836839698,
            "upper_bound": 1.05854702744257
          },
          "point_estimate": 0.6426748111701027,
          "standard_error": 0.2642433197371853
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414.81164844306875,
            "upper_bound": 415.4715613295384
          },
          "point_estimate": 415.1024276178388,
          "standard_error": 0.1687644802553317
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.29465198363957296,
            "upper_bound": 0.7415136013809488
          },
          "point_estimate": 0.5987925618214577,
          "standard_error": 0.1133951217442922
        }
      }
    },
    "memchr3/naive/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/never",
        "directory_name": "memchr3/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.2974478321892,
            "upper_bound": 335.67482997772504
          },
          "point_estimate": 335.4698416963737,
          "standard_error": 0.0967806168664772
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.2090747632195,
            "upper_bound": 335.6408539794816
          },
          "point_estimate": 335.4063557582391,
          "standard_error": 0.12239078299532606
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04443067524752281,
            "upper_bound": 0.5190426269666322
          },
          "point_estimate": 0.3130820235356463,
          "standard_error": 0.11793346611287808
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.2408760188042,
            "upper_bound": 335.58384705895185
          },
          "point_estimate": 335.3603446206126,
          "standard_error": 0.08834942581030447
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14620138118711248,
            "upper_bound": 0.44156767252936935
          },
          "point_estimate": 0.32288012313630926,
          "standard_error": 0.08453245268066766
        }
      }
    },
    "memchr3/naive/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/rare",
        "directory_name": "memchr3/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 362.03434143879906,
            "upper_bound": 362.31013598055426
          },
          "point_estimate": 362.1608323600672,
          "standard_error": 0.07100058729324273
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 362.0176201908709,
            "upper_bound": 362.33114051940635
          },
          "point_estimate": 362.07239843796697,
          "standard_error": 0.0745535502330505
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015640985157998347,
            "upper_bound": 0.3587358656002322
          },
          "point_estimate": 0.0982688726551662,
          "standard_error": 0.08766561890276514
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 362.0488801180688,
            "upper_bound": 362.2630655830957
          },
          "point_estimate": 362.14525913607423,
          "standard_error": 0.05427796370764219
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07446072847740569,
            "upper_bound": 0.3021003343223626
          },
          "point_estimate": 0.23698014317458463,
          "standard_error": 0.057357162488327326
        }
      }
    },
    "memchr3/naive/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/uncommon",
        "directory_name": "memchr3/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 411.1533747045681,
            "upper_bound": 411.65298900170035
          },
          "point_estimate": 411.37297490556057,
          "standard_error": 0.12983098426375608
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 411.148363724461,
            "upper_bound": 411.635805539853
          },
          "point_estimate": 411.2111498021481,
          "standard_error": 0.09875918940086108
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008868221888859081,
            "upper_bound": 0.5797644758574976
          },
          "point_estimate": 0.06866432557576645,
          "standard_error": 0.12526695379819736
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 411.16564057238,
            "upper_bound": 411.8552528107531
          },
          "point_estimate": 411.4302643360032,
          "standard_error": 0.20322824894955763
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06237341836519546,
            "upper_bound": 0.5694811936270624
          },
          "point_estimate": 0.4334305534225049,
          "standard_error": 0.13264116456702432
        }
      }
    },
    "memchr3/naive/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/never",
        "directory_name": "memchr3/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.24114139371435,
            "upper_bound": 44.29397133517596
          },
          "point_estimate": 44.26779580139496,
          "standard_error": 0.013518687781592726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.2224621843366,
            "upper_bound": 44.31225627791587
          },
          "point_estimate": 44.27158594408734,
          "standard_error": 0.02634553418671999
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00990105434618388,
            "upper_bound": 0.07154470906582085
          },
          "point_estimate": 0.062484540963874326,
          "standard_error": 0.01740976958261236
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.238312457531855,
            "upper_bound": 44.30793683249555
          },
          "point_estimate": 44.27735098301342,
          "standard_error": 0.018077751347594328
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029920512080933173,
            "upper_bound": 0.05161445351738468
          },
          "point_estimate": 0.04493709472765688,
          "standard_error": 0.00559705597129492
        }
      }
    },
    "memchr3/naive/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/rare",
        "directory_name": "memchr3/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.50564700199541,
            "upper_bound": 44.91882102191519
          },
          "point_estimate": 44.71643625261524,
          "standard_error": 0.10604970088887924
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.3674337621335,
            "upper_bound": 45.032334003602955
          },
          "point_estimate": 44.78901827049094,
          "standard_error": 0.1525414664724569
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04845129871586248,
            "upper_bound": 0.6076012571430486
          },
          "point_estimate": 0.3753205643116256,
          "standard_error": 0.14156092065716525
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.49979250823574,
            "upper_bound": 45.03151654910966
          },
          "point_estimate": 44.79083092521403,
          "standard_error": 0.13697137306917334
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.200336524214657,
            "upper_bound": 0.4306557434125856
          },
          "point_estimate": 0.35415435260502703,
          "standard_error": 0.05740273914735224
        }
      }
    },
    "memchr3/naive/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memchr3/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/uncommon",
        "directory_name": "memchr3/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.03844623306741,
            "upper_bound": 43.16096147689624
          },
          "point_estimate": 43.089177189239294,
          "standard_error": 0.03224523450740518
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.018953889067646,
            "upper_bound": 43.11763405074819
          },
          "point_estimate": 43.06396524968625,
          "standard_error": 0.027698676575478795
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009516247158114536,
            "upper_bound": 0.11484107378067517
          },
          "point_estimate": 0.07125491028599079,
          "standard_error": 0.025758107054436107
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.03162652555134,
            "upper_bound": 43.11381323054606
          },
          "point_estimate": 43.0766927635771,
          "standard_error": 0.021460700881420464
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0315561623099996,
            "upper_bound": 0.15953594582815173
          },
          "point_estimate": 0.1075428471800952,
          "standard_error": 0.04093223486043102
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51819.92390136055,
            "upper_bound": 52137.733848248296
          },
          "point_estimate": 51936.636222789115,
          "standard_error": 92.05839550528532
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51815.27781632653,
            "upper_bound": 51911.58728571428
          },
          "point_estimate": 51824.83541666667,
          "standard_error": 30.726615697228585
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6992850520782588,
            "upper_bound": 113.91048577768385
          },
          "point_estimate": 28.877931001604175,
          "standard_error": 35.97333466020526
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51816.14904242896,
            "upper_bound": 51840.54700836176
          },
          "point_estimate": 51825.24318738405,
          "standard_error": 6.295347939960425
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.921303757136346,
            "upper_bound": 469.8096561429168
          },
          "point_estimate": 306.4736342474923,
          "standard_error": 154.85890750747697
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53233.95388260485,
            "upper_bound": 53394.49199739329
          },
          "point_estimate": 53305.06869565471,
          "standard_error": 41.623875200751826
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53220.29516129033,
            "upper_bound": 53381.24181329423
          },
          "point_estimate": 53242.22762620445,
          "standard_error": 38.265344756963955
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.236873558560099,
            "upper_bound": 179.92520963560307
          },
          "point_estimate": 39.55882525076636,
          "standard_error": 42.54822010527104
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53232.625147094535,
            "upper_bound": 53272.91000323825
          },
          "point_estimate": 53245.892680047225,
          "standard_error": 10.495102282668844
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.09476539088531,
            "upper_bound": 182.57222533796823
          },
          "point_estimate": 138.18524385690563,
          "standard_error": 41.00918158251832
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53415.43983107493,
            "upper_bound": 53594.621437127986
          },
          "point_estimate": 53493.19791158964,
          "standard_error": 46.251246488682504
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53374.365000000005,
            "upper_bound": 53539.13998161765
          },
          "point_estimate": 53488.96680672269,
          "standard_error": 51.59640019580348
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6618039055832337,
            "upper_bound": 209.5617997795341
          },
          "point_estimate": 133.10692626555516,
          "standard_error": 51.76850783507351
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53397.9012847289,
            "upper_bound": 53516.66831043202
          },
          "point_estimate": 53457.31995798319,
          "standard_error": 30.620119959180396
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.1864987268736,
            "upper_bound": 222.81020751848152
          },
          "point_estimate": 154.04139669277822,
          "standard_error": 50.27260889374608
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15297.468396745158,
            "upper_bound": 15346.365127238205
          },
          "point_estimate": 15317.214500157124,
          "standard_error": 13.07501434203168
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15290.300688008989,
            "upper_bound": 15322.015886388252
          },
          "point_estimate": 15308.308146120004,
          "standard_error": 9.5895299659857
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.9836956327309174,
            "upper_bound": 39.90484992769964
          },
          "point_estimate": 22.866942208814994,
          "standard_error": 8.797574104228326
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15299.42075889378,
            "upper_bound": 15317.23881206808
          },
          "point_estimate": 15309.021930218056,
          "standard_error": 4.480235193872135
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.36023681682859,
            "upper_bound": 65.71856571430243
          },
          "point_estimate": 43.5589971337935,
          "standard_error": 17.993101207052657
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26992.100702610907,
            "upper_bound": 27032.487281775517
          },
          "point_estimate": 27010.039054311423,
          "standard_error": 10.42821754385578
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26984.075037147104,
            "upper_bound": 27026.32721290596
          },
          "point_estimate": 26999.98688707281,
          "standard_error": 10.38457140678125
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.9658447810058353,
            "upper_bound": 46.10646738352549
          },
          "point_estimate": 25.33383067021993,
          "standard_error": 10.470247905387955
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26992.24888812256,
            "upper_bound": 27021.33263251339
          },
          "point_estimate": 27005.40412767025,
          "standard_error": 7.394590679400674
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.082369733726145,
            "upper_bound": 49.006861928360195
          },
          "point_estimate": 34.72748359200497,
          "standard_error": 10.511850134248167
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16537.928603787466,
            "upper_bound": 16570.45135528076
          },
          "point_estimate": 16550.34511610081,
          "standard_error": 9.085928661166513
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16535.84952164009,
            "upper_bound": 16550.38838268793
          },
          "point_estimate": 16540.403254872184,
          "standard_error": 4.208574592311801
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9987237572266496,
            "upper_bound": 17.259324917579054
          },
          "point_estimate": 7.425103452839404,
          "standard_error": 4.5661916995974465
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16536.765634827098,
            "upper_bound": 16542.933740793098
          },
          "point_estimate": 16539.02204064728,
          "standard_error": 1.5875868031180769
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.0345256892705805,
            "upper_bound": 46.327989567634894
          },
          "point_estimate": 30.28788731930677,
          "standard_error": 14.175587611172524
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16692.448226264332,
            "upper_bound": 16710.558496367295
          },
          "point_estimate": 16700.08972709136,
          "standard_error": 4.738161150455006
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16689.839680989586,
            "upper_bound": 16705.02895220588
          },
          "point_estimate": 16696.510402077496,
          "standard_error": 4.059673044666281
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.674456217837362,
            "upper_bound": 17.592724788771356
          },
          "point_estimate": 10.452466082812874,
          "standard_error": 3.695582439993842
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16691.4931409525,
            "upper_bound": 16701.924283686436
          },
          "point_estimate": 16696.888476413293,
          "standard_error": 2.7299281503332433
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.091043711837717,
            "upper_bound": 23.334489674838913
          },
          "point_estimate": 15.866517451947852,
          "standard_error": 5.741976953735794
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19275.586912576247,
            "upper_bound": 19297.24125663482
          },
          "point_estimate": 19285.78873759394,
          "standard_error": 5.558683626111575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19270.13528397028,
            "upper_bound": 19302.79454175513
          },
          "point_estimate": 19280.611673491057,
          "standard_error": 8.33232540761544
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.324787366786086,
            "upper_bound": 31.300631233045436
          },
          "point_estimate": 16.53977723290079,
          "standard_error": 7.825134228169615
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19272.051174473105,
            "upper_bound": 19290.545020874593
          },
          "point_estimate": 19278.05176055367,
          "standard_error": 4.763762419004256
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.1186261019377,
            "upper_bound": 22.566125633597004
          },
          "point_estimate": 18.481346354021827,
          "standard_error": 3.3906893557876248
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39430.26425986253,
            "upper_bound": 39518.305765184385
          },
          "point_estimate": 39466.303253710015,
          "standard_error": 23.395492903728808
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39421.80987243053,
            "upper_bound": 39480.08248373102
          },
          "point_estimate": 39451.60913774403,
          "standard_error": 13.99546768695168
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.105433318293871,
            "upper_bound": 75.372841980741
          },
          "point_estimate": 35.03005200823978,
          "standard_error": 19.37907608476697
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39429.83631802584,
            "upper_bound": 39458.94431144833
          },
          "point_estimate": 39447.076679719416,
          "standard_error": 7.368187631544379
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.187764851659335,
            "upper_bound": 115.6214300583007
          },
          "point_estimate": 77.62489158232142,
          "standard_error": 30.67025099461046
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16641.199526982306,
            "upper_bound": 16677.842683396135
          },
          "point_estimate": 16658.541935452162,
          "standard_error": 9.400558006709426
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16627.304193971166,
            "upper_bound": 16676.906880733943
          },
          "point_estimate": 16660.650573394494,
          "standard_error": 12.444308593987447
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.462179003349085,
            "upper_bound": 55.62749591608517
          },
          "point_estimate": 34.89015099754844,
          "standard_error": 13.192808114336527
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16637.901656335234,
            "upper_bound": 16666.66479865227
          },
          "point_estimate": 16653.61436077684,
          "standard_error": 7.360654967066764
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.70154173321565,
            "upper_bound": 41.377258247674845
          },
          "point_estimate": 31.46694247432534,
          "standard_error": 6.851610836377281
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20181.40109698386,
            "upper_bound": 20223.220452089627
          },
          "point_estimate": 20201.066716280875,
          "standard_error": 10.69378470378348
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20174.779446298355,
            "upper_bound": 20220.40063033
          },
          "point_estimate": 20197.088125695216,
          "standard_error": 10.823530430128775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.168422703657784,
            "upper_bound": 57.3261682536713
          },
          "point_estimate": 33.81898312224981,
          "standard_error": 12.634763921255525
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20181.89372750923,
            "upper_bound": 20204.326550699607
          },
          "point_estimate": 20193.94826286061,
          "standard_error": 5.778413219382224
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.250432659103765,
            "upper_bound": 48.48397756702764
          },
          "point_estimate": 35.63782191445288,
          "standard_error": 8.812741064037443
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16647.756271267077,
            "upper_bound": 16687.861884538725
          },
          "point_estimate": 16664.06784810426,
          "standard_error": 10.72299598397435
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16646.7323883724,
            "upper_bound": 16667.625343721356
          },
          "point_estimate": 16655.400361543943,
          "standard_error": 5.816051263314261
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.705061726156526,
            "upper_bound": 30.582786200861815
          },
          "point_estimate": 15.13011845274365,
          "standard_error": 7.091916584834387
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16641.48554482602,
            "upper_bound": 16657.6204265088
          },
          "point_estimate": 16650.267243205923,
          "standard_error": 4.169930413653718
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.005936050885348,
            "upper_bound": 53.73408458489613
          },
          "point_estimate": 35.712309920983934,
          "standard_error": 14.756352557587867
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19340.74058868773,
            "upper_bound": 19366.44660835996
          },
          "point_estimate": 19353.76708629156,
          "standard_error": 6.567714125499795
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19340.738255827713,
            "upper_bound": 19368.75582401491
          },
          "point_estimate": 19353.838809016685,
          "standard_error": 8.695474305080044
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.014545386666315,
            "upper_bound": 37.77622102262971
          },
          "point_estimate": 18.85963085527194,
          "standard_error": 8.924165297652841
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19336.93210003779,
            "upper_bound": 19364.23887940318
          },
          "point_estimate": 19349.580234568413,
          "standard_error": 6.779844297526888
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.293122648796595,
            "upper_bound": 29.011122926149483
          },
          "point_estimate": 21.862805958075736,
          "standard_error": 4.550485064961779
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16574.307924849378,
            "upper_bound": 16614.65946316331
          },
          "point_estimate": 16593.139444263412,
          "standard_error": 10.36580364282204
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16563.44875304136,
            "upper_bound": 16620.41765510949
          },
          "point_estimate": 16586.204232924923,
          "standard_error": 12.79186998236766
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.295487052424447,
            "upper_bound": 57.0346225922084
          },
          "point_estimate": 31.88495525813836,
          "standard_error": 12.877729241054483
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16573.96308824402,
            "upper_bound": 16597.228048120698
          },
          "point_estimate": 16585.454530050243,
          "standard_error": 6.061672492466999
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.137312894863388,
            "upper_bound": 43.23786883911129
          },
          "point_estimate": 34.67817882885051,
          "standard_error": 7.415528479222953
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16477.663498832233,
            "upper_bound": 16512.672819989137
          },
          "point_estimate": 16494.592010383778,
          "standard_error": 9.009307633799663
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16467.127611262487,
            "upper_bound": 16526.21036431527
          },
          "point_estimate": 16490.108395776566,
          "standard_error": 14.465270157023545
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.056542302686556,
            "upper_bound": 48.20226670403013
          },
          "point_estimate": 34.17682577266783,
          "standard_error": 12.680221307160508
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16472.44425906181,
            "upper_bound": 16509.847817877326
          },
          "point_estimate": 16488.837910046357,
          "standard_error": 9.937423244648318
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.987766881123603,
            "upper_bound": 36.29943792553993
          },
          "point_estimate": 30.06422938899698,
          "standard_error": 4.9246522572178275
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16699.454927693747,
            "upper_bound": 16742.873634839976
          },
          "point_estimate": 16719.584869432234,
          "standard_error": 11.147002962320911
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16686.251066342935,
            "upper_bound": 16741.590864722093
          },
          "point_estimate": 16715.020956464043,
          "standard_error": 13.914314530667797
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.479414607062445,
            "upper_bound": 60.823240709331415
          },
          "point_estimate": 37.05731344407797,
          "standard_error": 13.835773356829264
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16689.788124038467,
            "upper_bound": 16721.88576519669
          },
          "point_estimate": 16703.720900321543,
          "standard_error": 8.283904083345963
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.60199247407745,
            "upper_bound": 49.62087291300011
          },
          "point_estimate": 37.28440861601517,
          "standard_error": 8.79331356390008
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19539.690606923523,
            "upper_bound": 19566.923497108663
          },
          "point_estimate": 19552.09624394836,
          "standard_error": 7.015981308912774
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19537.26213017751,
            "upper_bound": 19567.818719741794
          },
          "point_estimate": 19544.06828267886,
          "standard_error": 6.647692914687493
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.208231906732943,
            "upper_bound": 36.79409320906264
          },
          "point_estimate": 11.568506680039532,
          "standard_error": 8.705741136763654
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19532.51192578914,
            "upper_bound": 19553.441016132947
          },
          "point_estimate": 19543.128776118985,
          "standard_error": 5.496171187047941
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.445122140932611,
            "upper_bound": 30.232326179834853
          },
          "point_estimate": 23.377856986872015,
          "standard_error": 5.832431905532344
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16883.579139134137,
            "upper_bound": 16912.94337095061
          },
          "point_estimate": 16897.71004691096,
          "standard_error": 7.502817269143849
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16872.770446096656,
            "upper_bound": 16916.364776951672
          },
          "point_estimate": 16895.53889792441,
          "standard_error": 9.179811543815177
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.1448928554114097,
            "upper_bound": 40.36719781958145
          },
          "point_estimate": 32.31647688909111,
          "standard_error": 11.355112953330956
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16891.44632249307,
            "upper_bound": 16904.903984975055
          },
          "point_estimate": 16897.213372036884,
          "standard_error": 3.4048232850057327
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.248827070772975,
            "upper_bound": 32.44962537475724
          },
          "point_estimate": 25.019444958050613,
          "standard_error": 5.0879631873939815
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19835.566280151797,
            "upper_bound": 19863.838946116655
          },
          "point_estimate": 19848.306042186523,
          "standard_error": 7.271775999466839
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19831.841771288207,
            "upper_bound": 19865.862991266375
          },
          "point_estimate": 19838.120717664795,
          "standard_error": 8.329075531700413
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7930598071423436,
            "upper_bound": 35.36934306202172
          },
          "point_estimate": 14.76509182301889,
          "standard_error": 9.112235530145144
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19830.257681764575,
            "upper_bound": 19847.565528519863
          },
          "point_estimate": 19838.323066126013,
          "standard_error": 4.633943709842646
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.024975984618719,
            "upper_bound": 32.266475539355746
          },
          "point_estimate": 24.391455477354395,
          "standard_error": 6.450941365737063
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71820.3491913215,
            "upper_bound": 71927.23858788078
          },
          "point_estimate": 71874.83822266053,
          "standard_error": 27.395284469147885
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71801.87771203156,
            "upper_bound": 71954.30769230769
          },
          "point_estimate": 71877.80542406312,
          "standard_error": 41.4260047239181
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.186945061714106,
            "upper_bound": 154.65413620896652
          },
          "point_estimate": 99.87591745761476,
          "standard_error": 32.78694304393797
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71827.40861895724,
            "upper_bound": 71952.16945934488
          },
          "point_estimate": 71887.49981813059,
          "standard_error": 32.09129859597495
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.91919107593769,
            "upper_bound": 113.74315953756104
          },
          "point_estimate": 91.05540587035271,
          "standard_error": 15.499867993710115
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231223.0297636574,
            "upper_bound": 1232856.7059708333
          },
          "point_estimate": 1232008.4675462963,
          "standard_error": 419.3112650988561
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1230795.1444444444,
            "upper_bound": 1233677.2933333332
          },
          "point_estimate": 1231630.0703703703,
          "standard_error": 640.4235301462174
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.2899889646994,
            "upper_bound": 2173.620739799486
          },
          "point_estimate": 1408.6649094358183,
          "standard_error": 567.9277075136122
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231427.8945355192,
            "upper_bound": 1233014.6834212914
          },
          "point_estimate": 1232065.2283982683,
          "standard_error": 421.2252450111829
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 669.4847808728049,
            "upper_bound": 1649.6197057770828
          },
          "point_estimate": 1396.578898466166,
          "standard_error": 223.0646038734363
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203289.28897004965,
            "upper_bound": 203727.5934916201
          },
          "point_estimate": 203503.11358561675,
          "standard_error": 112.00743638912512
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203152.78212290505,
            "upper_bound": 203737.4977653631
          },
          "point_estimate": 203490.85686352753,
          "standard_error": 151.1599993616304
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.062785558041114,
            "upper_bound": 692.649793932085
          },
          "point_estimate": 370.45407842313097,
          "standard_error": 161.60108776348392
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203157.55824318624,
            "upper_bound": 203544.08841379223
          },
          "point_estimate": 203334.3943553653,
          "standard_error": 99.12600919637713
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207.1690914671372,
            "upper_bound": 482.07782340989104
          },
          "point_estimate": 371.64466174275645,
          "standard_error": 73.1980426315447
        }
      }
    },
    "memmem/bstr/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/bstr_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6677.53050690441,
            "upper_bound": 6698.4243014896865
          },
          "point_estimate": 6687.549828035015,
          "standard_error": 5.356685685373218
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6675.227094977786,
            "upper_bound": 6697.601091763405
          },
          "point_estimate": 6687.792804496038,
          "standard_error": 6.357018044169055
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.882054608310777,
            "upper_bound": 30.221666466405964
          },
          "point_estimate": 16.616363625521632,
          "standard_error": 6.419464236101815
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6667.53185070719,
            "upper_bound": 6685.9682536656865
          },
          "point_estimate": 6675.838006695718,
          "standard_error": 4.871002859137847
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.743830094033552,
            "upper_bound": 23.857411275016865
          },
          "point_estimate": 17.81034291998403,
          "standard_error": 3.9607102659197033
        }
      }
    },
    "memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/bstr_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6748.962495705608,
            "upper_bound": 6761.904758085188
          },
          "point_estimate": 6755.5989040446975,
          "standard_error": 3.3133296226785864
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6746.152627990579,
            "upper_bound": 6764.711014007686
          },
          "point_estimate": 6755.711778807841,
          "standard_error": 4.493596585472551
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5601091858536261,
            "upper_bound": 18.700105420144904
          },
          "point_estimate": 13.054603366225818,
          "standard_error": 4.463916324932806
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6746.245420945734,
            "upper_bound": 6758.725962022481
          },
          "point_estimate": 6753.190155660628,
          "standard_error": 3.1750042614590273
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.993931101263355,
            "upper_bound": 13.80999118493665
          },
          "point_estimate": 11.058058507669548,
          "standard_error": 1.966865273475328
        }
      }
    },
    "memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/bstr_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15588.443750698574,
            "upper_bound": 15617.283465483515
          },
          "point_estimate": 15601.862400597707,
          "standard_error": 7.398954809237642
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15580.929522173834,
            "upper_bound": 15621.028445684844
          },
          "point_estimate": 15596.96283276084,
          "standard_error": 10.21007815723248
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4428639577691262,
            "upper_bound": 41.229424690958425
          },
          "point_estimate": 22.629269971800788,
          "standard_error": 10.679527752826711
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15584.864223515138,
            "upper_bound": 15614.235389532672
          },
          "point_estimate": 15597.88621503014,
          "standard_error": 7.402671570072879
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.267552064485368,
            "upper_bound": 31.355142805391647
          },
          "point_estimate": 24.72343678211639,
          "standard_error": 5.23506632979178
        }
      }
    },
    "memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/bstr_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.63814828822323,
            "upper_bound": 58.14663241246012
          },
          "point_estimate": 57.9238362313866,
          "standard_error": 0.12991532174064604
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.86954781806954,
            "upper_bound": 58.09755694229881
          },
          "point_estimate": 57.95748965551503,
          "standard_error": 0.06744273979140619
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03615022043050721,
            "upper_bound": 0.5006819686304935
          },
          "point_estimate": 0.17608474840056523,
          "standard_error": 0.1076226000074592
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.85475733823036,
            "upper_bound": 58.07319061488561
          },
          "point_estimate": 58.00211481744581,
          "standard_error": 0.05649963578798204
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08494325659380322,
            "upper_bound": 0.6382715903768355
          },
          "point_estimate": 0.43394137355805623,
          "standard_error": 0.1581718584372693
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.12437378589488,
            "upper_bound": 33.29145792540975
          },
          "point_estimate": 33.20173388017261,
          "standard_error": 0.04294039295056939
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.088317025083064,
            "upper_bound": 33.30796050160909
          },
          "point_estimate": 33.16939076819161,
          "standard_error": 0.04908284161505093
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03105790002001292,
            "upper_bound": 0.23209547562499863
          },
          "point_estimate": 0.11417695158571728,
          "standard_error": 0.05260953188058593
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.13962754276011,
            "upper_bound": 33.319539936545105
          },
          "point_estimate": 33.225662027555856,
          "standard_error": 0.05053709880454474
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05843577498014459,
            "upper_bound": 0.18348197397753688
          },
          "point_estimate": 0.14319920136472974,
          "standard_error": 0.0322555311995264
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.8791389856285,
            "upper_bound": 37.08451559014856
          },
          "point_estimate": 36.98164232070157,
          "standard_error": 0.052247497426726235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.897864168191056,
            "upper_bound": 37.06143760474761
          },
          "point_estimate": 36.98695037168034,
          "standard_error": 0.04284106854974189
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03253295250506225,
            "upper_bound": 0.29176051299378725
          },
          "point_estimate": 0.10948252582717792,
          "standard_error": 0.05915816419996584
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.96903982530497,
            "upper_bound": 37.05775858296172
          },
          "point_estimate": 37.013519149702624,
          "standard_error": 0.022691957781143732
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06170413797646115,
            "upper_bound": 0.23997531795643068
          },
          "point_estimate": 0.17388798458601265,
          "standard_error": 0.046474100249170265
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.819710771181377,
            "upper_bound": 27.865493831857915
          },
          "point_estimate": 27.843639069237117,
          "standard_error": 0.01167217701051041
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.82533384414009,
            "upper_bound": 27.86377714230985
          },
          "point_estimate": 27.846678957954214,
          "standard_error": 0.00770581852319425
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004775843447441399,
            "upper_bound": 0.06244930090452453
          },
          "point_estimate": 0.013714326652414328,
          "standard_error": 0.015740026936175367
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.84213641920709,
            "upper_bound": 27.862351401757973
          },
          "point_estimate": 27.851679667444895,
          "standard_error": 0.005122620958637637
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013330894628662092,
            "upper_bound": 0.05484762747259186
          },
          "point_estimate": 0.039126623192112304,
          "standard_error": 0.010607046770990667
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.379907689435026,
            "upper_bound": 20.40021725014607
          },
          "point_estimate": 20.388562725515648,
          "standard_error": 0.005330620967611364
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.377452035888368,
            "upper_bound": 20.393095689974317
          },
          "point_estimate": 20.384806423974226,
          "standard_error": 0.003561805540443273
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001504543886672053,
            "upper_bound": 0.02011998375631325
          },
          "point_estimate": 0.00906163930279307,
          "standard_error": 0.004824841927445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.38094009437183,
            "upper_bound": 20.385909033819395
          },
          "point_estimate": 20.383873570319995,
          "standard_error": 0.0012610982730329855
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004834395379932207,
            "upper_bound": 0.025918159201657555
          },
          "point_estimate": 0.017807596728732007,
          "standard_error": 0.006342428948971791
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.990575841735517,
            "upper_bound": 31.012083185238392
          },
          "point_estimate": 31.00122317162816,
          "standard_error": 0.005498926429716642
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.98288284556706,
            "upper_bound": 31.01552961527556
          },
          "point_estimate": 31.003458949138015,
          "standard_error": 0.008028463886205667
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002204257724303031,
            "upper_bound": 0.03190772553570898
          },
          "point_estimate": 0.02292876650775497,
          "standard_error": 0.007580253977955253
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.983818261977333,
            "upper_bound": 31.009034884827187
          },
          "point_estimate": 30.995571108653177,
          "standard_error": 0.0065229294264740915
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0110731818329195,
            "upper_bound": 0.022564300375971003
          },
          "point_estimate": 0.018416993342371953,
          "standard_error": 0.0029568981889603364
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.01407835206495,
            "upper_bound": 45.04552241765037
          },
          "point_estimate": 45.029639291540875,
          "standard_error": 0.00805011549827492
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.00789808581921,
            "upper_bound": 45.04293747174555
          },
          "point_estimate": 45.03322038926483,
          "standard_error": 0.008282967162671335
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004431130470814322,
            "upper_bound": 0.04895474802082825
          },
          "point_estimate": 0.01477334233954672,
          "standard_error": 0.011758189873495344
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.00927832184148,
            "upper_bound": 45.038689139577365
          },
          "point_estimate": 45.027730500600654,
          "standard_error": 0.007806059807839744
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010854798787376206,
            "upper_bound": 0.0362135536650324
          },
          "point_estimate": 0.02677172158332181,
          "standard_error": 0.006214916361239586
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.27121671456806,
            "upper_bound": 57.314536382492555
          },
          "point_estimate": 57.29272581828118,
          "standard_error": 0.011096819893701109
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.269108115809246,
            "upper_bound": 57.32223292722539
          },
          "point_estimate": 57.28634364319595,
          "standard_error": 0.014156627009099202
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007472201917986101,
            "upper_bound": 0.06525443705710422
          },
          "point_estimate": 0.03013364484919367,
          "standard_error": 0.014679477584352478
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.28022961471685,
            "upper_bound": 57.308799831462906
          },
          "point_estimate": 57.29140384360726,
          "standard_error": 0.007220750609324466
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019636013349068056,
            "upper_bound": 0.048002545362918626
          },
          "point_estimate": 0.03690260165937702,
          "standard_error": 0.007243439721321996
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.47479115803911,
            "upper_bound": 46.56963640654829
          },
          "point_estimate": 46.5160638621021,
          "standard_error": 0.024700809801231377
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.468630501457454,
            "upper_bound": 46.55429364638795
          },
          "point_estimate": 46.48015536737391,
          "standard_error": 0.019432272117496895
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017427047701427177,
            "upper_bound": 0.10473475551001037
          },
          "point_estimate": 0.022269420909158098,
          "standard_error": 0.02357334412926279
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.46688191365279,
            "upper_bound": 46.503918750675425
          },
          "point_estimate": 46.47905199582005,
          "standard_error": 0.009553329678261564
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012902554720909306,
            "upper_bound": 0.11282408529302718
          },
          "point_estimate": 0.0823543515288185,
          "standard_error": 0.026658724704321943
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.4445721585919,
            "upper_bound": 63.46979095072149
          },
          "point_estimate": 63.456848160071885,
          "standard_error": 0.006459639676708216
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.438921391155304,
            "upper_bound": 63.470409382806736
          },
          "point_estimate": 63.45725988404124,
          "standard_error": 0.008898112888606521
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004080712155693694,
            "upper_bound": 0.038127925969992
          },
          "point_estimate": 0.022011781963542223,
          "standard_error": 0.007970820494699715
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.435314860534326,
            "upper_bound": 63.46393323518672
          },
          "point_estimate": 63.450054288305985,
          "standard_error": 0.00748857200223218
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011718276543290553,
            "upper_bound": 0.028056245428065393
          },
          "point_estimate": 0.021536678663251378,
          "standard_error": 0.0042929231850385675
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.373146639879934,
            "upper_bound": 44.4505608777782
          },
          "point_estimate": 44.40922435916102,
          "standard_error": 0.019857479325284
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.35105497179948,
            "upper_bound": 44.4451160792422
          },
          "point_estimate": 44.396432668221635,
          "standard_error": 0.02298631807004855
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011156197866533924,
            "upper_bound": 0.1086312606627804
          },
          "point_estimate": 0.06832892689236332,
          "standard_error": 0.02412455946833456
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.36883477798092,
            "upper_bound": 44.41169165795771
          },
          "point_estimate": 44.39215873927137,
          "standard_error": 0.01081583261096018
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03157360205773363,
            "upper_bound": 0.08874865525461134
          },
          "point_estimate": 0.06617698945782992,
          "standard_error": 0.01582361041171556
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.01661018074043,
            "upper_bound": 37.031939939539335
          },
          "point_estimate": 37.02418684709143,
          "standard_error": 0.0039052942326111783
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.01393799422141,
            "upper_bound": 37.030138914063855
          },
          "point_estimate": 37.02606415441976,
          "standard_error": 0.003800588154874974
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001190866695896211,
            "upper_bound": 0.023352758267549383
          },
          "point_estimate": 0.00718055351929577,
          "standard_error": 0.006118548307162989
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.0161189693954,
            "upper_bound": 37.029588064821155
          },
          "point_estimate": 37.02468964718643,
          "standard_error": 0.0034473492408970273
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006155151622904912,
            "upper_bound": 0.017448218121548096
          },
          "point_estimate": 0.012948809066756726,
          "standard_error": 0.002927494796283333
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.04232076206898,
            "upper_bound": 66.13708178297061
          },
          "point_estimate": 66.09032414256745,
          "standard_error": 0.02428357945128918
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.04207787207235,
            "upper_bound": 66.14543542539975
          },
          "point_estimate": 66.08936748833369,
          "standard_error": 0.02425438330269943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009297450560358998,
            "upper_bound": 0.13740399585575278
          },
          "point_estimate": 0.06647490794728309,
          "standard_error": 0.03161331958956354
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.04181158903513,
            "upper_bound": 66.12053089629012
          },
          "point_estimate": 66.08762708845202,
          "standard_error": 0.01996006280864961
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03663467819127005,
            "upper_bound": 0.10789827315934654
          },
          "point_estimate": 0.08074183646802723,
          "standard_error": 0.018027108602654135
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103622.93991045991,
            "upper_bound": 103778.86077009566
          },
          "point_estimate": 103695.4942290734,
          "standard_error": 40.14826857707491
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103594.07065527064,
            "upper_bound": 103768.5061050061
          },
          "point_estimate": 103687.87624643874,
          "standard_error": 43.01222639033016
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.86379946981928,
            "upper_bound": 212.22444610994677
          },
          "point_estimate": 110.42919923607136,
          "standard_error": 47.56456945234027
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103601.65875733926,
            "upper_bound": 103742.6362865007
          },
          "point_estimate": 103678.24104784105,
          "standard_error": 37.78340071285258
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.536153866769794,
            "upper_bound": 179.9720860888528
          },
          "point_estimate": 133.57308587816004,
          "standard_error": 32.51603166553767
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57521.99409721124,
            "upper_bound": 57637.226623175986
          },
          "point_estimate": 57578.2070547644,
          "standard_error": 29.421697344326535
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57506.17751406469,
            "upper_bound": 57655.53665611814
          },
          "point_estimate": 57563.18215981012,
          "standard_error": 35.65587705873589
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.943711200607577,
            "upper_bound": 177.1428394579347
          },
          "point_estimate": 93.0395995349262,
          "standard_error": 41.94426304402547
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57488.89316997077,
            "upper_bound": 57607.23444154013
          },
          "point_estimate": 57548.338936380074,
          "standard_error": 29.82291928003144
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.17806215456088,
            "upper_bound": 122.95769992035702
          },
          "point_estimate": 98.20594012862816,
          "standard_error": 17.70291678684855
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147603.93479674796,
            "upper_bound": 147749.80067353853
          },
          "point_estimate": 147677.68502968125,
          "standard_error": 37.38264663844757
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147575.55691056908,
            "upper_bound": 147776.48455284553
          },
          "point_estimate": 147676.88321073685,
          "standard_error": 52.36299139171325
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.49954114946588,
            "upper_bound": 225.7965416553138
          },
          "point_estimate": 122.34674136449058,
          "standard_error": 48.23054397300697
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147608.7558343569,
            "upper_bound": 147749.49470843846
          },
          "point_estimate": 147679.90445570688,
          "standard_error": 35.777147181653284
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.25147366007252,
            "upper_bound": 153.89494770166186
          },
          "point_estimate": 124.34917711594213,
          "standard_error": 20.841999956577705
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389047.0686330632,
            "upper_bound": 389501.51318642346
          },
          "point_estimate": 389272.16024653835,
          "standard_error": 116.12045695977037
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388942.1007978723,
            "upper_bound": 389640.2489361702
          },
          "point_estimate": 389245.19756838906,
          "standard_error": 150.6880615945713
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.78156537432301,
            "upper_bound": 686.8023456791559
          },
          "point_estimate": 432.0294570816995,
          "standard_error": 169.51395815877186
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389097.3659910895,
            "upper_bound": 389662.39645406976
          },
          "point_estimate": 389414.5987012987,
          "standard_error": 143.2905158842872
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 222.4230765243912,
            "upper_bound": 482.4277559785078
          },
          "point_estimate": 387.30116864857024,
          "standard_error": 67.08875233257153
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 576993.7293036659,
            "upper_bound": 578266.094143991
          },
          "point_estimate": 577549.3051769969,
          "standard_error": 333.4837809709884
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 576936.3541446208,
            "upper_bound": 578178.9778911564
          },
          "point_estimate": 577121.1581349206,
          "standard_error": 256.4707030700235
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.3060806747633,
            "upper_bound": 1429.251799030496
          },
          "point_estimate": 271.08974911578946,
          "standard_error": 284.9995514959662
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 576998.1853842527,
            "upper_bound": 577264.3398738466
          },
          "point_estimate": 577110.3225314368,
          "standard_error": 67.72513154256197
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167.1250863678079,
            "upper_bound": 1502.0517528044163
          },
          "point_estimate": 1113.1724732325686,
          "standard_error": 360.44901527597045
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43262.385242226475,
            "upper_bound": 43348.065986082765
          },
          "point_estimate": 43303.73560629251,
          "standard_error": 22.052359310690186
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43240.95369047619,
            "upper_bound": 43364.38452380952
          },
          "point_estimate": 43290.45683106576,
          "standard_error": 39.48836837455565
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.881518973480503,
            "upper_bound": 124.14627362929266
          },
          "point_estimate": 79.29569133032277,
          "standard_error": 32.41291584884416
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43244.81804206796,
            "upper_bound": 43342.551770833335
          },
          "point_estimate": 43283.96067099567,
          "standard_error": 25.25632922862675
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.38893795647727,
            "upper_bound": 89.16394872969109
          },
          "point_estimate": 73.52084638974372,
          "standard_error": 11.59910579257393
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99127.30858489036,
            "upper_bound": 99209.22398436486
          },
          "point_estimate": 99168.53356764416,
          "standard_error": 20.957471558798726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99093.34907659703,
            "upper_bound": 99229.0098092643
          },
          "point_estimate": 99171.421037693,
          "standard_error": 30.94843265206858
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.52012718173734,
            "upper_bound": 122.15627303565488
          },
          "point_estimate": 90.18521990433106,
          "standard_error": 29.117430247979964
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99120.55014257653,
            "upper_bound": 99190.7403312664
          },
          "point_estimate": 99159.68654941788,
          "standard_error": 18.567340156910795
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.28765589757029,
            "upper_bound": 85.21072785807236
          },
          "point_estimate": 69.67341973305167,
          "standard_error": 11.164974701142874
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70935.20652533637,
            "upper_bound": 71059.11168139649
          },
          "point_estimate": 70998.32331023685,
          "standard_error": 31.778822238957325
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70921.29788256448,
            "upper_bound": 71099.3291015625
          },
          "point_estimate": 70984.8095703125,
          "standard_error": 50.276851080685255
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.408966878429055,
            "upper_bound": 195.8085518557331
          },
          "point_estimate": 129.55596176241986,
          "standard_error": 45.7136110658551
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70870.18683464972,
            "upper_bound": 71025.11938086286
          },
          "point_estimate": 70936.27620231331,
          "standard_error": 40.54330971475537
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.93889918346088,
            "upper_bound": 132.82061569773307
          },
          "point_estimate": 105.7804801673372,
          "standard_error": 18.342163136687443
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308747.8004022482,
            "upper_bound": 309591.62807400123
          },
          "point_estimate": 309135.77812584076,
          "standard_error": 216.800577501838
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308650.5900423729,
            "upper_bound": 309515.3254237288
          },
          "point_estimate": 309077.895480226,
          "standard_error": 187.41562057368532
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.83979942025522,
            "upper_bound": 1158.4865004285273
          },
          "point_estimate": 457.8998875275023,
          "standard_error": 298.5929666498269
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308449.5845303074,
            "upper_bound": 309018.9205032886
          },
          "point_estimate": 308651.0081003742,
          "standard_error": 144.95619264656676
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 294.49649425312214,
            "upper_bound": 982.8126560923196
          },
          "point_estimate": 723.5706888435709,
          "standard_error": 188.1666003795824
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36270.36506784051,
            "upper_bound": 36383.419328485885
          },
          "point_estimate": 36319.01421621044,
          "standard_error": 29.403556886600256
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36257.508649367934,
            "upper_bound": 36366.91889554225
          },
          "point_estimate": 36276.85978043912,
          "standard_error": 24.22820022904432
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.533522891355044,
            "upper_bound": 123.56982499183376
          },
          "point_estimate": 31.414251538096615,
          "standard_error": 26.883260311972652
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36266.87697140771,
            "upper_bound": 36326.3212572136
          },
          "point_estimate": 36295.48252326516,
          "standard_error": 14.739553921192652
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.680196304327495,
            "upper_bound": 135.71885985379143
          },
          "point_estimate": 98.06807440620967,
          "standard_error": 32.252805322807845
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66136.72640718432,
            "upper_bound": 66272.57586460318
          },
          "point_estimate": 66198.20218362194,
          "standard_error": 34.878229447307326
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66123.57187012988,
            "upper_bound": 66247.28356060607
          },
          "point_estimate": 66173.47858585858,
          "standard_error": 42.82717235254983
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.941176109011627,
            "upper_bound": 169.76104567250303
          },
          "point_estimate": 99.07346281382816,
          "standard_error": 42.607965249032965
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66146.2268444425,
            "upper_bound": 66230.14284935653
          },
          "point_estimate": 66193.13397874852,
          "standard_error": 21.512608930023653
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.96153147579495,
            "upper_bound": 162.57288377431746
          },
          "point_estimate": 116.3148359007598,
          "standard_error": 33.04961318497471
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166223.45248325722,
            "upper_bound": 166395.7605278729
          },
          "point_estimate": 166303.12839041097,
          "standard_error": 44.29616590877247
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166193.21156773213,
            "upper_bound": 166372.9197108067
          },
          "point_estimate": 166287.26227168948,
          "standard_error": 44.95575259057417
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.415802627202957,
            "upper_bound": 220.67747460959905
          },
          "point_estimate": 118.41731342277858,
          "standard_error": 48.36420432538818
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166215.7934161307,
            "upper_bound": 166329.2604278898
          },
          "point_estimate": 166275.72389254582,
          "standard_error": 29.771901654539256
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.80453685201587,
            "upper_bound": 205.2542694355283
          },
          "point_estimate": 147.77380634826164,
          "standard_error": 39.90807389660836
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35688.53818680058,
            "upper_bound": 35764.73851079564
          },
          "point_estimate": 35724.190407945876,
          "standard_error": 19.53091526325172
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35677.72527040315,
            "upper_bound": 35766.14955752213
          },
          "point_estimate": 35708.25227384464,
          "standard_error": 20.27914340840415
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.08583174899336,
            "upper_bound": 102.38657564539344
          },
          "point_estimate": 51.67903854908474,
          "standard_error": 24.00509941294735
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35677.97140667476,
            "upper_bound": 35761.325060592026
          },
          "point_estimate": 35719.19833735586,
          "standard_error": 22.474358564690416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.8741115893003,
            "upper_bound": 86.81420854399663
          },
          "point_estimate": 65.01565124188757,
          "standard_error": 15.579421717409955
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/bstr_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12022.587267215971,
            "upper_bound": 12054.38460232125
          },
          "point_estimate": 12036.394634382925,
          "standard_error": 8.218107117853938
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12016.51436399867,
            "upper_bound": 12043.092604893169
          },
          "point_estimate": 12035.580413483893,
          "standard_error": 6.68113207141924
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7006855980179076,
            "upper_bound": 36.20959375823684
          },
          "point_estimate": 11.21849162248514,
          "standard_error": 8.361839963296063
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12017.147938639404,
            "upper_bound": 12036.78912234089
          },
          "point_estimate": 12028.384058452342,
          "standard_error": 5.338998908847237
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.580230180823673,
            "upper_bound": 39.72378957468343
          },
          "point_estimate": 27.279062746803746,
          "standard_error": 9.21282246340234
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/bstr_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506223.5442499035,
            "upper_bound": 507624.9248611111
          },
          "point_estimate": 506845.9525424383,
          "standard_error": 363.1610072814181
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506133.3186728395,
            "upper_bound": 507296.6458333333
          },
          "point_estimate": 506554.18315972225,
          "standard_error": 344.20856181498476
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182.2308414522616,
            "upper_bound": 1530.5445799107083
          },
          "point_estimate": 842.6315767069609,
          "standard_error": 347.44688585410125
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506183.3095711267,
            "upper_bound": 506765.9167555162
          },
          "point_estimate": 506399.01843434345,
          "standard_error": 148.74732587199327
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 442.0349372250435,
            "upper_bound": 1726.536186760051
          },
          "point_estimate": 1207.3921679054747,
          "standard_error": 385.28906640133994
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/bstr_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1023.0079808942209,
            "upper_bound": 1024.0420541225442
          },
          "point_estimate": 1023.5280521014528,
          "standard_error": 0.26575804673274434
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1022.651807658393,
            "upper_bound": 1024.3884281408104
          },
          "point_estimate": 1023.5764901221364,
          "standard_error": 0.541132838271504
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.15268743162453718,
            "upper_bound": 1.423452854787257
          },
          "point_estimate": 1.279892826731414,
          "standard_error": 0.36307596527047575
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1023.4474096005868,
            "upper_bound": 1024.3573059112632
          },
          "point_estimate": 1024.0791049216446,
          "standard_error": 0.22965771201588575
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5981333843477505,
            "upper_bound": 1.0024383552243992
          },
          "point_estimate": 0.8839597611292301,
          "standard_error": 0.10447341970711964
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51951.81299979621,
            "upper_bound": 52156.83299011616
          },
          "point_estimate": 52050.77610250663,
          "standard_error": 52.51120479556986
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51927.536923442705,
            "upper_bound": 52161.123843488895
          },
          "point_estimate": 52040.3470042796,
          "standard_error": 69.52982944901339
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.628235444191468,
            "upper_bound": 301.39731155355247
          },
          "point_estimate": 160.7016231455475,
          "standard_error": 65.48131139939146
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52007.272008023174,
            "upper_bound": 52260.32559837417
          },
          "point_estimate": 52155.37588232024,
          "standard_error": 64.33974563080385
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.8852120699589,
            "upper_bound": 228.92916220300137
          },
          "point_estimate": 174.92688810122664,
          "standard_error": 36.25824767835651
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52745.344291259455,
            "upper_bound": 52905.86358321202
          },
          "point_estimate": 52820.72586227213,
          "standard_error": 41.31221674934704
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52713.951715533374,
            "upper_bound": 52918.33854924794
          },
          "point_estimate": 52774.45422125182,
          "standard_error": 59.93109308652727
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.668177243775077,
            "upper_bound": 217.73861738618857
          },
          "point_estimate": 98.73266980464368,
          "standard_error": 58.95398467589702
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52713.67723306386,
            "upper_bound": 52840.12416220528
          },
          "point_estimate": 52755.53925783096,
          "standard_error": 33.15884681543454
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.51271496612625,
            "upper_bound": 172.00384073705746
          },
          "point_estimate": 137.36252457789655,
          "standard_error": 27.731021915662513
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53275.657195612264,
            "upper_bound": 53939.6689246787
          },
          "point_estimate": 53613.84881835505,
          "standard_error": 170.71289879660137
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52967.45959004392,
            "upper_bound": 54090.8446193265
          },
          "point_estimate": 53881.57127867252,
          "standard_error": 364.1219510779509
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.16870004650931,
            "upper_bound": 791.078698732512
          },
          "point_estimate": 537.5663660639074,
          "standard_error": 237.32383891619585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53019.393134324746,
            "upper_bound": 53601.39429029374
          },
          "point_estimate": 53193.750805270865,
          "standard_error": 150.95102623728633
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 367.02694555181273,
            "upper_bound": 629.7768190226517
          },
          "point_estimate": 566.7308224227038,
          "standard_error": 66.8169611338485
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15233.616460951578,
            "upper_bound": 15262.205013451685
          },
          "point_estimate": 15247.194509658646,
          "standard_error": 7.35399505653541
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15224.9043313709,
            "upper_bound": 15274.688323521166
          },
          "point_estimate": 15239.13020979348,
          "standard_error": 11.79730059364396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.624268456353103,
            "upper_bound": 38.21700695397588
          },
          "point_estimate": 22.66132792681074,
          "standard_error": 9.607381442641964
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15231.958933279888,
            "upper_bound": 15252.205960148947
          },
          "point_estimate": 15239.983389948991,
          "standard_error": 5.1466039544826065
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.167818405379643,
            "upper_bound": 29.27797500384157
          },
          "point_estimate": 24.466998078283726,
          "standard_error": 4.061019214823376
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26953.88264764023,
            "upper_bound": 27015.877611811215
          },
          "point_estimate": 26983.52122436373,
          "standard_error": 15.944628343050836
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26942.763763281444,
            "upper_bound": 27028.270570793185
          },
          "point_estimate": 26961.36100815419,
          "standard_error": 26.01832298238637
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.952046935804405,
            "upper_bound": 87.50522470666144
          },
          "point_estimate": 45.476528644075835,
          "standard_error": 22.6209274112355
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26943.929417502484,
            "upper_bound": 26999.96610143316
          },
          "point_estimate": 26967.948845224455,
          "standard_error": 15.197587276984708
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.77613224435264,
            "upper_bound": 65.77781317194967
          },
          "point_estimate": 53.10413963259242,
          "standard_error": 9.582276530497843
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16353.833640489049,
            "upper_bound": 16381.298260093869
          },
          "point_estimate": 16367.135698730588,
          "standard_error": 7.069569697801759
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16348.360457920791,
            "upper_bound": 16394.595184518454
          },
          "point_estimate": 16361.559763476347,
          "standard_error": 11.177652499257654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8865772214779946,
            "upper_bound": 39.4462670809672
          },
          "point_estimate": 24.48478122297338,
          "standard_error": 9.4365998857566
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16354.317053927614,
            "upper_bound": 16380.868720156474
          },
          "point_estimate": 16365.669869194711,
          "standard_error": 6.760889106832731
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.4226412672917,
            "upper_bound": 28.071126980515565
          },
          "point_estimate": 23.556114627364103,
          "standard_error": 3.74374312155674
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16500.583804078957,
            "upper_bound": 16525.722813043136
          },
          "point_estimate": 16512.357636229564,
          "standard_error": 6.464682322798084
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16495.0732053612,
            "upper_bound": 16528.985152852598
          },
          "point_estimate": 16505.326272149025,
          "standard_error": 8.235894417874484
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.8125081899242383,
            "upper_bound": 35.115573850299604
          },
          "point_estimate": 15.388088608754837,
          "standard_error": 8.294073467875805
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16497.65218199779,
            "upper_bound": 16518.765621419065
          },
          "point_estimate": 16505.30727709365,
          "standard_error": 5.437574069332343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.761723498044198,
            "upper_bound": 26.477975584221547
          },
          "point_estimate": 21.501244568143555,
          "standard_error": 4.328154819852353
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19109.42357533326,
            "upper_bound": 19141.691439513303
          },
          "point_estimate": 19126.0098374975,
          "standard_error": 8.25455932376604
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19102.203939075633,
            "upper_bound": 19145.88112745098
          },
          "point_estimate": 19133.133293942577,
          "standard_error": 12.514151114421
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.595819938157725,
            "upper_bound": 45.21628182592886
          },
          "point_estimate": 22.2182534395791,
          "standard_error": 11.435901732762725
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19097.29354339627,
            "upper_bound": 19144.015692334626
          },
          "point_estimate": 19117.01911628288,
          "standard_error": 11.977408460181374
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.220228864627856,
            "upper_bound": 33.8789930472285
          },
          "point_estimate": 27.56469648595648,
          "standard_error": 4.617962337408015
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37540.62224977817,
            "upper_bound": 37608.72419156069
          },
          "point_estimate": 37574.56244035296,
          "standard_error": 17.39074554416329
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37520.36438923395,
            "upper_bound": 37615.866114561766
          },
          "point_estimate": 37585.561723602485,
          "standard_error": 26.591855089783778
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.360792442508869,
            "upper_bound": 103.12321100564004
          },
          "point_estimate": 66.00343454248856,
          "standard_error": 23.768448864688892
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37519.746401144555,
            "upper_bound": 37598.52723939235
          },
          "point_estimate": 37559.27449382915,
          "standard_error": 21.090743411238165
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.31210208130268,
            "upper_bound": 72.56593148210486
          },
          "point_estimate": 58.10969757249241,
          "standard_error": 9.715579139883918
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15773.116848433738,
            "upper_bound": 15808.878969461171
          },
          "point_estimate": 15790.456519198002,
          "standard_error": 9.145910295652154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15762.687092568449,
            "upper_bound": 15814.68915688831
          },
          "point_estimate": 15791.964411608478,
          "standard_error": 13.734084681133211
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.755309366532513,
            "upper_bound": 52.96848157302071
          },
          "point_estimate": 39.84111443102316,
          "standard_error": 13.100243529579116
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15767.738159529494,
            "upper_bound": 15807.024063872575
          },
          "point_estimate": 15789.23256856138,
          "standard_error": 9.876648923474772
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.661353789523904,
            "upper_bound": 37.42901547844369
          },
          "point_estimate": 30.42556918109532,
          "standard_error": 5.094063739957932
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20099.982302953165,
            "upper_bound": 20145.98607547356
          },
          "point_estimate": 20123.0930997106,
          "standard_error": 11.74859728026742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20093.586464088396,
            "upper_bound": 20152.377071823204
          },
          "point_estimate": 20122.10744014733,
          "standard_error": 13.454506344743812
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.771105910577957,
            "upper_bound": 66.7139271305135
          },
          "point_estimate": 35.29199348946319,
          "standard_error": 15.968021667278114
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20093.741062038207,
            "upper_bound": 20133.26851205853
          },
          "point_estimate": 20113.014759273876,
          "standard_error": 10.175873391114482
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.769867446075843,
            "upper_bound": 50.51601302565292
          },
          "point_estimate": 39.220312885020746,
          "standard_error": 7.6015203913892995
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16770.745816038638,
            "upper_bound": 16806.34470056721
          },
          "point_estimate": 16786.261300216916,
          "standard_error": 9.28136709581873
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16767.8147305398,
            "upper_bound": 16803.138504155126
          },
          "point_estimate": 16775.505701754388,
          "standard_error": 7.294780646168955
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6768595785148195,
            "upper_bound": 38.56248691925648
          },
          "point_estimate": 9.61354169220639,
          "standard_error": 7.99649499611136
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16765.5540263635,
            "upper_bound": 16777.134603315444
          },
          "point_estimate": 16769.83310429183,
          "standard_error": 2.977017490744223
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.63000796765089,
            "upper_bound": 41.88401328254831
          },
          "point_estimate": 30.884488069487087,
          "standard_error": 9.780235401072508
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19105.82688801203,
            "upper_bound": 19136.74055238596
          },
          "point_estimate": 19121.163631734355,
          "standard_error": 7.898925529510116
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19103.782673973423,
            "upper_bound": 19151.429322122964
          },
          "point_estimate": 19115.78868891225,
          "standard_error": 11.17774010422894
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.5559416341907455,
            "upper_bound": 52.78474015589539
          },
          "point_estimate": 22.28781766789816,
          "standard_error": 12.354457047981128
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19104.559937915343,
            "upper_bound": 19136.366178766453
          },
          "point_estimate": 19117.89517985955,
          "standard_error": 8.142211469546407
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.047698424766438,
            "upper_bound": 32.71942482722109
          },
          "point_estimate": 26.245059096212835,
          "standard_error": 4.520150198430865
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16539.936275618526,
            "upper_bound": 16590.535930066293
          },
          "point_estimate": 16564.590186749854,
          "standard_error": 12.952782230208603
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16519.584394904457,
            "upper_bound": 16596.189604185623
          },
          "point_estimate": 16566.472513973742,
          "standard_error": 17.988635240482765
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.665930950367936,
            "upper_bound": 77.67035414427953
          },
          "point_estimate": 46.831450810976165,
          "standard_error": 18.73223834177237
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16528.145643807682,
            "upper_bound": 16569.664761596363
          },
          "point_estimate": 16549.867933067842,
          "standard_error": 10.867368463772909
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.033668269075314,
            "upper_bound": 54.61445114724162
          },
          "point_estimate": 43.26256226632166,
          "standard_error": 7.857305935904295
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16703.7909419401,
            "upper_bound": 16733.81699146478
          },
          "point_estimate": 16717.82795786423,
          "standard_error": 7.735208317900442
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16699.64996933456,
            "upper_bound": 16734.947654093834
          },
          "point_estimate": 16713.27905550445,
          "standard_error": 6.844192594155018
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.44129848600912,
            "upper_bound": 45.22418663961011
          },
          "point_estimate": 12.834320946630443,
          "standard_error": 11.04587927762574
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16706.435844347336,
            "upper_bound": 16741.571751631927
          },
          "point_estimate": 16721.76012616638,
          "standard_error": 9.28521872447639
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.761927372200322,
            "upper_bound": 34.01106126778634
          },
          "point_estimate": 25.85959577581974,
          "standard_error": 6.070245912653106
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16361.620405484622,
            "upper_bound": 16385.447446559894
          },
          "point_estimate": 16373.41937824732,
          "standard_error": 6.133169854489066
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16358.414431508758,
            "upper_bound": 16391.99732665466
          },
          "point_estimate": 16368.860918505176,
          "standard_error": 9.606064052759558
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.02300554342128,
            "upper_bound": 34.95490704564424
          },
          "point_estimate": 24.668310277943014,
          "standard_error": 7.891397984394429
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16353.19431736168,
            "upper_bound": 16386.72894505682
          },
          "point_estimate": 16368.671570662566,
          "standard_error": 8.513597283034008
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.535317500195386,
            "upper_bound": 24.90319917688961
          },
          "point_estimate": 20.421276343880816,
          "standard_error": 3.1858478961719836
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19513.491259179653,
            "upper_bound": 19531.64084721476
          },
          "point_estimate": 19522.949267866738,
          "standard_error": 4.650208080575533
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19509.072004298763,
            "upper_bound": 19537.88044062332
          },
          "point_estimate": 19524.87660084184,
          "standard_error": 6.7505470035223105
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.123984459874863,
            "upper_bound": 24.86110100945684
          },
          "point_estimate": 19.59517238854735,
          "standard_error": 6.524541480218555
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19513.588593022403,
            "upper_bound": 19533.67903152336
          },
          "point_estimate": 19526.119826653732,
          "standard_error": 5.153074217966624
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.572700503228685,
            "upper_bound": 18.68833715212017
          },
          "point_estimate": 15.508129689701692,
          "standard_error": 2.620545977531748
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17020.450841147576,
            "upper_bound": 17035.53140081085
          },
          "point_estimate": 17028.15513115321,
          "standard_error": 3.8638340401897153
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17016.39812335431,
            "upper_bound": 17038.7633552015
          },
          "point_estimate": 17030.35538698844,
          "standard_error": 5.0748430241612565
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.97952107544934,
            "upper_bound": 23.1993117718218
          },
          "point_estimate": 13.138239607423404,
          "standard_error": 5.033953511683126
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17025.112234410048,
            "upper_bound": 17040.190175246877
          },
          "point_estimate": 17033.247308268114,
          "standard_error": 4.01167706580077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.13151542619942,
            "upper_bound": 15.999021641948096
          },
          "point_estimate": 12.881605810309717,
          "standard_error": 2.2517650533709315
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19607.07341963697,
            "upper_bound": 19643.133846700763
          },
          "point_estimate": 19624.161109098077,
          "standard_error": 9.28253410282916
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19601.191563230797,
            "upper_bound": 19642.09622534029
          },
          "point_estimate": 19621.34012412304,
          "standard_error": 11.70633809014604
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.580134197803151,
            "upper_bound": 50.75366567476488
          },
          "point_estimate": 25.75945444553787,
          "standard_error": 11.295729329008488
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19600.402148810324,
            "upper_bound": 19632.517383419003
          },
          "point_estimate": 19619.01193852019,
          "standard_error": 8.2554912583233
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.141846143562647,
            "upper_bound": 40.87585324818947
          },
          "point_estimate": 30.958598047460356,
          "standard_error": 6.9033084670794365
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71580.68559288539,
            "upper_bound": 71660.90948353097
          },
          "point_estimate": 71621.93666996047,
          "standard_error": 20.611712071728583
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71547.9117259552,
            "upper_bound": 71673.78458498024
          },
          "point_estimate": 71647.12859025033,
          "standard_error": 35.73726739492376
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.992783373869688,
            "upper_bound": 110.1572757122897
          },
          "point_estimate": 62.379563714679904,
          "standard_error": 28.670868518348527
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71588.36841043219,
            "upper_bound": 71680.13410688895
          },
          "point_estimate": 71643.91678045275,
          "standard_error": 23.348558863112345
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.93482085358076,
            "upper_bound": 80.80379232580253
          },
          "point_estimate": 68.4107017776926,
          "standard_error": 10.332830892391708
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1233133.392579365,
            "upper_bound": 1236736.4003019177
          },
          "point_estimate": 1234870.740708995,
          "standard_error": 922.5010651419888
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1232100.8925,
            "upper_bound": 1236719.9916666667
          },
          "point_estimate": 1235066.7648148148,
          "standard_error": 1596.751767695753
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 312.9157658336207,
            "upper_bound": 6137.729146033709
          },
          "point_estimate": 3424.6144367009633,
          "standard_error": 1470.7897602523728
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1232686.109122807,
            "upper_bound": 1235896.9240522876
          },
          "point_estimate": 1234575.401038961,
          "standard_error": 826.1118701939475
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1845.9723429673056,
            "upper_bound": 3951.070246387998
          },
          "point_estimate": 3088.5710686362872,
          "standard_error": 581.9773845088282
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202997.2227667598,
            "upper_bound": 203726.8193009444
          },
          "point_estimate": 203281.39103218945,
          "standard_error": 200.7774377791084
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202890.9972067039,
            "upper_bound": 203274.3894652833
          },
          "point_estimate": 203128.2560521415,
          "standard_error": 105.97903915278242
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.348664126260886,
            "upper_bound": 446.0904503484622
          },
          "point_estimate": 270.91221167076645,
          "standard_error": 108.1820172256585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203015.98300180497,
            "upper_bound": 203186.7618190507
          },
          "point_estimate": 203104.63919320903,
          "standard_error": 43.89485381221128
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.07479023240563,
            "upper_bound": 1023.266839828284
          },
          "point_estimate": 670.3713656479605,
          "standard_error": 303.927624448241
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/bstr_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6526.630585775611,
            "upper_bound": 6547.773817110911
          },
          "point_estimate": 6537.090242944138,
          "standard_error": 5.419543621863251
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6518.625285585932,
            "upper_bound": 6550.530338424678
          },
          "point_estimate": 6540.043890386343,
          "standard_error": 9.156471285420546
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9658881863372604,
            "upper_bound": 30.763261214849287
          },
          "point_estimate": 18.280115776091996,
          "standard_error": 7.541867988307509
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6519.056421372194,
            "upper_bound": 6538.32215419711
          },
          "point_estimate": 6527.196587671089,
          "standard_error": 4.9642450720543625
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.185664786858004,
            "upper_bound": 22.39165269609444
          },
          "point_estimate": 18.11777255698697,
          "standard_error": 2.906015854959053
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/bstr_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6645.295951922589,
            "upper_bound": 6663.9597727947685
          },
          "point_estimate": 6654.067736279934,
          "standard_error": 4.792676894827843
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6640.888263698282,
            "upper_bound": 6667.105154010369
          },
          "point_estimate": 6652.563664880408,
          "standard_error": 6.0147049541877085
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.0944890209998395,
            "upper_bound": 27.68663890462299
          },
          "point_estimate": 14.691034296212328,
          "standard_error": 6.315328393543731
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6640.876683698438,
            "upper_bound": 6651.567472047902
          },
          "point_estimate": 6644.558374068749,
          "standard_error": 2.74719740671477
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.21523196210001,
            "upper_bound": 19.97054640382123
          },
          "point_estimate": 16.00321128036635,
          "standard_error": 3.2741488546255377
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/bstr_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15557.911051625606,
            "upper_bound": 15581.085777205775
          },
          "point_estimate": 15568.74108676704,
          "standard_error": 5.924942549602281
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15555.344266715696,
            "upper_bound": 15578.072694122691
          },
          "point_estimate": 15566.708607416942,
          "standard_error": 5.244059134012921
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.875234991261128,
            "upper_bound": 32.643478814141545
          },
          "point_estimate": 12.21618491825472,
          "standard_error": 7.4163593071794605
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15556.19937354184,
            "upper_bound": 15577.233056011286
          },
          "point_estimate": 15567.791828934684,
          "standard_error": 5.462493080674151
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.368393911354277,
            "upper_bound": 26.90775883042817
          },
          "point_estimate": 19.66367986077661,
          "standard_error": 4.963147434617195
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/bstr_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.4820370614578,
            "upper_bound": 35.73473340061962
          },
          "point_estimate": 35.5772840324657,
          "standard_error": 0.07109796364651029
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.462070772922615,
            "upper_bound": 35.552201419745046
          },
          "point_estimate": 35.52966211642746,
          "standard_error": 0.03250314944942547
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010021461639728498,
            "upper_bound": 0.12102565653813548
          },
          "point_estimate": 0.06912014577104268,
          "standard_error": 0.033870768378742123
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.456807009301485,
            "upper_bound": 35.536047721849336
          },
          "point_estimate": 35.496004784393506,
          "standard_error": 0.02063661583002764
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03609721388947926,
            "upper_bound": 0.36507685696396225
          },
          "point_estimate": 0.23757062360631773,
          "standard_error": 0.11486262642630007
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834838000071898,
            "upper_bound": 8.839751735050594
          },
          "point_estimate": 8.836866224730148,
          "standard_error": 0.0013110384353600343
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.83454194234863,
            "upper_bound": 8.837970676894782
          },
          "point_estimate": 8.835218762046757,
          "standard_error": 0.0009593645673557744
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00019347128269612995,
            "upper_bound": 0.004125335816961822
          },
          "point_estimate": 0.0018445605399849971,
          "standard_error": 0.0010739738037552895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834649499333523,
            "upper_bound": 8.83773311991296
          },
          "point_estimate": 8.836005699072253,
          "standard_error": 0.0008848293091960268
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009975668815834162,
            "upper_bound": 0.0064866987582005145
          },
          "point_estimate": 0.004360189481664541,
          "standard_error": 0.0017168696985690663
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.836972251604713,
            "upper_bound": 8.842411806732915
          },
          "point_estimate": 8.839357234060722,
          "standard_error": 0.0014134087631915062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.83647474080934,
            "upper_bound": 8.841405879405256
          },
          "point_estimate": 8.83815273910203,
          "standard_error": 0.001117248200342268
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00021082869457930475,
            "upper_bound": 0.00566541568035543
          },
          "point_estimate": 0.0025817672167459116,
          "standard_error": 0.0015293446708999528
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.837090082302892,
            "upper_bound": 8.838911691480888
          },
          "point_estimate": 8.837957314924925,
          "standard_error": 0.0004610502800944155
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015791271829913131,
            "upper_bound": 0.006783224807547764
          },
          "point_estimate": 0.004720034675503766,
          "standard_error": 0.0015627894797152486
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.838354047074038,
            "upper_bound": 8.844066048953277
          },
          "point_estimate": 8.8408757211284,
          "standard_error": 0.0014763384285732923
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.836959510914124,
            "upper_bound": 8.842476787411837
          },
          "point_estimate": 8.840514467122055,
          "standard_error": 0.0016042906194419812
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006941991916766952,
            "upper_bound": 0.007105245556115886
          },
          "point_estimate": 0.0038238669300814185,
          "standard_error": 0.0015286536304491508
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.837925109715139,
            "upper_bound": 8.840798706676644
          },
          "point_estimate": 8.839053575142579,
          "standard_error": 0.0007293910979024261
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002021899481791792,
            "upper_bound": 0.00698217789939283
          },
          "point_estimate": 0.004888909118627885,
          "standard_error": 0.0015092711618651348
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.838062039475119,
            "upper_bound": 8.843000869417494
          },
          "point_estimate": 8.840116211644332,
          "standard_error": 0.0013006185762839718
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.837524997938878,
            "upper_bound": 8.840972681045425
          },
          "point_estimate": 8.838970056950187,
          "standard_error": 0.0008639588664559003
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000504826067642803,
            "upper_bound": 0.0042618691755397925
          },
          "point_estimate": 0.001915760991651815,
          "standard_error": 0.0010048203739556048
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.838453560330697,
            "upper_bound": 8.84011307535394
          },
          "point_estimate": 8.839322729432629,
          "standard_error": 0.00041832584445211434
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011645886414922437,
            "upper_bound": 0.006400237768781088
          },
          "point_estimate": 0.004326087119544884,
          "standard_error": 0.0016434050319815764
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.579706786174924,
            "upper_bound": 9.584737150968598
          },
          "point_estimate": 9.581900693298596,
          "standard_error": 0.0013026440661701372
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.57910665849852,
            "upper_bound": 9.58321150951548
          },
          "point_estimate": 9.581208762846297,
          "standard_error": 0.0010065553279633398
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007408283729965739,
            "upper_bound": 0.0053067290838632345
          },
          "point_estimate": 0.0020658666030737437,
          "standard_error": 0.001181978245988384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.57875384767692,
            "upper_bound": 9.581544066443607
          },
          "point_estimate": 9.580145342019138,
          "standard_error": 0.0007128993932282384
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014366132964924866,
            "upper_bound": 0.006240624675653068
          },
          "point_estimate": 0.004330123192523893,
          "standard_error": 0.001444529628155383
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.315368753411486,
            "upper_bound": 10.322597525726923
          },
          "point_estimate": 10.318706469377394,
          "standard_error": 0.0018568570338412917
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.314187432695192,
            "upper_bound": 10.323972364227089
          },
          "point_estimate": 10.317646710745333,
          "standard_error": 0.0020304453700765294
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000740868909060906,
            "upper_bound": 0.00891698544342383
          },
          "point_estimate": 0.004283467536673921,
          "standard_error": 0.002139032365793993
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.314987285654656,
            "upper_bound": 10.321242837577094
          },
          "point_estimate": 10.317618202187992,
          "standard_error": 0.0015858235262119049
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00222814978764277,
            "upper_bound": 0.007783399786933793
          },
          "point_estimate": 0.006174722955272736,
          "standard_error": 0.0014338783939615433
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.8037334957672755,
            "upper_bound": 7.812200680465144
          },
          "point_estimate": 7.807317732717736,
          "standard_error": 0.002217090269863965
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.802920968857125,
            "upper_bound": 7.809258954120312
          },
          "point_estimate": 7.804528173616442,
          "standard_error": 0.0015555032773578003
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00016653013224277852,
            "upper_bound": 0.007811537179046544
          },
          "point_estimate": 0.0025596642278477182,
          "standard_error": 0.00220941088397988
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.8032433389486515,
            "upper_bound": 7.805595338820397
          },
          "point_estimate": 7.804334474635049,
          "standard_error": 0.000593757497970182
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015750695541327433,
            "upper_bound": 0.01068739271718328
          },
          "point_estimate": 0.007385970165761218,
          "standard_error": 0.002625031952805044
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.753538383997356,
            "upper_bound": 9.758489993511096
          },
          "point_estimate": 9.755747168873745,
          "standard_error": 0.0012788411336793265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.75335236094217,
            "upper_bound": 9.757152923223629
          },
          "point_estimate": 9.754929967472066,
          "standard_error": 0.000947236625500157
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004420401433241631,
            "upper_bound": 0.0059194767768480155
          },
          "point_estimate": 0.0024397724617245924,
          "standard_error": 0.0013819625884389367
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.753740509427107,
            "upper_bound": 9.755995141928665
          },
          "point_estimate": 9.754989585253888,
          "standard_error": 0.0005885453511446633
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014635780227650742,
            "upper_bound": 0.006026818919860824
          },
          "point_estimate": 0.004263329605376399,
          "standard_error": 0.0013100104192157328
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.548127257981454,
            "upper_bound": 12.553962502269735
          },
          "point_estimate": 12.55080296422053,
          "standard_error": 0.0015006548351446714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.546471507422188,
            "upper_bound": 12.5544450143166
          },
          "point_estimate": 12.550330918931506,
          "standard_error": 0.001482336539091336
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00010174529110774769,
            "upper_bound": 0.007285568368749142
          },
          "point_estimate": 0.0038214853932816138,
          "standard_error": 0.0021477295631231585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.547999737524874,
            "upper_bound": 12.553134935183005
          },
          "point_estimate": 12.550293283298776,
          "standard_error": 0.0012880310338479744
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001896679961649423,
            "upper_bound": 0.006423785138213403
          },
          "point_estimate": 0.005008675925633367,
          "standard_error": 0.0011973655573243305
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.835453284738156,
            "upper_bound": 8.84008847066763
          },
          "point_estimate": 8.837587677283363,
          "standard_error": 0.0011847263934240098
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.83442344898843,
            "upper_bound": 8.83907987350563
          },
          "point_estimate": 8.837409005830786,
          "standard_error": 0.0012976464578942765
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00011377229353788784,
            "upper_bound": 0.006226769393947042
          },
          "point_estimate": 0.0025069735517139883,
          "standard_error": 0.0014717286563671111
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.836016106052593,
            "upper_bound": 8.838590113766639
          },
          "point_estimate": 8.837479462754086,
          "standard_error": 0.0006620997847377328
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016386045608765608,
            "upper_bound": 0.005496859830937225
          },
          "point_estimate": 0.003933371333347977,
          "standard_error": 0.0010802250051518034
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.825182011884062,
            "upper_bound": 9.833376505013018
          },
          "point_estimate": 9.828757817081245,
          "standard_error": 0.0021236544786389982
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.82423810563963,
            "upper_bound": 9.832264077659142
          },
          "point_estimate": 9.826408603958598,
          "standard_error": 0.001677038128558268
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00028789570864194686,
            "upper_bound": 0.008808255645458386
          },
          "point_estimate": 0.0034747413407856596,
          "standard_error": 0.0021280984095218463
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.8238168645777,
            "upper_bound": 9.827315316964285
          },
          "point_estimate": 9.825524167108147,
          "standard_error": 0.0008984506179656203
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018588196793669545,
            "upper_bound": 0.00980053145714162
          },
          "point_estimate": 0.007057095054194649,
          "standard_error": 0.0021997850293285744
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.188442637601565,
            "upper_bound": 19.22299719675587
          },
          "point_estimate": 19.2031007673844,
          "standard_error": 0.009045001240648031
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.18524543047281,
            "upper_bound": 19.21732982663023
          },
          "point_estimate": 19.191816334412437,
          "standard_error": 0.008148394496782927
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016134533694695406,
            "upper_bound": 0.03716395325845938
          },
          "point_estimate": 0.009990381031309102,
          "standard_error": 0.008735438189819686
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.184197948880968,
            "upper_bound": 19.19351535423799
          },
          "point_estimate": 19.187700669727725,
          "standard_error": 0.002370825077608322
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006904616321714011,
            "upper_bound": 0.04398042484847544
          },
          "point_estimate": 0.03020798556491882,
          "standard_error": 0.01069504557675442
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102525.01986780124,
            "upper_bound": 102730.7357048122
          },
          "point_estimate": 102623.5057425665,
          "standard_error": 52.654447230919054
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102484.45399061032,
            "upper_bound": 102753.7323943662
          },
          "point_estimate": 102590.36088028167,
          "standard_error": 71.73644507174585
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.002356661454773,
            "upper_bound": 298.42649301173475
          },
          "point_estimate": 157.885307858947,
          "standard_error": 66.24467093993862
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102461.12295060686,
            "upper_bound": 102630.31847826087
          },
          "point_estimate": 102541.55376623376,
          "standard_error": 44.00627216818844
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.19605946908304,
            "upper_bound": 222.50528579279327
          },
          "point_estimate": 176.05884840145933,
          "standard_error": 33.73030676169545
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57335.77627685144,
            "upper_bound": 57372.22662933754
          },
          "point_estimate": 57351.60073531621,
          "standard_error": 9.46655472238129
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57331.35055205047,
            "upper_bound": 57364.43756196485
          },
          "point_estimate": 57344.52214773922,
          "standard_error": 7.8306854438488305
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4676820756642077,
            "upper_bound": 41.75541716090255
          },
          "point_estimate": 15.281614712919325,
          "standard_error": 9.52491834478365
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57332.25245212115,
            "upper_bound": 57365.160317996975
          },
          "point_estimate": 57347.634815027246,
          "standard_error": 9.25027154954584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.429860319526997,
            "upper_bound": 44.780506618759574
          },
          "point_estimate": 31.518707316186845,
          "standard_error": 10.156948894207472
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147640.98378513358,
            "upper_bound": 147858.82987159636
          },
          "point_estimate": 147744.96424716094,
          "standard_error": 55.87515682531385
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147582.89083672088,
            "upper_bound": 147883.83333333334
          },
          "point_estimate": 147723.5886759582,
          "standard_error": 82.92608872390878
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.12193984880248,
            "upper_bound": 337.11994462468834
          },
          "point_estimate": 194.38753971967463,
          "standard_error": 71.26201020522136
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147615.720559636,
            "upper_bound": 147805.23717263268
          },
          "point_estimate": 147696.8350332594,
          "standard_error": 50.5934374539105
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.32937835910984,
            "upper_bound": 242.374501136544
          },
          "point_estimate": 186.4086145731713,
          "standard_error": 38.107002440394915
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 372361.9114536768,
            "upper_bound": 373473.3124471574
          },
          "point_estimate": 372855.5884604794,
          "standard_error": 289.20052867766964
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 372255.53401360544,
            "upper_bound": 373278.0471938775
          },
          "point_estimate": 372478.39994331065,
          "standard_error": 289.8879782767641
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.77294647471862,
            "upper_bound": 1295.2399740286876
          },
          "point_estimate": 525.1440967482653,
          "standard_error": 327.8670838730214
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 372307.25931831065,
            "upper_bound": 373044.6272028811
          },
          "point_estimate": 372710.3358865624,
          "standard_error": 191.12482826105395
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.6413677565266,
            "upper_bound": 1349.427936911791
          },
          "point_estimate": 964.9455946392242,
          "standard_error": 289.01636645531335
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483716.2506945358,
            "upper_bound": 485158.0785018275
          },
          "point_estimate": 484404.6291410819,
          "standard_error": 369.5673012695752
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483547.7799707602,
            "upper_bound": 485443.3552631579
          },
          "point_estimate": 483750.8396381579,
          "standard_error": 632.0856595585279
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.86635721719403,
            "upper_bound": 1969.6217100321844
          },
          "point_estimate": 826.868358116216,
          "standard_error": 576.3104439748519
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483575.63113230455,
            "upper_bound": 485694.63733071
          },
          "point_estimate": 484563.167874231,
          "standard_error": 600.6233530337543
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 673.7188843997598,
            "upper_bound": 1511.4135648972788
          },
          "point_estimate": 1228.484811595171,
          "standard_error": 215.04980631966075
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43115.075862459,
            "upper_bound": 43235.91808556018
          },
          "point_estimate": 43176.010002686344,
          "standard_error": 30.77010031156876
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43096.892051238545,
            "upper_bound": 43264.79532858274
          },
          "point_estimate": 43168.612707838474,
          "standard_error": 34.75607502549925
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.307428629480054,
            "upper_bound": 177.3986082400997
          },
          "point_estimate": 96.69848206889772,
          "standard_error": 47.5976399724446
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43086.46597348362,
            "upper_bound": 43177.36721422019
          },
          "point_estimate": 43135.99734398618,
          "standard_error": 22.892778187890844
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.09334644386813,
            "upper_bound": 132.38414072228176
          },
          "point_estimate": 102.74990289055124,
          "standard_error": 19.993286032561368
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99111.32732515894,
            "upper_bound": 99271.57391218212
          },
          "point_estimate": 99189.92039282472,
          "standard_error": 41.00653561567635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99076.32116257946,
            "upper_bound": 99278.91371480472
          },
          "point_estimate": 99185.3654972752,
          "standard_error": 51.11567553320177
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.0283333670696,
            "upper_bound": 232.0764673716354
          },
          "point_estimate": 122.64374005696672,
          "standard_error": 52.19307667056523
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99079.26930696076,
            "upper_bound": 99223.3433196556
          },
          "point_estimate": 99159.03538695636,
          "standard_error": 38.04352620368475
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.02385074078785,
            "upper_bound": 176.61413765962743
          },
          "point_estimate": 136.71600372575475,
          "standard_error": 26.777364337865336
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69630.02753768701,
            "upper_bound": 69770.37990980776
          },
          "point_estimate": 69694.6554107371,
          "standard_error": 36.14055472200608
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69606.01954022989,
            "upper_bound": 69774.24931581828
          },
          "point_estimate": 69647.10280970627,
          "standard_error": 46.6495341139871
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.411399826715352,
            "upper_bound": 199.0324192634002
          },
          "point_estimate": 105.84021804434649,
          "standard_error": 45.64528466725596
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69640.58833647532,
            "upper_bound": 69757.81729215714
          },
          "point_estimate": 69699.71095188336,
          "standard_error": 29.21701302134801
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.565330922691786,
            "upper_bound": 162.00254966070727
          },
          "point_estimate": 120.75757161006638,
          "standard_error": 29.814197868707208
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263320.99161039933,
            "upper_bound": 263929.5717770876
          },
          "point_estimate": 263564.63092563837,
          "standard_error": 166.62773666591391
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263301.586352657,
            "upper_bound": 263641.99523809523
          },
          "point_estimate": 263361.1918780193,
          "standard_error": 80.85791883787599
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.097319988165204,
            "upper_bound": 416.3190542574223
          },
          "point_estimate": 104.93938409706738,
          "standard_error": 99.07703234387488
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263282.6880001475,
            "upper_bound": 263508.25126464583
          },
          "point_estimate": 263365.3749294184,
          "standard_error": 57.704052015374344
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.8115097944946,
            "upper_bound": 832.6288497153661
          },
          "point_estimate": 556.383115081079,
          "standard_error": 238.49151525853975
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35751.008961722975,
            "upper_bound": 35818.08396715928
          },
          "point_estimate": 35779.13841825006,
          "standard_error": 17.633304418792594
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35745.95467980296,
            "upper_bound": 35789.34330049261
          },
          "point_estimate": 35762.927298850576,
          "standard_error": 11.09151319989577
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.771318036499098,
            "upper_bound": 59.45729832373047
          },
          "point_estimate": 30.790615734142232,
          "standard_error": 14.841486242915934
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35744.2317868808,
            "upper_bound": 35787.353042654584
          },
          "point_estimate": 35767.341106775,
          "standard_error": 11.349139646556951
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.905648113261712,
            "upper_bound": 87.39220526269551
          },
          "point_estimate": 58.950872090512455,
          "standard_error": 22.335264128360052
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65666.00156494878,
            "upper_bound": 65857.72663426766
          },
          "point_estimate": 65748.12648181635,
          "standard_error": 49.9866756605239
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65649.17836045811,
            "upper_bound": 65783.4681283906
          },
          "point_estimate": 65720.28460920232,
          "standard_error": 39.322135263033616
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.247884050845409,
            "upper_bound": 183.2599020629541
          },
          "point_estimate": 92.84361565126808,
          "standard_error": 40.518138291809606
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65652.64697210451,
            "upper_bound": 65732.60949130486
          },
          "point_estimate": 65689.41169066016,
          "standard_error": 20.497145016477056
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.31537932312634,
            "upper_bound": 243.7822813605196
          },
          "point_estimate": 166.77928665668145,
          "standard_error": 59.68898889593272
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154218.8708656595,
            "upper_bound": 154436.4817633256
          },
          "point_estimate": 154326.62676990853,
          "standard_error": 55.67395971557519
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154184.58305084746,
            "upper_bound": 154483.72245762713
          },
          "point_estimate": 154321.0913707291,
          "standard_error": 79.53324573349438
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.75535848510203,
            "upper_bound": 309.8095071551752
          },
          "point_estimate": 216.7731628605559,
          "standard_error": 67.8244646920148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154204.7247905525,
            "upper_bound": 154385.1179378531
          },
          "point_estimate": 154282.03439357254,
          "standard_error": 46.45742512085436
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.3227459298764,
            "upper_bound": 234.43273446165065
          },
          "point_estimate": 185.65031204136776,
          "standard_error": 32.82520256817862
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35777.14814733656,
            "upper_bound": 35856.126720044944
          },
          "point_estimate": 35815.52680873562,
          "standard_error": 20.199962221613323
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35780.786381514255,
            "upper_bound": 35865.45120452311
          },
          "point_estimate": 35802.11887495903,
          "standard_error": 17.43483788964198
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.347686109582827,
            "upper_bound": 120.6617706011909
          },
          "point_estimate": 31.208820848755767,
          "standard_error": 31.919364931017732
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35766.94988378877,
            "upper_bound": 35812.72556053688
          },
          "point_estimate": 35792.71560612445,
          "standard_error": 11.448175354952856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.474542612626497,
            "upper_bound": 88.71302132652266
          },
          "point_estimate": 67.27357049992752,
          "standard_error": 15.290039682555438
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11580.33836694693,
            "upper_bound": 11604.144684387922
          },
          "point_estimate": 11591.515339265445,
          "standard_error": 6.121357695181851
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11577.438282247766,
            "upper_bound": 11603.891137186038
          },
          "point_estimate": 11588.915303054491,
          "standard_error": 6.111685714272808
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.369841529221229,
            "upper_bound": 32.72425116793874
          },
          "point_estimate": 12.523401741267572,
          "standard_error": 7.518271113277826
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11572.324942165138,
            "upper_bound": 11609.886759758936
          },
          "point_estimate": 11586.603854638335,
          "standard_error": 9.63648332157158
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.912764783176305,
            "upper_bound": 27.65824740010938
          },
          "point_estimate": 20.512999687841297,
          "standard_error": 4.946358670667739
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499195.42734398786,
            "upper_bound": 500406.3971587981
          },
          "point_estimate": 499781.4566280713,
          "standard_error": 310.3682994262349
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498935.5269406393,
            "upper_bound": 500672.294520548
          },
          "point_estimate": 499549.6517857143,
          "standard_error": 477.0420479042025
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.70468201155245,
            "upper_bound": 1737.602655909334
          },
          "point_estimate": 1229.123444206104,
          "standard_error": 378.1979034257254
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498898.7873390398,
            "upper_bound": 500172.4826621746
          },
          "point_estimate": 499476.6603807152,
          "standard_error": 336.7988522080614
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 596.2591291790956,
            "upper_bound": 1303.8212395691623
          },
          "point_estimate": 1035.9488321186452,
          "standard_error": 184.66975099732483
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 961.0441910433656,
            "upper_bound": 963.5394111964569
          },
          "point_estimate": 962.1241462825276,
          "standard_error": 0.6491998658626433
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 960.7589552156448,
            "upper_bound": 962.890001324328
          },
          "point_estimate": 961.2483644550392,
          "standard_error": 0.59117232097537
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.21920184652551977,
            "upper_bound": 2.7134484097572127
          },
          "point_estimate": 0.8283303097545923,
          "standard_error": 0.6811277452092833
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 960.8580931278454,
            "upper_bound": 965.404206172284
          },
          "point_estimate": 963.269599519802,
          "standard_error": 1.2513304570084551
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4381267497857735,
            "upper_bound": 3.0676611979980577
          },
          "point_estimate": 2.1661006439093566,
          "standard_error": 0.7069140029335315
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51370.38515613617,
            "upper_bound": 51430.884757632986
          },
          "point_estimate": 51399.966745188634,
          "standard_error": 15.513129978489692
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51371.74900653132,
            "upper_bound": 51445.99362606232
          },
          "point_estimate": 51385.21501416431,
          "standard_error": 18.44197718278589
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.23366613933305,
            "upper_bound": 84.93975682536086
          },
          "point_estimate": 39.125915972043146,
          "standard_error": 23.600893420608916
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51353.10788852945,
            "upper_bound": 51393.66726109198
          },
          "point_estimate": 51374.01149332254,
          "standard_error": 10.27873793824732
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.65410602903988,
            "upper_bound": 67.4181032076851
          },
          "point_estimate": 51.901071789567965,
          "standard_error": 10.619499336018968
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53193.029856840745,
            "upper_bound": 53229.103183841136
          },
          "point_estimate": 53210.35060284925,
          "standard_error": 9.251779910204446
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53185.1691068814,
            "upper_bound": 53246.47031072067
          },
          "point_estimate": 53205.15226939971,
          "standard_error": 14.43942768345284
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.54566309645315,
            "upper_bound": 50.24285778541041
          },
          "point_estimate": 31.54052359817919,
          "standard_error": 12.419149281596365
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53201.26237556996,
            "upper_bound": 53247.252235718726
          },
          "point_estimate": 53230.46419729612,
          "standard_error": 11.599411438365612
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.42840933666099,
            "upper_bound": 36.760823515684336
          },
          "point_estimate": 30.74768614348713,
          "standard_error": 4.987878367224344
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53182.14349422193,
            "upper_bound": 53311.56210192254
          },
          "point_estimate": 53236.47764914359,
          "standard_error": 34.02409459349763
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53164.72602489019,
            "upper_bound": 53269.8908282786
          },
          "point_estimate": 53187.98438262567,
          "standard_error": 32.45442287810853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.068978602477472,
            "upper_bound": 133.2902651299662
          },
          "point_estimate": 46.07603914050039,
          "standard_error": 32.90268415956588
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53164.72631786064,
            "upper_bound": 53259.10375753552
          },
          "point_estimate": 53210.541936833295,
          "standard_error": 25.56044228417623
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.70571894603311,
            "upper_bound": 166.4664566229428
          },
          "point_estimate": 113.3212857670731,
          "standard_error": 41.24866382385058
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15407.88997257987,
            "upper_bound": 15432.216571715026
          },
          "point_estimate": 15420.25834089913,
          "standard_error": 6.20882989526735
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15402.814525326054,
            "upper_bound": 15434.756121726825
          },
          "point_estimate": 15423.303231894315,
          "standard_error": 8.224532539471072
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.776357225889924,
            "upper_bound": 35.89241105775761
          },
          "point_estimate": 20.630943483692413,
          "standard_error": 8.047944344731242
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15401.91882266266,
            "upper_bound": 15436.795405926674
          },
          "point_estimate": 15418.194562549976,
          "standard_error": 9.099856653456111
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.590968505833336,
            "upper_bound": 26.06612108332011
          },
          "point_estimate": 20.774659619003177,
          "standard_error": 3.659380446186347
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26304.57495267405,
            "upper_bound": 26364.956877349057
          },
          "point_estimate": 26330.96098065584,
          "standard_error": 15.638534300285274
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26294.97475949105,
            "upper_bound": 26357.446536326333
          },
          "point_estimate": 26309.14361573739,
          "standard_error": 15.910614101340594
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.657879649384112,
            "upper_bound": 68.08153166604043
          },
          "point_estimate": 31.01461012280736,
          "standard_error": 16.860197384394333
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26299.542173815455,
            "upper_bound": 26329.90919760734
          },
          "point_estimate": 26310.42418161129,
          "standard_error": 7.787509317412171
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.062012508043072,
            "upper_bound": 73.82963691669394
          },
          "point_estimate": 52.33867813685458,
          "standard_error": 16.18676163401763
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16646.356213126866,
            "upper_bound": 16678.77441136051
          },
          "point_estimate": 16660.151740541995,
          "standard_error": 8.52201300887932
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16644.4268946913,
            "upper_bound": 16672.150996335316
          },
          "point_estimate": 16649.374843487552,
          "standard_error": 5.765414291336957
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.573403279440724,
            "upper_bound": 31.388074625983645
          },
          "point_estimate": 7.778739717607567,
          "standard_error": 7.174468009955836
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16646.759690197792,
            "upper_bound": 16652.603906469256
          },
          "point_estimate": 16649.01359025766,
          "standard_error": 1.4974745615921996
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.488577503370501,
            "upper_bound": 40.19557644906008
          },
          "point_estimate": 28.46575241174847,
          "standard_error": 10.012815260691296
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16491.715334071298,
            "upper_bound": 16521.511694813424
          },
          "point_estimate": 16506.660709329903,
          "standard_error": 7.652443445402649
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16478.075953678475,
            "upper_bound": 16532.02497729337
          },
          "point_estimate": 16508.91882379655,
          "standard_error": 12.683177858722765
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.172056749277009,
            "upper_bound": 43.0280962217924
          },
          "point_estimate": 35.014515437632454,
          "standard_error": 10.29720724266747
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16498.080426591692,
            "upper_bound": 16526.47300540179
          },
          "point_estimate": 16513.252300741948,
          "standard_error": 7.169769085824361
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.756339862663562,
            "upper_bound": 30.27949652669323
          },
          "point_estimate": 25.520866063504005,
          "standard_error": 3.672928976747453
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/krate_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19297.762858577426,
            "upper_bound": 19324.029164915653
          },
          "point_estimate": 19308.982592761364,
          "standard_error": 6.8701057306444335
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19293.141945773525,
            "upper_bound": 19313.72281081491
          },
          "point_estimate": 19306.32097288676,
          "standard_error": 5.786464514640903
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.2932872621055296,
            "upper_bound": 26.654612907968435
          },
          "point_estimate": 14.991132796054709,
          "standard_error": 5.717982906309873
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19297.295248140777,
            "upper_bound": 19312.51909364346
          },
          "point_estimate": 19304.888884746302,
          "standard_error": 3.881544286909942
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.689088133722011,
            "upper_bound": 33.66934344738944
          },
          "point_estimate": 22.943414702443395,
          "standard_error": 8.179845733083425
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39848.41304146411,
            "upper_bound": 39943.21953530918
          },
          "point_estimate": 39889.60309654488,
          "standard_error": 24.763447554794755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39844.66048298573,
            "upper_bound": 39936.40293241336
          },
          "point_estimate": 39858.41218441274,
          "standard_error": 18.794597520920323
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.508566750199438,
            "upper_bound": 106.24257473291532
          },
          "point_estimate": 14.886893204424116,
          "standard_error": 21.370645667035426
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39844.53698675819,
            "upper_bound": 39865.45456105521
          },
          "point_estimate": 39853.619251001466,
          "standard_error": 5.469891085075881
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.61546936434362,
            "upper_bound": 109.89845668255974
          },
          "point_estimate": 82.42029135723361,
          "standard_error": 26.45300725030299
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16435.59989242774,
            "upper_bound": 16455.577537851867
          },
          "point_estimate": 16445.655150420334,
          "standard_error": 5.122464008638932
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16430.645861601086,
            "upper_bound": 16460.503000150762
          },
          "point_estimate": 16446.68428959101,
          "standard_error": 9.881534236843754
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.56106404883376,
            "upper_bound": 25.66964172579016
          },
          "point_estimate": 21.847203139953432,
          "standard_error": 6.686987293412346
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16438.591492988144,
            "upper_bound": 16458.263813559795
          },
          "point_estimate": 16447.771150152425,
          "standard_error": 5.023680057980593
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.231320312547222,
            "upper_bound": 20.28299242321972
          },
          "point_estimate": 17.075270073213026,
          "standard_error": 2.3314381322252458
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20002.121206401043,
            "upper_bound": 20049.136926567655
          },
          "point_estimate": 20023.651352851557,
          "standard_error": 12.097930347448033
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19991.67216721672,
            "upper_bound": 20046.905500550056
          },
          "point_estimate": 20018.646786107183,
          "standard_error": 14.779308411053576
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.0811459341473535,
            "upper_bound": 61.43145884336882
          },
          "point_estimate": 35.12863192126359,
          "standard_error": 14.917258058019076
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19994.515085908228,
            "upper_bound": 20026.395830059195
          },
          "point_estimate": 20010.599124198136,
          "standard_error": 8.238718259523216
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.842814046599507,
            "upper_bound": 54.40864287986647
          },
          "point_estimate": 40.32501574753149,
          "standard_error": 10.23988619754477
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16633.456856992707,
            "upper_bound": 16658.25400227535
          },
          "point_estimate": 16645.122184463024,
          "standard_error": 6.346185852183239
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16631.670128682475,
            "upper_bound": 16662.94589239445
          },
          "point_estimate": 16639.257735101357,
          "standard_error": 7.522063199331765
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.714211216639365,
            "upper_bound": 35.71184345987469
          },
          "point_estimate": 12.432980981830823,
          "standard_error": 8.779998409387563
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16624.916155071147,
            "upper_bound": 16645.12963912361
          },
          "point_estimate": 16632.839962232556,
          "standard_error": 5.107554484440646
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.698134245179943,
            "upper_bound": 28.021768001440265
          },
          "point_estimate": 21.197633389005507,
          "standard_error": 4.816014017820909
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19148.84310942837,
            "upper_bound": 19191.40621674076
          },
          "point_estimate": 19168.440566083147,
          "standard_error": 10.921660810368044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19143.676371308014,
            "upper_bound": 19184.425764767933
          },
          "point_estimate": 19165.296773909984,
          "standard_error": 9.713264031037504
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.7722446480449205,
            "upper_bound": 54.68381984492051
          },
          "point_estimate": 20.391065893701136,
          "standard_error": 12.629620521691724
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19142.93650678177,
            "upper_bound": 19174.686195411927
          },
          "point_estimate": 19160.81483916927,
          "standard_error": 8.06001918374318
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.725797124088428,
            "upper_bound": 50.48892593208188
          },
          "point_estimate": 36.38103917887644,
          "standard_error": 9.971739141514863
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16648.50082478037,
            "upper_bound": 16667.063814821948
          },
          "point_estimate": 16656.995046347896,
          "standard_error": 4.803627984717432
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16642.718678292797,
            "upper_bound": 16668.448103105402
          },
          "point_estimate": 16653.113793279284,
          "standard_error": 5.284208859163501
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9012434901969728,
            "upper_bound": 24.90136921715744
          },
          "point_estimate": 12.263896541106076,
          "standard_error": 6.123758794177197
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16650.475806916278,
            "upper_bound": 16662.571672398946
          },
          "point_estimate": 16655.651117216883,
          "standard_error": 3.0423050132376996
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.110064041270285,
            "upper_bound": 20.915657332749593
          },
          "point_estimate": 15.986706651552502,
          "standard_error": 3.93559896060176
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16696.564935354025,
            "upper_bound": 16720.685998774505
          },
          "point_estimate": 16707.72241033132,
          "standard_error": 6.207011647092093
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16691.866061580884,
            "upper_bound": 16725.831313189337
          },
          "point_estimate": 16698.933193277313,
          "standard_error": 8.520350571417755
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.61739779068082,
            "upper_bound": 33.70910659363657
          },
          "point_estimate": 13.107765778322518,
          "standard_error": 8.898660511451899
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16693.077348505238,
            "upper_bound": 16710.466945765478
          },
          "point_estimate": 16701.609147011077,
          "standard_error": 4.5649519758389765
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.977224228764968,
            "upper_bound": 25.761416207049347
          },
          "point_estimate": 20.64385988722284,
          "standard_error": 4.56674899803343
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16684.16539957412,
            "upper_bound": 16728.468755371392
          },
          "point_estimate": 16703.138110548392,
          "standard_error": 11.631798362392258
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16681.0264732965,
            "upper_bound": 16720.13750767342
          },
          "point_estimate": 16689.6572053407,
          "standard_error": 8.261351564223721
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.281668829859827,
            "upper_bound": 42.10760641941058
          },
          "point_estimate": 12.04894997792378,
          "standard_error": 10.060627786384336
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16679.667427176257,
            "upper_bound": 16693.63997586071
          },
          "point_estimate": 16685.85544593528,
          "standard_error": 3.5901075294209
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.975586872383443,
            "upper_bound": 54.65844866614387
          },
          "point_estimate": 38.70745397489004,
          "standard_error": 13.333734045119382
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19519.44972963634,
            "upper_bound": 19568.08252346872
          },
          "point_estimate": 19541.507338373,
          "standard_error": 12.531850978239635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19511.763754696727,
            "upper_bound": 19556.216794894735
          },
          "point_estimate": 19539.284970477725,
          "standard_error": 13.127250658475331
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.406793565805933,
            "upper_bound": 58.718578868972706
          },
          "point_estimate": 27.095847506070555,
          "standard_error": 13.215402742859606
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19521.13335112917,
            "upper_bound": 19551.32259653649
          },
          "point_estimate": 19538.315270022515,
          "standard_error": 7.678781892670074
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.754679537829254,
            "upper_bound": 59.180302811741726
          },
          "point_estimate": 41.72960900170801,
          "standard_error": 12.514737821697638
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16867.947699091284,
            "upper_bound": 16911.484974216488
          },
          "point_estimate": 16885.660151759897,
          "standard_error": 11.667422995563024
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16860.78032063197,
            "upper_bound": 16889.042263011153
          },
          "point_estimate": 16876.051068773235,
          "standard_error": 7.22694900226784
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.717994696946564,
            "upper_bound": 32.97794398082182
          },
          "point_estimate": 20.878108353781847,
          "standard_error": 8.3091471939501
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16865.46969688588,
            "upper_bound": 16883.95840457569
          },
          "point_estimate": 16874.57307367354,
          "standard_error": 4.74267556258507
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.366679923089832,
            "upper_bound": 58.63559842979087
          },
          "point_estimate": 39.0152506839275,
          "standard_error": 16.01869967842139
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19666.045771122463,
            "upper_bound": 19707.342188924144
          },
          "point_estimate": 19685.90387016544,
          "standard_error": 10.607543387925745
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19650.60527266161,
            "upper_bound": 19709.718846734253
          },
          "point_estimate": 19687.463103406764,
          "standard_error": 16.005902084445356
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.708875895691369,
            "upper_bound": 65.11219187580699
          },
          "point_estimate": 45.65943100547409,
          "standard_error": 15.742301769193151
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19662.26519448516,
            "upper_bound": 19701.236661761617
          },
          "point_estimate": 19681.05477058153,
          "standard_error": 10.021315003301815
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.10624823160468,
            "upper_bound": 45.155994745401394
          },
          "point_estimate": 35.316554847886636,
          "standard_error": 6.7756521051735925
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71916.52400715227,
            "upper_bound": 72049.32159412057
          },
          "point_estimate": 71978.26041690193,
          "standard_error": 34.15553816592729
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71903.28491436101,
            "upper_bound": 72062.9824957651
          },
          "point_estimate": 71940.00441370223,
          "standard_error": 45.876356099347575
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.922981017311813,
            "upper_bound": 189.9513616079244
          },
          "point_estimate": 73.78645155563714,
          "standard_error": 48.22373999498579
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71893.50550120138,
            "upper_bound": 72017.02002313652
          },
          "point_estimate": 71948.07665417586,
          "standard_error": 31.142086993947768
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.90152977674335,
            "upper_bound": 152.92718661959654
          },
          "point_estimate": 114.12823823279837,
          "standard_error": 27.479553039161356
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1233493.102452381,
            "upper_bound": 1237305.5659284391
          },
          "point_estimate": 1235318.6648597885,
          "standard_error": 977.2969620522254
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1232664.0958333332,
            "upper_bound": 1237898.6222222222
          },
          "point_estimate": 1234579.6347619048,
          "standard_error": 1216.2659467956785
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 618.0772592491426,
            "upper_bound": 5531.782770791043
          },
          "point_estimate": 3225.2487279626475,
          "standard_error": 1337.1720558910613
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1233112.9120405577,
            "upper_bound": 1235086.9554979843
          },
          "point_estimate": 1234163.122857143,
          "standard_error": 508.9146850379194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1610.9230850150966,
            "upper_bound": 4136.378640957698
          },
          "point_estimate": 3270.769601982067,
          "standard_error": 637.005375726311
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228143.8012619047,
            "upper_bound": 229020.95461595984
          },
          "point_estimate": 228546.6202948909,
          "standard_error": 225.06744508902065
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227978.5015625,
            "upper_bound": 228974.78671875
          },
          "point_estimate": 228338.89186507935,
          "standard_error": 293.0710335694438
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175.6394490692851,
            "upper_bound": 1249.005266935112
          },
          "point_estimate": 757.1136143710565,
          "standard_error": 266.6747762032798
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228078.25717123077,
            "upper_bound": 228664.88789287757
          },
          "point_estimate": 228336.8475974026,
          "standard_error": 150.83356001890175
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 361.829577824989,
            "upper_bound": 1013.6947365409852
          },
          "point_estimate": 751.3315701418064,
          "standard_error": 184.89417074441843
        }
      }
    },
    "memmem/krate/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6030.0607471789435,
            "upper_bound": 6046.372853488824
          },
          "point_estimate": 6038.425753138241,
          "standard_error": 4.169884578745526
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6031.028917662683,
            "upper_bound": 6048.505478087649
          },
          "point_estimate": 6036.890991589199,
          "standard_error": 5.817837268605361
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2673104107686617,
            "upper_bound": 25.05853607404526
          },
          "point_estimate": 11.619766541715675,
          "standard_error": 6.155452743969948
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6027.4374164564015,
            "upper_bound": 6052.151415023003
          },
          "point_estimate": 6042.463231489626,
          "standard_error": 6.270518637129177
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.187872100951532,
            "upper_bound": 18.6035696566018
          },
          "point_estimate": 13.89026599306784,
          "standard_error": 3.0193977316421683
        }
      }
    },
    "memmem/krate/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6013.8966954404605,
            "upper_bound": 6047.515064676809
          },
          "point_estimate": 6030.700969672895,
          "standard_error": 8.43638875681251
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6006.803992363878,
            "upper_bound": 6057.901524852969
          },
          "point_estimate": 6026.940405046481,
          "standard_error": 11.522266706206748
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8358865162113311,
            "upper_bound": 47.71895520527659
          },
          "point_estimate": 30.264837406748747,
          "standard_error": 11.511974294038415
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6026.0787034684745,
            "upper_bound": 6057.243308097027
          },
          "point_estimate": 6042.634065900898,
          "standard_error": 7.983803337287792
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.27574159782921,
            "upper_bound": 35.39800725903338
          },
          "point_estimate": 28.06668356337907,
          "standard_error": 4.952685832819667
        }
      }
    },
    "memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15601.012514778678,
            "upper_bound": 15629.706031801432
          },
          "point_estimate": 15613.905175929336,
          "standard_error": 7.377727962388362
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15599.010166093927,
            "upper_bound": 15623.73787705231
          },
          "point_estimate": 15609.40007900712,
          "standard_error": 6.363978430823694
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.721526695099764,
            "upper_bound": 33.77825362604684
          },
          "point_estimate": 17.559496053162725,
          "standard_error": 7.666555258469447
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15591.07710528755,
            "upper_bound": 15617.300233055472
          },
          "point_estimate": 15604.19234614183,
          "standard_error": 7.011342655904533
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.224335530528103,
            "upper_bound": 34.509042685101356
          },
          "point_estimate": 24.53018596970858,
          "standard_error": 7.26975993412892
        }
      }
    },
    "memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.031129959183,
            "upper_bound": 58.15833323387478
          },
          "point_estimate": 58.08621100338576,
          "standard_error": 0.03277214116940453
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.028235774458295,
            "upper_bound": 58.11719186100292
          },
          "point_estimate": 58.06912988738013,
          "standard_error": 0.021530929157014017
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0133448484567154,
            "upper_bound": 0.12715150160912972
          },
          "point_estimate": 0.06594314578480717,
          "standard_error": 0.0289362032848145
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.030915292189114,
            "upper_bound": 58.09173056640042
          },
          "point_estimate": 58.05402401957172,
          "standard_error": 0.01543369983118808
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03074725963466315,
            "upper_bound": 0.1591709976131304
          },
          "point_estimate": 0.1089741645534035,
          "standard_error": 0.03801551129794275
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.3350170584782,
            "upper_bound": 26.958887597052552
          },
          "point_estimate": 26.64270667768618,
          "standard_error": 0.15939808579179035
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.2771920206518,
            "upper_bound": 27.092123474103616
          },
          "point_estimate": 26.54109279464768,
          "standard_error": 0.19385059182605605
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10135661746574907,
            "upper_bound": 0.956028688928762
          },
          "point_estimate": 0.39521873181233114,
          "standard_error": 0.219657201213066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.41895850054997,
            "upper_bound": 26.86374754121069
          },
          "point_estimate": 26.59761475444115,
          "standard_error": 0.1132729024718764
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2712157331070211,
            "upper_bound": 0.687904738466481
          },
          "point_estimate": 0.5300072134250023,
          "standard_error": 0.10589129443033334
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.84059488505512,
            "upper_bound": 24.85956336049154
          },
          "point_estimate": 24.849242742131715,
          "standard_error": 0.004893875281421475
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.834815190131927,
            "upper_bound": 24.855797828828504
          },
          "point_estimate": 24.84768159725997,
          "standard_error": 0.004897590807060812
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002515492475856121,
            "upper_bound": 0.024106719821088265
          },
          "point_estimate": 0.014148111854944544,
          "standard_error": 0.005637395356109982
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.84157634582171,
            "upper_bound": 24.850873792594204
          },
          "point_estimate": 24.847011895280065,
          "standard_error": 0.0023650191270921683
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006981368232055888,
            "upper_bound": 0.02283544083363843
          },
          "point_estimate": 0.01634108790437732,
          "standard_error": 0.004641370883485312
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.196269144049523,
            "upper_bound": 28.96241914435474
          },
          "point_estimate": 28.58257413762525,
          "standard_error": 0.1958477606561369
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.066122868473627,
            "upper_bound": 28.98928946198124
          },
          "point_estimate": 28.718287602473623,
          "standard_error": 0.2464204250937293
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.126390300722955,
            "upper_bound": 1.14523999789356
          },
          "point_estimate": 0.5887777371538623,
          "standard_error": 0.2635482357965066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.986154278799575,
            "upper_bound": 28.792092118717115
          },
          "point_estimate": 28.340436998550153,
          "standard_error": 0.20531782756668376
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3469441451888812,
            "upper_bound": 0.8484766577313292
          },
          "point_estimate": 0.652714004130495,
          "standard_error": 0.12806574321900954
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.00417392274678,
            "upper_bound": 29.492918937075707
          },
          "point_estimate": 29.25617290113476,
          "standard_error": 0.12471590511933836
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.88135891613367,
            "upper_bound": 29.579349386064436
          },
          "point_estimate": 29.414555290334533,
          "standard_error": 0.16843431934953718
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002916157664897074,
            "upper_bound": 0.676819055966853
          },
          "point_estimate": 0.4431143495646079,
          "standard_error": 0.18987987151691485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.15935238997497,
            "upper_bound": 29.544130744139277
          },
          "point_estimate": 29.384902265448385,
          "standard_error": 0.09639087838364856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2197338131576896,
            "upper_bound": 0.5236579936786238
          },
          "point_estimate": 0.4155097678152662,
          "standard_error": 0.07852953801914903
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.487659226762528,
            "upper_bound": 20.502564494635248
          },
          "point_estimate": 20.493570091127623,
          "standard_error": 0.0040439454129925995
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.486314934290206,
            "upper_bound": 20.494669592822103
          },
          "point_estimate": 20.490670630478444,
          "standard_error": 0.00237794051731898
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005391360324203434,
            "upper_bound": 0.00983659653692148
          },
          "point_estimate": 0.006112803964380591,
          "standard_error": 0.002492633948018378
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.48705107880453,
            "upper_bound": 20.492344518410135
          },
          "point_estimate": 20.48963686999484,
          "standard_error": 0.0013748603933340649
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002903123751287516,
            "upper_bound": 0.020422198166932456
          },
          "point_estimate": 0.013471245510428505,
          "standard_error": 0.005800520300004264
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.012774268970553,
            "upper_bound": 27.044946131747658
          },
          "point_estimate": 27.027200497280617,
          "standard_error": 0.008239495071713095
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.01019801118426,
            "upper_bound": 27.049599278612316
          },
          "point_estimate": 27.01668496219255,
          "standard_error": 0.008274498762321924
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002504402022573272,
            "upper_bound": 0.03965293801331725
          },
          "point_estimate": 0.014765955983178257,
          "standard_error": 0.008284087761577375
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.013056272631452,
            "upper_bound": 27.024835466927605
          },
          "point_estimate": 27.01740625287212,
          "standard_error": 0.003019230131647038
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00759593681279032,
            "upper_bound": 0.034572293613608716
          },
          "point_estimate": 0.02737349425439658,
          "standard_error": 0.00708512814859532
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.56393978270708,
            "upper_bound": 41.007068022809406
          },
          "point_estimate": 40.81677312360458,
          "standard_error": 0.11546185885663136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.70759107809181,
            "upper_bound": 41.09790525132631
          },
          "point_estimate": 40.900060187298706,
          "standard_error": 0.10444219436304962
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06125954472745094,
            "upper_bound": 0.4573886955956285
          },
          "point_estimate": 0.28933989148192535,
          "standard_error": 0.10017443019215504
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.75401834359448,
            "upper_bound": 41.03562382295589
          },
          "point_estimate": 40.88763952902753,
          "standard_error": 0.07141026062734608
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13179986184046538,
            "upper_bound": 0.5614460743984128
          },
          "point_estimate": 0.38566504328920337,
          "standard_error": 0.13295754688269265
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.098561684217376,
            "upper_bound": 28.12660895869818
          },
          "point_estimate": 28.112541287350417,
          "standard_error": 0.007207388714345142
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.092983620336028,
            "upper_bound": 28.13773966353823
          },
          "point_estimate": 28.10820891313904,
          "standard_error": 0.01405403041607984
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002182978686401979,
            "upper_bound": 0.03838853627294041
          },
          "point_estimate": 0.028261084528366977,
          "standard_error": 0.010020060552787992
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.09997404435802,
            "upper_bound": 28.13511844945638
          },
          "point_estimate": 28.115113450495706,
          "standard_error": 0.008944540759195008
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015722146924533602,
            "upper_bound": 0.02825875923654352
          },
          "point_estimate": 0.024015193584946953,
          "standard_error": 0.0031961070085240787
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.369971067312704,
            "upper_bound": 37.39403830799148
          },
          "point_estimate": 37.38167353885655,
          "standard_error": 0.006126160743564776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.37003922633689,
            "upper_bound": 37.390164272685865
          },
          "point_estimate": 37.38172072933073,
          "standard_error": 0.005659899135428875
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003861929624427543,
            "upper_bound": 0.03312267746074248
          },
          "point_estimate": 0.014060599358830106,
          "standard_error": 0.006915018749324803
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.36904376426157,
            "upper_bound": 37.38607339089974
          },
          "point_estimate": 37.378697911032326,
          "standard_error": 0.004308495904568472
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007486223503269388,
            "upper_bound": 0.02834357258077306
          },
          "point_estimate": 0.020356246148774137,
          "standard_error": 0.005487054287456774
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.157675828917142,
            "upper_bound": 29.718467568338717
          },
          "point_estimate": 29.454242251720817,
          "standard_error": 0.1438765594475082
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.160943785000065,
            "upper_bound": 29.76409641371813
          },
          "point_estimate": 29.54168272388152,
          "standard_error": 0.1515661007570761
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07556911859860238,
            "upper_bound": 0.7804188315061408
          },
          "point_estimate": 0.3401991600517552,
          "standard_error": 0.17319131342554733
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.37507999227477,
            "upper_bound": 29.785412566322297
          },
          "point_estimate": 29.569247486978156,
          "standard_error": 0.1052667631073178
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2008268865074436,
            "upper_bound": 0.6459255646639961
          },
          "point_estimate": 0.4760786520484494,
          "standard_error": 0.1179634765075361
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.461366810527277,
            "upper_bound": 19.48382431564563
          },
          "point_estimate": 19.470142454464,
          "standard_error": 0.006145392514810506
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.46083172514581,
            "upper_bound": 19.468407689781102
          },
          "point_estimate": 19.466247385487087,
          "standard_error": 0.002611100605246165
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00010044258213277914,
            "upper_bound": 0.011849912987644996
          },
          "point_estimate": 0.004089943081862559,
          "standard_error": 0.003554187291659668
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.461174548935332,
            "upper_bound": 19.467987079510216
          },
          "point_estimate": 19.465003432820055,
          "standard_error": 0.0017261630524153474
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0026565464053147263,
            "upper_bound": 0.03129236781110464
          },
          "point_estimate": 0.020457138600521996,
          "standard_error": 0.009407190972962304
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.64567157993888,
            "upper_bound": 31.6788275063257
          },
          "point_estimate": 31.660600420261595,
          "standard_error": 0.008558740567280091
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.63916662518666,
            "upper_bound": 31.676049847560975
          },
          "point_estimate": 31.648037166085945,
          "standard_error": 0.009851275902525934
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002142107279041818,
            "upper_bound": 0.041985679346068554
          },
          "point_estimate": 0.013586212817740431,
          "standard_error": 0.010747368796423486
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.640447864363217,
            "upper_bound": 31.66519668692588
          },
          "point_estimate": 31.649422120910447,
          "standard_error": 0.006311915979212363
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008056989090579036,
            "upper_bound": 0.03779828949545166
          },
          "point_estimate": 0.02850763289730833,
          "standard_error": 0.007476998020386917
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103531.7776702873,
            "upper_bound": 103728.4480352564
          },
          "point_estimate": 103628.94881393298,
          "standard_error": 50.37326213660522
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103494.43945868946,
            "upper_bound": 103792.83190883193
          },
          "point_estimate": 103574.13498846831,
          "standard_error": 88.70557148916741
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.98957644722284,
            "upper_bound": 269.82794043750147
          },
          "point_estimate": 228.83037282489812,
          "standard_error": 69.45854078374586
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103485.73854920008,
            "upper_bound": 103707.032513659
          },
          "point_estimate": 103600.71455951456,
          "standard_error": 57.70625840494004
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.53159615139325,
            "upper_bound": 200.4928006778939
          },
          "point_estimate": 167.86913069638462,
          "standard_error": 24.10948907794119
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56586.00501707955,
            "upper_bound": 56695.97946036684
          },
          "point_estimate": 56640.61288293959,
          "standard_error": 28.21319106249917
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56557.11482633489,
            "upper_bound": 56695.23950233281
          },
          "point_estimate": 56654.47600533215,
          "standard_error": 34.9436837766306
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.43057972164833,
            "upper_bound": 168.3662063291927
          },
          "point_estimate": 77.72354228005987,
          "standard_error": 37.935721475893125
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56577.47774167665,
            "upper_bound": 56668.03306888892
          },
          "point_estimate": 56624.98336935227,
          "standard_error": 23.259259773697504
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.380806954466614,
            "upper_bound": 123.73424113937332
          },
          "point_estimate": 94.08222063715776,
          "standard_error": 19.149227585911436
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147200.4274831309,
            "upper_bound": 147606.47886832463
          },
          "point_estimate": 147404.1562830152,
          "standard_error": 103.68950647386428
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147194.5516194332,
            "upper_bound": 147592.35806824756
          },
          "point_estimate": 147442.1176788124,
          "standard_error": 104.31507587063076
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.66283191748586,
            "upper_bound": 592.8399585235754
          },
          "point_estimate": 240.5778133807277,
          "standard_error": 133.75247444242575
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147100.47329668872,
            "upper_bound": 147519.06036222106
          },
          "point_estimate": 147312.66105473475,
          "standard_error": 106.65953267237448
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.74011946486507,
            "upper_bound": 466.34149351981495
          },
          "point_estimate": 345.4431423265073,
          "standard_error": 79.20771898662957
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390421.11540477246,
            "upper_bound": 391096.04235237255
          },
          "point_estimate": 390752.4848509794,
          "standard_error": 173.6451146231455
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390168.05319148937,
            "upper_bound": 391256.4072695036
          },
          "point_estimate": 390729.69072948326,
          "standard_error": 325.8797215399704
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.25663432873853,
            "upper_bound": 943.9449282415832
          },
          "point_estimate": 769.6349959106926,
          "standard_error": 236.9417476577449
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390360.5001340688,
            "upper_bound": 391076.6193341975
          },
          "point_estimate": 390707.9832550428,
          "standard_error": 182.89837952356484
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 374.8520800059803,
            "upper_bound": 680.5782938153375
          },
          "point_estimate": 578.9046436612435,
          "standard_error": 78.94415479463268
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579804.3132003811,
            "upper_bound": 581273.7258778029
          },
          "point_estimate": 580500.0581034266,
          "standard_error": 376.5372921484197
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579650.4460978836,
            "upper_bound": 581960.3333333334
          },
          "point_estimate": 580012.4959183673,
          "standard_error": 535.7976696800057
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.35416388455698,
            "upper_bound": 1814.722539210738
          },
          "point_estimate": 615.3023543142706,
          "standard_error": 520.4090998336678
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579369.4936921531,
            "upper_bound": 580368.673317943
          },
          "point_estimate": 579811.9565862708,
          "standard_error": 263.0313166357808
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 476.1049739996534,
            "upper_bound": 1503.7182703605404
          },
          "point_estimate": 1256.955093371206,
          "standard_error": 227.19630740859563
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49386.76305919763,
            "upper_bound": 49582.7580495169
          },
          "point_estimate": 49475.26131189613,
          "standard_error": 50.70626168980626
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49349.75855978261,
            "upper_bound": 49608.69904891304
          },
          "point_estimate": 49421.116338315216,
          "standard_error": 60.25133443096513
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.477207262268776,
            "upper_bound": 277.13500007986374
          },
          "point_estimate": 116.60882127896252,
          "standard_error": 62.78005750251323
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49356.12239993339,
            "upper_bound": 49423.48956574504
          },
          "point_estimate": 49378.32430124224,
          "standard_error": 17.442177002478424
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.43309829803044,
            "upper_bound": 227.67670059916705
          },
          "point_estimate": 168.83031485683614,
          "standard_error": 44.827832555760075
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100270.61566210646,
            "upper_bound": 100491.4368203543
          },
          "point_estimate": 100383.99902339296,
          "standard_error": 56.360238477365975
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100162.93038674034,
            "upper_bound": 100550.41712707182
          },
          "point_estimate": 100414.3537446286,
          "standard_error": 90.2900899763944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.065106234262466,
            "upper_bound": 320.89245024225335
          },
          "point_estimate": 215.4702860457191,
          "standard_error": 75.39527469304046
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100222.23600145293,
            "upper_bound": 100487.21190346927
          },
          "point_estimate": 100358.15768099304,
          "standard_error": 66.32735340445092
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.92397442992812,
            "upper_bound": 224.26798186704053
          },
          "point_estimate": 188.09409364104755,
          "standard_error": 28.57641046167283
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70897.75352518717,
            "upper_bound": 71036.5629985894
          },
          "point_estimate": 70972.82342176649,
          "standard_error": 35.674802261455596
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70921.15991210938,
            "upper_bound": 71092.629296875
          },
          "point_estimate": 70977.39529079861,
          "standard_error": 39.0781831761587
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.101546443524525,
            "upper_bound": 198.39359425125144
          },
          "point_estimate": 84.98440089193903,
          "standard_error": 46.78544143082349
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70928.9823016827,
            "upper_bound": 71020.04970237496
          },
          "point_estimate": 70968.06137378246,
          "standard_error": 22.975002210756813
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.52756197818039,
            "upper_bound": 164.92023469537125
          },
          "point_estimate": 118.85603012725886,
          "standard_error": 32.19999294246346
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308622.7709490012,
            "upper_bound": 309163.02740496374
          },
          "point_estimate": 308876.49215698143,
          "standard_error": 139.00652323149362
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308508.3788841808,
            "upper_bound": 309142.01906779665
          },
          "point_estimate": 308811.11822033895,
          "standard_error": 219.29858473564715
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.14038578131528,
            "upper_bound": 843.6651686660372
          },
          "point_estimate": 418.5360879084873,
          "standard_error": 203.8546143774172
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308522.84772940655,
            "upper_bound": 308938.80069306254
          },
          "point_estimate": 308728.84450803435,
          "standard_error": 109.12788892064872
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 251.57031502614691,
            "upper_bound": 608.139953695314
          },
          "point_estimate": 461.2563543117342,
          "standard_error": 101.45661756074004
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36522.703726598505,
            "upper_bound": 36574.28558518445
          },
          "point_estimate": 36544.75809887833,
          "standard_error": 13.35275378956977
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36517.503024193546,
            "upper_bound": 36551.90650201613
          },
          "point_estimate": 36542.77382632488,
          "standard_error": 9.14938483194611
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.978597578166019,
            "upper_bound": 52.38967651192818
          },
          "point_estimate": 18.72483040093631,
          "standard_error": 12.929295982898692
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36522.32298243986,
            "upper_bound": 36543.97188931056
          },
          "point_estimate": 36536.23079440721,
          "standard_error": 5.572668375970061
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.150827984642914,
            "upper_bound": 65.17168400135259
          },
          "point_estimate": 44.450706752198,
          "standard_error": 15.840429428277329
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67093.81822990933,
            "upper_bound": 67319.46436385441
          },
          "point_estimate": 67194.8320931256,
          "standard_error": 58.29275206487935
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67063.21919285275,
            "upper_bound": 67301.17569316083
          },
          "point_estimate": 67122.78082915237,
          "standard_error": 59.74723509266733
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.40607486877351,
            "upper_bound": 282.42022234278494
          },
          "point_estimate": 134.1781751224828,
          "standard_error": 66.06417993682446
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67089.28540281726,
            "upper_bound": 67183.61008786368
          },
          "point_estimate": 67138.87125333077,
          "standard_error": 24.252445124438108
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.17065076085008,
            "upper_bound": 263.74888653325894
          },
          "point_estimate": 193.8917931562653,
          "standard_error": 53.32387238986678
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166548.28171559036,
            "upper_bound": 166866.16606816702
          },
          "point_estimate": 166712.58323385517,
          "standard_error": 81.79707464857653
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166457.10502283106,
            "upper_bound": 166921.7048515982
          },
          "point_estimate": 166813.48249619483,
          "standard_error": 121.99397154033647
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.79925571403496,
            "upper_bound": 451.50875225808903
          },
          "point_estimate": 257.76408109728504,
          "standard_error": 111.90668786014275
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166703.76079156864,
            "upper_bound": 166928.98922362458
          },
          "point_estimate": 166837.42455079168,
          "standard_error": 56.78733926986294
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146.67212616257055,
            "upper_bound": 327.86193278126996
          },
          "point_estimate": 272.2531883895323,
          "standard_error": 44.631733443681064
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36009.274296004245,
            "upper_bound": 36077.18826313649
          },
          "point_estimate": 36046.37307991514,
          "standard_error": 17.482360214240188
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36012.90884016973,
            "upper_bound": 36092.61267326733
          },
          "point_estimate": 36054.48178217822,
          "standard_error": 15.665381769005688
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.349187101005136,
            "upper_bound": 86.01575312638235
          },
          "point_estimate": 36.34579156265503,
          "standard_error": 23.27539486894775
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36014.59073532817,
            "upper_bound": 36085.16465635088
          },
          "point_estimate": 36051.41226179761,
          "standard_error": 18.515788658776184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.37937786284216,
            "upper_bound": 78.28927882415441
          },
          "point_estimate": 58.21762448334654,
          "standard_error": 15.547307802678985
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11969.5263813426,
            "upper_bound": 12019.722571344835
          },
          "point_estimate": 11996.478819872931,
          "standard_error": 12.946503443317685
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11965.08973670576,
            "upper_bound": 12025.487306297397
          },
          "point_estimate": 12011.188571546323,
          "standard_error": 12.92230647386905
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.0012947689511837,
            "upper_bound": 70.63512416635085
          },
          "point_estimate": 22.918738453150823,
          "standard_error": 15.432037647555871
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11957.130159261427,
            "upper_bound": 12010.901954547158
          },
          "point_estimate": 11986.529679156978,
          "standard_error": 13.334334816926829
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.440999165071666,
            "upper_bound": 55.4706502624872
          },
          "point_estimate": 43.046230077732815,
          "standard_error": 10.483965610444082
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 505857.97472718253,
            "upper_bound": 506962.18959606485
          },
          "point_estimate": 506364.80716104497,
          "standard_error": 284.0317938696238
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 505672.5353009259,
            "upper_bound": 507033.07787698414
          },
          "point_estimate": 506139.4592592593,
          "standard_error": 296.8682536495226
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153.09474901817114,
            "upper_bound": 1474.413389032289
          },
          "point_estimate": 582.6759464262866,
          "standard_error": 320.2351092612562
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 505630.4642443783,
            "upper_bound": 506834.8010986683
          },
          "point_estimate": 506082.2505050505,
          "standard_error": 309.62910500667164
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 333.6972617851559,
            "upper_bound": 1224.9433076347866
          },
          "point_estimate": 944.178625472827,
          "standard_error": 232.28309959405115
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1012.5932255884989,
            "upper_bound": 1013.8311203835392
          },
          "point_estimate": 1013.1652282362976,
          "standard_error": 0.3183991827296006
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1012.3211427296594,
            "upper_bound": 1013.8975772392488
          },
          "point_estimate": 1012.9208706321984,
          "standard_error": 0.4136008707964102
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.20041331248534552,
            "upper_bound": 1.7223992480775163
          },
          "point_estimate": 0.9705004345396122,
          "standard_error": 0.3856983980955261
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1012.2409734110514,
            "upper_bound": 1013.2576549965088
          },
          "point_estimate": 1012.654957892438,
          "standard_error": 0.2668832162318332
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.49038867415123955,
            "upper_bound": 1.391795799014294
          },
          "point_estimate": 1.0608416709456678,
          "standard_error": 0.2420822788783699
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51340.86170115281,
            "upper_bound": 51424.97067550249
          },
          "point_estimate": 51383.0152593979,
          "standard_error": 21.52067811129662
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51320.89419263456,
            "upper_bound": 51461.17110481586
          },
          "point_estimate": 51368.3340710576,
          "standard_error": 41.01665729859678
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.809056075854285,
            "upper_bound": 120.6942020239203
          },
          "point_estimate": 107.5475605906498,
          "standard_error": 31.38033225331413
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51321.601682453926,
            "upper_bound": 51424.13846046235
          },
          "point_estimate": 51362.64265479563,
          "standard_error": 25.812974327927204
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.74882333726229,
            "upper_bound": 83.78494511876328
          },
          "point_estimate": 71.90468515298518,
          "standard_error": 9.37154464938707
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52631.19427073842,
            "upper_bound": 52693.24092909766
          },
          "point_estimate": 52660.41169547965,
          "standard_error": 15.900153186655386
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52615.2411231884,
            "upper_bound": 52694.01902173913
          },
          "point_estimate": 52651.370113871635,
          "standard_error": 21.766616884346377
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.348067298537645,
            "upper_bound": 89.97547263450366
          },
          "point_estimate": 54.88032708727466,
          "standard_error": 19.38052170866917
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52625.290817239744,
            "upper_bound": 52683.394208037826
          },
          "point_estimate": 52652.40927159797,
          "standard_error": 14.79711322845724
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.560845538371947,
            "upper_bound": 68.6758181725371
          },
          "point_estimate": 52.973281131094055,
          "standard_error": 11.216647915552766
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52693.893157324856,
            "upper_bound": 52828.666438709486
          },
          "point_estimate": 52755.590026896585,
          "standard_error": 34.60123288986464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52678.03402676988,
            "upper_bound": 52849.05605950653
          },
          "point_estimate": 52711.17893081761,
          "standard_error": 39.36511527408835
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.209995543600789,
            "upper_bound": 201.8174996826229
          },
          "point_estimate": 58.96032348154895,
          "standard_error": 45.726334642380614
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52692.97919207205,
            "upper_bound": 52783.31807783836
          },
          "point_estimate": 52731.70528339585,
          "standard_error": 23.58411260414167
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.004317902630575,
            "upper_bound": 153.0912243879882
          },
          "point_estimate": 115.44827796110398,
          "standard_error": 28.731056074865272
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15226.06636964635,
            "upper_bound": 15268.187339560547
          },
          "point_estimate": 15246.155351338495,
          "standard_error": 10.780663633796133
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15213.93922883487,
            "upper_bound": 15268.031538139145
          },
          "point_estimate": 15246.10657705664,
          "standard_error": 11.854963986775273
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.004560962131638,
            "upper_bound": 60.99537045525389
          },
          "point_estimate": 33.24309632236242,
          "standard_error": 15.097915802728908
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15225.821642177409,
            "upper_bound": 15258.099870776448
          },
          "point_estimate": 15243.119605708624,
          "standard_error": 8.164169688112182
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.912296607271873,
            "upper_bound": 47.25194803337712
          },
          "point_estimate": 35.97321313478001,
          "standard_error": 7.842423565329127
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26268.54280287368,
            "upper_bound": 26322.4157472205
          },
          "point_estimate": 26293.39902217858,
          "standard_error": 13.84152805147595
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26257.050892426436,
            "upper_bound": 26321.053261319
          },
          "point_estimate": 26289.408918234443,
          "standard_error": 16.78667338850962
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.2154052442143986,
            "upper_bound": 71.93716374456798
          },
          "point_estimate": 44.32478350516768,
          "standard_error": 17.065330269171277
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26265.6093732606,
            "upper_bound": 26312.324068801074
          },
          "point_estimate": 26289.006677692785,
          "standard_error": 11.811842862673576
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.349715097728193,
            "upper_bound": 62.00971803251584
          },
          "point_estimate": 46.043337920107255,
          "standard_error": 11.265541803512416
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16483.13740724251,
            "upper_bound": 16512.019166414826
          },
          "point_estimate": 16496.327567096952,
          "standard_error": 7.415188203719172
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16478.970769114538,
            "upper_bound": 16508.935448776065
          },
          "point_estimate": 16490.514304422282,
          "standard_error": 8.494336276791712
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.043807483361058,
            "upper_bound": 37.04031387851447
          },
          "point_estimate": 22.826685388034637,
          "standard_error": 8.195612075777772
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16481.444027518533,
            "upper_bound": 16507.301415462025
          },
          "point_estimate": 16492.15708516325,
          "standard_error": 6.567825991672627
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.95420243744855,
            "upper_bound": 33.98051023704311
          },
          "point_estimate": 24.658420892915736,
          "standard_error": 6.597441965449959
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16619.908047749752,
            "upper_bound": 16644.652206557967
          },
          "point_estimate": 16629.77352372239,
          "standard_error": 6.672115364232895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16618.13426349497,
            "upper_bound": 16632.18328758768
          },
          "point_estimate": 16623.32597590729,
          "standard_error": 3.8740572471121313
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3677764716002296,
            "upper_bound": 17.84714356237946
          },
          "point_estimate": 7.899174196451733,
          "standard_error": 4.449360604214504
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16617.27790941404,
            "upper_bound": 16627.006698022888
          },
          "point_estimate": 16621.780991195446,
          "standard_error": 2.5754096562940174
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.2344092073963635,
            "upper_bound": 33.639294190153315
          },
          "point_estimate": 22.321972563633175,
          "standard_error": 9.394237399225196
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19284.032310454793,
            "upper_bound": 19318.628609489264
          },
          "point_estimate": 19301.662156779737,
          "standard_error": 8.88415500455573
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19275.003142060967,
            "upper_bound": 19328.081269167255
          },
          "point_estimate": 19302.03609341826,
          "standard_error": 12.480900039495571
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.348907243975741,
            "upper_bound": 54.03777865846812
          },
          "point_estimate": 29.654954880456497,
          "standard_error": 12.630828577211656
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19267.07717075684,
            "upper_bound": 19314.91176122056
          },
          "point_estimate": 19288.397268867015,
          "standard_error": 12.518300732712165
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.885929235955196,
            "upper_bound": 36.2804407234626
          },
          "point_estimate": 29.512339510675467,
          "standard_error": 4.991176242776738
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38196.21912654114,
            "upper_bound": 38271.23220758025
          },
          "point_estimate": 38228.62068192416,
          "standard_error": 19.51639077633911
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38182.659403969184,
            "upper_bound": 38247.63082647865
          },
          "point_estimate": 38223.52021151586,
          "standard_error": 16.769534999568634
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.632396933032209,
            "upper_bound": 78.98182389743603
          },
          "point_estimate": 45.085102509408024,
          "standard_error": 17.29770033681837
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38190.60180144245,
            "upper_bound": 38227.886996163266
          },
          "point_estimate": 38209.1987547118,
          "standard_error": 9.514021532890894
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.72203804282196,
            "upper_bound": 94.2717465311708
          },
          "point_estimate": 64.98338246182539,
          "standard_error": 22.190948743982577
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15942.40352782483,
            "upper_bound": 15964.24865312023
          },
          "point_estimate": 15952.294958435285,
          "standard_error": 5.633806191074882
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15939.95470648514,
            "upper_bound": 15960.12607963695
          },
          "point_estimate": 15949.034239914676,
          "standard_error": 5.246030605455482
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4668645765056654,
            "upper_bound": 29.16624078654616
          },
          "point_estimate": 13.662407810802485,
          "standard_error": 6.467684379663718
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15942.707973590888,
            "upper_bound": 15953.643272453655
          },
          "point_estimate": 15948.022769764271,
          "standard_error": 2.7767563611987756
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.968619823421867,
            "upper_bound": 25.899436503630103
          },
          "point_estimate": 18.84276923328324,
          "standard_error": 5.233836141156677
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19923.984322587978,
            "upper_bound": 19944.671970990315
          },
          "point_estimate": 19934.446580259024,
          "standard_error": 5.315308596501643
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19913.167307692307,
            "upper_bound": 19949.81758241758
          },
          "point_estimate": 19936.61196886447,
          "standard_error": 9.612428438644155
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0879697403000723,
            "upper_bound": 29.477584504140054
          },
          "point_estimate": 21.88737863889249,
          "standard_error": 7.274458261794651
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19925.207097242794,
            "upper_bound": 19946.91762145749
          },
          "point_estimate": 19937.83546596261,
          "standard_error": 5.592748711207342
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.24907699272774,
            "upper_bound": 20.711144697616724
          },
          "point_estimate": 17.699009032678298,
          "standard_error": 2.395984169081769
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16607.124376220232,
            "upper_bound": 16638.643526248175
          },
          "point_estimate": 16622.00481517771,
          "standard_error": 8.04993241603033
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16600.39542752629,
            "upper_bound": 16642.759144947417
          },
          "point_estimate": 16616.83210312017,
          "standard_error": 10.263194928039916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.425609086278096,
            "upper_bound": 46.36775973922138
          },
          "point_estimate": 23.70793959281742,
          "standard_error": 10.07655581009518
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16599.988951725336,
            "upper_bound": 16626.497080078992
          },
          "point_estimate": 16611.479720188363,
          "standard_error": 6.732634979719982
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.491146769241336,
            "upper_bound": 34.73641937493515
          },
          "point_estimate": 26.982884564378228,
          "standard_error": 5.571379059969733
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19093.536793075255,
            "upper_bound": 19132.211254941027
          },
          "point_estimate": 19111.728327786535,
          "standard_error": 9.951516191636053
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19085.715317919075,
            "upper_bound": 19139.438153208383
          },
          "point_estimate": 19095.846111403047,
          "standard_error": 15.46599828526285
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.036414867268486,
            "upper_bound": 53.582355049773966
          },
          "point_estimate": 21.467334755503128,
          "standard_error": 14.301206064924427
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19087.38150795331,
            "upper_bound": 19115.943708264924
          },
          "point_estimate": 19100.04713814824,
          "standard_error": 7.321144894337551
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.145528947040203,
            "upper_bound": 39.89144510885859
          },
          "point_estimate": 33.10011314046622,
          "standard_error": 6.172535023187291
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16468.54289795918,
            "upper_bound": 16515.525186696897
          },
          "point_estimate": 16489.456399740848,
          "standard_error": 12.129458617470933
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16458.089229024943,
            "upper_bound": 16507.868518518517
          },
          "point_estimate": 16478.83201814059,
          "standard_error": 10.910724240084468
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9959916380354703,
            "upper_bound": 54.972308008175126
          },
          "point_estimate": 30.163328369255197,
          "standard_error": 13.241475107678788
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16468.58432723739,
            "upper_bound": 16498.55792296288
          },
          "point_estimate": 16485.679787967136,
          "standard_error": 7.611023236068788
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.84564045189986,
            "upper_bound": 56.78006992477138
          },
          "point_estimate": 40.528860474163864,
          "standard_error": 12.100060111683092
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16661.875356960827,
            "upper_bound": 16687.15294533639
          },
          "point_estimate": 16674.84685406655,
          "standard_error": 6.474806930505114
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16653.113646788992,
            "upper_bound": 16691.44764908257
          },
          "point_estimate": 16685.091743119265,
          "standard_error": 11.854436856591846
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.031514289234268,
            "upper_bound": 31.59759078459503
          },
          "point_estimate": 17.662511215481604,
          "standard_error": 9.121277309965109
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16652.301473082916,
            "upper_bound": 16686.454926506856
          },
          "point_estimate": 16668.965185273442,
          "standard_error": 8.813199405407298
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.985086960519562,
            "upper_bound": 25.319537104301663
          },
          "point_estimate": 21.577576520847817,
          "standard_error": 3.1101368604903556
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16484.009765073606,
            "upper_bound": 16520.50733298764
          },
          "point_estimate": 16501.907020424627,
          "standard_error": 9.360085743186042
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16471.128078817736,
            "upper_bound": 16521.5117274148
          },
          "point_estimate": 16502.509698275862,
          "standard_error": 13.343460811483771
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.904953756722353,
            "upper_bound": 56.182165930124874
          },
          "point_estimate": 32.90998600828888,
          "standard_error": 12.207100224818705
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16482.517091081776,
            "upper_bound": 16515.080349199572
          },
          "point_estimate": 16499.727388219766,
          "standard_error": 8.246182739959412
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.014216907807217,
            "upper_bound": 39.96901812532235
          },
          "point_estimate": 31.236313501670832,
          "standard_error": 5.810549741722057
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19466.056271064725,
            "upper_bound": 19494.769566875188
          },
          "point_estimate": 19480.49853815056,
          "standard_error": 7.363737896868001
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19450.09204647006,
            "upper_bound": 19498.04584450402
          },
          "point_estimate": 19485.496974339338,
          "standard_error": 11.849716471038576
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.34987062828216337,
            "upper_bound": 46.107725417350366
          },
          "point_estimate": 21.709914129317305,
          "standard_error": 11.353607544444866
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19463.831134941913,
            "upper_bound": 19491.273324918373
          },
          "point_estimate": 19476.225703840395,
          "standard_error": 7.012831549184557
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.62421700821891,
            "upper_bound": 30.189481463856843
          },
          "point_estimate": 24.543205522647483,
          "standard_error": 4.016568694132106
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16849.775313852813,
            "upper_bound": 16902.47227272727
          },
          "point_estimate": 16872.57033240569,
          "standard_error": 13.67312396113512
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16838.27303648732,
            "upper_bound": 16887.957792207795
          },
          "point_estimate": 16862.150440630798,
          "standard_error": 13.131035861663015
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.296007031081518,
            "upper_bound": 56.88451636022544
          },
          "point_estimate": 32.76034779501285,
          "standard_error": 12.24455391528764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16845.075648987342,
            "upper_bound": 16869.527856941782
          },
          "point_estimate": 16855.770829819532,
          "standard_error": 6.190289908601645
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.571083639883803,
            "upper_bound": 65.93761466215071
          },
          "point_estimate": 45.59661634909764,
          "standard_error": 15.259064217163994
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19609.931250249487,
            "upper_bound": 19653.777590586695
          },
          "point_estimate": 19630.03519160263,
          "standard_error": 11.323060964542115
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19602.05099238472,
            "upper_bound": 19656.45709660011
          },
          "point_estimate": 19616.62043693624,
          "standard_error": 13.084395144056892
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.891834415761639,
            "upper_bound": 61.098649899270846
          },
          "point_estimate": 27.881680692263274,
          "standard_error": 14.161288116543478
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19605.34929843497,
            "upper_bound": 19622.353446577406
          },
          "point_estimate": 19612.131445672516,
          "standard_error": 4.298062838481606
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.65579745034309,
            "upper_bound": 50.316082383649864
          },
          "point_estimate": 37.6446528576075,
          "standard_error": 9.407559322071297
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71556.50567991502,
            "upper_bound": 71615.43093113361
          },
          "point_estimate": 71583.64167072866,
          "standard_error": 15.081409383798604
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71550.5678040245,
            "upper_bound": 71602.60472440945
          },
          "point_estimate": 71580.67772309712,
          "standard_error": 12.322495120233446
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.272291411724536,
            "upper_bound": 74.1034078707588
          },
          "point_estimate": 38.57496839652456,
          "standard_error": 16.9646887811334
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71555.15445293514,
            "upper_bound": 71594.18855638201
          },
          "point_estimate": 71574.64047959914,
          "standard_error": 10.25715630053075
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.86508464816985,
            "upper_bound": 70.59951869934599
          },
          "point_estimate": 50.24187440535924,
          "standard_error": 14.247466756401924
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1233957.427,
            "upper_bound": 1236342.861222222
          },
          "point_estimate": 1235155.7623333333,
          "standard_error": 609.7054748757914
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1233869.683333333,
            "upper_bound": 1236785.7333333334
          },
          "point_estimate": 1235019.3638888889,
          "standard_error": 606.7225071798628
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.56935479222118,
            "upper_bound": 3696.344124376808
          },
          "point_estimate": 1380.2593921622024,
          "standard_error": 954.180332845775
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1233053.1695353657,
            "upper_bound": 1236612.084175926
          },
          "point_estimate": 1234550.047878788,
          "standard_error": 918.0837318311168
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1055.618229322158,
            "upper_bound": 2634.0843307031496
          },
          "point_estimate": 2038.3135758960304,
          "standard_error": 408.11797603367575
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201376.5932057024,
            "upper_bound": 201895.91263395597
          },
          "point_estimate": 201641.80175304745,
          "standard_error": 133.0392601552615
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201180.97513812155,
            "upper_bound": 202066.76550030697
          },
          "point_estimate": 201671.27223756904,
          "standard_error": 240.94342801554305
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.79866225460283,
            "upper_bound": 746.4669023552749
          },
          "point_estimate": 590.7856229460834,
          "standard_error": 188.4800823265209
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201257.23481714123,
            "upper_bound": 202020.32091795615
          },
          "point_estimate": 201708.36059410204,
          "standard_error": 196.9694813608716
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272.234138253645,
            "upper_bound": 521.7314873999106
          },
          "point_estimate": 442.2620870963415,
          "standard_error": 62.86510854204347
        }
      }
    },
    "memmem/krate/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5974.005482100746,
            "upper_bound": 5985.238509186137
          },
          "point_estimate": 5979.611681748383,
          "standard_error": 2.8721230360994925
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5972.37910496874,
            "upper_bound": 5987.855314248108
          },
          "point_estimate": 5978.085766973785,
          "standard_error": 4.091822875658022
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5788084076555644,
            "upper_bound": 16.56558899102834
          },
          "point_estimate": 10.822418771338471,
          "standard_error": 3.5782410829443227
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5974.084802669879,
            "upper_bound": 5986.349285857617
          },
          "point_estimate": 5979.141009730644,
          "standard_error": 3.1955901585408513
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.656391456239912,
            "upper_bound": 11.927937623909616
          },
          "point_estimate": 9.593448749897194,
          "standard_error": 1.6092803798386683
        }
      }
    },
    "memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5998.057267253592,
            "upper_bound": 6025.8925811668605
          },
          "point_estimate": 6011.869577422678,
          "standard_error": 7.135954937023908
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5989.647127007783,
            "upper_bound": 6032.053982447425
          },
          "point_estimate": 6012.23945189601,
          "standard_error": 12.773441725538593
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.368461197950725,
            "upper_bound": 39.27191450232062
          },
          "point_estimate": 30.04086587998044,
          "standard_error": 9.014644983080752
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5988.731929794835,
            "upper_bound": 6017.480474740275
          },
          "point_estimate": 6002.705215665275,
          "standard_error": 7.487351794791559
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.215392266752213,
            "upper_bound": 28.73168661685713
          },
          "point_estimate": 23.74362741708377,
          "standard_error": 3.4978226742442535
        }
      }
    },
    "memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15586.915231339775,
            "upper_bound": 15628.478574248478
          },
          "point_estimate": 15606.721235611683,
          "standard_error": 10.618218565955704
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15584.095272070015,
            "upper_bound": 15632.585999928651
          },
          "point_estimate": 15599.39444563356,
          "standard_error": 11.79619052469943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.876602828116478,
            "upper_bound": 59.142511272357176
          },
          "point_estimate": 26.509025394810024,
          "standard_error": 13.688068172314528
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15582.721312766298,
            "upper_bound": 15611.626211894656
          },
          "point_estimate": 15596.52037115282,
          "standard_error": 7.237676658887069
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.904872225097485,
            "upper_bound": 46.34242719169413
          },
          "point_estimate": 35.48146743889616,
          "standard_error": 7.732517130400285
        }
      }
    },
    "memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.908955372421474,
            "upper_bound": 35.0614799207966
          },
          "point_estimate": 34.989013470018165,
          "standard_error": 0.039187221796523806
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.883928496105455,
            "upper_bound": 35.103846066934864
          },
          "point_estimate": 35.022132577058784,
          "standard_error": 0.06675714815081009
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0074701312993765215,
            "upper_bound": 0.2199779704305398
          },
          "point_estimate": 0.12173973205718532,
          "standard_error": 0.06970378631798876
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.872902402758584,
            "upper_bound": 35.0657781361716
          },
          "point_estimate": 34.971695474232185,
          "standard_error": 0.051918189573112235
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06973322112526778,
            "upper_bound": 0.1589598275153038
          },
          "point_estimate": 0.13100372399027824,
          "standard_error": 0.02295735042685876
        }
      }
    },
    "memmem/krate/prebuilt/sliceslice-haystack/words": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/sliceslice-haystack/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/sliceslice-haystack/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/sliceslice-haystack/words",
        "directory_name": "memmem/krate_prebuilt_sliceslice-haystack_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180542.224009901,
            "upper_bound": 180778.32432578976
          },
          "point_estimate": 180656.5268835455,
          "standard_error": 60.39597209498713
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180503.6051980198,
            "upper_bound": 180773.08952145217
          },
          "point_estimate": 180646.33745874587,
          "standard_error": 60.208373688075554
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.78214144394774,
            "upper_bound": 341.44326324509905
          },
          "point_estimate": 128.378216384196,
          "standard_error": 82.03866722903365
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180601.63580522657,
            "upper_bound": 180736.1085483548
          },
          "point_estimate": 180680.3888646007,
          "standard_error": 34.000391391156825
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.37419589801728,
            "upper_bound": 267.5133448670915
          },
          "point_estimate": 200.9337248624909,
          "standard_error": 44.51903488253775
        }
      }
    },
    "memmem/krate/prebuilt/sliceslice-i386/words": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/krate_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25032056.766071428,
            "upper_bound": 25058648.61107143
          },
          "point_estimate": 25044899.021904763,
          "standard_error": 6844.613793933859
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25028663.4375,
            "upper_bound": 25061204.083333332
          },
          "point_estimate": 25040079.70535714,
          "standard_error": 7677.87602191134
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1059.529481190775,
            "upper_bound": 37739.18079249193
          },
          "point_estimate": 20657.930283249345,
          "standard_error": 11596.124941000617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25037660.741975307,
            "upper_bound": 25068135.271323107
          },
          "point_estimate": 25052241.92987013,
          "standard_error": 8469.33246938289
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10820.396415373034,
            "upper_bound": 28538.72317293959
          },
          "point_estimate": 22816.55889161484,
          "standard_error": 4379.4844713542425
        }
      }
    },
    "memmem/krate/prebuilt/sliceslice-words/words": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/krate_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80335725.11749999,
            "upper_bound": 80359073.33416668
          },
          "point_estimate": 80347018.66666667,
          "standard_error": 5968.524815122771
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80330865.66666667,
            "upper_bound": 80361580.0
          },
          "point_estimate": 80345489.16666666,
          "standard_error": 7885.97165261741
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6623.268282417164,
            "upper_bound": 34595.97618579496
          },
          "point_estimate": 20927.39282846819,
          "standard_error": 7264.135039437484
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10867.969848238252,
            "upper_bound": 25055.224855144883
          },
          "point_estimate": 19965.941629697976,
          "standard_error": 3615.195245785389
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.838924450845573,
            "upper_bound": 8.847274936987613
          },
          "point_estimate": 8.842620156847623,
          "standard_error": 0.002148326982999936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.837917640372911,
            "upper_bound": 8.847586648039783
          },
          "point_estimate": 8.839617452551114,
          "standard_error": 0.0020411337576947356
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002758132340714535,
            "upper_bound": 0.009401118045694152
          },
          "point_estimate": 0.002658048107697097,
          "standard_error": 0.002114035817168082
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.838859797888725,
            "upper_bound": 8.841432677266056
          },
          "point_estimate": 8.839946353423485,
          "standard_error": 0.0006608586982487113
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014049724470370354,
            "upper_bound": 0.009254027684341411
          },
          "point_estimate": 0.0071749234774673805,
          "standard_error": 0.0021141524766655332
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.844336293996122,
            "upper_bound": 8.85264330534754
          },
          "point_estimate": 8.848026468282237,
          "standard_error": 0.002152792037350647
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.843424059638348,
            "upper_bound": 8.851833507381365
          },
          "point_estimate": 8.845190923307971,
          "standard_error": 0.0019350642853484669
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004298762329014194,
            "upper_bound": 0.009576493751038188
          },
          "point_estimate": 0.0026985850614549916,
          "standard_error": 0.002206898258306647
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.844471396094683,
            "upper_bound": 8.847120408370428
          },
          "point_estimate": 8.845405589793776,
          "standard_error": 0.0006821008437318158
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001307478793330673,
            "upper_bound": 0.00949251033811497
          },
          "point_estimate": 0.007167947254016709,
          "standard_error": 0.0021170205068170534
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.845481331513934,
            "upper_bound": 8.852226987317314
          },
          "point_estimate": 8.848210774890047,
          "standard_error": 0.0018017653639640508
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.844597413166852,
            "upper_bound": 8.84926631168656
          },
          "point_estimate": 8.846443782697815,
          "standard_error": 0.0013723187419205263
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00047057092421014593,
            "upper_bound": 0.0053915201191354675
          },
          "point_estimate": 0.0027947260827926607,
          "standard_error": 0.0014131074434001886
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.844846442278401,
            "upper_bound": 8.848316059601128
          },
          "point_estimate": 8.846257746588739,
          "standard_error": 0.0008910214606459839
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014580675221937684,
            "upper_bound": 0.008952973140674461
          },
          "point_estimate": 0.005977598751813435,
          "standard_error": 0.002404424780146276
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.837797825950512,
            "upper_bound": 8.843004958833818
          },
          "point_estimate": 8.840174602704634,
          "standard_error": 0.001331202083831612
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.8378549371359,
            "upper_bound": 8.842375099119947
          },
          "point_estimate": 8.839167778897657,
          "standard_error": 0.0010801647876755903
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00035152387510746535,
            "upper_bound": 0.00673155039791317
          },
          "point_estimate": 0.0022354401460853847,
          "standard_error": 0.0016940758788220526
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.83850099586754,
            "upper_bound": 8.841059425921772
          },
          "point_estimate": 8.839533732125723,
          "standard_error": 0.0006588874155306111
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016686013196771298,
            "upper_bound": 0.006207272625823978
          },
          "point_estimate": 0.004434922493188207,
          "standard_error": 0.0012639160095475406
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.761477402782551,
            "upper_bound": 9.76345013026562
          },
          "point_estimate": 9.762418024267754,
          "standard_error": 0.0005031130168070136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.761363580422444,
            "upper_bound": 9.763094535154206
          },
          "point_estimate": 9.762390350629031,
          "standard_error": 0.0004020702994662592
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00013579339898121237,
            "upper_bound": 0.0027562293641486793
          },
          "point_estimate": 0.0009639610632115284,
          "standard_error": 0.0006694422451570836
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.76072429178536,
            "upper_bound": 9.7640989215922
          },
          "point_estimate": 9.762135357948576,
          "standard_error": 0.0008778993580309145
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006419921841295912,
            "upper_bound": 0.0023172964876974618
          },
          "point_estimate": 0.0016776974725258523,
          "standard_error": 0.0004349607445802116
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.085359640725247,
            "upper_bound": 10.08968920910932
          },
          "point_estimate": 10.087371394420938,
          "standard_error": 0.0011054293927411942
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.085076448456777,
            "upper_bound": 10.08909831696636
          },
          "point_estimate": 10.086795060187605,
          "standard_error": 0.0009054391637150704
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00032730495395432373,
            "upper_bound": 0.005913733630034651
          },
          "point_estimate": 0.0018661372459808467,
          "standard_error": 0.0013914064432975954
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.08612220280913,
            "upper_bound": 10.09232266711881
          },
          "point_estimate": 10.088942050976266,
          "standard_error": 0.001816480734855077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014518153819660638,
            "upper_bound": 0.005076154656018824
          },
          "point_estimate": 0.003685144576000995,
          "standard_error": 0.0009809626157934628
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.805107121682307,
            "upper_bound": 7.825894800130317
          },
          "point_estimate": 7.814550480171684,
          "standard_error": 0.00535886438648994
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.802122466721373,
            "upper_bound": 7.828627753143322
          },
          "point_estimate": 7.804915550944917,
          "standard_error": 0.007074955790629597
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006351972244048221,
            "upper_bound": 0.029830717668697545
          },
          "point_estimate": 0.00506490311499336,
          "standard_error": 0.007664443816774276
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.802779951308377,
            "upper_bound": 7.811988555281101
          },
          "point_estimate": 7.805476443281549,
          "standard_error": 0.0024490911953618023
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004316728121915195,
            "upper_bound": 0.023266991993482925
          },
          "point_estimate": 0.017912104099800874,
          "standard_error": 0.004388428538245078
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.771259671621609,
            "upper_bound": 9.808182534564311
          },
          "point_estimate": 9.789936463585798,
          "standard_error": 0.009451186768367271
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.754514103291491,
            "upper_bound": 9.815314565536053
          },
          "point_estimate": 9.810468335830098,
          "standard_error": 0.021763096038745183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001152271351047564,
            "upper_bound": 0.046548496503224776
          },
          "point_estimate": 0.01045773403639439,
          "standard_error": 0.015529124042846437
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.756916151495137,
            "upper_bound": 9.792904121281634
          },
          "point_estimate": 9.768060995817669,
          "standard_error": 0.00927822082909895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01925146446705327,
            "upper_bound": 0.03346596332022142
          },
          "point_estimate": 0.031517269925873136,
          "standard_error": 0.00369358229297137
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.352522610620248,
            "upper_bound": 12.407775800627991
          },
          "point_estimate": 12.380909874788063,
          "standard_error": 0.014142717054999018
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.347998361199526,
            "upper_bound": 12.419843585145085
          },
          "point_estimate": 12.381535285919371,
          "standard_error": 0.021827900093270517
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01056108309895184,
            "upper_bound": 0.08224874343386675
          },
          "point_estimate": 0.048018640502893846,
          "standard_error": 0.01798538773069138
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.368577407523466,
            "upper_bound": 12.424665771994444
          },
          "point_estimate": 12.399192862769263,
          "standard_error": 0.014241343471134723
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027613368115169217,
            "upper_bound": 0.059730562667316134
          },
          "point_estimate": 0.04707975949139379,
          "standard_error": 0.008360279352932902
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.861335653625982,
            "upper_bound": 8.86479016908896
          },
          "point_estimate": 8.862919687416898,
          "standard_error": 0.0008884717316980814
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.860624850087461,
            "upper_bound": 8.864319889469364
          },
          "point_estimate": 8.862508903342246,
          "standard_error": 0.000888470653945597
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006128271336252189,
            "upper_bound": 0.004392247528621981
          },
          "point_estimate": 0.002186029692137803,
          "standard_error": 0.0010039368831951262
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.860300216398656,
            "upper_bound": 8.865697457933994
          },
          "point_estimate": 8.862961960134097,
          "standard_error": 0.0013850838943812185
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012743495456650714,
            "upper_bound": 0.0040841200305401825
          },
          "point_estimate": 0.002969824012110778,
          "standard_error": 0.0007886218549015466
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.870670529350887,
            "upper_bound": 9.878052133018516
          },
          "point_estimate": 9.873886218792428,
          "standard_error": 0.0019105349108592036
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.869982614118443,
            "upper_bound": 9.87715174389817
          },
          "point_estimate": 9.870832596585892,
          "standard_error": 0.0020885145881439824
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002145341015543701,
            "upper_bound": 0.008023087693518909
          },
          "point_estimate": 0.0027102103452234664,
          "standard_error": 0.0023581066032133703
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.870344995219728,
            "upper_bound": 9.874078590954964
          },
          "point_estimate": 9.872003583471828,
          "standard_error": 0.000979113103743264
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002096904735786144,
            "upper_bound": 0.008943421974849123
          },
          "point_estimate": 0.0063508629569917876,
          "standard_error": 0.001959722978052346
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.19071208947282,
            "upper_bound": 19.21953915292391
          },
          "point_estimate": 19.20242549745073,
          "standard_error": 0.007680709372683548
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.18861040374084,
            "upper_bound": 19.20760946951816
          },
          "point_estimate": 19.19468770220029,
          "standard_error": 0.00504291680170483
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032127361799345186,
            "upper_bound": 0.02247888700142256
          },
          "point_estimate": 0.01006224063454014,
          "standard_error": 0.005429667944733783
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.18654486516279,
            "upper_bound": 19.19776778418064
          },
          "point_estimate": 19.190777101761167,
          "standard_error": 0.0028425077124340547
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006120267279650016,
            "upper_bound": 0.03823620158206617
          },
          "point_estimate": 0.025493895759069515,
          "standard_error": 0.010282409742372723
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104602.78353332532,
            "upper_bound": 104772.11301404852
          },
          "point_estimate": 104677.19085887611,
          "standard_error": 43.89091540481066
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104580.91594827586,
            "upper_bound": 104748.81494252873
          },
          "point_estimate": 104598.21206896551,
          "standard_error": 49.01993575066888
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.3120985187976735,
            "upper_bound": 201.26779848617156
          },
          "point_estimate": 37.97106816207596,
          "standard_error": 57.357801554064864
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104582.99313125695,
            "upper_bound": 104690.5835176435
          },
          "point_estimate": 104643.10971040452,
          "standard_error": 28.00760856505467
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.417807631585084,
            "upper_bound": 204.65071123519056
          },
          "point_estimate": 146.43631896963046,
          "standard_error": 44.530714311103274
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56504.3845641339,
            "upper_bound": 56638.182974922245
          },
          "point_estimate": 56568.46525549877,
          "standard_error": 34.29689329589388
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56465.96382285417,
            "upper_bound": 56653.34618973562
          },
          "point_estimate": 56553.63271124935,
          "standard_error": 53.69736937212991
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.55749764539795,
            "upper_bound": 200.16344751947844
          },
          "point_estimate": 121.64265868799993,
          "standard_error": 46.784491724800915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56473.28945854696,
            "upper_bound": 56586.425075220366
          },
          "point_estimate": 56530.58502958938,
          "standard_error": 29.562374569471665
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.33347899855353,
            "upper_bound": 144.95609908323814
          },
          "point_estimate": 114.48393544996456,
          "standard_error": 21.798673786878037
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148524.77483628524,
            "upper_bound": 148684.44615889213
          },
          "point_estimate": 148608.68071898285,
          "standard_error": 40.96955277761971
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148534.19489795918,
            "upper_bound": 148715.74020408164
          },
          "point_estimate": 148608.36293731778,
          "standard_error": 37.51764572658402
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.815897240704516,
            "upper_bound": 229.90851770402625
          },
          "point_estimate": 84.689723343402,
          "standard_error": 57.530056845139754
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148579.15323793012,
            "upper_bound": 148710.8677499571
          },
          "point_estimate": 148645.47005565863,
          "standard_error": 36.11684418739503
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.79383150924956,
            "upper_bound": 184.2233339329213
          },
          "point_estimate": 136.16456553090543,
          "standard_error": 33.00936929072974
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396328.8112192028,
            "upper_bound": 397045.5356737836
          },
          "point_estimate": 396697.32838509313,
          "standard_error": 183.85899595461436
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395996.66847826086,
            "upper_bound": 397234.3017080745
          },
          "point_estimate": 396749.4366847826,
          "standard_error": 295.81880693016706
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.5711121764799,
            "upper_bound": 1013.6458129823916
          },
          "point_estimate": 752.3740762622378,
          "standard_error": 265.1241210132187
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396327.9666218913,
            "upper_bound": 397155.58998774586
          },
          "point_estimate": 396787.7736024845,
          "standard_error": 218.23042683988825
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321.97817152167676,
            "upper_bound": 737.059309254589
          },
          "point_estimate": 614.7483925250895,
          "standard_error": 97.81233741343996
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578336.3560029762,
            "upper_bound": 579821.7761286376
          },
          "point_estimate": 578990.5304188713,
          "standard_error": 386.3170004720806
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 577920.5582010582,
            "upper_bound": 579580.0917107584
          },
          "point_estimate": 578779.5307539683,
          "standard_error": 374.06811999574944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.72686283997884,
            "upper_bound": 1693.3829703994472
          },
          "point_estimate": 915.9962517934032,
          "standard_error": 464.1486234242562
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578076.4808307093,
            "upper_bound": 578942.689781746
          },
          "point_estimate": 578503.3163883735,
          "standard_error": 229.23930091770333
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 461.7423012412645,
            "upper_bound": 1804.0630171658593
          },
          "point_estimate": 1287.4353469684204,
          "standard_error": 388.58845823752495
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49693.35422286822,
            "upper_bound": 49809.8340135279
          },
          "point_estimate": 49750.97923544613,
          "standard_error": 29.75930043637914
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49664.81901504788,
            "upper_bound": 49814.007090743275
          },
          "point_estimate": 49758.559963520296,
          "standard_error": 33.90847500916659
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.357360986206203,
            "upper_bound": 176.85506125828272
          },
          "point_estimate": 76.62899879916318,
          "standard_error": 41.41813669120348
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49677.32704026082,
            "upper_bound": 49799.62448966606
          },
          "point_estimate": 49749.41307584345,
          "standard_error": 31.779051655011816
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.35307451678768,
            "upper_bound": 129.6132600364575
          },
          "point_estimate": 99.2132148087476,
          "standard_error": 20.175578573045005
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100049.66216187851,
            "upper_bound": 100240.47246833867
          },
          "point_estimate": 100142.0658020683,
          "standard_error": 48.759984443822944
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100029.5746556474,
            "upper_bound": 100242.39890244436
          },
          "point_estimate": 100142.3699724518,
          "standard_error": 44.87656403859055
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.806147600010515,
            "upper_bound": 290.1463189076449
          },
          "point_estimate": 100.49550014422694,
          "standard_error": 79.00633823499439
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100071.98970993356,
            "upper_bound": 100319.91722770056
          },
          "point_estimate": 100203.03547636936,
          "standard_error": 63.11908967939212
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.38416054905018,
            "upper_bound": 214.85619854696063
          },
          "point_estimate": 162.8214508791956,
          "standard_error": 35.40610840926641
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70653.87007565933,
            "upper_bound": 70772.91247606695
          },
          "point_estimate": 70711.41643582855,
          "standard_error": 30.508154880338335
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70630.04961089494,
            "upper_bound": 70775.70319622013
          },
          "point_estimate": 70706.2354355815,
          "standard_error": 34.442985905043095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.879039658503473,
            "upper_bound": 173.345358167647
          },
          "point_estimate": 100.60839772745716,
          "standard_error": 41.93450428770069
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70658.54369143794,
            "upper_bound": 70731.87042670984
          },
          "point_estimate": 70696.21501844459,
          "standard_error": 18.274513452368765
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.61092708865136,
            "upper_bound": 130.88966720988796
          },
          "point_estimate": 101.80725641853876,
          "standard_error": 20.26285869186422
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308660.26482132764,
            "upper_bound": 309091.61636352405
          },
          "point_estimate": 308872.7860966505,
          "standard_error": 110.21543220825328
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308656.6687853107,
            "upper_bound": 309158.7711864407
          },
          "point_estimate": 308826.6605326877,
          "standard_error": 117.69701487529063
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.22544405675899,
            "upper_bound": 620.4600118660724
          },
          "point_estimate": 253.38710499298568,
          "standard_error": 148.00119173250496
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308716.21067552356,
            "upper_bound": 308928.26650364796
          },
          "point_estimate": 308824.43350209115,
          "standard_error": 54.06456304582695
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165.74874387705313,
            "upper_bound": 484.747566169384
          },
          "point_estimate": 365.9405427032864,
          "standard_error": 79.02706709711043
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36195.867389015366,
            "upper_bound": 36253.28035262521
          },
          "point_estimate": 36223.75970348763,
          "standard_error": 14.728235728523842
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36183.30641323594,
            "upper_bound": 36269.495219123506
          },
          "point_estimate": 36216.32934926959,
          "standard_error": 23.19245671655976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.670682831944616,
            "upper_bound": 83.42716063042778
          },
          "point_estimate": 51.405043957900766,
          "standard_error": 18.493604993807576
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36188.69401587051,
            "upper_bound": 36258.89572186929
          },
          "point_estimate": 36223.17789620738,
          "standard_error": 18.206379198626273
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.70715711474001,
            "upper_bound": 60.17922473936878
          },
          "point_estimate": 49.08523575679417,
          "standard_error": 8.007523532619661
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67264.34593584656,
            "upper_bound": 67385.19172729275
          },
          "point_estimate": 67322.5266920194,
          "standard_error": 30.87118876491696
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67246.66712962963,
            "upper_bound": 67421.35308641975
          },
          "point_estimate": 67297.60687830688,
          "standard_error": 48.757303224128336
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.921705635605082,
            "upper_bound": 164.83413151805516
          },
          "point_estimate": 77.68253505604022,
          "standard_error": 44.497769759652634
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67251.65841269841,
            "upper_bound": 67355.85618357488
          },
          "point_estimate": 67293.18684944685,
          "standard_error": 26.647570060673228
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.93742858714872,
            "upper_bound": 123.8626227560701
          },
          "point_estimate": 102.76679992002856,
          "standard_error": 16.99736319868508
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169663.61503931167,
            "upper_bound": 169900.40688797104
          },
          "point_estimate": 169786.08500037086,
          "standard_error": 60.68788450711566
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169641.36728971964,
            "upper_bound": 169985.7098130841
          },
          "point_estimate": 169790.47880507342,
          "standard_error": 80.81887084769545
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.23132552498055,
            "upper_bound": 343.0114547748339
          },
          "point_estimate": 255.26110803828857,
          "standard_error": 81.33191226875908
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169703.83806365475,
            "upper_bound": 169913.50928910478
          },
          "point_estimate": 169821.54851316908,
          "standard_error": 54.43672290118856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.55980792887291,
            "upper_bound": 257.748309101257
          },
          "point_estimate": 202.55166179998915,
          "standard_error": 38.41477370224985
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36103.209149426235,
            "upper_bound": 36184.29206150399
          },
          "point_estimate": 36140.05448613672,
          "standard_error": 20.917127387467197
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36090.68123138034,
            "upper_bound": 36178.13590422597
          },
          "point_estimate": 36122.09183926798,
          "standard_error": 16.57078041802806
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.798094587860102,
            "upper_bound": 106.30827048107028
          },
          "point_estimate": 25.684275454306228,
          "standard_error": 29.17812541403202
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36107.61646094067,
            "upper_bound": 36153.52397576593
          },
          "point_estimate": 36128.053781967785,
          "standard_error": 11.486784202600578
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.675281696517622,
            "upper_bound": 93.05284147230162
          },
          "point_estimate": 69.52418592291038,
          "standard_error": 18.70450882434073
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11834.70080073881,
            "upper_bound": 11882.282946135609
          },
          "point_estimate": 11857.56132465698,
          "standard_error": 12.207264717919678
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11826.073554976098,
            "upper_bound": 11890.608295306389
          },
          "point_estimate": 11847.931355932204,
          "standard_error": 15.949459564363718
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.002546676701359,
            "upper_bound": 66.845526251326
          },
          "point_estimate": 42.308931608709905,
          "standard_error": 16.017696520984217
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11821.117819926149,
            "upper_bound": 11850.495330843743
          },
          "point_estimate": 11832.147247667586,
          "standard_error": 7.536759571915838
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.303623929514742,
            "upper_bound": 50.48922359840475
          },
          "point_estimate": 40.68322826739304,
          "standard_error": 7.432100477277523
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 507178.26039021165,
            "upper_bound": 508049.3861736112
          },
          "point_estimate": 507647.5015613978,
          "standard_error": 223.81784492002524
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 507279.0245949074,
            "upper_bound": 508120.8944444445
          },
          "point_estimate": 507759.0039682539,
          "standard_error": 212.73020549661427
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139.35181372975015,
            "upper_bound": 1168.5917709200785
          },
          "point_estimate": 516.1084945872368,
          "standard_error": 256.3956086502021
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 507310.46108963346,
            "upper_bound": 508014.16319005814
          },
          "point_estimate": 507746.2442640693,
          "standard_error": 182.99617229260437
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 293.3661350362811,
            "upper_bound": 1028.920190886297
          },
          "point_estimate": 747.3180902448538,
          "standard_error": 199.5685110538131
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 997.2487881148588,
            "upper_bound": 1000.5585558981512
          },
          "point_estimate": 999.0870198886826,
          "standard_error": 0.8571958805589842
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 997.2524499248824,
            "upper_bound": 1000.9406047730916
          },
          "point_estimate": 999.8323170172156,
          "standard_error": 0.7382248186948644
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.22067929592522265,
            "upper_bound": 3.4305893169855053
          },
          "point_estimate": 1.364056737152988,
          "standard_error": 0.8527221335970255
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 996.5618385553412,
            "upper_bound": 1001.1394981421294
          },
          "point_estimate": 999.3764319536866,
          "standard_error": 1.169396793156024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6955969860177574,
            "upper_bound": 3.775965335008199
          },
          "point_estimate": 2.8626490962300855,
          "standard_error": 0.8263652948100721
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51371.68271892655,
            "upper_bound": 51488.18896421845
          },
          "point_estimate": 51427.01193469196,
          "standard_error": 29.863198059971957
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51346.75949858757,
            "upper_bound": 51506.70856873823
          },
          "point_estimate": 51393.736380145274,
          "standard_error": 47.39401664480059
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.570661112166978,
            "upper_bound": 161.1693143349973
          },
          "point_estimate": 90.41565090186208,
          "standard_error": 41.077836002853
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51367.49778081422,
            "upper_bound": 51482.99088192309
          },
          "point_estimate": 51414.61528358647,
          "standard_error": 29.528892284414106
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.22466532555252,
            "upper_bound": 122.99628016127409
          },
          "point_estimate": 99.53290449786384,
          "standard_error": 18.02588620803343
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53122.09066617862,
            "upper_bound": 53250.83309821865
          },
          "point_estimate": 53171.04007483326,
          "standard_error": 36.15458482535146
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53117.63360175696,
            "upper_bound": 53161.62249877989
          },
          "point_estimate": 53139.584602245,
          "standard_error": 12.6897468632619
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4711313501641328,
            "upper_bound": 65.7016457588896
          },
          "point_estimate": 21.84513499528234,
          "standard_error": 19.724911416876527
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53125.64649689173,
            "upper_bound": 53148.72156513319
          },
          "point_estimate": 53138.25614268601,
          "standard_error": 5.83869767254179
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.428975657481494,
            "upper_bound": 184.04899729668688
          },
          "point_estimate": 120.0288929033803,
          "standard_error": 57.0885910910933
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52771.88307309881,
            "upper_bound": 52853.93214705578
          },
          "point_estimate": 52812.534799456305,
          "standard_error": 21.125416838608945
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52749.329172714075,
            "upper_bound": 52883.631608957076
          },
          "point_estimate": 52796.94541203031,
          "standard_error": 36.700153838769715
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.978863375477076,
            "upper_bound": 113.79514269671596
          },
          "point_estimate": 99.90553051949829,
          "standard_error": 26.145980796202977
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52748.21252008605,
            "upper_bound": 52824.58455814595
          },
          "point_estimate": 52777.76824684749,
          "standard_error": 19.47178973154935
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.78572335170605,
            "upper_bound": 82.63035696968468
          },
          "point_estimate": 70.31938695165489,
          "standard_error": 9.648600074475564
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15423.342551212265,
            "upper_bound": 15458.472833475234
          },
          "point_estimate": 15440.346410748916,
          "standard_error": 8.983516787477496
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15416.79407650672,
            "upper_bound": 15463.35573648361
          },
          "point_estimate": 15434.95003547609,
          "standard_error": 14.095309649814524
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.64762652811752,
            "upper_bound": 49.74052619760021
          },
          "point_estimate": 32.64770559322582,
          "standard_error": 11.848128763326732
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15409.764189563011,
            "upper_bound": 15448.893648577849
          },
          "point_estimate": 15425.832322126576,
          "standard_error": 10.07670357374987
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.146424017062365,
            "upper_bound": 38.14317619548892
          },
          "point_estimate": 29.885725972430883,
          "standard_error": 5.529205986658789
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26281.88773553744,
            "upper_bound": 26329.94498941799
          },
          "point_estimate": 26305.034747009435,
          "standard_error": 12.30117528684
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26270.49486714976,
            "upper_bound": 26334.724516908213
          },
          "point_estimate": 26301.577093397747,
          "standard_error": 18.810164655935537
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.051078950906064,
            "upper_bound": 70.73815475863711
          },
          "point_estimate": 42.96804813209262,
          "standard_error": 15.605814870994724
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26290.17003887379,
            "upper_bound": 26339.04870992534
          },
          "point_estimate": 26314.549260304913,
          "standard_error": 12.22572396947521
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.29406554190127,
            "upper_bound": 52.13094583617529
          },
          "point_estimate": 40.92646253860299,
          "standard_error": 7.668294837006835
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16500.438538155417,
            "upper_bound": 16537.598192944843
          },
          "point_estimate": 16518.758160292917,
          "standard_error": 9.520254805275476
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16487.95577535243,
            "upper_bound": 16543.928982871003
          },
          "point_estimate": 16519.757766156334,
          "standard_error": 17.916810878193473
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.457557553556262,
            "upper_bound": 53.20381378751733
          },
          "point_estimate": 36.87112598782026,
          "standard_error": 13.276707442854692
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16497.16668893264,
            "upper_bound": 16534.475768690838
          },
          "point_estimate": 16517.01540015237,
          "standard_error": 9.535855082201175
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.530482796296184,
            "upper_bound": 38.24622873350213
          },
          "point_estimate": 31.629463356074677,
          "standard_error": 4.686720823311851
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16489.12721727136,
            "upper_bound": 16526.593724412596
          },
          "point_estimate": 16506.44269551262,
          "standard_error": 9.634875785737362
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16482.46917082766,
            "upper_bound": 16526.71697684975
          },
          "point_estimate": 16496.192374035407,
          "standard_error": 12.345058452563022
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.873485125361141,
            "upper_bound": 51.88193372484693
          },
          "point_estimate": 24.79842782233509,
          "standard_error": 11.732398399565827
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16483.538528842735,
            "upper_bound": 16506.178626197692
          },
          "point_estimate": 16494.454075021666,
          "standard_error": 5.753632604709402
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.392305824997328,
            "upper_bound": 42.272419734012345
          },
          "point_estimate": 32.1988516163247,
          "standard_error": 7.408193678266687
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19284.116622631067,
            "upper_bound": 19305.111060894884
          },
          "point_estimate": 19294.238703064115,
          "standard_error": 5.379367045681346
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19282.103081827845,
            "upper_bound": 19312.000106269923
          },
          "point_estimate": 19287.801992561108,
          "standard_error": 7.68456286935118
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7970879205862142,
            "upper_bound": 29.38316990873346
          },
          "point_estimate": 18.14215520394786,
          "standard_error": 7.911332351856425
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19282.543856886183,
            "upper_bound": 19298.536340850773
          },
          "point_estimate": 19289.05427632941,
          "standard_error": 4.0499586148505715
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.144113239461284,
            "upper_bound": 22.214511447401556
          },
          "point_estimate": 17.914112608545064,
          "standard_error": 3.226422414782782
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95113.96442558747,
            "upper_bound": 95201.08729329852
          },
          "point_estimate": 95153.1041797215,
          "standard_error": 22.556777804961065
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95096.80156657964,
            "upper_bound": 95190.18842471714
          },
          "point_estimate": 95128.62679503916,
          "standard_error": 24.90432692545735
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.498743590117344,
            "upper_bound": 107.16425192905449
          },
          "point_estimate": 48.94462195742369,
          "standard_error": 25.280730177999597
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95095.37937014471,
            "upper_bound": 95160.0829278424
          },
          "point_estimate": 95118.83797090636,
          "standard_error": 16.59398230807354
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.632088906009937,
            "upper_bound": 103.4237418168421
          },
          "point_estimate": 75.10083451368158,
          "standard_error": 21.219445783130432
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102218.87194060996,
            "upper_bound": 102267.48836405386
          },
          "point_estimate": 102240.29861646156,
          "standard_error": 12.580972470018182
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102212.89171348314,
            "upper_bound": 102259.45856741571
          },
          "point_estimate": 102232.69600499376,
          "standard_error": 11.142052871767618
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.061495497287345,
            "upper_bound": 55.09589527653364
          },
          "point_estimate": 34.09139581329165,
          "standard_error": 12.455783937198156
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102212.04927819622,
            "upper_bound": 102240.86227065852
          },
          "point_estimate": 102224.89494382024,
          "standard_error": 7.513599200435561
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.644162016510554,
            "upper_bound": 59.74467462568203
          },
          "point_estimate": 41.93151685657362,
          "standard_error": 13.1293691299685
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19998.02650483358,
            "upper_bound": 20036.346290119574
          },
          "point_estimate": 20015.26443102755,
          "standard_error": 9.85873791906594
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19992.432268722467,
            "upper_bound": 20027.253235132157
          },
          "point_estimate": 20011.80154185022,
          "standard_error": 11.18613944478273
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.634524870631831,
            "upper_bound": 47.49361245527521
          },
          "point_estimate": 22.894388672102743,
          "standard_error": 11.121904758749393
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19996.3406264793,
            "upper_bound": 20017.27180985383
          },
          "point_estimate": 20005.95229561188,
          "standard_error": 5.310938048659386
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.943119078377183,
            "upper_bound": 46.599638424505464
          },
          "point_estimate": 33.087121953025445,
          "standard_error": 9.723754365143236
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16778.671702168238,
            "upper_bound": 16820.517523233724
          },
          "point_estimate": 16795.23477980225,
          "standard_error": 11.380943026865676
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16774.91108287123,
            "upper_bound": 16799.028155532083
          },
          "point_estimate": 16782.410492400904,
          "standard_error": 6.252601546288059
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.701542881798626,
            "upper_bound": 27.0092708929457
          },
          "point_estimate": 11.952472669811112,
          "standard_error": 7.043653000487743
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16778.431309502215,
            "upper_bound": 16793.150365212743
          },
          "point_estimate": 16786.03876059246,
          "standard_error": 3.742753755552255
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.861774395387343,
            "upper_bound": 57.178286415533606
          },
          "point_estimate": 37.89388394944773,
          "standard_error": 16.326432605974873
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19347.306809811027,
            "upper_bound": 19389.921140711456
          },
          "point_estimate": 19367.184021220335,
          "standard_error": 10.937579467573942
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19347.06140234167,
            "upper_bound": 19384.964461001713
          },
          "point_estimate": 19360.154585772572,
          "standard_error": 8.243054127959805
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9940436225592637,
            "upper_bound": 61.148729196460366
          },
          "point_estimate": 17.490215056703633,
          "standard_error": 15.452138677048827
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19336.468235316283,
            "upper_bound": 19363.79538772064
          },
          "point_estimate": 19349.387983384364,
          "standard_error": 6.919134712868572
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.462538787901272,
            "upper_bound": 49.380197605003104
          },
          "point_estimate": 36.380057140846866,
          "standard_error": 9.4182052217223
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16776.031492737377,
            "upper_bound": 16803.723406264417
          },
          "point_estimate": 16790.049753774143,
          "standard_error": 7.112602968504213
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16761.1978376953,
            "upper_bound": 16805.482464236273
          },
          "point_estimate": 16799.688086448237,
          "standard_error": 11.9713705933652
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6409776166819466,
            "upper_bound": 40.75456602773261
          },
          "point_estimate": 21.512836916178276,
          "standard_error": 11.66141866563632
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16770.745513885897,
            "upper_bound": 16801.770311299177
          },
          "point_estimate": 16790.47389712272,
          "standard_error": 7.874876598195971
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.060820433574742,
            "upper_bound": 28.88632768390315
          },
          "point_estimate": 23.676193403143664,
          "standard_error": 3.757409751852611
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16688.760287182522,
            "upper_bound": 16724.056829542216
          },
          "point_estimate": 16703.46068870116,
          "standard_error": 9.333136979811703
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16687.860465562204,
            "upper_bound": 16714.46922073938
          },
          "point_estimate": 16691.00681661298,
          "standard_error": 5.994490831516988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2933234238863414,
            "upper_bound": 31.452508999824087
          },
          "point_estimate": 7.417775846219203,
          "standard_error": 7.617304852910884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16687.797657937535,
            "upper_bound": 16717.789368659574
          },
          "point_estimate": 16701.340734277226,
          "standard_error": 8.71843256794361
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.3962457548342115,
            "upper_bound": 44.91694817111309
          },
          "point_estimate": 31.13260533618942,
          "standard_error": 11.59628975534009
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16649.912080579372,
            "upper_bound": 16680.248948433145
          },
          "point_estimate": 16664.84045461321,
          "standard_error": 7.749318348198277
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16645.356530259072,
            "upper_bound": 16693.133760879522
          },
          "point_estimate": 16662.56040426019,
          "standard_error": 11.31821900094121
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.942135743685596,
            "upper_bound": 45.73462663422219
          },
          "point_estimate": 27.535388414415863,
          "standard_error": 10.893633300197225
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16642.842728848005,
            "upper_bound": 16669.520986322583
          },
          "point_estimate": 16655.114445151732,
          "standard_error": 6.744616737398835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.906761881051697,
            "upper_bound": 31.840145628452063
          },
          "point_estimate": 25.899378196054556,
          "standard_error": 4.253907336195208
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19510.65337830635,
            "upper_bound": 19541.543884164166
          },
          "point_estimate": 19525.352177282868,
          "standard_error": 7.923631808150716
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19503.270059235325,
            "upper_bound": 19550.84185963023
          },
          "point_estimate": 19518.35117797523,
          "standard_error": 11.625510844760734
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.364834467476297,
            "upper_bound": 43.56882558083024
          },
          "point_estimate": 25.025727831150384,
          "standard_error": 10.23611498315906
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19512.27086362413,
            "upper_bound": 19535.8992268462
          },
          "point_estimate": 19522.7545153823,
          "standard_error": 5.9081090913973835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.78687546459956,
            "upper_bound": 31.655133492976667
          },
          "point_estimate": 26.485442662432593,
          "standard_error": 4.714005571882727
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16879.546354124774,
            "upper_bound": 16903.20850270688
          },
          "point_estimate": 16890.588827054096,
          "standard_error": 6.065683480276612
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16875.621397267336,
            "upper_bound": 16899.737064965197
          },
          "point_estimate": 16888.96498839907,
          "standard_error": 5.461892369619774
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5572957597071313,
            "upper_bound": 31.695957892040337
          },
          "point_estimate": 16.122462513461066,
          "standard_error": 7.507557321281626
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16872.572818277076,
            "upper_bound": 16890.739376664438
          },
          "point_estimate": 16881.426488685327,
          "standard_error": 4.650323674458134
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.582711748133558,
            "upper_bound": 27.747064224355476
          },
          "point_estimate": 20.264032254501355,
          "standard_error": 5.228752364379983
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19665.902420074493,
            "upper_bound": 19694.27673139598
          },
          "point_estimate": 19680.27609380254,
          "standard_error": 7.234489640535319
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19663.999663743823,
            "upper_bound": 19695.481912004347
          },
          "point_estimate": 19681.91149511135,
          "standard_error": 8.267681925482627
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.491881298579249,
            "upper_bound": 40.88280644419932
          },
          "point_estimate": 17.81204413244147,
          "standard_error": 9.210087007759478
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19663.498076266656,
            "upper_bound": 19689.301113243328
          },
          "point_estimate": 19677.71603377611,
          "standard_error": 6.779202327779126
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.516461594904335,
            "upper_bound": 31.93126475453935
          },
          "point_estimate": 24.09933713041794,
          "standard_error": 5.18090740005875
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58652.37998407561,
            "upper_bound": 58831.242466738586
          },
          "point_estimate": 58753.772084168086,
          "standard_error": 45.58336392985241
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58740.720671521034,
            "upper_bound": 58807.14221503056
          },
          "point_estimate": 58779.3306364617,
          "standard_error": 22.259592489521143
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.47817349183124,
            "upper_bound": 171.57975957520677
          },
          "point_estimate": 51.72656562697353,
          "standard_error": 35.69206600572094
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58591.29663486459,
            "upper_bound": 58799.056378973015
          },
          "point_estimate": 58729.13353507334,
          "standard_error": 56.23301003833708
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.195558267382708,
            "upper_bound": 223.52929951641028
          },
          "point_estimate": 151.69075113386893,
          "standard_error": 57.595022672995974
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 884775.1280905139,
            "upper_bound": 885352.5801886102
          },
          "point_estimate": 885044.6666883975,
          "standard_error": 148.11311221537483
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 884625.4149659864,
            "upper_bound": 885347.8670634921
          },
          "point_estimate": 884967.0694444445,
          "standard_error": 196.2617639302768
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.45842007950166,
            "upper_bound": 817.4950354865373
          },
          "point_estimate": 520.6762514704045,
          "standard_error": 191.79649715386063
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 884613.8921920146,
            "upper_bound": 884926.9855124102
          },
          "point_estimate": 884710.5975262832,
          "standard_error": 80.65362788607537
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.7690604143135,
            "upper_bound": 646.2015303306217
          },
          "point_estimate": 493.5077965986026,
          "standard_error": 109.79716353301023
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201636.78841666665,
            "upper_bound": 202102.62294544748
          },
          "point_estimate": 201846.17267056913,
          "standard_error": 119.88836555165778
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201587.85324585636,
            "upper_bound": 202037.0179952644
          },
          "point_estimate": 201693.2682627379,
          "standard_error": 113.36924244764644
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.48988601018992,
            "upper_bound": 577.9999213639888
          },
          "point_estimate": 228.42094243641577,
          "standard_error": 154.94681591609867
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201519.80580110496,
            "upper_bound": 201940.81496840663
          },
          "point_estimate": 201690.47937145727,
          "standard_error": 107.97144525319912
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.47189888101582,
            "upper_bound": 540.6546261625339
          },
          "point_estimate": 398.3395868808298,
          "standard_error": 108.62190503667937
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6031.799382383241,
            "upper_bound": 6055.472149870662
          },
          "point_estimate": 6043.613894039988,
          "standard_error": 6.084765826016338
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6025.949560676392,
            "upper_bound": 6066.248756631299
          },
          "point_estimate": 6039.850677866195,
          "standard_error": 10.569509752983736
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9973368748375284,
            "upper_bound": 33.94335266882609
          },
          "point_estimate": 27.757913399771184,
          "standard_error": 8.199544114093149
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6025.427254892463,
            "upper_bound": 6047.68970469186
          },
          "point_estimate": 6035.423198783975,
          "standard_error": 5.620264316421135
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.99592385923345,
            "upper_bound": 23.69125387074566
          },
          "point_estimate": 20.265862755765017,
          "standard_error": 2.7449085751778135
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6069.272003872847,
            "upper_bound": 6085.775985556108
          },
          "point_estimate": 6077.385737216182,
          "standard_error": 4.235006362331808
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6063.888741502238,
            "upper_bound": 6092.787800070008
          },
          "point_estimate": 6077.147570883767,
          "standard_error": 6.858241085400157
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.1702374271369163,
            "upper_bound": 23.166483237361344
          },
          "point_estimate": 19.720876403324944,
          "standard_error": 5.812173293189594
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6070.014874182417,
            "upper_bound": 6087.96141215837
          },
          "point_estimate": 6079.203593123839,
          "standard_error": 4.532708256487436
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.395892826536128,
            "upper_bound": 16.804126643386734
          },
          "point_estimate": 14.082208303727882,
          "standard_error": 2.0888448078309367
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15752.622589608509,
            "upper_bound": 15806.087191405846
          },
          "point_estimate": 15776.247581861378,
          "standard_error": 13.848796982087226
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15744.242819956617,
            "upper_bound": 15800.720194194815
          },
          "point_estimate": 15757.22514461316,
          "standard_error": 14.253196761395747
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7664655937663558,
            "upper_bound": 65.925774981427
          },
          "point_estimate": 26.89086721456564,
          "standard_error": 15.917486311802618
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15744.649879062446,
            "upper_bound": 15768.024599364324
          },
          "point_estimate": 15754.766046708171,
          "standard_error": 6.062051939931352
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.16732845194228,
            "upper_bound": 63.06150919314345
          },
          "point_estimate": 46.107812687472034,
          "standard_error": 13.269612305255825
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.84548803208973,
            "upper_bound": 56.876066676015554
          },
          "point_estimate": 56.85953451087471,
          "standard_error": 0.007847835882089507
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.8389222560331,
            "upper_bound": 56.875672454539334
          },
          "point_estimate": 56.85098818515972,
          "standard_error": 0.011175490799636124
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0034045638183691387,
            "upper_bound": 0.04599078500975913
          },
          "point_estimate": 0.024070561542366347,
          "standard_error": 0.010203814909280592
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.839925762381974,
            "upper_bound": 56.86345328153684
          },
          "point_estimate": 56.84813071595892,
          "standard_error": 0.0060615038616222555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01260647846725489,
            "upper_bound": 0.03502925293636137
          },
          "point_estimate": 0.026084561915398572,
          "standard_error": 0.006364560214811552
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.67326324170339,
            "upper_bound": 34.76794244984816
          },
          "point_estimate": 34.71795082460826,
          "standard_error": 0.02426786260997
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.657920595808974,
            "upper_bound": 34.77867808021049
          },
          "point_estimate": 34.70950171848098,
          "standard_error": 0.021972741989607525
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004844148563424948,
            "upper_bound": 0.13850594649990086
          },
          "point_estimate": 0.045890460879133434,
          "standard_error": 0.03881109624824737
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.67656528613359,
            "upper_bound": 34.735850160914566
          },
          "point_estimate": 34.70711684453285,
          "standard_error": 0.01477623014995822
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03331620087580404,
            "upper_bound": 0.10442762571128716
          },
          "point_estimate": 0.0810929604282906,
          "standard_error": 0.01829029227201911
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.434848986561605,
            "upper_bound": 38.508296361497585
          },
          "point_estimate": 38.473145707832686,
          "standard_error": 0.018779077645844567
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.43329865569685,
            "upper_bound": 38.51943647267539
          },
          "point_estimate": 38.47761280748441,
          "standard_error": 0.021223260164399665
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01441518874292737,
            "upper_bound": 0.10380943191028832
          },
          "point_estimate": 0.06385396259255384,
          "standard_error": 0.021702104153010337
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.449555871223986,
            "upper_bound": 38.50098576609248
          },
          "point_estimate": 38.47459093513634,
          "standard_error": 0.01316329781436414
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030052587672817738,
            "upper_bound": 0.0843092785963845
          },
          "point_estimate": 0.06250668875991042,
          "standard_error": 0.014493092641652484
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.26014569853977,
            "upper_bound": 29.291048051280686
          },
          "point_estimate": 29.2752902269557,
          "standard_error": 0.007895144002821012
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.25371609403983,
            "upper_bound": 29.299063314751855
          },
          "point_estimate": 29.27082867409263,
          "standard_error": 0.013145094159410462
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00509153059804067,
            "upper_bound": 0.044213737842587135
          },
          "point_estimate": 0.02741948529143527,
          "standard_error": 0.010684924128409537
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.258639327771384,
            "upper_bound": 29.298450768629458
          },
          "point_estimate": 29.28133136299949,
          "standard_error": 0.010290442131153371
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015709730971932914,
            "upper_bound": 0.03163524888141497
          },
          "point_estimate": 0.026292469977336655,
          "standard_error": 0.003999833492206763
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.28838485474513,
            "upper_bound": 21.30044852530766
          },
          "point_estimate": 21.29387496410808,
          "standard_error": 0.0031016039266362244
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.28871414198391,
            "upper_bound": 21.30011140847372
          },
          "point_estimate": 21.290228309969265,
          "standard_error": 0.0028501751940132563
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005926240944388318,
            "upper_bound": 0.01545551144019319
          },
          "point_estimate": 0.002694251994408373,
          "standard_error": 0.004496147631248472
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.28896127587601,
            "upper_bound": 21.29325248579039
          },
          "point_estimate": 21.290487927412983,
          "standard_error": 0.00111104805298198
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0036454264322165727,
            "upper_bound": 0.014276613344972544
          },
          "point_estimate": 0.010369981972975782,
          "standard_error": 0.0029140096983985
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.655190355022846,
            "upper_bound": 34.68707334904899
          },
          "point_estimate": 34.66854563884728,
          "standard_error": 0.008394559665048507
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.653621937475464,
            "upper_bound": 34.67566011282839
          },
          "point_estimate": 34.65619804864674,
          "standard_error": 0.006262845643219393
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007644307149198799,
            "upper_bound": 0.02885445584343357
          },
          "point_estimate": 0.004156823404921397,
          "standard_error": 0.007600478108323303
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.65504925216378,
            "upper_bound": 34.66152149737395
          },
          "point_estimate": 34.657204902784905,
          "standard_error": 0.0016821262037674943
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002185625337591624,
            "upper_bound": 0.04117438633471939
          },
          "point_estimate": 0.02803432954244933,
          "standard_error": 0.010778733578490091
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.41330078933176,
            "upper_bound": 49.45613271185346
          },
          "point_estimate": 49.43306473326002,
          "standard_error": 0.0110282201616714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.4026594896293,
            "upper_bound": 49.45423158320254
          },
          "point_estimate": 49.4271124880256,
          "standard_error": 0.013888075538278404
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004177378840023434,
            "upper_bound": 0.059014320661107206
          },
          "point_estimate": 0.03378581121356655,
          "standard_error": 0.01403843192622712
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.403430487045945,
            "upper_bound": 49.44438698331368
          },
          "point_estimate": 49.42234850095111,
          "standard_error": 0.010475253735670386
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016646060317101537,
            "upper_bound": 0.04921871039801003
          },
          "point_estimate": 0.03675668398441572,
          "standard_error": 0.00900331556666243
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.2625361622649,
            "upper_bound": 51.478087309982016
          },
          "point_estimate": 51.36515824754714,
          "standard_error": 0.05580826406823831
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.228700444151,
            "upper_bound": 51.50906153403934
          },
          "point_estimate": 51.25328931066073,
          "standard_error": 0.09212173257549292
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00973722901831046,
            "upper_bound": 0.3676692143817083
          },
          "point_estimate": 0.06039723816879575,
          "standard_error": 0.09677703622044392
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.22647877439474,
            "upper_bound": 51.3100226547882
          },
          "point_estimate": 51.2524661996118,
          "standard_error": 0.0219036990650963
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08854692192200495,
            "upper_bound": 0.24445982801653124
          },
          "point_estimate": 0.18589220042709137,
          "standard_error": 0.04202905222513303
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.829092781993765,
            "upper_bound": 40.885350710612464
          },
          "point_estimate": 40.85238301822648,
          "standard_error": 0.014737609070667344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.834096764829646,
            "upper_bound": 40.85411073882137
          },
          "point_estimate": 40.84202344850313,
          "standard_error": 0.006691893465187338
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002295875747111013,
            "upper_bound": 0.04503127562679149
          },
          "point_estimate": 0.013931537864981351,
          "standard_error": 0.009593740501124156
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.83580350883751,
            "upper_bound": 40.84957834111958
          },
          "point_estimate": 40.84226920769247,
          "standard_error": 0.003553953984941964
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007506107409890336,
            "upper_bound": 0.07360585297820751
          },
          "point_estimate": 0.04912952639512542,
          "standard_error": 0.020416340784435553
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.22022137463081,
            "upper_bound": 59.26064685379053
          },
          "point_estimate": 59.237154663483565,
          "standard_error": 0.010613626412755477
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.21293323102287,
            "upper_bound": 59.24631020514224
          },
          "point_estimate": 59.22810057394087,
          "standard_error": 0.010166842862112084
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032144984316322387,
            "upper_bound": 0.04081870842988031
          },
          "point_estimate": 0.022390697162675447,
          "standard_error": 0.009121224137950806
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.21914589793752,
            "upper_bound": 59.243368209608995
          },
          "point_estimate": 59.232265937113304,
          "standard_error": 0.006323303862773263
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011497993475732117,
            "upper_bound": 0.052153862880378855
          },
          "point_estimate": 0.035309393759523376,
          "standard_error": 0.012971201682310264
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.32799725550363,
            "upper_bound": 44.379190965887105
          },
          "point_estimate": 44.348309706864995,
          "standard_error": 0.013868630850689866
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.328064107339735,
            "upper_bound": 44.348116395753856
          },
          "point_estimate": 44.33750953317137,
          "standard_error": 0.006378702998970095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002873088812379676,
            "upper_bound": 0.033277451101021586
          },
          "point_estimate": 0.0142127611374756,
          "standard_error": 0.007967603481987186
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.33077732244854,
            "upper_bound": 44.3487638684244
          },
          "point_estimate": 44.34021111799717,
          "standard_error": 0.004755342492779413
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007818932296889604,
            "upper_bound": 0.0701707496580099
          },
          "point_estimate": 0.04613618306193397,
          "standard_error": 0.020287873016438886
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.86762266130385,
            "upper_bound": 39.90252871911504
          },
          "point_estimate": 39.88321141077506,
          "standard_error": 0.009001703047638419
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.86339260472434,
            "upper_bound": 39.90248653357553
          },
          "point_estimate": 39.87163508392284,
          "standard_error": 0.008432610519868718
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002751975398244327,
            "upper_bound": 0.043237257675372544
          },
          "point_estimate": 0.014250046085176956,
          "standard_error": 0.00978946624451972
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.8627801828839,
            "upper_bound": 39.88276340536901
          },
          "point_estimate": 39.87105284074919,
          "standard_error": 0.005147514236034397
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007772689039234237,
            "upper_bound": 0.03898343050624748
          },
          "point_estimate": 0.030011284290097463,
          "standard_error": 0.008158749844960208
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.72491682013393,
            "upper_bound": 66.78429983738285
          },
          "point_estimate": 66.7493379147933,
          "standard_error": 0.01565005689030122
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.7174441157099,
            "upper_bound": 66.75997475976177
          },
          "point_estimate": 66.73364462858476,
          "standard_error": 0.01263119717162907
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0051240651878526194,
            "upper_bound": 0.05263278497829286
          },
          "point_estimate": 0.030843263701007727,
          "standard_error": 0.011330281359951956
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.72193968245847,
            "upper_bound": 66.75057274432497
          },
          "point_estimate": 66.73723095459421,
          "standard_error": 0.007277536597683895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014884479018130092,
            "upper_bound": 0.07743394138419389
          },
          "point_estimate": 0.05192878723152962,
          "standard_error": 0.02012073764112048
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149198.4892980419,
            "upper_bound": 149552.95367793716
          },
          "point_estimate": 149363.65082081055,
          "standard_error": 90.91751363133844
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149126.8679417122,
            "upper_bound": 149530.68673155736
          },
          "point_estimate": 149317.90573770492,
          "standard_error": 116.09786056006548
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.32081632664709,
            "upper_bound": 497.5271994185086
          },
          "point_estimate": 287.84259904278207,
          "standard_error": 103.11546071661542
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149162.4568715039,
            "upper_bound": 149452.66062691863
          },
          "point_estimate": 149275.3290930381,
          "standard_error": 75.88419604269315
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.0419422822267,
            "upper_bound": 407.3713610558944
          },
          "point_estimate": 302.50741011003566,
          "standard_error": 73.76026677702431
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57534.6980060089,
            "upper_bound": 57601.396489790546
          },
          "point_estimate": 57565.518491372815,
          "standard_error": 17.15664855716895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57530.703887884265,
            "upper_bound": 57603.47373417721
          },
          "point_estimate": 57553.98364978903,
          "standard_error": 14.447513909238138
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1162116732176697,
            "upper_bound": 92.27464519724312
          },
          "point_estimate": 28.847821362853416,
          "standard_error": 22.288739733231576
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57513.970024363705,
            "upper_bound": 57555.67160103141
          },
          "point_estimate": 57536.69037070524,
          "standard_error": 11.361642854980952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.919443489800447,
            "upper_bound": 74.68206508340863
          },
          "point_estimate": 57.260330124813834,
          "standard_error": 14.319839706949482
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231395.4703282161,
            "upper_bound": 231832.30288238396
          },
          "point_estimate": 231587.8308285614,
          "standard_error": 111.48650813651724
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231353.9216772152,
            "upper_bound": 231843.9300281294
          },
          "point_estimate": 231433.4208860759,
          "standard_error": 107.17293916943682
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.54065905567081,
            "upper_bound": 527.5442589253395
          },
          "point_estimate": 122.492533822126,
          "standard_error": 108.79394795001411
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231368.17942765984,
            "upper_bound": 231489.13264708285
          },
          "point_estimate": 231419.4629459149,
          "standard_error": 30.701916111634933
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.25758004122316,
            "upper_bound": 467.4604331111729
          },
          "point_estimate": 370.2434029488339,
          "standard_error": 104.72098829787068
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 729595.774940992,
            "upper_bound": 730993.9762918253
          },
          "point_estimate": 730306.4325349206,
          "standard_error": 359.5522978660809
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 729104.6,
            "upper_bound": 731155.9342857143
          },
          "point_estimate": 730490.917,
          "standard_error": 527.1042677644782
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.97796377543087,
            "upper_bound": 2107.136316990948
          },
          "point_estimate": 1009.7995833724744,
          "standard_error": 511.7883176945659
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 729155.3311724085,
            "upper_bound": 730671.4617676768
          },
          "point_estimate": 729897.3759480519,
          "standard_error": 391.5853765296211
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 660.972728120112,
            "upper_bound": 1477.9939949542209
          },
          "point_estimate": 1195.6940866757695,
          "standard_error": 200.96210662405184
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1584325.4307453416,
            "upper_bound": 1586285.4563405798
          },
          "point_estimate": 1585324.4122929606,
          "standard_error": 503.357896677008
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1583685.9989648033,
            "upper_bound": 1586782.4076086956
          },
          "point_estimate": 1585811.322463768,
          "standard_error": 919.0484481067544
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399.94745811685954,
            "upper_bound": 2692.263229920177
          },
          "point_estimate": 2127.939214395623,
          "standard_error": 621.9255062708061
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1584356.9791624926,
            "upper_bound": 1586583.537187789
          },
          "point_estimate": 1585766.166233766,
          "standard_error": 569.8120683872274
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1068.958217826947,
            "upper_bound": 1939.887873233591
          },
          "point_estimate": 1674.7652643045424,
          "standard_error": 222.7562869034346
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68271.40048454469,
            "upper_bound": 68453.50754610485
          },
          "point_estimate": 68363.14131161236,
          "standard_error": 46.80031509878894
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68188.23496240602,
            "upper_bound": 68488.32027986634
          },
          "point_estimate": 68399.93703007518,
          "standard_error": 71.02646054646901
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.46654976718493,
            "upper_bound": 274.1890293426939
          },
          "point_estimate": 191.31433831401569,
          "standard_error": 69.80672800938525
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68268.71135061013,
            "upper_bound": 68405.87831346612
          },
          "point_estimate": 68343.7293672493,
          "standard_error": 34.629770571403625
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.92247005469652,
            "upper_bound": 189.37927295701851
          },
          "point_estimate": 156.51175083157736,
          "standard_error": 24.20312427528281
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 196679.32220120123,
            "upper_bound": 197281.4251602799
          },
          "point_estimate": 196977.8438260403,
          "standard_error": 154.2233198217512
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 196483.94216216216,
            "upper_bound": 197502.5972972973
          },
          "point_estimate": 196974.5141141141,
          "standard_error": 312.58941780811955
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.687835494383174,
            "upper_bound": 825.2481691349651
          },
          "point_estimate": 727.9059872302215,
          "standard_error": 220.4283610442519
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 196504.96400548017,
            "upper_bound": 197093.6633272196
          },
          "point_estimate": 196733.61885573887,
          "standard_error": 150.9370029115152
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 343.61990402380303,
            "upper_bound": 596.3438449344617
          },
          "point_estimate": 515.6812530703353,
          "standard_error": 64.70418911581676
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132438.4814649134,
            "upper_bound": 132682.03739999997
          },
          "point_estimate": 132554.58833997115,
          "standard_error": 62.00779065252804
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132330.39,
            "upper_bound": 132656.26545454544
          },
          "point_estimate": 132582.62262626263,
          "standard_error": 93.23630385105442
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.051318438015701,
            "upper_bound": 387.45601290918125
          },
          "point_estimate": 152.748770960895,
          "standard_error": 104.79457154101802
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132396.41574743114,
            "upper_bound": 132616.47458359823
          },
          "point_estimate": 132519.1782384888,
          "standard_error": 56.16786182734041
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.73974246111628,
            "upper_bound": 273.4230718515426
          },
          "point_estimate": 206.89501692959232,
          "standard_error": 44.91860003074199
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 793678.517584541,
            "upper_bound": 795837.0897115037
          },
          "point_estimate": 794678.4298852657,
          "standard_error": 552.2336603627115
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 793112.95,
            "upper_bound": 795648.0217391305
          },
          "point_estimate": 794687.6317934783,
          "standard_error": 729.2880566218729
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 365.1171022135103,
            "upper_bound": 3126.137472670337
          },
          "point_estimate": 1978.9099839977496,
          "standard_error": 689.9829231979072
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 793403.5118381403,
            "upper_bound": 794825.8930752025
          },
          "point_estimate": 794118.1564652738,
          "standard_error": 366.51345702093533
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 919.8655202834228,
            "upper_bound": 2482.3850549514186
          },
          "point_estimate": 1840.3095449873615,
          "standard_error": 445.5880105470952
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59935.944171708834,
            "upper_bound": 60104.5110671067
          },
          "point_estimate": 60016.31043735922,
          "standard_error": 43.39607654469585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59899.09735973597,
            "upper_bound": 60173.23597359736
          },
          "point_estimate": 59988.26870187018,
          "standard_error": 60.434062703733346
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.291813843677796,
            "upper_bound": 231.9531600459248
          },
          "point_estimate": 140.37814632516884,
          "standard_error": 58.2322117674457
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59885.77694748968,
            "upper_bound": 60018.610113311675
          },
          "point_estimate": 59932.61812181218,
          "standard_error": 33.88264871970617
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.16377263193009,
            "upper_bound": 177.1798132532784
          },
          "point_estimate": 144.68395477825445,
          "standard_error": 25.788821895217275
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118471.12174489106,
            "upper_bound": 118712.34829571894
          },
          "point_estimate": 118588.6111187374,
          "standard_error": 61.77611529667641
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118463.06535016288,
            "upper_bound": 118764.2996742671
          },
          "point_estimate": 118528.52312703582,
          "standard_error": 84.69945541096219
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.445363514057107,
            "upper_bound": 347.71430750761266
          },
          "point_estimate": 168.10093980877227,
          "standard_error": 91.5176097318556
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118369.76504744373,
            "upper_bound": 118608.8487405768
          },
          "point_estimate": 118479.63896949956,
          "standard_error": 60.770074409123424
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.3361887762521,
            "upper_bound": 262.5094651885851
          },
          "point_estimate": 205.5341768173788,
          "standard_error": 38.599658129506395
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 358288.55021942116,
            "upper_bound": 359042.3111721522
          },
          "point_estimate": 358646.41827497666,
          "standard_error": 193.58100705433628
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 358081.54761904763,
            "upper_bound": 359059.0326797386
          },
          "point_estimate": 358519.81348039216,
          "standard_error": 255.28497495436395
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.19513566794888,
            "upper_bound": 1135.7616467479038
          },
          "point_estimate": 691.1438962101124,
          "standard_error": 250.53214670700024
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 358239.23475075595,
            "upper_bound": 358779.01972273283
          },
          "point_estimate": 358510.9026228673,
          "standard_error": 136.9445658660644
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 338.49999275645627,
            "upper_bound": 854.9288328485522
          },
          "point_estimate": 647.6770851664315,
          "standard_error": 141.96124937768576
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56821.560429427074,
            "upper_bound": 56867.807878348205
          },
          "point_estimate": 56843.01975161211,
          "standard_error": 11.856805364228252
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56817.29151785714,
            "upper_bound": 56862.03734375
          },
          "point_estimate": 56836.29333767361,
          "standard_error": 13.145471472946593
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.604691212995546,
            "upper_bound": 60.23546389154702
          },
          "point_estimate": 33.64752274899146,
          "standard_error": 13.386560723380391
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56822.28905330882,
            "upper_bound": 56853.54072705518
          },
          "point_estimate": 56839.44633522727,
          "standard_error": 8.199893520069526
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.489435706693257,
            "upper_bound": 54.39936284649291
          },
          "point_estimate": 39.47828668315309,
          "standard_error": 10.42469198606949
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22364.16376596802,
            "upper_bound": 22427.83639743927
          },
          "point_estimate": 22398.02029690043,
          "standard_error": 16.37675107656677
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22358.41252302026,
            "upper_bound": 22442.112128387267
          },
          "point_estimate": 22409.703550235317,
          "standard_error": 22.69652953829925
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.36290727524886,
            "upper_bound": 96.61465508032695
          },
          "point_estimate": 51.65918924861353,
          "standard_error": 19.714825543810807
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22376.94228911119,
            "upper_bound": 22433.342240815266
          },
          "point_estimate": 22400.14554383615,
          "standard_error": 14.238513417038655
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.808668145152925,
            "upper_bound": 72.05678193572665
          },
          "point_estimate": 54.66663169046722,
          "standard_error": 12.16265518131572
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-repeated-rare-huge_common-m"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1857889.4616951388,
            "upper_bound": 1861510.5860555556
          },
          "point_estimate": 1859742.9928948416,
          "standard_error": 926.4281075571382
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1856383.6666666667,
            "upper_bound": 1862024.7256944445
          },
          "point_estimate": 1860686.509375,
          "standard_error": 1455.707976658995
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 544.0863915906392,
            "upper_bound": 4962.128677904481
          },
          "point_estimate": 2972.0116705695173,
          "standard_error": 1210.7710871303025
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1858408.0550884956,
            "upper_bound": 1862108.1857064397
          },
          "point_estimate": 1860586.9698701296,
          "standard_error": 947.630710491102
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1779.6321106323792,
            "upper_bound": 3709.9805961784978
          },
          "point_estimate": 3079.428007984132,
          "standard_error": 484.5648889099465
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-repeated-rare-small_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3686.71580990428,
            "upper_bound": 3695.200119487607
          },
          "point_estimate": 3691.489208109563,
          "standard_error": 2.2186950396854153
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3687.184346766826,
            "upper_bound": 3696.0742691097353
          },
          "point_estimate": 3694.788237798757,
          "standard_error": 2.1509603452890467
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.18255204815848391,
            "upper_bound": 11.127876878371842
          },
          "point_estimate": 1.999693692123188,
          "standard_error": 2.5619152714422975
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3693.520597269689,
            "upper_bound": 3695.894572254497
          },
          "point_estimate": 3695.120826813021,
          "standard_error": 0.6244170043519888
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8027872094314852,
            "upper_bound": 10.510141353164585
          },
          "point_estimate": 7.387589050521286,
          "standard_error": 2.446486023727109
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51926.74525120859,
            "upper_bound": 52254.50707639879
          },
          "point_estimate": 52068.296347677926,
          "standard_error": 85.39636730029704
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51892.292261055634,
            "upper_bound": 52167.30977175463
          },
          "point_estimate": 51973.32089079093,
          "standard_error": 71.9097087367314
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.751597653442886,
            "upper_bound": 352.949737813771
          },
          "point_estimate": 146.1864077589218,
          "standard_error": 83.66214447357378
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51898.51369787438,
            "upper_bound": 52040.61799496474
          },
          "point_estimate": 51971.765915111995,
          "standard_error": 36.138212364601706
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.71683392982388,
            "upper_bound": 402.52155784257377
          },
          "point_estimate": 283.80907359133033,
          "standard_error": 93.1070954283792
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52655.89881003277,
            "upper_bound": 52741.164485041416
          },
          "point_estimate": 52693.78667034736,
          "standard_error": 21.962821270672148
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52629.38502415459,
            "upper_bound": 52720.3481884058
          },
          "point_estimate": 52680.646159420285,
          "standard_error": 19.97760588916688
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.63267689202592,
            "upper_bound": 99.38841698913107
          },
          "point_estimate": 67.43099246228284,
          "standard_error": 25.447326271498948
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52659.91199820587,
            "upper_bound": 52708.792827854304
          },
          "point_estimate": 52685.50854507811,
          "standard_error": 12.244593238722988
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.425329764905698,
            "upper_bound": 104.55107651215997
          },
          "point_estimate": 73.34255173870527,
          "standard_error": 22.825531890625538
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53181.69150142927,
            "upper_bound": 53349.52942963031
          },
          "point_estimate": 53255.900559564485,
          "standard_error": 43.372381226596694
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53146.58167747333,
            "upper_bound": 53332.451903367495
          },
          "point_estimate": 53225.131863103954,
          "standard_error": 38.36274064899076
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.489827758690845,
            "upper_bound": 189.01659105131296
          },
          "point_estimate": 78.90182985836482,
          "standard_error": 48.75605651668961
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53181.44548595009,
            "upper_bound": 53239.36604834448
          },
          "point_estimate": 53212.65774372041,
          "standard_error": 14.798582789292006
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.37404848659319,
            "upper_bound": 198.8691271385189
          },
          "point_estimate": 144.65057892693426,
          "standard_error": 42.5286179678215
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15226.098484583452,
            "upper_bound": 15248.035220169792
          },
          "point_estimate": 15236.497148871164,
          "standard_error": 5.622153653235871
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15222.17882381752,
            "upper_bound": 15253.968324937028
          },
          "point_estimate": 15229.179275702025,
          "standard_error": 8.51430646064821
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6468719775690603,
            "upper_bound": 30.457377406378345
          },
          "point_estimate": 17.24173933705538,
          "standard_error": 7.564845258964713
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15228.447912105506,
            "upper_bound": 15244.666150478695
          },
          "point_estimate": 15236.535359350975,
          "standard_error": 4.121225851165215
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.23928964790647,
            "upper_bound": 22.598686772427104
          },
          "point_estimate": 18.726306542865427,
          "standard_error": 3.3407602450685214
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26404.219575699855,
            "upper_bound": 26453.64792582973
          },
          "point_estimate": 26427.67723007215,
          "standard_error": 12.703345943458336
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26397.355894949495,
            "upper_bound": 26463.825316017315
          },
          "point_estimate": 26409.984545454543,
          "standard_error": 17.52271830860534
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.1918492528513775,
            "upper_bound": 68.00536920720812
          },
          "point_estimate": 41.72774160100228,
          "standard_error": 18.484186901317848
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26408.213166361016,
            "upper_bound": 26448.954850682996
          },
          "point_estimate": 26425.853225029517,
          "standard_error": 10.332513139419673
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.960738851952204,
            "upper_bound": 53.39031377450888
          },
          "point_estimate": 42.346421187955286,
          "standard_error": 8.256732141545369
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16615.011066758834,
            "upper_bound": 16633.56512733782
          },
          "point_estimate": 16623.53797179543,
          "standard_error": 4.751662888165156
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16612.232646834476,
            "upper_bound": 16634.093412880025
          },
          "point_estimate": 16618.007866132724,
          "standard_error": 5.656104826827558
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.194609846623127,
            "upper_bound": 24.175352394597382
          },
          "point_estimate": 10.18997690719523,
          "standard_error": 5.7702105754080995
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16614.45486270023,
            "upper_bound": 16621.809522861986
          },
          "point_estimate": 16617.497554162084,
          "standard_error": 1.8601863188550671
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.431406750279978,
            "upper_bound": 20.06982642957933
          },
          "point_estimate": 15.867130087626837,
          "standard_error": 3.6474065909283544
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16616.81207580404,
            "upper_bound": 16644.381942148764
          },
          "point_estimate": 16627.93155521667,
          "standard_error": 7.411467350363742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16615.480716253445,
            "upper_bound": 16633.582552800734
          },
          "point_estimate": 16618.08019589838,
          "standard_error": 4.37214679653832
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.610687558029182,
            "upper_bound": 20.51818298283548
          },
          "point_estimate": 5.880324706207522,
          "standard_error": 5.275883041012874
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16617.644896030477,
            "upper_bound": 16626.650078769344
          },
          "point_estimate": 16621.519184763813,
          "standard_error": 2.2589255192953237
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.5013535798316604,
            "upper_bound": 36.728830928638
          },
          "point_estimate": 24.684777320328855,
          "standard_error": 10.106967834910604
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19272.06121159245,
            "upper_bound": 19319.896533646915
          },
          "point_estimate": 19290.5221356498,
          "standard_error": 13.27183639622382
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19270.94184811471,
            "upper_bound": 19290.714020180563
          },
          "point_estimate": 19275.29102496017,
          "standard_error": 6.117730715872094
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3532239359154197,
            "upper_bound": 26.960997848485164
          },
          "point_estimate": 7.582085553123902,
          "standard_error": 7.722200039247515
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19270.740505867776,
            "upper_bound": 19277.789374694712
          },
          "point_estimate": 19274.25600623487,
          "standard_error": 1.8200052822860435
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.042749252029785,
            "upper_bound": 67.35143752543874
          },
          "point_estimate": 44.157938374737,
          "standard_error": 20.24407996065673
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93441.0677529762,
            "upper_bound": 93541.72474725274
          },
          "point_estimate": 93480.79904212456,
          "standard_error": 27.420132338925175
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93434.2435897436,
            "upper_bound": 93491.96128205128
          },
          "point_estimate": 93449.6673076923,
          "standard_error": 16.313043931811134
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.371378348417467,
            "upper_bound": 67.03157611764206
          },
          "point_estimate": 33.8539665784627,
          "standard_error": 17.06609248299309
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93440.53765247932,
            "upper_bound": 93468.60031003854
          },
          "point_estimate": 93452.91124875125,
          "standard_error": 7.107780612406713
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.603967805185704,
            "upper_bound": 138.2675741701396
          },
          "point_estimate": 91.33358917874388,
          "standard_error": 39.4960252868119
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101102.205323206,
            "upper_bound": 101438.1407182595
          },
          "point_estimate": 101296.73324790563,
          "standard_error": 84.90589369945452
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101092.32763888888,
            "upper_bound": 101449.83892857144
          },
          "point_estimate": 101422.48867669753,
          "standard_error": 73.9813446813537
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.3883585532504386,
            "upper_bound": 456.0496051535006
          },
          "point_estimate": 53.01124455887051,
          "standard_error": 75.95594706947188
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101332.66885487529,
            "upper_bound": 101431.43357451764
          },
          "point_estimate": 101399.98675324676,
          "standard_error": 26.09071544787166
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.602689200952465,
            "upper_bound": 354.4650953691662
          },
          "point_estimate": 282.9780512486907,
          "standard_error": 89.5177845002184
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19919.119427761227,
            "upper_bound": 19950.80402517837
          },
          "point_estimate": 19933.875598897077,
          "standard_error": 8.104789712882743
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19913.04363336992,
            "upper_bound": 19948.675288144896
          },
          "point_estimate": 19930.694420051223,
          "standard_error": 8.684378806588192
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.431787756282918,
            "upper_bound": 43.958506052727365
          },
          "point_estimate": 24.63059393703462,
          "standard_error": 9.665272723458765
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19918.91060190352,
            "upper_bound": 19947.74094401756
          },
          "point_estimate": 19931.194574251214,
          "standard_error": 7.339593973351739
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.613216263108017,
            "upper_bound": 36.05440025639105
          },
          "point_estimate": 26.9784328871466,
          "standard_error": 6.350484431019474
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16751.74297547597,
            "upper_bound": 16766.209223149704
          },
          "point_estimate": 16758.38717907708,
          "standard_error": 3.7226797146999377
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16748.24394044321,
            "upper_bound": 16766.71754385965
          },
          "point_estimate": 16755.51816712835,
          "standard_error": 4.656414037668197
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.143715099290973,
            "upper_bound": 20.773283657054215
          },
          "point_estimate": 11.726751464250617,
          "standard_error": 4.593273416276942
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16750.34545013322,
            "upper_bound": 16758.652665029527
          },
          "point_estimate": 16753.98631147246,
          "standard_error": 2.088366047107463
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.200155404282276,
            "upper_bound": 16.48552220444291
          },
          "point_estimate": 12.39433875281264,
          "standard_error": 3.000272707276833
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19280.17930416962,
            "upper_bound": 19322.299127123144
          },
          "point_estimate": 19298.160838051423,
          "standard_error": 10.966356911228347
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19273.480021821186,
            "upper_bound": 19308.130626326965
          },
          "point_estimate": 19291.59686836518,
          "standard_error": 12.10376078978246
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.530618249421388,
            "upper_bound": 45.56404566135185
          },
          "point_estimate": 24.273679202815295,
          "standard_error": 11.325859289619148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19275.37701313166,
            "upper_bound": 19299.231004553676
          },
          "point_estimate": 19286.236560785288,
          "standard_error": 6.577211592484564
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.748574334816649,
            "upper_bound": 53.513982614686675
          },
          "point_estimate": 36.69498835428666,
          "standard_error": 12.658638045465064
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16481.155498520682,
            "upper_bound": 16510.46433021542
          },
          "point_estimate": 16495.66936803081,
          "standard_error": 7.548506986142994
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16473.412674117266,
            "upper_bound": 16518.31232048375
          },
          "point_estimate": 16491.778987150414,
          "standard_error": 13.01220146844373
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.933291132759146,
            "upper_bound": 41.7216856084984
          },
          "point_estimate": 36.06068074074637,
          "standard_error": 9.40020199019244
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16473.85866802615,
            "upper_bound": 16515.823264807226
          },
          "point_estimate": 16496.32562829461,
          "standard_error": 10.8353390511366
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.06743364294475,
            "upper_bound": 30.011481859692907
          },
          "point_estimate": 25.148867960088715,
          "standard_error": 3.5818064414908473
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16656.855934507425,
            "upper_bound": 16672.423560943647
          },
          "point_estimate": 16664.04278393039,
          "standard_error": 4.001963642276289
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16653.192889908256,
            "upper_bound": 16672.545045871557
          },
          "point_estimate": 16658.884272608128,
          "standard_error": 4.783400982469253
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8103562853118784,
            "upper_bound": 20.82180583447261
          },
          "point_estimate": 9.26633889512667,
          "standard_error": 5.026022108022181
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16656.78046966599,
            "upper_bound": 16673.35394213608
          },
          "point_estimate": 16664.38870487311,
          "standard_error": 4.704766987800306
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.144413955537286,
            "upper_bound": 17.24338971596281
          },
          "point_estimate": 13.346972703230684,
          "standard_error": 3.087524631355293
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16482.50452417429,
            "upper_bound": 16516.550340156642
          },
          "point_estimate": 16497.863124995496,
          "standard_error": 8.782091718006708
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16478.835678620064,
            "upper_bound": 16515.544560447874
          },
          "point_estimate": 16484.363895013295,
          "standard_error": 10.26734814345536
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5557706891830516,
            "upper_bound": 43.29009467705418
          },
          "point_estimate": 19.786357090849535,
          "standard_error": 11.228353285005277
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16479.52408555915,
            "upper_bound": 16503.13702089889
          },
          "point_estimate": 16489.90972758517,
          "standard_error": 6.217871543015209
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.593933335357734,
            "upper_bound": 39.71945832479202
          },
          "point_estimate": 29.231953981137902,
          "standard_error": 7.7310367745150845
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19476.634724394866,
            "upper_bound": 19515.450237137255
          },
          "point_estimate": 19495.29750724212,
          "standard_error": 9.94243613415886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19467.57630208999,
            "upper_bound": 19526.22682948649
          },
          "point_estimate": 19487.89979200215,
          "standard_error": 16.427417323460688
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.823466509744056,
            "upper_bound": 55.886710789881725
          },
          "point_estimate": 37.724949536008765,
          "standard_error": 13.324396541190572
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19472.05057729762,
            "upper_bound": 19511.241119514096
          },
          "point_estimate": 19485.71869837087,
          "standard_error": 10.02091422892262
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.85914816748037,
            "upper_bound": 40.30711806829067
          },
          "point_estimate": 33.158585432557665,
          "standard_error": 5.464581343973237
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16840.607372584578,
            "upper_bound": 16868.64078146311
          },
          "point_estimate": 16854.479749666392,
          "standard_error": 7.18852792996871
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16834.359956207285,
            "upper_bound": 16870.780100118696
          },
          "point_estimate": 16856.326966248645,
          "standard_error": 9.79872973331824
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.119871329703506,
            "upper_bound": 41.30184774561053
          },
          "point_estimate": 25.93306611336943,
          "standard_error": 8.775435454189935
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16834.280896710894,
            "upper_bound": 16863.10441341539
          },
          "point_estimate": 16848.328691466453,
          "standard_error": 7.620886559676813
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.797001805981854,
            "upper_bound": 30.44976607644662
          },
          "point_estimate": 24.014791411341257,
          "standard_error": 4.311977134320044
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19603.15352001921,
            "upper_bound": 19622.31727809078
          },
          "point_estimate": 19612.29410863283,
          "standard_error": 4.908431776815212
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19598.533825559756,
            "upper_bound": 19625.85476319107
          },
          "point_estimate": 19607.32588562167,
          "standard_error": 7.839995049619523
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.950325185723852,
            "upper_bound": 27.999793587188016
          },
          "point_estimate": 15.035253784936131,
          "standard_error": 7.215705856705373
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19599.861369305316,
            "upper_bound": 19622.059521749034
          },
          "point_estimate": 19608.231821339115,
          "standard_error": 5.674367487463739
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.54034886119377,
            "upper_bound": 19.893530328754057
          },
          "point_estimate": 16.44062929330969,
          "standard_error": 2.8504362717683427
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-freq_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57957.23935259643,
            "upper_bound": 57982.58042949399
          },
          "point_estimate": 57968.397329706306,
          "standard_error": 6.575127642983796
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57950.710191082806,
            "upper_bound": 57976.98425336165
          },
          "point_estimate": 57965.64477176221,
          "standard_error": 6.014330810553317
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.0402865641000743,
            "upper_bound": 27.69926683838259
          },
          "point_estimate": 16.747989145336618,
          "standard_error": 7.495297619115612
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57954.90474394368,
            "upper_bound": 57976.08604762233
          },
          "point_estimate": 57964.73142939863,
          "standard_error": 5.535100206497625
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.883841036969599,
            "upper_bound": 31.292665164782488
          },
          "point_estimate": 22.006992825939324,
          "standard_error": 6.95627662527191
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-repeated_"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 884095.556814059,
            "upper_bound": 884547.7081309524
          },
          "point_estimate": 884283.7008087679,
          "standard_error": 118.89226282535306
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 884063.5466269841,
            "upper_bound": 884381.5598639456
          },
          "point_estimate": 884179.5037037036,
          "standard_error": 78.69196389207714
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.65582272949701,
            "upper_bound": 415.17413162025855
          },
          "point_estimate": 157.3512180397812,
          "standard_error": 98.11992193357716
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 884125.6308198113,
            "upper_bound": 884335.7919874537
          },
          "point_estimate": 884214.5299319727,
          "standard_error": 53.41208271858548
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.82364214839234,
            "upper_bound": 581.6157297331436
          },
          "point_estimate": 395.4893066163019,
          "standard_error": 147.81632844735535
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector_rare-alph"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202386.5687380953,
            "upper_bound": 202853.65225810185
          },
          "point_estimate": 202594.3482261905,
          "standard_error": 120.53081711800532
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202337.90694444443,
            "upper_bound": 202770.9296296296
          },
          "point_estimate": 202468.4247222222,
          "standard_error": 143.05173863628488
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.817008088065965,
            "upper_bound": 594.7409297467874
          },
          "point_estimate": 305.7758114602821,
          "standard_error": 142.61916312235172
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202311.66591398147,
            "upper_bound": 202524.08836773896
          },
          "point_estimate": 202411.7444011544,
          "standard_error": 53.05600845001496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163.1227563078967,
            "upper_bound": 566.2794729821686
          },
          "point_estimate": 400.2883712996048,
          "standard_error": 121.129302060002
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5970.183045567551,
            "upper_bound": 5987.642509516745
          },
          "point_estimate": 5978.644325141737,
          "standard_error": 4.493759931594391
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5967.869258557223,
            "upper_bound": 5993.065930443382
          },
          "point_estimate": 5974.055040473966,
          "standard_error": 6.851990537799522
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8701772764575654,
            "upper_bound": 24.47282035696025
          },
          "point_estimate": 12.328314447355524,
          "standard_error": 6.2096195343147595
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5968.885557540074,
            "upper_bound": 5987.85966725036
          },
          "point_estimate": 5978.530844102329,
          "standard_error": 4.775079840178751
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.19090040344449,
            "upper_bound": 18.70521121371663
          },
          "point_estimate": 15.00124546048687,
          "standard_error": 2.657896846537514
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6006.449277471413,
            "upper_bound": 6046.138967964074
          },
          "point_estimate": 6026.015834789392,
          "standard_error": 10.182898995754998
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5999.469129909421,
            "upper_bound": 6051.685843156788
          },
          "point_estimate": 6029.549199497165,
          "standard_error": 12.710224171558446
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.433216121503438,
            "upper_bound": 59.60243280785889
          },
          "point_estimate": 38.59101739666957,
          "standard_error": 15.319611804947272
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5989.007809785714,
            "upper_bound": 6027.920125395025
          },
          "point_estimate": 6004.947957504889,
          "standard_error": 9.987730351034235
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.567167664654615,
            "upper_bound": 42.94700464554565
          },
          "point_estimate": 34.044805607821345,
          "standard_error": 6.282050643316781
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-repeated-rare-huge_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15575.26835326464,
            "upper_bound": 15616.940926896476
          },
          "point_estimate": 15595.40306391335,
          "standard_error": 10.75055209215762
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15570.20785530822,
            "upper_bound": 15640.368055555557
          },
          "point_estimate": 15576.544648972602,
          "standard_error": 19.264302465551896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.919312970431765,
            "upper_bound": 55.05598553797196
          },
          "point_estimate": 25.165357464182367,
          "standard_error": 16.477943202196236
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15571.71926999131,
            "upper_bound": 15618.145150285098
          },
          "point_estimate": 15591.030950898416,
          "standard_error": 12.635653392298195
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.02662342564309,
            "upper_bound": 41.55487569529369
          },
          "point_estimate": 35.81134811197518,
          "standard_error": 5.415710379293146
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-repeated-rare-small_never-tric"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.97666394068239,
            "upper_bound": 35.07358040170896
          },
          "point_estimate": 35.033999087257804,
          "standard_error": 0.02594143802627532
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.01992660906464,
            "upper_bound": 35.073279766219834
          },
          "point_estimate": 35.06301314982788,
          "standard_error": 0.013630970545024964
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004515770987989668,
            "upper_bound": 0.07586650850613254
          },
          "point_estimate": 0.017903189525251436,
          "standard_error": 0.019668860401468816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.000941712434276,
            "upper_bound": 35.063004190035656
          },
          "point_estimate": 35.03343419184355,
          "standard_error": 0.0165909891719257
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013600482848900126,
            "upper_bound": 0.12879374427325518
          },
          "point_estimate": 0.08661004303943674,
          "standard_error": 0.0350007111082164
        }
      }
    },
    "memmem/krate_nopre/prebuilt/sliceslice-haystack/words": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/sliceslice-haystack/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/sliceslice-haystack/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/sliceslice-haystack/words",
        "directory_name": "memmem/krate_nopre_prebuilt_sliceslice-haystack_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180317.52723597363,
            "upper_bound": 180543.25506600665
          },
          "point_estimate": 180413.22740649065,
          "standard_error": 59.244841633312184
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180284.48143564357,
            "upper_bound": 180469.4183168317
          },
          "point_estimate": 180356.14367436743,
          "standard_error": 50.458124558073514
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.225543664835342,
            "upper_bound": 218.95608084873737
          },
          "point_estimate": 114.16219597654258,
          "standard_error": 53.12494362289042
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180300.8268141629,
            "upper_bound": 180390.91931123548
          },
          "point_estimate": 180340.45391539152,
          "standard_error": 22.68537066817353
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.996563762179406,
            "upper_bound": 287.7316102931093
          },
          "point_estimate": 197.4303667858474,
          "standard_error": 70.12483010324269
        }
      }
    },
    "memmem/krate_nopre/prebuilt/sliceslice-i386/words": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/krate_nopre_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25036135.94684524,
            "upper_bound": 25063973.063241072
          },
          "point_estimate": 25050380.34297619,
          "standard_error": 7149.651291006728
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25026945.5,
            "upper_bound": 25069496.375
          },
          "point_estimate": 25052411.875,
          "standard_error": 9187.265223726232
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4808.865964625168,
            "upper_bound": 41979.324054714336
          },
          "point_estimate": 30076.04159854242,
          "standard_error": 10399.248287291985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25033798.220698256,
            "upper_bound": 25058781.463836476
          },
          "point_estimate": 25047111.45064935,
          "standard_error": 6308.869777894619
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13273.904043290617,
            "upper_bound": 30164.304570724616
          },
          "point_estimate": 23897.262972656943,
          "standard_error": 4286.681938607429
        }
      }
    },
    "memmem/krate_nopre/prebuilt/sliceslice-words/words": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/krate_nopre_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79928704.85416666,
            "upper_bound": 79953807.07833333
          },
          "point_estimate": 79940727.06666666,
          "standard_error": 6426.2522917719125
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79923664.66666667,
            "upper_bound": 79964018.33333333
          },
          "point_estimate": 79930056.0,
          "standard_error": 11331.252770831848
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1080.8153808116913,
            "upper_bound": 33905.82589804758
          },
          "point_estimate": 16952.542299024848,
          "standard_error": 9051.873957213646
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12486.525792060189,
            "upper_bound": 25319.284200095048
          },
          "point_estimate": 21462.178196957495,
          "standard_error": 3325.564456088121
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.841653148291288,
            "upper_bound": 8.845120556587606
          },
          "point_estimate": 8.843397715110273,
          "standard_error": 0.0008879154433840002
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.841137009763447,
            "upper_bound": 8.84601880613505
          },
          "point_estimate": 8.843144075936124,
          "standard_error": 0.001295860098880697
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000325615447815271,
            "upper_bound": 0.00510162420127228
          },
          "point_estimate": 0.0029900832315827605,
          "standard_error": 0.0011548010437143486
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.841091062739718,
            "upper_bound": 8.84572451521547
          },
          "point_estimate": 8.843660954543074,
          "standard_error": 0.0011688109393035132
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017398114207072466,
            "upper_bound": 0.003706035632240108
          },
          "point_estimate": 0.0029593486493401018,
          "standard_error": 0.0005088249180936604
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.84442422937727,
            "upper_bound": 8.849758977226662
          },
          "point_estimate": 8.846881924104022,
          "standard_error": 0.001370274418189508
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.843206113668016,
            "upper_bound": 8.850724719489325
          },
          "point_estimate": 8.84538897216228,
          "standard_error": 0.0019730652368199013
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004566535742253654,
            "upper_bound": 0.007744575274111394
          },
          "point_estimate": 0.003261737380791331,
          "standard_error": 0.0019699575280545715
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.844492637273039,
            "upper_bound": 8.84813011407284
          },
          "point_estimate": 8.846002338393149,
          "standard_error": 0.0009240153455235648
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018530688334921011,
            "upper_bound": 0.005955850652463714
          },
          "point_estimate": 0.00455457317664053,
          "standard_error": 0.0010335612889842926
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.84449176657386,
            "upper_bound": 8.849366519182707
          },
          "point_estimate": 8.846632674587797,
          "standard_error": 0.0012582616347221877
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.844074152357784,
            "upper_bound": 8.84795354968007
          },
          "point_estimate": 8.8459219216939,
          "standard_error": 0.0011572553953896692
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005049435555783343,
            "upper_bound": 0.005341731113302963
          },
          "point_estimate": 0.0028061114488569876,
          "standard_error": 0.0011739207261634648
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.844239588605125,
            "upper_bound": 8.847193229629841
          },
          "point_estimate": 8.845739635539747,
          "standard_error": 0.0007504586871615547
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015005964482486293,
            "upper_bound": 0.005996320245014987
          },
          "point_estimate": 0.004174390935115386,
          "standard_error": 0.001343060158267796
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.838066457903174,
            "upper_bound": 8.843045592637864
          },
          "point_estimate": 8.840224175510329,
          "standard_error": 0.0012912194318517175
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.837297247712124,
            "upper_bound": 8.841951561149216
          },
          "point_estimate": 8.838903651213037,
          "standard_error": 0.001474098306201489
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004649000633625878,
            "upper_bound": 0.006045806104435004
          },
          "point_estimate": 0.003297069189155323,
          "standard_error": 0.0013332756469097973
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.837637398495124,
            "upper_bound": 8.84107917328967
          },
          "point_estimate": 8.839352678633112,
          "standard_error": 0.0008902452118166779
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001661385287927239,
            "upper_bound": 0.006171620046617002
          },
          "point_estimate": 0.004294455276404531,
          "standard_error": 0.0013853429389256596
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.761378478467009,
            "upper_bound": 9.768568162936678
          },
          "point_estimate": 9.764446126182424,
          "standard_error": 0.0018868825740840616
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.76039429320176,
            "upper_bound": 9.7665418736949
          },
          "point_estimate": 9.762604891146095,
          "standard_error": 0.0015013573531996848
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008575581616034992,
            "upper_bound": 0.007025168179249052
          },
          "point_estimate": 0.003512584089624526,
          "standard_error": 0.0016454556467796332
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.76080667492523,
            "upper_bound": 9.764991355836813
          },
          "point_estimate": 9.76290007283886,
          "standard_error": 0.0010656002667834654
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019094688028410904,
            "upper_bound": 0.009227041949219942
          },
          "point_estimate": 0.006307154113578822,
          "standard_error": 0.002243688488367217
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.08849966738038,
            "upper_bound": 10.093047204857328
          },
          "point_estimate": 10.09063904411292,
          "standard_error": 0.0011640905283199
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.087807249914311,
            "upper_bound": 10.092577323307102
          },
          "point_estimate": 10.090243874798848,
          "standard_error": 0.0012455944735478462
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008424900723679672,
            "upper_bound": 0.006371672346059325
          },
          "point_estimate": 0.0034432835064609796,
          "standard_error": 0.001405971356093909
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.087938894875284,
            "upper_bound": 10.090614754177308
          },
          "point_estimate": 10.0892186582794,
          "standard_error": 0.0006711954753250879
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018458016934284843,
            "upper_bound": 0.005209276260100962
          },
          "point_estimate": 0.003867355299887486,
          "standard_error": 0.0009238950319129512
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.835005017347457,
            "upper_bound": 7.8574272053783005
          },
          "point_estimate": 7.847926636342436,
          "standard_error": 0.005869635969269216
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.842028146388516,
            "upper_bound": 7.856671080458697
          },
          "point_estimate": 7.853749849038806,
          "standard_error": 0.0031416018034245473
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005741917343953031,
            "upper_bound": 0.022587183771438674
          },
          "point_estimate": 0.003040489572313557,
          "standard_error": 0.005940595791862855
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.830599040054049,
            "upper_bound": 7.854233612005733
          },
          "point_estimate": 7.845494353503538,
          "standard_error": 0.006126692786416393
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002834086091241812,
            "upper_bound": 0.02821820286661145
          },
          "point_estimate": 0.01961052551127476,
          "standard_error": 0.006973497791808084
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.778199477166828,
            "upper_bound": 9.812606901836366
          },
          "point_estimate": 9.796403501373572,
          "standard_error": 0.008905257111461728
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.756231779405494,
            "upper_bound": 9.819422942489178
          },
          "point_estimate": 9.811561606574712,
          "standard_error": 0.015488960005004089
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002732051216743354,
            "upper_bound": 0.04316935935399854
          },
          "point_estimate": 0.017339951363168047,
          "standard_error": 0.0112322913333056
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.76268488601721,
            "upper_bound": 9.816016528409024
          },
          "point_estimate": 9.78592797206289,
          "standard_error": 0.01303165711984035
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011548248712613672,
            "upper_bound": 0.033872213342307236
          },
          "point_estimate": 0.02970669993065031,
          "standard_error": 0.0049640161074446815
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.363721821760809,
            "upper_bound": 12.437416144748996
          },
          "point_estimate": 12.400804080030772,
          "standard_error": 0.018837439058366965
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.341206750593956,
            "upper_bound": 12.453827857708529
          },
          "point_estimate": 12.411417501843305,
          "standard_error": 0.0343919678980621
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011137994570459008,
            "upper_bound": 0.10205277180384471
          },
          "point_estimate": 0.08907300074211315,
          "standard_error": 0.024071534597774814
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.374856963332908,
            "upper_bound": 12.445311826076706
          },
          "point_estimate": 12.4077980870409,
          "standard_error": 0.017955502735227766
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04099791686637433,
            "upper_bound": 0.07490076560213177
          },
          "point_estimate": 0.06302547507352309,
          "standard_error": 0.008701962498004721
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.862363905821582,
            "upper_bound": 8.86742866853866
          },
          "point_estimate": 8.864738124481718,
          "standard_error": 0.001295093966830857
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.861082069435493,
            "upper_bound": 8.868554157579634
          },
          "point_estimate": 8.863593273352455,
          "standard_error": 0.0017464200261408666
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008716249873286769,
            "upper_bound": 0.007307772141251099
          },
          "point_estimate": 0.004075700805588301,
          "standard_error": 0.0016185451956124365
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.861274241096988,
            "upper_bound": 8.864009586760504
          },
          "point_estimate": 8.862446549734859,
          "standard_error": 0.0007069495537601519
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002070687458886578,
            "upper_bound": 0.005572861373774499
          },
          "point_estimate": 0.0043146259226602935,
          "standard_error": 0.0009207262345397896
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.868531354016088,
            "upper_bound": 9.875792803125249
          },
          "point_estimate": 9.871885107484008,
          "standard_error": 0.001868838692508569
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.868766936181904,
            "upper_bound": 9.875984967299676
          },
          "point_estimate": 9.870073952931588,
          "standard_error": 0.0014126347720577208
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0001154752781525658,
            "upper_bound": 0.009997556760932794
          },
          "point_estimate": 0.002316562039530175,
          "standard_error": 0.002489164425616095
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.869580726034846,
            "upper_bound": 9.872284302017215
          },
          "point_estimate": 9.87090551912947,
          "standard_error": 0.0006781319525043127
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00160671739222289,
            "upper_bound": 0.008329589413608193
          },
          "point_estimate": 0.00624337769106028,
          "standard_error": 0.001647223735320915
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.19207747543681,
            "upper_bound": 19.2050433356741
          },
          "point_estimate": 19.19854946432452,
          "standard_error": 0.0033093927587713504
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.189271785280013,
            "upper_bound": 19.20619811883251
          },
          "point_estimate": 19.198132957722834,
          "standard_error": 0.0032149657091413374
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011846911575518417,
            "upper_bound": 0.021905764747441853
          },
          "point_estimate": 0.005829990757458142,
          "standard_error": 0.005857345780241891
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.185698287849537,
            "upper_bound": 19.201515009957113
          },
          "point_estimate": 19.191886276433326,
          "standard_error": 0.0040962577390109434
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005666551790458234,
            "upper_bound": 0.014221173112743804
          },
          "point_estimate": 0.011028884534182447,
          "standard_error": 0.0022030354275664387
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103050.45447601013,
            "upper_bound": 103178.3503379216
          },
          "point_estimate": 103115.61714263169,
          "standard_error": 32.72307568502206
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103027.3810290404,
            "upper_bound": 103210.13068181818
          },
          "point_estimate": 103118.7478896104,
          "standard_error": 38.79421765580903
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.349355853214448,
            "upper_bound": 210.5815111288495
          },
          "point_estimate": 107.56361087444984,
          "standard_error": 52.17477451687595
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103073.7610536498,
            "upper_bound": 103140.97352607684
          },
          "point_estimate": 103109.63892414403,
          "standard_error": 16.82263658950617
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.12652131673248,
            "upper_bound": 136.678306456014
          },
          "point_estimate": 109.23363115194398,
          "standard_error": 19.826125557810208
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56578.97503634476,
            "upper_bound": 56686.36344399939
          },
          "point_estimate": 56632.47340392127,
          "standard_error": 27.53591754208752
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56556.45303738318,
            "upper_bound": 56718.137071651094
          },
          "point_estimate": 56613.808047767394,
          "standard_error": 37.88107124964548
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.773670831161214,
            "upper_bound": 170.17488671139287
          },
          "point_estimate": 110.8936925535701,
          "standard_error": 44.28256985132659
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56586.36371984941,
            "upper_bound": 56710.62246062042
          },
          "point_estimate": 56649.70503297326,
          "standard_error": 31.547315374142286
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.27014184975391,
            "upper_bound": 112.85910687785127
          },
          "point_estimate": 91.55296961971304,
          "standard_error": 15.19154779442749
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148226.64818048297,
            "upper_bound": 148767.68260315043
          },
          "point_estimate": 148482.93652874563,
          "standard_error": 138.27204281611913
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148210.5056620209,
            "upper_bound": 148722.15650406503
          },
          "point_estimate": 148469.06512533876,
          "standard_error": 115.72446110384726
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.23038489589569,
            "upper_bound": 724.5725225021254
          },
          "point_estimate": 354.08314407961086,
          "standard_error": 169.0862747203193
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148177.7760309534,
            "upper_bound": 148557.28764068603
          },
          "point_estimate": 148361.90666244325,
          "standard_error": 101.23300657576917
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.95496940944315,
            "upper_bound": 645.6666335947507
          },
          "point_estimate": 463.11630680560984,
          "standard_error": 122.65370469857449
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397636.61953502416,
            "upper_bound": 398542.5723524845
          },
          "point_estimate": 398051.89511818497,
          "standard_error": 231.65238739802112
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397381.5303442029,
            "upper_bound": 398488.99565217394
          },
          "point_estimate": 398070.45108695654,
          "standard_error": 322.7406271869323
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.08706755347178,
            "upper_bound": 1342.6769152932338
          },
          "point_estimate": 844.6609749499335,
          "standard_error": 324.8590053065428
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397465.7282648747,
            "upper_bound": 398505.6581239197
          },
          "point_estimate": 397957.87523997744,
          "standard_error": 267.14649080608456
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 385.3124221434677,
            "upper_bound": 1047.5525375883833
          },
          "point_estimate": 771.439170638727,
          "standard_error": 190.62430584463635
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578310.0198450491,
            "upper_bound": 579402.1029280046
          },
          "point_estimate": 578830.8568997228,
          "standard_error": 280.43229137427767
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578222.3498866213,
            "upper_bound": 579480.925925926
          },
          "point_estimate": 578629.9670194003,
          "standard_error": 298.65095692977405
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.57090248630705,
            "upper_bound": 1619.0697712558724
          },
          "point_estimate": 820.566826490229,
          "standard_error": 421.62479706293385
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578268.716556651,
            "upper_bound": 578966.4184370203
          },
          "point_estimate": 578617.5508142652,
          "standard_error": 174.93582030586174
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 422.7569708687984,
            "upper_bound": 1183.4865784414922
          },
          "point_estimate": 936.0936945705836,
          "standard_error": 188.7444101045626
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49448.0914957672,
            "upper_bound": 49543.37140233237
          },
          "point_estimate": 49488.46946668826,
          "standard_error": 24.986625271890517
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49441.312683295546,
            "upper_bound": 49517.49342403628
          },
          "point_estimate": 49460.33890184645,
          "standard_error": 17.673891731182543
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.2734640137285265,
            "upper_bound": 90.5262320436274
          },
          "point_estimate": 32.81210584604227,
          "standard_error": 21.674803931092256
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49439.22695432849,
            "upper_bound": 49467.20313853421
          },
          "point_estimate": 49450.2516582737,
          "standard_error": 7.1450519259483185
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.7870468441179,
            "upper_bound": 119.02403523887992
          },
          "point_estimate": 83.12603433274843,
          "standard_error": 29.100541650563084
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100031.00594928484,
            "upper_bound": 100488.37999165468
          },
          "point_estimate": 100233.33761490494,
          "standard_error": 118.66767282344674
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99955.51312576311,
            "upper_bound": 100367.28399725274
          },
          "point_estimate": 100234.70274725274,
          "standard_error": 115.13962909241069
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.677416819302657,
            "upper_bound": 514.7681468225712
          },
          "point_estimate": 362.7901430784187,
          "standard_error": 139.2317948496957
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100000.16412367667,
            "upper_bound": 100233.40598529077
          },
          "point_estimate": 100119.61511345796,
          "standard_error": 59.70622018427731
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148.36912027338488,
            "upper_bound": 567.6051950182473
          },
          "point_estimate": 396.1712155632515,
          "standard_error": 124.5944488943318
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70795.2135564722,
            "upper_bound": 71058.40488980553
          },
          "point_estimate": 70912.31040131192,
          "standard_error": 68.3139912856247
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70767.05449967511,
            "upper_bound": 71053.04515919427
          },
          "point_estimate": 70806.22627556545,
          "standard_error": 67.71852222783309
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.765979416212213,
            "upper_bound": 323.42651095394837
          },
          "point_estimate": 117.67146299212877,
          "standard_error": 78.3828130413657
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70777.04949297885,
            "upper_bound": 70850.67713036189
          },
          "point_estimate": 70802.60380243538,
          "standard_error": 19.13516574256289
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.56377405937,
            "upper_bound": 307.412092981917
          },
          "point_estimate": 227.9296724157435,
          "standard_error": 65.2835747422184
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308669.0152118644,
            "upper_bound": 309136.9180508474
          },
          "point_estimate": 308904.1215254237,
          "standard_error": 120.51850914461704
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308446.6836158192,
            "upper_bound": 309293.7966101695
          },
          "point_estimate": 308874.5116525424,
          "standard_error": 211.3269349221853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.60193478482428,
            "upper_bound": 682.5875620342139
          },
          "point_estimate": 627.9648515632863,
          "standard_error": 206.67862338690784
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308626.101014185,
            "upper_bound": 309231.64941936266
          },
          "point_estimate": 308995.7362976007,
          "standard_error": 153.75630209811646
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.81410275092705,
            "upper_bound": 469.6442745949841
          },
          "point_estimate": 402.9016423612998,
          "standard_error": 55.49122283248072
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36404.496711393935,
            "upper_bound": 36513.82916666666
          },
          "point_estimate": 36454.22433500961,
          "standard_error": 28.09679755569564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36379.863363363365,
            "upper_bound": 36519.64371514371
          },
          "point_estimate": 36426.25994744745,
          "standard_error": 32.72072364497492
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.0750315582234125,
            "upper_bound": 135.40387187037513
          },
          "point_estimate": 66.33818635979942,
          "standard_error": 34.94059622268022
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36394.81206174335,
            "upper_bound": 36483.439336112315
          },
          "point_estimate": 36433.39208039208,
          "standard_error": 22.39205932887115
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.66071424292071,
            "upper_bound": 121.76424003955648
          },
          "point_estimate": 93.72312354284344,
          "standard_error": 23.600970612677106
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66210.29660319265,
            "upper_bound": 66452.61668268398
          },
          "point_estimate": 66326.5494069264,
          "standard_error": 62.082905468998554
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66160.04983766234,
            "upper_bound": 66499.59386363637
          },
          "point_estimate": 66269.01193939394,
          "standard_error": 81.09296960661665
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.07076893843204,
            "upper_bound": 357.21908174900045
          },
          "point_estimate": 234.6282747799773,
          "standard_error": 83.65995017264919
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66127.44499453869,
            "upper_bound": 66380.35695897024
          },
          "point_estimate": 66246.9507012987,
          "standard_error": 66.73548223822769
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.85730707369916,
            "upper_bound": 264.07051623207326
          },
          "point_estimate": 207.6066420883864,
          "standard_error": 40.051953462427264
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170158.76402138037,
            "upper_bound": 170367.61343457943
          },
          "point_estimate": 170265.55351987836,
          "standard_error": 53.429244079071886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170154.16592493694,
            "upper_bound": 170385.99883177568
          },
          "point_estimate": 170265.9313084112,
          "standard_error": 57.973285009715376
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.5658609265356,
            "upper_bound": 306.61068100517
          },
          "point_estimate": 134.67988966501613,
          "standard_error": 64.73534551808976
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170170.27462174353,
            "upper_bound": 170386.04512577347
          },
          "point_estimate": 170271.69264473845,
          "standard_error": 57.403531331358856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.38765470923525,
            "upper_bound": 238.88344242848245
          },
          "point_estimate": 178.0261155173149,
          "standard_error": 39.666922689356895
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35821.04315253123,
            "upper_bound": 35925.807002712034
          },
          "point_estimate": 35867.04867380483,
          "standard_error": 26.77162817251727
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35820.366987179485,
            "upper_bound": 35924.87785291631
          },
          "point_estimate": 35830.51427514793,
          "standard_error": 22.787932025157907
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4894227960442015,
            "upper_bound": 132.54042723274304
          },
          "point_estimate": 15.902456507615788,
          "standard_error": 27.921950762276992
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35821.4134840437,
            "upper_bound": 35851.59872031215
          },
          "point_estimate": 35836.09822741361,
          "standard_error": 7.701972713612706
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.523500821600695,
            "upper_bound": 114.46478587305648
          },
          "point_estimate": 89.41114183625349,
          "standard_error": 25.96945654151304
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11618.137544720785,
            "upper_bound": 11639.46019931602
          },
          "point_estimate": 11628.612467276856,
          "standard_error": 5.468157873137619
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11613.733919601826,
            "upper_bound": 11644.822443090734
          },
          "point_estimate": 11626.558334223932,
          "standard_error": 7.577518367694709
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.2939473571193165,
            "upper_bound": 31.702411839649315
          },
          "point_estimate": 18.215890422836864,
          "standard_error": 6.688006979456037
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11621.628989544985,
            "upper_bound": 11639.51375768385
          },
          "point_estimate": 11630.506127088684,
          "standard_error": 4.567531550678539
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.54187449012811,
            "upper_bound": 22.56687554569597
          },
          "point_estimate": 18.223558622263724,
          "standard_error": 3.0713414727129136
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-repeated-rare-huge_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506289.4870732474,
            "upper_bound": 507060.04762885807
          },
          "point_estimate": 506658.33650683414,
          "standard_error": 197.8178448724233
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506033.24166666664,
            "upper_bound": 507141.9930555555
          },
          "point_estimate": 506568.7798721341,
          "standard_error": 261.07785074958787
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.77090130968988,
            "upper_bound": 1122.624185277818
          },
          "point_estimate": 817.6549150670818,
          "standard_error": 271.4262676548692
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506179.54124228394,
            "upper_bound": 506887.5183080808
          },
          "point_estimate": 506493.816053391,
          "standard_error": 182.65536249421305
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.82472646252324,
            "upper_bound": 813.286226168557
          },
          "point_estimate": 659.4201655488995,
          "standard_error": 118.12768932188752
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-repeated-rare-small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1034.75220563336,
            "upper_bound": 1036.4569880228617
          },
          "point_estimate": 1035.6367787446793,
          "standard_error": 0.4365564225957363
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1034.6279160729553,
            "upper_bound": 1036.9691032582882
          },
          "point_estimate": 1035.6273610715302,
          "standard_error": 0.5675608530083719
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.27151564408233597,
            "upper_bound": 2.5690292565419397
          },
          "point_estimate": 1.392023811921301,
          "standard_error": 0.5528490861684555
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1034.3504019451502,
            "upper_bound": 1036.166437115497
          },
          "point_estimate": 1035.3544054065055,
          "standard_error": 0.45785606023639
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7880066731819108,
            "upper_bound": 1.8886488792269716
          },
          "point_estimate": 1.4552523946207887,
          "standard_error": 0.291984487502268
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248420.17584602095,
            "upper_bound": 249087.8595588152
          },
          "point_estimate": 248730.73176843752,
          "standard_error": 171.3932075443111
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248265.648202138,
            "upper_bound": 249074.8378684807
          },
          "point_estimate": 248625.7082388511,
          "standard_error": 238.84699339304632
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.42585571036048,
            "upper_bound": 992.7939358664488
          },
          "point_estimate": 563.2141955338395,
          "standard_error": 209.15375298406036
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248295.59936314952,
            "upper_bound": 248838.0850890518
          },
          "point_estimate": 248509.4999734959,
          "standard_error": 139.20823631530874
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289.13042121591684,
            "upper_bound": 757.5288893761506
          },
          "point_estimate": 571.2230935450134,
          "standard_error": 130.42842438298584
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194997.31764069264,
            "upper_bound": 195332.0623885918
          },
          "point_estimate": 195163.70795136236,
          "standard_error": 85.39523722823222
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194904.862394958,
            "upper_bound": 195316.6024955437
          },
          "point_estimate": 195241.54634581105,
          "standard_error": 119.51780353746976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.118157771806416,
            "upper_bound": 546.9763218400435
          },
          "point_estimate": 235.90272254985575,
          "standard_error": 134.89983789341002
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195005.25051029876,
            "upper_bound": 195277.94127736855
          },
          "point_estimate": 195142.666254601,
          "standard_error": 71.36237502308634
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.97062234123868,
            "upper_bound": 369.56140638007133
          },
          "point_estimate": 284.67793503815045,
          "standard_error": 55.25353259898518
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176095.95453301127,
            "upper_bound": 176379.50720111572
          },
          "point_estimate": 176240.83663944484,
          "standard_error": 72.64595810523222
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176031.4347826087,
            "upper_bound": 176427.92807300054
          },
          "point_estimate": 176303.23876811593,
          "standard_error": 115.95164553275684
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.98842130710644,
            "upper_bound": 390.68980306386374
          },
          "point_estimate": 298.31435452672247,
          "standard_error": 91.19371488738258
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176205.50726676045,
            "upper_bound": 176488.94312604654
          },
          "point_estimate": 176387.27501097936,
          "standard_error": 72.64943819757174
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146.26158660094953,
            "upper_bound": 295.7444206496781
          },
          "point_estimate": 241.4681811258692,
          "standard_error": 38.33098017914852
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/libc_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37434.73612004683,
            "upper_bound": 37509.98525621905
          },
          "point_estimate": 37471.198809156,
          "standard_error": 19.328026848631666
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37416.865430827325,
            "upper_bound": 37502.07651905252
          },
          "point_estimate": 37473.030430091705,
          "standard_error": 19.08749437448912
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.570851662977525,
            "upper_bound": 114.24468530439766
          },
          "point_estimate": 39.78231040597339,
          "standard_error": 26.746810709361135
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37418.86845707203,
            "upper_bound": 37491.96070629805
          },
          "point_estimate": 37461.732068960904,
          "standard_error": 19.82618545918564
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.05207154934367,
            "upper_bound": 88.16786544772273
          },
          "point_estimate": 64.52717364995665,
          "standard_error": 16.064836062712523
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/libc_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72593.89813333334,
            "upper_bound": 72823.55285761906
          },
          "point_estimate": 72688.37672047621,
          "standard_error": 61.00754720698032
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72555.988,
            "upper_bound": 72732.09057142858
          },
          "point_estimate": 72638.6759,
          "standard_error": 49.1910710343076
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.448060703265621,
            "upper_bound": 204.3832545714719
          },
          "point_estimate": 125.3171087151772,
          "standard_error": 53.483244344103
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72611.5681684203,
            "upper_bound": 72708.3937443609
          },
          "point_estimate": 72658.88782857143,
          "standard_error": 24.54474946414476
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.076627468371534,
            "upper_bound": 304.78802005260644
          },
          "point_estimate": 204.26278435619793,
          "standard_error": 80.81827660254372
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69400.66117944043,
            "upper_bound": 69518.23494767812
          },
          "point_estimate": 69443.6026356325,
          "standard_error": 34.1827365285805
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69395.88158396946,
            "upper_bound": 69429.87849872773
          },
          "point_estimate": 69408.81420392584,
          "standard_error": 12.053927887063209
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.13444506019009,
            "upper_bound": 39.70830910419403
          },
          "point_estimate": 20.615228264727712,
          "standard_error": 12.439693206918086
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69397.00175011758,
            "upper_bound": 69414.06012014997
          },
          "point_estimate": 69404.26966392386,
          "standard_error": 4.465346170208255
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.844703071016507,
            "upper_bound": 175.39070874751
          },
          "point_estimate": 114.14344195011532,
          "standard_error": 58.43316617495148
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/libc_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82713.33962214584,
            "upper_bound": 82890.13697412953
          },
          "point_estimate": 82797.68200048812,
          "standard_error": 45.267991466464075
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82657.34783599089,
            "upper_bound": 82897.75284738041
          },
          "point_estimate": 82780.86729173447,
          "standard_error": 70.47471004597666
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.62196177746568,
            "upper_bound": 283.6555974698079
          },
          "point_estimate": 178.21223177915232,
          "standard_error": 59.9955320380104
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82690.03888458334,
            "upper_bound": 82824.59695664681
          },
          "point_estimate": 82743.81376800875,
          "standard_error": 34.31416536407606
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.7588124330219,
            "upper_bound": 195.00602912715817
          },
          "point_estimate": 151.03004627894484,
          "standard_error": 30.17916644434066
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/libc_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322654.9676786943,
            "upper_bound": 322803.0328817249
          },
          "point_estimate": 322723.3827215901,
          "standard_error": 38.054319045757886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322628.9557522124,
            "upper_bound": 322789.03694690263
          },
          "point_estimate": 322714.0631760078,
          "standard_error": 40.894797454967375
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.305912625682889,
            "upper_bound": 197.80846662095152
          },
          "point_estimate": 89.1312488600886,
          "standard_error": 47.25020859548556
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322662.8624063989,
            "upper_bound": 322746.25446528115
          },
          "point_estimate": 322704.3466038386,
          "standard_error": 21.019893843503468
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.7697492843768,
            "upper_bound": 173.31170023755448
          },
          "point_estimate": 127.17068746547828,
          "standard_error": 32.84236042578474
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47912.5417412154,
            "upper_bound": 48060.02202574784
          },
          "point_estimate": 47981.7697678205,
          "standard_error": 37.94139459137281
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47873.96625518281,
            "upper_bound": 48087.990875109936
          },
          "point_estimate": 47958.48870382586,
          "standard_error": 51.53631910542567
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.96609256549404,
            "upper_bound": 217.63922154510004
          },
          "point_estimate": 121.39049162677388,
          "standard_error": 50.3149708297969
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47880.229798976536,
            "upper_bound": 47972.51085138801
          },
          "point_estimate": 47913.712092656686,
          "standard_error": 23.56590742275845
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.089483009706306,
            "upper_bound": 156.0907925956015
          },
          "point_estimate": 126.59632646351848,
          "standard_error": 24.99959846766707
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33905.984168959076,
            "upper_bound": 34081.40269138791
          },
          "point_estimate": 33979.89377686605,
          "standard_error": 45.73336293089001
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33897.931648506856,
            "upper_bound": 33999.92689370161
          },
          "point_estimate": 33953.24198053986,
          "standard_error": 22.040110045945948
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.65534968371384,
            "upper_bound": 163.2586734888949
          },
          "point_estimate": 40.98118526113988,
          "standard_error": 40.954151833431034
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33906.05791228438,
            "upper_bound": 33964.4540098587
          },
          "point_estimate": 33942.365086702375,
          "standard_error": 15.144259694612348
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.640872943704224,
            "upper_bound": 226.51654552935295
          },
          "point_estimate": 152.9452780542648,
          "standard_error": 57.815344767194894
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37285.578426862034,
            "upper_bound": 37352.90949954416
          },
          "point_estimate": 37313.16561741962,
          "standard_error": 17.932650308691212
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37286.91948717949,
            "upper_bound": 37320.16393162393
          },
          "point_estimate": 37293.735897435894,
          "standard_error": 9.055424790855016
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9204284759668028,
            "upper_bound": 57.962186232508806
          },
          "point_estimate": 13.794127050827816,
          "standard_error": 15.41753319496484
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37278.824154002024,
            "upper_bound": 37310.08679630538
          },
          "point_estimate": 37292.24153979354,
          "standard_error": 7.911064868862397
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.852504315232476,
            "upper_bound": 89.6731083848291
          },
          "point_estimate": 59.77427269678527,
          "standard_error": 24.350535696325643
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83876.38853678654,
            "upper_bound": 84167.60928166813
          },
          "point_estimate": 84006.72171193958,
          "standard_error": 74.97173748668234
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83814.70636753547,
            "upper_bound": 84144.81131639723
          },
          "point_estimate": 83896.34917885554,
          "standard_error": 89.79184810641432
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.152024377116454,
            "upper_bound": 386.02265400123343
          },
          "point_estimate": 158.8375987004862,
          "standard_error": 91.37778858528873
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83842.16886502702,
            "upper_bound": 84035.71918197945
          },
          "point_estimate": 83937.1741399478,
          "standard_error": 50.75507746202903
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.81031362553767,
            "upper_bound": 344.90814900254577
          },
          "point_estimate": 250.10668603042663,
          "standard_error": 69.22977575421531
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65704.55144697652,
            "upper_bound": 65826.12981255015
          },
          "point_estimate": 65762.72532841956,
          "standard_error": 31.11372630428443
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65686.25584386282,
            "upper_bound": 65835.82245286804
          },
          "point_estimate": 65750.67184115524,
          "standard_error": 36.54070601946603
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.808253188321405,
            "upper_bound": 173.67329525602435
          },
          "point_estimate": 94.05745365505696,
          "standard_error": 38.52382142796133
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65711.16569754183,
            "upper_bound": 65869.44715450282
          },
          "point_estimate": 65789.43750293028,
          "standard_error": 40.48960625371149
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.68771502455075,
            "upper_bound": 132.91991364175468
          },
          "point_estimate": 103.56472385859932,
          "standard_error": 20.789173441974345
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151037.41975344328,
            "upper_bound": 151234.71373648313
          },
          "point_estimate": 151131.51726372354,
          "standard_error": 50.56567491510139
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150998.2855324074,
            "upper_bound": 151286.8401388889
          },
          "point_estimate": 151099.415,
          "standard_error": 59.61233133046702
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.27985631114527,
            "upper_bound": 319.2000420934726
          },
          "point_estimate": 116.6158313983962,
          "standard_error": 68.24350430910464
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151056.1072270349,
            "upper_bound": 151248.9964663689
          },
          "point_estimate": 151147.9091125541,
          "standard_error": 48.54923640655488
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.71461781741094,
            "upper_bound": 212.43786704712707
          },
          "point_estimate": 168.7903550871554,
          "standard_error": 33.96815694445384
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198665.5460261081,
            "upper_bound": 199034.98545431087
          },
          "point_estimate": 198831.2892926533,
          "standard_error": 94.7800839049156
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198616.00683060108,
            "upper_bound": 199078.89699453552
          },
          "point_estimate": 198727.32559198543,
          "standard_error": 94.41236874451164
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.168373552394954,
            "upper_bound": 422.273188313744
          },
          "point_estimate": 161.10480875185246,
          "standard_error": 91.67361775775662
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198596.7557389112,
            "upper_bound": 198757.50142216615
          },
          "point_estimate": 198648.8472642112,
          "standard_error": 41.09769461182977
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.45482427074279,
            "upper_bound": 399.5856671318739
          },
          "point_estimate": 315.7441149782588,
          "standard_error": 83.67062337073754
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122108.46901434164,
            "upper_bound": 122676.89964619954
          },
          "point_estimate": 122375.55167572174,
          "standard_error": 145.9000160711962
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122020.10738255034,
            "upper_bound": 122760.1096196868
          },
          "point_estimate": 122245.92902684564,
          "standard_error": 191.7368412607231
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.09344628581281,
            "upper_bound": 755.0408472217318
          },
          "point_estimate": 339.02565978646106,
          "standard_error": 194.30473172658924
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122111.45971258484,
            "upper_bound": 122852.72201715138
          },
          "point_estimate": 122444.6973503007,
          "standard_error": 200.4771029812563
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203.2764479838314,
            "upper_bound": 604.9162923006339
          },
          "point_estimate": 486.1289094458311,
          "standard_error": 97.3213353278862
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52677.12593971562,
            "upper_bound": 52799.257106271565
          },
          "point_estimate": 52735.123881009895,
          "standard_error": 31.49104373507194
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52642.17717391304,
            "upper_bound": 52816.2752484472
          },
          "point_estimate": 52699.10107689211,
          "standard_error": 51.48342739411583
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.68098959599742,
            "upper_bound": 187.5364222985774
          },
          "point_estimate": 103.69066367240887,
          "standard_error": 42.8098697458138
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52673.879044551795,
            "upper_bound": 52781.730476125784
          },
          "point_estimate": 52732.78089215133,
          "standard_error": 27.77994183934284
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.63354549278059,
            "upper_bound": 132.89684038566622
          },
          "point_estimate": 104.6370732568171,
          "standard_error": 20.12551364888562
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77580.32991496024,
            "upper_bound": 77796.16832455102
          },
          "point_estimate": 77679.62932700786,
          "standard_error": 55.401977546660234
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77543.87200854701,
            "upper_bound": 77748.65097680097
          },
          "point_estimate": 77674.4328258547,
          "standard_error": 49.351761031312336
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.358686237937555,
            "upper_bound": 270.3582651004577
          },
          "point_estimate": 116.23332557711038,
          "standard_error": 62.031068242971294
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77614.95789778346,
            "upper_bound": 77723.84958145084
          },
          "point_estimate": 77671.25369075369,
          "standard_error": 27.68913204393825
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.59779239448035,
            "upper_bound": 258.85991928411255
          },
          "point_estimate": 184.94178190667824,
          "standard_error": 52.289200857508554
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40867.93324345484,
            "upper_bound": 41014.9127467744
          },
          "point_estimate": 40931.106512276085,
          "standard_error": 38.308669593749265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40850.56854565952,
            "upper_bound": 40979.17136414882
          },
          "point_estimate": 40877.48765345109,
          "standard_error": 39.83242087341584
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3078080988301566,
            "upper_bound": 156.46235218278375
          },
          "point_estimate": 57.05661241526324,
          "standard_error": 43.601513726786266
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40859.455454341914,
            "upper_bound": 40934.312939941345
          },
          "point_estimate": 40889.32367091758,
          "standard_error": 19.06552054129373
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.475240105734834,
            "upper_bound": 183.20591086527605
          },
          "point_estimate": 127.29105895673617,
          "standard_error": 42.543232556254885
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93466.55786387564,
            "upper_bound": 93642.99363906232
          },
          "point_estimate": 93536.23193199903,
          "standard_error": 48.36179675757575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93458.43557197944,
            "upper_bound": 93554.9471171502
          },
          "point_estimate": 93481.70340616966,
          "standard_error": 21.270248395920227
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.760074655040355,
            "upper_bound": 102.12598353396297
          },
          "point_estimate": 26.485752871690643,
          "standard_error": 28.043882003955805
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93453.11084837056,
            "upper_bound": 93494.10690573504
          },
          "point_estimate": 93472.38726004072,
          "standard_error": 10.889103449177025
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.981498745006988,
            "upper_bound": 243.3613269496933
          },
          "point_estimate": 161.81357554104065,
          "standard_error": 70.92787115161346
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1319136.8909454718,
            "upper_bound": 1323436.1310314625
          },
          "point_estimate": 1321130.501571712,
          "standard_error": 1102.747845127746
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1318619.776785714,
            "upper_bound": 1323969.3392857143
          },
          "point_estimate": 1319938.3120535717,
          "standard_error": 1317.4129242329325
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 615.3451765754639,
            "upper_bound": 5976.400868272857
          },
          "point_estimate": 2050.2880183919547,
          "standard_error": 1459.742269263416
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1318642.4658275351,
            "upper_bound": 1321797.422943723
          },
          "point_estimate": 1319964.891929499,
          "standard_error": 790.3947934768603
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1533.4342710730298,
            "upper_bound": 4802.460070186737
          },
          "point_estimate": 3680.48282175937,
          "standard_error": 835.4213754120283
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156730.50733036565,
            "upper_bound": 157178.23309121002
          },
          "point_estimate": 156946.34555247676,
          "standard_error": 114.87609031241624
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156628.15086206896,
            "upper_bound": 157286.45641762452
          },
          "point_estimate": 156851.61022167487,
          "standard_error": 175.65602414996363
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.2321636820351,
            "upper_bound": 643.3471039518785
          },
          "point_estimate": 396.0676666752867,
          "standard_error": 147.7233276821922
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156849.73683464548,
            "upper_bound": 157261.3187385148
          },
          "point_estimate": 157059.22315270937,
          "standard_error": 103.34960760429888
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202.17573612913424,
            "upper_bound": 458.92472620752255
          },
          "point_estimate": 383.4575043764164,
          "standard_error": 62.73178730658119
        }
      }
    },
    "memmem/libc/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/libc_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6701.67769045858,
            "upper_bound": 6712.991457408778
          },
          "point_estimate": 6707.2821924309665,
          "standard_error": 2.8829364380732208
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6702.06961908284,
            "upper_bound": 6713.7428346893485
          },
          "point_estimate": 6706.01939718935,
          "standard_error": 2.674656002257356
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6555334173546157,
            "upper_bound": 16.21986668689187
          },
          "point_estimate": 6.276495239900849,
          "standard_error": 3.9522080916107583
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6704.084072468993,
            "upper_bound": 6712.138657672441
          },
          "point_estimate": 6707.883223987551,
          "standard_error": 2.04401074665169
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.179386127883767,
            "upper_bound": 12.93340206798774
          },
          "point_estimate": 9.603883482158436,
          "standard_error": 2.200981223144266
        }
      }
    },
    "memmem/libc/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/libc_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7363.034228878997,
            "upper_bound": 7380.753628238531
          },
          "point_estimate": 7371.761907246587,
          "standard_error": 4.552605305396681
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7361.382555724418,
            "upper_bound": 7387.150962512665
          },
          "point_estimate": 7366.452541371158,
          "standard_error": 7.207569720013485
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9956502518263712,
            "upper_bound": 25.568070300889925
          },
          "point_estimate": 14.029134439382757,
          "standard_error": 6.6894274570863645
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7362.728281489665,
            "upper_bound": 7380.882214107134
          },
          "point_estimate": 7371.685424808221,
          "standard_error": 4.881140036149868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.882428933621311,
            "upper_bound": 18.55740023746761
          },
          "point_estimate": 15.13786299223976,
          "standard_error": 2.447934380152194
        }
      }
    },
    "memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/libc_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47500.94272829131,
            "upper_bound": 47547.33659812869
          },
          "point_estimate": 47522.468770774976,
          "standard_error": 11.87518336179103
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47493.41165577342,
            "upper_bound": 47544.559041394336
          },
          "point_estimate": 47513.44176470589,
          "standard_error": 13.160610825398964
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.861756923933471,
            "upper_bound": 62.36286617388048
          },
          "point_estimate": 36.204602233055695,
          "standard_error": 13.946538212842157
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47497.50682569493,
            "upper_bound": 47531.28621328313
          },
          "point_estimate": 47515.53672863085,
          "standard_error": 8.595716528291662
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.037226537360336,
            "upper_bound": 53.442057881463285
          },
          "point_estimate": 39.48316533872608,
          "standard_error": 9.88261919906072
        }
      }
    },
    "memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/libc_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.164744063148,
            "upper_bound": 116.34299778012486
          },
          "point_estimate": 116.25752524337716,
          "standard_error": 0.04572326959516879
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.21235508683066,
            "upper_bound": 116.354665095486
          },
          "point_estimate": 116.2600929570814,
          "standard_error": 0.028948493601391696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005534081104827244,
            "upper_bound": 0.2566528781789325
          },
          "point_estimate": 0.04697399913726197,
          "standard_error": 0.06523917703886509
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.07126189297034,
            "upper_bound": 116.30403774816348
          },
          "point_estimate": 116.20261309253172,
          "standard_error": 0.06447107611469614
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03837945737966561,
            "upper_bound": 0.2134645513339283
          },
          "point_estimate": 0.15231209601078757,
          "standard_error": 0.04189792334505058
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.195155382646615,
            "upper_bound": 19.241609165487947
          },
          "point_estimate": 19.220105178380685,
          "standard_error": 0.011956704277201613
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.191253049475055,
            "upper_bound": 19.242044914790817
          },
          "point_estimate": 19.23036134761984,
          "standard_error": 0.010735062015096728
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0027041280008852048,
            "upper_bound": 0.06293312897498586
          },
          "point_estimate": 0.017636484024036572,
          "standard_error": 0.014907886790155173
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.190204312057805,
            "upper_bound": 19.233779530056076
          },
          "point_estimate": 19.21841588831812,
          "standard_error": 0.011420801919491482
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010806395593919252,
            "upper_bound": 0.051414049008525725
          },
          "point_estimate": 0.03992374013629572,
          "standard_error": 0.009884733075057987
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.373091456786277,
            "upper_bound": 19.47281978266825
          },
          "point_estimate": 19.422336887865193,
          "standard_error": 0.02552151361360936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.359280156505385,
            "upper_bound": 19.49565362330386
          },
          "point_estimate": 19.427560403249323,
          "standard_error": 0.033388793117865474
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013449176498646664,
            "upper_bound": 0.14322703944558726
          },
          "point_estimate": 0.09379601098606372,
          "standard_error": 0.0375241070756244
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.338669005801087,
            "upper_bound": 19.49684676547519
          },
          "point_estimate": 19.421945257903428,
          "standard_error": 0.04163677893747053
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04513859926622767,
            "upper_bound": 0.10747758672323768
          },
          "point_estimate": 0.08479540426513184,
          "standard_error": 0.015658149664232443
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.579917306456,
            "upper_bound": 18.632152003795355
          },
          "point_estimate": 18.60446367837484,
          "standard_error": 0.013399101433274304
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.576640482544743,
            "upper_bound": 18.635098336474847
          },
          "point_estimate": 18.591877725586333,
          "standard_error": 0.014455511001953183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006506574049403886,
            "upper_bound": 0.07349768849118805
          },
          "point_estimate": 0.026563180847989858,
          "standard_error": 0.017625406309523656
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.57435608314141,
            "upper_bound": 18.60713838430393
          },
          "point_estimate": 18.589953798874912,
          "standard_error": 0.008253342764656273
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018115897351395304,
            "upper_bound": 0.0582035799080344
          },
          "point_estimate": 0.04448345966357671,
          "standard_error": 0.010158913372682307
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.71991731445353,
            "upper_bound": 18.12770709544731
          },
          "point_estimate": 17.925899909176234,
          "standard_error": 0.1040949813239293
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.72439377273612,
            "upper_bound": 18.212330399177564
          },
          "point_estimate": 17.916897384477718,
          "standard_error": 0.10882956791979442
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02530932406229731,
            "upper_bound": 0.6040402754769668
          },
          "point_estimate": 0.31214189546582743,
          "standard_error": 0.14456451269751056
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.818819258137843,
            "upper_bound": 18.298032045222673
          },
          "point_estimate": 18.08983187311951,
          "standard_error": 0.1215710959802693
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.168708667747845,
            "upper_bound": 0.4631919586022523
          },
          "point_estimate": 0.34683726873140885,
          "standard_error": 0.07612403347704111
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.24631809394107,
            "upper_bound": 25.34380210365969
          },
          "point_estimate": 25.29389937169494,
          "standard_error": 0.025053788829767044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.21753728411577,
            "upper_bound": 25.38985150321962
          },
          "point_estimate": 25.269981756129702,
          "standard_error": 0.05135100805075088
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006357002315384451,
            "upper_bound": 0.13299653047507376
          },
          "point_estimate": 0.09207035431756416,
          "standard_error": 0.03946102411925551
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.222758693926483,
            "upper_bound": 25.282380043073516
          },
          "point_estimate": 25.240572515670852,
          "standard_error": 0.01562684997464598
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05456809999784719,
            "upper_bound": 0.09501123994036095
          },
          "point_estimate": 0.08386643262919995,
          "standard_error": 0.010529810188333433
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.90353892705886,
            "upper_bound": 27.98849908660544
          },
          "point_estimate": 27.94907054113228,
          "standard_error": 0.021589373904159707
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.879505018614022,
            "upper_bound": 27.993276240231392
          },
          "point_estimate": 27.98575622679617,
          "standard_error": 0.02803017275037838
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002401301989576848,
            "upper_bound": 0.11562668366031267
          },
          "point_estimate": 0.01138856770994086,
          "standard_error": 0.029111072024534293
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.972145923021184,
            "upper_bound": 27.993585538863552
          },
          "point_estimate": 27.986821676850017,
          "standard_error": 0.005644715951042901
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010410603519464273,
            "upper_bound": 0.09323563958932292
          },
          "point_estimate": 0.07170917880169063,
          "standard_error": 0.018047477712063404
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.098931044219285,
            "upper_bound": 24.13528440545
          },
          "point_estimate": 24.11500966484696,
          "standard_error": 0.009432957760207084
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.091831765595565,
            "upper_bound": 24.12720285438518
          },
          "point_estimate": 24.108192130059585,
          "standard_error": 0.009431792611966246
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005498822347782018,
            "upper_bound": 0.04121254242202588
          },
          "point_estimate": 0.0213006464363908,
          "standard_error": 0.009195034397353323
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.09970767800646,
            "upper_bound": 24.126628380349366
          },
          "point_estimate": 24.111647621092256,
          "standard_error": 0.006931337657886794
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011699827250148852,
            "upper_bound": 0.044662262896339504
          },
          "point_estimate": 0.03157476049029875,
          "standard_error": 0.00973925848874599
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.004621036900527,
            "upper_bound": 29.058304220553328
          },
          "point_estimate": 29.032655152785384,
          "standard_error": 0.013831931566256056
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.9843069216874,
            "upper_bound": 29.07082788881604
          },
          "point_estimate": 29.049021584528543,
          "standard_error": 0.02329160222492595
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0042189934352179536,
            "upper_bound": 0.0735905266524879
          },
          "point_estimate": 0.04038279416009381,
          "standard_error": 0.018956467513491473
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.00859603233173,
            "upper_bound": 29.06612984967834
          },
          "point_estimate": 29.04072248658106,
          "standard_error": 0.016358328860641133
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02406934148132785,
            "upper_bound": 0.054065565096396255
          },
          "point_estimate": 0.045976236980938265,
          "standard_error": 0.007254550809561321
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.387159918682045,
            "upper_bound": 38.4937161220513
          },
          "point_estimate": 38.445276903617,
          "standard_error": 0.027540573272698033
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.40060868499746,
            "upper_bound": 38.4942661843898
          },
          "point_estimate": 38.47698325680538,
          "standard_error": 0.024850108833026945
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005657825745399791,
            "upper_bound": 0.12583827663185063
          },
          "point_estimate": 0.028643208300990364,
          "standard_error": 0.037242536039884554
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.34284270191404,
            "upper_bound": 38.487418108340286
          },
          "point_estimate": 38.42562737474944,
          "standard_error": 0.03715826040667823
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02697935942578904,
            "upper_bound": 0.12274767393596292
          },
          "point_estimate": 0.09193213464515831,
          "standard_error": 0.02448602909410148
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.031020557049068,
            "upper_bound": 21.13636118196146
          },
          "point_estimate": 21.092995165366847,
          "standard_error": 0.028247007011583294
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.070324307049347,
            "upper_bound": 21.14510431926493
          },
          "point_estimate": 21.125093432634237,
          "standard_error": 0.017355889769723607
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0050934561675713376,
            "upper_bound": 0.08016723557943067
          },
          "point_estimate": 0.02653033272247624,
          "standard_error": 0.01990610605503496
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.991278138459673,
            "upper_bound": 21.139995390107025
          },
          "point_estimate": 21.082026752013725,
          "standard_error": 0.03947685219697983
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015428767155758171,
            "upper_bound": 0.13969833442385585
          },
          "point_estimate": 0.09473032094273022,
          "standard_error": 0.037590870518948265
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.18961208961419,
            "upper_bound": 25.22620078435804
          },
          "point_estimate": 25.20550277806766,
          "standard_error": 0.009459738237079986
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.183433795062232,
            "upper_bound": 25.21929265640202
          },
          "point_estimate": 25.196896902724585,
          "standard_error": 0.009452282374450205
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005539602788462565,
            "upper_bound": 0.04104399571443844
          },
          "point_estimate": 0.02297840991254388,
          "standard_error": 0.008825835518563674
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.185769607293853,
            "upper_bound": 25.215654480110572
          },
          "point_estimate": 25.203521530069622,
          "standard_error": 0.007728536707978235
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011641503047218536,
            "upper_bound": 0.04519359435176721
          },
          "point_estimate": 0.03143755321175984,
          "standard_error": 0.010210075069095688
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.97901736446746,
            "upper_bound": 36.13034169542939
          },
          "point_estimate": 35.54088631728449,
          "standard_error": 0.2951638394908373
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.76143535165661,
            "upper_bound": 36.4410625993738
          },
          "point_estimate": 35.37302814301944,
          "standard_error": 0.43339598790347195
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14794968025249502,
            "upper_bound": 1.6336551194364803
          },
          "point_estimate": 0.9227876922806548,
          "standard_error": 0.3917665575645824
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.237683986741224,
            "upper_bound": 36.49580215814544
          },
          "point_estimate": 35.860002266253396,
          "standard_error": 0.32660272058085316
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.543610895121495,
            "upper_bound": 1.2073650441594423
          },
          "point_estimate": 0.9852177459187426,
          "standard_error": 0.16372364784031168
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 894835.8554359269,
            "upper_bound": 895318.9445679442
          },
          "point_estimate": 895053.8724080527,
          "standard_error": 124.83283153541986
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 894738.3902439025,
            "upper_bound": 895278.8048780488
          },
          "point_estimate": 894901.2597560976,
          "standard_error": 134.3331848567112
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.795301994230634,
            "upper_bound": 605.4562620152202
          },
          "point_estimate": 282.3151888902923,
          "standard_error": 142.64770732333034
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 894740.7556524941,
            "upper_bound": 895181.1862427945
          },
          "point_estimate": 894916.4205258157,
          "standard_error": 115.46435041746246
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.478292694363,
            "upper_bound": 563.6964179527288
          },
          "point_estimate": 414.6433919968191,
          "standard_error": 112.26162340834112
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167059.1652497452,
            "upper_bound": 167273.87162796257
          },
          "point_estimate": 167155.4633575433,
          "standard_error": 55.19511798963385
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167032.8883792049,
            "upper_bound": 167240.7780963303
          },
          "point_estimate": 167139.99375637103,
          "standard_error": 53.89173892269881
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6633141093,
            "upper_bound": 257.6193144630426
          },
          "point_estimate": 155.68935722063017,
          "standard_error": 67.48765753980531
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167066.17337009116,
            "upper_bound": 167172.1090689313
          },
          "point_estimate": 167116.225163827,
          "standard_error": 27.748963468561055
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.42480439825525,
            "upper_bound": 255.2402731797223
          },
          "point_estimate": 183.7344326463131,
          "standard_error": 52.544675195090385
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 702712.8644905754,
            "upper_bound": 704337.7533088371
          },
          "point_estimate": 703497.0733295177,
          "standard_error": 415.9641042780262
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 702361.9278846154,
            "upper_bound": 704683.9560439561
          },
          "point_estimate": 703257.8814102564,
          "standard_error": 543.6833053369121
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298.66358719761536,
            "upper_bound": 2487.2309410993153
          },
          "point_estimate": 1356.2776247994757,
          "standard_error": 535.1201702814801
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 702639.2715672218,
            "upper_bound": 704106.5767497933
          },
          "point_estimate": 703349.7379120879,
          "standard_error": 372.87185005867167
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 737.3782268268473,
            "upper_bound": 1725.606457585501
          },
          "point_estimate": 1382.8180920435568,
          "standard_error": 252.0260921268003
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355297.7043993874,
            "upper_bound": 355735.7240620666
          },
          "point_estimate": 355524.1848709354,
          "standard_error": 111.722378407138
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355282.9813915858,
            "upper_bound": 355828.9395900755
          },
          "point_estimate": 355590.4811893204,
          "standard_error": 138.361221663345
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.03715981802509,
            "upper_bound": 634.1447138872912
          },
          "point_estimate": 357.6159684487771,
          "standard_error": 134.86997713702678
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355067.3499081128,
            "upper_bound": 355757.2290882641
          },
          "point_estimate": 355407.9217248771,
          "standard_error": 186.58402602181565
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.88203207715983,
            "upper_bound": 490.8644644583335
          },
          "point_estimate": 373.3651978345364,
          "standard_error": 79.00404926982704
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 539033.9079379814,
            "upper_bound": 540108.0630094976
          },
          "point_estimate": 539477.1255012839,
          "standard_error": 283.0018680547921
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 539009.7522058825,
            "upper_bound": 539548.6132352941
          },
          "point_estimate": 539358.8970588235,
          "standard_error": 170.89764131496634
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.65586248303872,
            "upper_bound": 848.4522524371538
          },
          "point_estimate": 424.6831314309098,
          "standard_error": 207.0157063497125
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 538910.2329564774,
            "upper_bound": 539555.1857210626
          },
          "point_estimate": 539256.9341100076,
          "standard_error": 165.0796479447457
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 210.13410421418243,
            "upper_bound": 1407.5927481404929
          },
          "point_estimate": 940.3722731292766,
          "standard_error": 376.833366756465
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233434.11185188367,
            "upper_bound": 234198.5912075321
          },
          "point_estimate": 233808.0470301689,
          "standard_error": 196.0920329984536
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233210.6717948718,
            "upper_bound": 234416.96634615384
          },
          "point_estimate": 233724.3283653846,
          "standard_error": 274.77276513233403
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189.27717106274127,
            "upper_bound": 1139.147818979167
          },
          "point_estimate": 677.3572794648767,
          "standard_error": 253.43726229238095
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233187.70304177224,
            "upper_bound": 233873.11342919312
          },
          "point_estimate": 233446.6194971695,
          "standard_error": 176.2561897225263
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 369.041698037104,
            "upper_bound": 808.0649033348718
          },
          "point_estimate": 653.3727224603188,
          "standard_error": 111.86924871287536
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401144.3608759376,
            "upper_bound": 402009.4231004709
          },
          "point_estimate": 401506.59454779344,
          "standard_error": 226.99882443415984
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401067.99267399264,
            "upper_bound": 401681.07623626373
          },
          "point_estimate": 401174.2395604396,
          "standard_error": 177.81406812444587
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.38141778015608,
            "upper_bound": 821.8037850895322
          },
          "point_estimate": 170.75876876330466,
          "standard_error": 210.6761783205593
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401120.2999021526,
            "upper_bound": 401718.40335557464
          },
          "point_estimate": 401356.29978592834,
          "standard_error": 157.4141464236173
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.79231760571584,
            "upper_bound": 1102.3547731943047
          },
          "point_estimate": 757.4545001383194,
          "standard_error": 278.06178918577086
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389358.7857556568,
            "upper_bound": 390633.54220292973
          },
          "point_estimate": 390064.6162369132,
          "standard_error": 329.76309797767067
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389549.29122340423,
            "upper_bound": 390687.6317375887
          },
          "point_estimate": 390456.4327507599,
          "standard_error": 281.27245808801104
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.11572389645828,
            "upper_bound": 1533.0576613466517
          },
          "point_estimate": 396.99565976040645,
          "standard_error": 386.8501641359952
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390320.4627971017,
            "upper_bound": 390927.8018848485
          },
          "point_estimate": 390611.1231831998,
          "standard_error": 160.9304401390035
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 313.39469675838825,
            "upper_bound": 1498.923633148614
          },
          "point_estimate": 1098.5159458482983,
          "standard_error": 317.23277642110884
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292648.88792517915,
            "upper_bound": 293546.9913119719
          },
          "point_estimate": 293046.300016001,
          "standard_error": 233.2184264661807
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292521.5468637993,
            "upper_bound": 293405.6821236559
          },
          "point_estimate": 292889.6911962365,
          "standard_error": 200.94164934120775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.33904655879184,
            "upper_bound": 1051.4780068163534
          },
          "point_estimate": 429.6885259332855,
          "standard_error": 246.1227551386839
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292594.2822406636,
            "upper_bound": 292935.46691347857
          },
          "point_estimate": 292734.8326351068,
          "standard_error": 86.83259957642332
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 245.7092180622771,
            "upper_bound": 1083.7086982092585
          },
          "point_estimate": 777.8797911183042,
          "standard_error": 236.4861106183383
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296511.9143537553,
            "upper_bound": 297379.21385859465
          },
          "point_estimate": 296853.2459852885,
          "standard_error": 237.84877598418703
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296437.701897019,
            "upper_bound": 296824.78211382113
          },
          "point_estimate": 296705.7598722416,
          "standard_error": 112.24444985172894
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.76523149847674,
            "upper_bound": 535.9407050786765
          },
          "point_estimate": 258.5252565484773,
          "standard_error": 134.9386118518831
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296440.50997740595,
            "upper_bound": 296811.7775080013
          },
          "point_estimate": 296625.84928729804,
          "standard_error": 94.49481644391628
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153.52642762350655,
            "upper_bound": 1210.4716329589255
          },
          "point_estimate": 794.6132591762819,
          "standard_error": 356.5777372068395
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323615.62742098613,
            "upper_bound": 324154.52947724407
          },
          "point_estimate": 323865.82575326593,
          "standard_error": 138.1096847692082
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323484.0530973451,
            "upper_bound": 324285.15984513273
          },
          "point_estimate": 323751.86224188795,
          "standard_error": 176.0702145402577
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.53995726130138,
            "upper_bound": 742.7449204640951
          },
          "point_estimate": 374.2174176041007,
          "standard_error": 171.30400304007077
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323645.89248643693,
            "upper_bound": 323933.3081254655
          },
          "point_estimate": 323775.14405240776,
          "standard_error": 72.02154522531983
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 185.29333261126024,
            "upper_bound": 570.6804870951164
          },
          "point_estimate": 459.0506680359274,
          "standard_error": 99.43624734956931
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152949.89502038315,
            "upper_bound": 153286.70967787114
          },
          "point_estimate": 153115.64197979192,
          "standard_error": 86.20517509018869
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152817.69927971187,
            "upper_bound": 153361.64705882352
          },
          "point_estimate": 153072.66922268906,
          "standard_error": 136.15572986767853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.74719878543177,
            "upper_bound": 451.13399199076446
          },
          "point_estimate": 394.5105756220729,
          "standard_error": 115.69080961506336
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152845.61215731065,
            "upper_bound": 153240.6276831047
          },
          "point_estimate": 153037.39546000218,
          "standard_error": 106.19854653132924
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.94692432670507,
            "upper_bound": 353.89570473333407
          },
          "point_estimate": 287.10710403819695,
          "standard_error": 46.86225506725588
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223516.5071785836,
            "upper_bound": 224007.4893171195
          },
          "point_estimate": 223741.4182008959,
          "standard_error": 125.9244865163764
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223440.4818872334,
            "upper_bound": 223916.102931152
          },
          "point_estimate": 223670.4165644172,
          "standard_error": 100.3034688245256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.36488163079208,
            "upper_bound": 620.4112624824558
          },
          "point_estimate": 214.37920621445372,
          "standard_error": 160.88237259279325
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223426.91837538136,
            "upper_bound": 223735.8063179421
          },
          "point_estimate": 223603.2582423711,
          "standard_error": 80.62810909793282
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161.97960019896786,
            "upper_bound": 584.3635140204768
          },
          "point_estimate": 419.9343973381097,
          "standard_error": 118.43563689145658
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/libc_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84635.89458397935,
            "upper_bound": 84676.78502642579
          },
          "point_estimate": 84651.94131801403,
          "standard_error": 11.205432216316895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84634.97771317829,
            "upper_bound": 84653.09825581395
          },
          "point_estimate": 84640.6286498708,
          "standard_error": 5.76039486373182
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0716012600489533,
            "upper_bound": 25.949665760216053
          },
          "point_estimate": 8.818671839569985,
          "standard_error": 6.8011332618368625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84636.77981812487,
            "upper_bound": 84653.91083333333
          },
          "point_estimate": 84645.06327393536,
          "standard_error": 4.684338506080425
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.43097040030634,
            "upper_bound": 56.47033448674118
          },
          "point_estimate": 37.1520211615231,
          "standard_error": 16.516951780291475
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/libc_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1074284.4835375815,
            "upper_bound": 1077560.3882644724
          },
          "point_estimate": 1075746.3560725958,
          "standard_error": 847.0199512682993
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1073613.5205882352,
            "upper_bound": 1076886.2536764706
          },
          "point_estimate": 1075114.3308823528,
          "standard_error": 888.5258666119621
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 535.0114625603901,
            "upper_bound": 3903.3021373692823
          },
          "point_estimate": 2227.190084847036,
          "standard_error": 825.9902434045653
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1074096.3301430007,
            "upper_bound": 1076479.3967109425
          },
          "point_estimate": 1075349.2873185638,
          "standard_error": 609.6111468832571
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1136.1695171241483,
            "upper_bound": 3987.160488086218
          },
          "point_estimate": 2822.8658824061167,
          "standard_error": 851.5544480885396
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/libc_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2152.9304483672217,
            "upper_bound": 2160.1571238488273
          },
          "point_estimate": 2156.791610956537,
          "standard_error": 1.8359769265174215
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2155.076301882311,
            "upper_bound": 2158.6997862359717
          },
          "point_estimate": 2157.5370326386,
          "standard_error": 0.8726053718475753
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3539398213688052,
            "upper_bound": 7.895466241277032
          },
          "point_estimate": 1.475370820550231,
          "standard_error": 2.00531236103585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2154.7564109377136,
            "upper_bound": 2158.2822181050046
          },
          "point_estimate": 2157.140628969036,
          "standard_error": 0.9195722113503276
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8767653725769202,
            "upper_bound": 8.968987489893207
          },
          "point_estimate": 6.13103720653949,
          "standard_error": 1.9942134060551004
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51826.5505646961,
            "upper_bound": 51885.008696581186
          },
          "point_estimate": 51852.74395570479,
          "standard_error": 15.087339702847592
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51812.16071937322,
            "upper_bound": 51879.82336182336
          },
          "point_estimate": 51842.83911613078,
          "standard_error": 20.707313197986164
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.75726112265802,
            "upper_bound": 81.9504872630362
          },
          "point_estimate": 45.78923327539929,
          "standard_error": 20.436438352940147
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51815.50297385955,
            "upper_bound": 51862.605912134575
          },
          "point_estimate": 51836.044092944096,
          "standard_error": 12.55777606834457
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.49268073461231,
            "upper_bound": 69.91955796792391
          },
          "point_estimate": 50.24730588855496,
          "standard_error": 14.064905722487456
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53213.422330943096,
            "upper_bound": 53411.331793270794
          },
          "point_estimate": 53300.30864394784,
          "standard_error": 51.25082788047608
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53164.08187134503,
            "upper_bound": 53357.46081871345
          },
          "point_estimate": 53272.80902777778,
          "standard_error": 50.26219182865876
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.703989363004112,
            "upper_bound": 223.81049589499813
          },
          "point_estimate": 143.35181113920845,
          "standard_error": 51.86864922406134
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53183.43880491032,
            "upper_bound": 53306.67015422699
          },
          "point_estimate": 53237.36091744513,
          "standard_error": 31.785814104771113
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.09612002520288,
            "upper_bound": 242.4881862157035
          },
          "point_estimate": 170.19009855578872,
          "standard_error": 53.48381269367928
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52899.271192332686,
            "upper_bound": 52974.35530864197
          },
          "point_estimate": 52934.4256012717,
          "standard_error": 19.28835787277669
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52874.84210526316,
            "upper_bound": 52973.27452891488
          },
          "point_estimate": 52932.43216374269,
          "standard_error": 22.802655483284664
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.206523028907991,
            "upper_bound": 103.31332701572757
          },
          "point_estimate": 64.53829214368795,
          "standard_error": 25.288359454347383
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52907.88770992715,
            "upper_bound": 52988.49786628734
          },
          "point_estimate": 52948.64000531632,
          "standard_error": 20.30870056795169
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.595932621361683,
            "upper_bound": 85.13647213216039
          },
          "point_estimate": 64.14513154447619,
          "standard_error": 14.691586555966998
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15393.508177768348,
            "upper_bound": 15419.49965009664
          },
          "point_estimate": 15405.678089497003,
          "standard_error": 6.689235738085539
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15388.424211568376,
            "upper_bound": 15425.64225710649
          },
          "point_estimate": 15399.239587752792,
          "standard_error": 9.144685414086227
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7419920670797269,
            "upper_bound": 38.129615572534014
          },
          "point_estimate": 17.722091361229342,
          "standard_error": 8.88807865159521
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15388.804638164418,
            "upper_bound": 15408.73250079473
          },
          "point_estimate": 15397.758216751428,
          "standard_error": 5.158249838564937
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.882345848690774,
            "upper_bound": 27.52121319899883
          },
          "point_estimate": 22.26728398099044,
          "standard_error": 4.48473460836843
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26996.71863747966,
            "upper_bound": 27040.657608469544
          },
          "point_estimate": 27017.54043258685,
          "standard_error": 11.237273370255274
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26990.208110450716,
            "upper_bound": 27037.583333333332
          },
          "point_estimate": 27014.070356612185,
          "standard_error": 8.493551769984222
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.1955940740240494,
            "upper_bound": 62.78748470996806
          },
          "point_estimate": 12.15569640380668,
          "standard_error": 18.031065469773683
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26989.855382231544,
            "upper_bound": 27021.961943536404
          },
          "point_estimate": 27006.554134424267,
          "standard_error": 8.198623105137093
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.294262553075246,
            "upper_bound": 50.4566773658904
          },
          "point_estimate": 37.39464763209613,
          "standard_error": 9.334911542667063
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16534.634182654376,
            "upper_bound": 16555.916291615187
          },
          "point_estimate": 16545.541394217224,
          "standard_error": 5.458419869903165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16532.349548626917,
            "upper_bound": 16562.521707089756
          },
          "point_estimate": 16546.055826126536,
          "standard_error": 8.693392882012331
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.3264304484553264,
            "upper_bound": 32.69213359119702
          },
          "point_estimate": 19.989793545883273,
          "standard_error": 6.976365692831782
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16529.386410019346,
            "upper_bound": 16551.986270358953
          },
          "point_estimate": 16540.563367992952,
          "standard_error": 5.733848922405249
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.846359381069425,
            "upper_bound": 23.08235157809324
          },
          "point_estimate": 18.26287210728444,
          "standard_error": 3.250163172367181
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16532.548163778607,
            "upper_bound": 16589.069312363055
          },
          "point_estimate": 16557.207340456305,
          "standard_error": 14.686044442416575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16527.350325414904,
            "upper_bound": 16580.92884586181
          },
          "point_estimate": 16535.607232346243,
          "standard_error": 13.557545622855857
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.933164594847086,
            "upper_bound": 61.79209889841495
          },
          "point_estimate": 25.47175987808431,
          "standard_error": 15.983699015559186
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16529.046302254785,
            "upper_bound": 16564.12523422664
          },
          "point_estimate": 16544.006331982368,
          "standard_error": 9.038545013231907
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.78896955826504,
            "upper_bound": 68.68258564922941
          },
          "point_estimate": 48.98747922339295,
          "standard_error": 15.367667264859358
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19104.27258894147,
            "upper_bound": 19140.220192494737
          },
          "point_estimate": 19121.60312229816,
          "standard_error": 9.18711343879866
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19096.81335436383,
            "upper_bound": 19146.06069634303
          },
          "point_estimate": 19115.22362425517,
          "standard_error": 12.565823193899824
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.535458572842372,
            "upper_bound": 51.67374400768639
          },
          "point_estimate": 30.168986709396464,
          "standard_error": 11.60110309615039
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19100.58897412318,
            "upper_bound": 19132.111283171533
          },
          "point_estimate": 19116.90851735016,
          "standard_error": 8.070003350201636
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.765255312400637,
            "upper_bound": 37.57081550240197
          },
          "point_estimate": 30.66111104689869,
          "standard_error": 5.375865670923618
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23038.5105060596,
            "upper_bound": 23094.154022910803
          },
          "point_estimate": 23063.454300825437,
          "standard_error": 14.321004825968112
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23033.863668150032,
            "upper_bound": 23087.511549057
          },
          "point_estimate": 23041.55701571156,
          "standard_error": 15.28688048893106
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.1742396766384653,
            "upper_bound": 70.31133765826998
          },
          "point_estimate": 17.793391066813957,
          "standard_error": 17.669206752582934
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23028.654034458497,
            "upper_bound": 23047.529204719893
          },
          "point_estimate": 23035.108494810975,
          "standard_error": 4.836837245806515
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.363264745382578,
            "upper_bound": 63.450675668651016
          },
          "point_estimate": 47.5935932132892,
          "standard_error": 12.799827491467807
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15788.261638151533,
            "upper_bound": 15814.7880092312
          },
          "point_estimate": 15801.045862015968,
          "standard_error": 6.786524814110667
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15782.663011314187,
            "upper_bound": 15820.50506133698
          },
          "point_estimate": 15795.89898281114,
          "standard_error": 9.096784733887036
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.033594788443048,
            "upper_bound": 39.56262176498355
          },
          "point_estimate": 23.770393481747156,
          "standard_error": 8.56080408657915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15794.141647376106,
            "upper_bound": 15813.024676270796
          },
          "point_estimate": 15803.414108259018,
          "standard_error": 4.784756038765727
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.850111457285209,
            "upper_bound": 28.350873243851108
          },
          "point_estimate": 22.639444606074544,
          "standard_error": 4.1573125926145815
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19932.51101407065,
            "upper_bound": 19984.966149683496
          },
          "point_estimate": 19954.115784443053,
          "standard_error": 13.85715272576318
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19924.47065277016,
            "upper_bound": 19959.582247668677
          },
          "point_estimate": 19948.566935715593,
          "standard_error": 10.26793563110831
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.647512055307361,
            "upper_bound": 46.403550598735016
          },
          "point_estimate": 21.490664791254954,
          "standard_error": 10.520511319875483
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19929.17979038584,
            "upper_bound": 19953.28503134917
          },
          "point_estimate": 19939.9075905992,
          "standard_error": 6.088695760047048
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.105691193302697,
            "upper_bound": 69.08110872380428
          },
          "point_estimate": 46.144370334419435,
          "standard_error": 18.19783648292936
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16728.741782405887,
            "upper_bound": 16749.896560212906
          },
          "point_estimate": 16738.061580374047,
          "standard_error": 5.478836076171561
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16727.649828548034,
            "upper_bound": 16749.65598802395
          },
          "point_estimate": 16732.215228882895,
          "standard_error": 4.351233834343149
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0836546974833277,
            "upper_bound": 23.446522992310133
          },
          "point_estimate": 5.037774398508895,
          "standard_error": 5.235677698054568
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16729.15883685243,
            "upper_bound": 16733.940396674967
          },
          "point_estimate": 16731.424967846524,
          "standard_error": 1.2227627734774584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.5236039149047955,
            "upper_bound": 23.902381540923116
          },
          "point_estimate": 18.302007687469754,
          "standard_error": 5.460647345410136
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19254.996203772047,
            "upper_bound": 19276.259727080014
          },
          "point_estimate": 19264.85175892279,
          "standard_error": 5.457321220374833
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19249.72142730966,
            "upper_bound": 19274.95861155273
          },
          "point_estimate": 19262.34825687032,
          "standard_error": 6.417494599058789
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8789643723120166,
            "upper_bound": 28.872200600293485
          },
          "point_estimate": 16.787193903081498,
          "standard_error": 6.973732895764074
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19250.313336284227,
            "upper_bound": 19271.254980774047
          },
          "point_estimate": 19260.500357194476,
          "standard_error": 5.502608380692998
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.683293259000898,
            "upper_bound": 23.60887827404312
          },
          "point_estimate": 18.2168604088836,
          "standard_error": 4.1075324521027845
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16375.056872986812,
            "upper_bound": 16390.598503971385
          },
          "point_estimate": 16383.04386043692,
          "standard_error": 3.9769608883230334
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16373.357879997424,
            "upper_bound": 16395.11401532222
          },
          "point_estimate": 16383.866756797355,
          "standard_error": 6.324479489655526
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3076070128842985,
            "upper_bound": 23.17041013280835
          },
          "point_estimate": 15.564668927142444,
          "standard_error": 5.101943677160161
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16371.24903064647,
            "upper_bound": 16382.912228903702
          },
          "point_estimate": 16376.38342414683,
          "standard_error": 2.9510858077617015
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.756675539039636,
            "upper_bound": 16.85797029882422
          },
          "point_estimate": 13.28103670687613,
          "standard_error": 2.3981856986455994
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16622.41166214656,
            "upper_bound": 16682.186413250263
          },
          "point_estimate": 16648.67973781568,
          "standard_error": 15.450858532850402
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16617.04373284538,
            "upper_bound": 16669.8016068161
          },
          "point_estimate": 16637.215061756633,
          "standard_error": 10.057724916742222
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6187108078576957,
            "upper_bound": 68.30471492603388
          },
          "point_estimate": 13.608372603780955,
          "standard_error": 19.953412504829792
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16610.461914607076,
            "upper_bound": 16641.8614015518
          },
          "point_estimate": 16627.538604579317,
          "standard_error": 8.479114442419322
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.482835742335658,
            "upper_bound": 71.45694802566318
          },
          "point_estimate": 51.373388100759286,
          "standard_error": 15.655746611305924
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16528.568207176602,
            "upper_bound": 16541.505479575404
          },
          "point_estimate": 16534.391175877205,
          "standard_error": 3.352445478866597
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16527.14987873276,
            "upper_bound": 16541.263165075034
          },
          "point_estimate": 16531.984326840484,
          "standard_error": 2.726240826341161
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.65478970529359,
            "upper_bound": 13.833939727110533
          },
          "point_estimate": 4.849009670242393,
          "standard_error": 3.829575292892357
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16526.87828068017,
            "upper_bound": 16542.850654602575
          },
          "point_estimate": 16532.94198897964,
          "standard_error": 4.189466673832084
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.223489363584602,
            "upper_bound": 14.888721441782902
          },
          "point_estimate": 11.196872575832034,
          "standard_error": 3.1353424072141016
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19681.973564092143,
            "upper_bound": 19735.913902439024
          },
          "point_estimate": 19705.07099090205,
          "standard_error": 13.606737510803145
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19679.75834688347,
            "upper_bound": 19740.349232158987
          },
          "point_estimate": 19684.53817266744,
          "standard_error": 12.522652389355487
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2116136451585495,
            "upper_bound": 67.7347293665772
          },
          "point_estimate": 10.784876854601414,
          "standard_error": 11.98836652716508
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19682.564885524513,
            "upper_bound": 19694.579021807127
          },
          "point_estimate": 19687.568894520115,
          "standard_error": 3.1436281290901724
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.430103451091791,
            "upper_bound": 56.958113602267105
          },
          "point_estimate": 45.4909962516126,
          "standard_error": 13.69183487741944
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17018.295362246692,
            "upper_bound": 17052.563759906658
          },
          "point_estimate": 17035.519535309817,
          "standard_error": 8.791730103166774
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17008.433119357025,
            "upper_bound": 17062.723549788636
          },
          "point_estimate": 17035.737868214455,
          "standard_error": 17.624302997207103
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.088901395231708,
            "upper_bound": 45.23292219483672
          },
          "point_estimate": 36.2780974430614,
          "standard_error": 11.318320324092571
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17020.392938058267,
            "upper_bound": 17053.91946822691
          },
          "point_estimate": 17034.796710851384,
          "standard_error": 8.552665211471448
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.810080518734207,
            "upper_bound": 33.99671901948934
          },
          "point_estimate": 29.435700807492324,
          "standard_error": 3.663452125803059
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19616.686733933933,
            "upper_bound": 19667.286451781427
          },
          "point_estimate": 19638.455245087946,
          "standard_error": 13.148377641728136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19608.852527027026,
            "upper_bound": 19650.666531531533
          },
          "point_estimate": 19633.657432432432,
          "standard_error": 10.658308496853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.687440493621023,
            "upper_bound": 51.312302337672214
          },
          "point_estimate": 29.19039442050276,
          "standard_error": 12.096957888899942
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19606.155156642908,
            "upper_bound": 19640.90186161265
          },
          "point_estimate": 19620.525728325727,
          "standard_error": 8.939628168172964
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.979223053713802,
            "upper_bound": 63.83282981525229
          },
          "point_estimate": 43.9520927691306,
          "standard_error": 15.090198299506602
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71585.8511920385,
            "upper_bound": 71730.19084802875
          },
          "point_estimate": 71652.61278597987,
          "standard_error": 37.16569175933571
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71556.35428149605,
            "upper_bound": 71726.71122047244
          },
          "point_estimate": 71642.49159284777,
          "standard_error": 35.373931262081996
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.572053417472178,
            "upper_bound": 205.49696593831183
          },
          "point_estimate": 84.99742487386862,
          "standard_error": 50.786162256992
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71553.76847365234,
            "upper_bound": 71647.79645163465
          },
          "point_estimate": 71604.71903057572,
          "standard_error": 25.033395951377976
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.4187771847908,
            "upper_bound": 165.31880246928353
          },
          "point_estimate": 123.94530801866354,
          "standard_error": 30.84368826401409
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231026.8337817458,
            "upper_bound": 1233251.4447887898
          },
          "point_estimate": 1232090.1187658731,
          "standard_error": 570.6907832453682
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1230331.9523809524,
            "upper_bound": 1234039.98
          },
          "point_estimate": 1231719.7033333334,
          "standard_error": 878.0851450844375
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.71907879343956,
            "upper_bound": 3037.537441489601
          },
          "point_estimate": 2094.81398147626,
          "standard_error": 776.869247112946
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231171.0472320376,
            "upper_bound": 1233404.126140947
          },
          "point_estimate": 1232226.9164502164,
          "standard_error": 562.3005887216489
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 902.6732870421996,
            "upper_bound": 2317.916776880683
          },
          "point_estimate": 1907.3187936472764,
          "standard_error": 330.42873546030813
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203207.5591630753,
            "upper_bound": 203571.06523510243
          },
          "point_estimate": 203369.93413075287,
          "standard_error": 93.41063787484114
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203116.1545623836,
            "upper_bound": 203437.19366852887
          },
          "point_estimate": 203380.9944134078,
          "standard_error": 83.04331362983346
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.52337166716181,
            "upper_bound": 441.3180284778974
          },
          "point_estimate": 125.95877411852491,
          "standard_error": 107.48573709542512
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203178.22899537755,
            "upper_bound": 203415.9002432156
          },
          "point_estimate": 203323.70875716463,
          "standard_error": 60.51565709111811
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.96923182095286,
            "upper_bound": 446.92123166420834
          },
          "point_estimate": 311.3213083863276,
          "standard_error": 98.84784287897682
        }
      }
    },
    "memmem/regex/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/regex_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6576.0902494993825,
            "upper_bound": 6581.92265158342
          },
          "point_estimate": 6578.594449436229,
          "standard_error": 1.5201416097184388
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6575.844888526373,
            "upper_bound": 6580.179282218597
          },
          "point_estimate": 6576.656360139367,
          "standard_error": 1.2299576703996824
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.33568484389872966,
            "upper_bound": 6.007868794311059
          },
          "point_estimate": 2.23601981892133,
          "standard_error": 1.5175279131962816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6575.66953309164,
            "upper_bound": 6578.738926687799
          },
          "point_estimate": 6576.903588671615,
          "standard_error": 0.7732526181136711
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4894907458224993,
            "upper_bound": 7.318909382337151
          },
          "point_estimate": 5.0769627631528635,
          "standard_error": 1.7305584309874475
        }
      }
    },
    "memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/regex_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6709.924404681265,
            "upper_bound": 6729.465825129977
          },
          "point_estimate": 6719.0857994715825,
          "standard_error": 5.014354090836338
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6707.3915111767965,
            "upper_bound": 6732.132820165445
          },
          "point_estimate": 6715.413282837613,
          "standard_error": 4.878780086422902
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.012816796687778,
            "upper_bound": 27.400874233655713
          },
          "point_estimate": 8.540258688210647,
          "standard_error": 6.5453297323949275
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6706.786253697834,
            "upper_bound": 6723.360668434353
          },
          "point_estimate": 6714.675843867937,
          "standard_error": 4.159853084004531
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.455548861756371,
            "upper_bound": 21.43707004682598
          },
          "point_estimate": 16.675538153765874,
          "standard_error": 3.8135069383756472
        }
      }
    },
    "memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/regex_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15568.70628594581,
            "upper_bound": 15606.70882692216
          },
          "point_estimate": 15585.491618897806,
          "standard_error": 9.836958141206164
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15564.132799466131,
            "upper_bound": 15601.123552123552
          },
          "point_estimate": 15572.66038181038,
          "standard_error": 9.602035917192248
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.1907282939673145,
            "upper_bound": 44.723400585953485
          },
          "point_estimate": 15.251235755261222,
          "standard_error": 10.848036471973929
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15567.821052963913,
            "upper_bound": 15588.68732290888
          },
          "point_estimate": 15575.17586900444,
          "standard_error": 5.404480107761724
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.256956227179604,
            "upper_bound": 45.72105538668845
          },
          "point_estimate": 32.71298911585567,
          "standard_error": 9.952047663557272
        }
      }
    },
    "memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/regex_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.14133704487055,
            "upper_bound": 41.71685258277404
          },
          "point_estimate": 41.39269032447568,
          "standard_error": 0.14781513423275144
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.114049929035986,
            "upper_bound": 41.494000863131745
          },
          "point_estimate": 41.3466616498165,
          "standard_error": 0.09283245752865237
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.058780707847643617,
            "upper_bound": 0.598450415723251
          },
          "point_estimate": 0.1853538121361832,
          "standard_error": 0.13383527162127284
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.09318110001429,
            "upper_bound": 41.749763483503905
          },
          "point_estimate": 41.34353820398956,
          "standard_error": 0.16857528478952438
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12846068892494736,
            "upper_bound": 0.7115376421601716
          },
          "point_estimate": 0.4883339645997515,
          "standard_error": 0.16882950349210343
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.998154370816582,
            "upper_bound": 16.014621121376614
          },
          "point_estimate": 16.006621049100154,
          "standard_error": 0.004203106015041789
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.998837431657412,
            "upper_bound": 16.014958374464346
          },
          "point_estimate": 16.007166540329166,
          "standard_error": 0.005210642330673625
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004555260467745194,
            "upper_bound": 0.02350763856085299
          },
          "point_estimate": 0.01159740031265545,
          "standard_error": 0.0057498604242789825
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.000968082376108,
            "upper_bound": 16.016084863442394
          },
          "point_estimate": 16.007503729744148,
          "standard_error": 0.0038982554520445886
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006336842241667739,
            "upper_bound": 0.019060640036018026
          },
          "point_estimate": 0.01402682753517556,
          "standard_error": 0.003404484544773491
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.966098396572033,
            "upper_bound": 15.978515516625713
          },
          "point_estimate": 15.972508125343111,
          "standard_error": 0.00316151473346126
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.967922988229438,
            "upper_bound": 15.979002772603042
          },
          "point_estimate": 15.971928150177654,
          "standard_error": 0.0027737921500031225
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001747318779654886,
            "upper_bound": 0.017650074150091503
          },
          "point_estimate": 0.0068117123888309485,
          "standard_error": 0.004072397398218026
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.968454500401286,
            "upper_bound": 15.974709648147256
          },
          "point_estimate": 15.971285215854907,
          "standard_error": 0.001610088241654401
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004378125591698245,
            "upper_bound": 0.014577103967760794
          },
          "point_estimate": 0.010583920320969062,
          "standard_error": 0.0026407541540162025
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.94790082550166,
            "upper_bound": 15.97806622017732
          },
          "point_estimate": 15.96679404002424,
          "standard_error": 0.008595892534201196
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.969995508117783,
            "upper_bound": 15.980127349384922
          },
          "point_estimate": 15.974291846032546,
          "standard_error": 0.0031507634846528604
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007412519552243606,
            "upper_bound": 0.012386448538787898
          },
          "point_estimate": 0.004583373743332176,
          "standard_error": 0.003634946616210972
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.972232779764644,
            "upper_bound": 15.978112951766756
          },
          "point_estimate": 15.974585865884968,
          "standard_error": 0.001512937462613857
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0034570255278959456,
            "upper_bound": 0.04409518649802916
          },
          "point_estimate": 0.028693007334290983,
          "standard_error": 0.014191772073659818
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.952158374374042,
            "upper_bound": 15.956026180294993
          },
          "point_estimate": 15.9539298555463,
          "standard_error": 0.000996287361374307
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.95139517921279,
            "upper_bound": 15.956243570703826
          },
          "point_estimate": 15.95267849584716,
          "standard_error": 0.0011598461070698944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00013145008848079086,
            "upper_bound": 0.005352294597580942
          },
          "point_estimate": 0.002330287246497937,
          "standard_error": 0.0012704079622686223
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.951866538896695,
            "upper_bound": 15.953703332275056
          },
          "point_estimate": 15.952785022574265,
          "standard_error": 0.00046460262013727833
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001144042696106023,
            "upper_bound": 0.004328165827772247
          },
          "point_estimate": 0.003308799498923231,
          "standard_error": 0.000789843306608151
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.434527755780735,
            "upper_bound": 17.441889215600742
          },
          "point_estimate": 17.437796808932273,
          "standard_error": 0.0018955525340346164
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.433489529980932,
            "upper_bound": 17.440380550088435
          },
          "point_estimate": 17.436787545623297,
          "standard_error": 0.0015581511456380886
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010081729165340562,
            "upper_bound": 0.008432097509653713
          },
          "point_estimate": 0.0031725034029885567,
          "standard_error": 0.0020051543452055333
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.433877328048744,
            "upper_bound": 17.438914222565018
          },
          "point_estimate": 17.43653570062102,
          "standard_error": 0.0012655242987386229
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022808492117608816,
            "upper_bound": 0.008963813080102925
          },
          "point_estimate": 0.006331733386307856,
          "standard_error": 0.001954233434469346
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.04737484785842,
            "upper_bound": 18.173586036556056
          },
          "point_estimate": 18.090601787038363,
          "standard_error": 0.03888706502650381
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.045811137869418,
            "upper_bound": 18.056077128841736
          },
          "point_estimate": 18.049280555658218,
          "standard_error": 0.008954093429136821
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008060763529002978,
            "upper_bound": 0.0122690701347705
          },
          "point_estimate": 0.005160048702615866,
          "standard_error": 0.010997564972369322
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.04596464915368,
            "upper_bound": 18.19549353205648
          },
          "point_estimate": 18.08704335749433,
          "standard_error": 0.04273468854598771
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003104449144463302,
            "upper_bound": 0.19874907927538651
          },
          "point_estimate": 0.1296835610924544,
          "standard_error": 0.07237888261156668
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.16790805577404,
            "upper_bound": 15.225114859313836
          },
          "point_estimate": 15.196568203585496,
          "standard_error": 0.014664909363526824
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.156013722952084,
            "upper_bound": 15.22852000551466
          },
          "point_estimate": 15.19477190641674,
          "standard_error": 0.016580552349081033
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006914732984090508,
            "upper_bound": 0.08221158703459393
          },
          "point_estimate": 0.03878715176111963,
          "standard_error": 0.021092407215478393
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.17698547853048,
            "upper_bound": 15.203988580617198
          },
          "point_estimate": 15.192020030839792,
          "standard_error": 0.006865218870100359
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024569821922135054,
            "upper_bound": 0.06310669129028511
          },
          "point_estimate": 0.04872323881920861,
          "standard_error": 0.00972635214365508
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.423011628185506,
            "upper_bound": 17.488385166605998
          },
          "point_estimate": 17.45498369031596,
          "standard_error": 0.016775179087811294
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.41171804758788,
            "upper_bound": 17.52003198377484
          },
          "point_estimate": 17.42625383048599,
          "standard_error": 0.0360125423646419
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003732887968344842,
            "upper_bound": 0.08140650773490019
          },
          "point_estimate": 0.042372221302338135,
          "standard_error": 0.02508003569585411
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.42168634795897,
            "upper_bound": 17.502900725304116
          },
          "point_estimate": 17.453366499299943,
          "standard_error": 0.020548672985426102
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03569031664614217,
            "upper_bound": 0.06253487792119378
          },
          "point_estimate": 0.055984981569895606,
          "standard_error": 0.006772317265569264
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.494982284650696,
            "upper_bound": 20.550197831805
          },
          "point_estimate": 20.51671049102424,
          "standard_error": 0.015047374535455424
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.496323023977176,
            "upper_bound": 20.515977050449532
          },
          "point_estimate": 20.504449267314357,
          "standard_error": 0.006076361506204576
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025126480685382945,
            "upper_bound": 0.03344400401854449
          },
          "point_estimate": 0.00911284534089924,
          "standard_error": 0.008145862887496302
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.49638796018536,
            "upper_bound": 20.50656500453244
          },
          "point_estimate": 20.5004860605099,
          "standard_error": 0.0026072577035711458
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006672335479862315,
            "upper_bound": 0.07635013864678657
          },
          "point_estimate": 0.050128282977301965,
          "standard_error": 0.02261043387183293
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.96404091351106,
            "upper_bound": 16.09626166608094
          },
          "point_estimate": 16.02687508428304,
          "standard_error": 0.03282667179774245
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.95987564194342,
            "upper_bound": 16.128363151570174
          },
          "point_estimate": 15.965772206995968,
          "standard_error": 0.043979497919720535
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0029613595121349033,
            "upper_bound": 0.14843762350985285
          },
          "point_estimate": 0.010936310457345454,
          "standard_error": 0.04475096846888889
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.96228443709399,
            "upper_bound": 16.06383711399219
          },
          "point_estimate": 15.999728410855068,
          "standard_error": 0.02733706870268457
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007943531753345504,
            "upper_bound": 0.13754826450670316
          },
          "point_estimate": 0.1093796121249992,
          "standard_error": 0.02628063987103474
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.78230154810782,
            "upper_bound": 17.892467967211566
          },
          "point_estimate": 17.819618737719743,
          "standard_error": 0.03437278056217314
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.780806241528193,
            "upper_bound": 17.786985730308842
          },
          "point_estimate": 17.78353813963642,
          "standard_error": 0.00789540495634675
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00030909978334563317,
            "upper_bound": 0.00617779476155917
          },
          "point_estimate": 0.003520217252259607,
          "standard_error": 0.009516581966403764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.78259756363901,
            "upper_bound": 17.788725099355293
          },
          "point_estimate": 17.78466780218971,
          "standard_error": 0.0015953437768497108
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016324576282288194,
            "upper_bound": 0.17498711675161133
          },
          "point_estimate": 0.11431481045579664,
          "standard_error": 0.06468303781972773
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.255914010611235,
            "upper_bound": 27.314449726882103
          },
          "point_estimate": 27.279731583924978,
          "standard_error": 0.015597730472461095
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.248762663062596,
            "upper_bound": 27.286155026561776
          },
          "point_estimate": 27.266001153522783,
          "standard_error": 0.009945265922549512
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005234299034480239,
            "upper_bound": 0.045186091823116
          },
          "point_estimate": 0.02771895856983205,
          "standard_error": 0.011116085246621372
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.25218350017018,
            "upper_bound": 27.28179512091871
          },
          "point_estimate": 27.26767157872147,
          "standard_error": 0.007401375442664201
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012079133856524423,
            "upper_bound": 0.07794460375043857
          },
          "point_estimate": 0.05213665030512553,
          "standard_error": 0.02089654195988184
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138374.88406736602,
            "upper_bound": 140662.36444212083
          },
          "point_estimate": 139487.9604998793,
          "standard_error": 565.382043142043
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138243.9953263625,
            "upper_bound": 142164.2323616392
          },
          "point_estimate": 138410.60836501903,
          "standard_error": 961.8775826553268
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.933610964211752,
            "upper_bound": 2859.9573032306307
          },
          "point_estimate": 268.104434783928,
          "standard_error": 805.9647824204193
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138368.6577648657,
            "upper_bound": 140949.39760893575
          },
          "point_estimate": 139432.80673546984,
          "standard_error": 706.944760561117
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.57999964312418,
            "upper_bound": 2073.6310490640612
          },
          "point_estimate": 1877.8469293140024,
          "standard_error": 373.8900365865315
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57212.46860203545,
            "upper_bound": 57617.470663150634
          },
          "point_estimate": 57404.654227692656,
          "standard_error": 104.37199421955351
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57114.39573475289,
            "upper_bound": 57815.116719242906
          },
          "point_estimate": 57296.40596364729,
          "standard_error": 169.48767709146523
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.13709608940598,
            "upper_bound": 521.1382754088299
          },
          "point_estimate": 289.86348944380376,
          "standard_error": 132.59007467009351
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57143.434181386,
            "upper_bound": 57326.48307351485
          },
          "point_estimate": 57217.649035191935,
          "standard_error": 47.545401602460416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.8741454009862,
            "upper_bound": 407.03764643978263
          },
          "point_estimate": 348.2120308338111,
          "standard_error": 58.503438393150326
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205414.7602399448,
            "upper_bound": 208316.52680477084
          },
          "point_estimate": 206814.84584566404,
          "standard_error": 720.3809678061981
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205176.24670433145,
            "upper_bound": 209968.1920903955
          },
          "point_estimate": 205472.06850282487,
          "standard_error": 1164.0696452944414
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.26119686243999,
            "upper_bound": 3710.62012885026
          },
          "point_estimate": 510.5445390998738,
          "standard_error": 978.6105777630136
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205196.8813997818,
            "upper_bound": 208688.0853837635
          },
          "point_estimate": 206647.813955536,
          "standard_error": 944.9790252475492
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277.47783210590643,
            "upper_bound": 2717.9766787337867
          },
          "point_estimate": 2402.927571339276,
          "standard_error": 477.19687331493594
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 670026.1815455246,
            "upper_bound": 671882.9121913579
          },
          "point_estimate": 670947.6069356261,
          "standard_error": 476.80286169591153
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 669746.025,
            "upper_bound": 672282.8549382717
          },
          "point_estimate": 670769.8135802469,
          "standard_error": 701.3904920242727
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415.7324724340352,
            "upper_bound": 2732.7687684280913
          },
          "point_estimate": 1822.377339405561,
          "standard_error": 590.047041598473
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 669614.5797580631,
            "upper_bound": 671439.3072253624
          },
          "point_estimate": 670597.3762866763,
          "standard_error": 459.8854461154137
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 933.0140347067144,
            "upper_bound": 1940.6644870463788
          },
          "point_estimate": 1585.1470798727555,
          "standard_error": 255.14502903360227
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1377016.2366460906,
            "upper_bound": 1378265.246917181
          },
          "point_estimate": 1377578.6362727806,
          "standard_error": 322.14209046522114
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1376798.638888889,
            "upper_bound": 1377923.861111111
          },
          "point_estimate": 1377533.018518519,
          "standard_error": 306.00959107663647
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134.02625317613766,
            "upper_bound": 1503.2109275069552
          },
          "point_estimate": 677.8887675947454,
          "standard_error": 343.53694109894127
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1376777.056727089,
            "upper_bound": 1377629.5431107
          },
          "point_estimate": 1377205.84021164,
          "standard_error": 215.66133238943172
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401.4222419525621,
            "upper_bound": 1520.2595664437974
          },
          "point_estimate": 1070.0165590036054,
          "standard_error": 324.0262288590659
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53968.20986313088,
            "upper_bound": 54412.54789289279
          },
          "point_estimate": 54163.42579792795,
          "standard_error": 113.59261610574607
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53915.8630881095,
            "upper_bound": 54455.0562000998
          },
          "point_estimate": 54020.88460578842,
          "standard_error": 112.59026724008378
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.00432676397125,
            "upper_bound": 505.2097201849253
          },
          "point_estimate": 153.68968685229825,
          "standard_error": 110.40450478617942
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53928.81888063909,
            "upper_bound": 54474.54139072605
          },
          "point_estimate": 54126.69430360059,
          "standard_error": 145.86802032367098
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.36539305038161,
            "upper_bound": 480.56701292429887
          },
          "point_estimate": 378.46587036352184,
          "standard_error": 107.73695793265092
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166003.4399869537,
            "upper_bound": 166257.02845455531
          },
          "point_estimate": 166127.56034192216,
          "standard_error": 65.01986022042946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165969.8295281583,
            "upper_bound": 166289.79229452054
          },
          "point_estimate": 166117.86953685584,
          "standard_error": 64.12096229502656
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.546242579202776,
            "upper_bound": 428.0874283359914
          },
          "point_estimate": 146.22644338571027,
          "standard_error": 110.63143545133366
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166032.51220437203,
            "upper_bound": 166188.74768393676
          },
          "point_estimate": 166097.75910573444,
          "standard_error": 39.77195380432699
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.74627483220485,
            "upper_bound": 276.0847152982377
          },
          "point_estimate": 216.921242369781,
          "standard_error": 42.57963866331624
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111602.27135358848,
            "upper_bound": 111812.30930904907
          },
          "point_estimate": 111697.7792129224,
          "standard_error": 54.11820328263344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111587.2619631902,
            "upper_bound": 111787.42281989484
          },
          "point_estimate": 111648.86073619632,
          "standard_error": 47.10515569117836
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.487005474173454,
            "upper_bound": 263.02108037645536
          },
          "point_estimate": 104.495202521115,
          "standard_error": 65.99973475380608
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111581.2878753088,
            "upper_bound": 111701.51402875491
          },
          "point_estimate": 111631.00997530077,
          "standard_error": 30.475394143158745
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.611868783365544,
            "upper_bound": 241.9764415805033
          },
          "point_estimate": 180.06369941402903,
          "standard_error": 48.693078156964454
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 687792.48786478,
            "upper_bound": 689151.7414667566
          },
          "point_estimate": 688457.3139233303,
          "standard_error": 347.07358814766275
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 687654.5188679246,
            "upper_bound": 689350.6198113208
          },
          "point_estimate": 688385.566786463,
          "standard_error": 330.5199111633921
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.0969842164406,
            "upper_bound": 2104.8443852731134
          },
          "point_estimate": 496.7633040108719,
          "standard_error": 560.5931805063085
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 687776.0443853379,
            "upper_bound": 688653.9677249275
          },
          "point_estimate": 688289.5309973046,
          "standard_error": 230.5009627059909
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 568.5660985563782,
            "upper_bound": 1503.9465308183378
          },
          "point_estimate": 1151.3667757111518,
          "standard_error": 241.4591428528013
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48617.108771463674,
            "upper_bound": 48725.54622617851
          },
          "point_estimate": 48667.43151107493,
          "standard_error": 27.81764307777425
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48602.45294906166,
            "upper_bound": 48724.29172252011
          },
          "point_estimate": 48637.757516277285,
          "standard_error": 33.288519110777194
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.699515958871077,
            "upper_bound": 149.1070376510801
          },
          "point_estimate": 74.25421114821188,
          "standard_error": 33.1377670786262
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48619.071415062186,
            "upper_bound": 48701.397853185874
          },
          "point_estimate": 48658.86402632221,
          "standard_error": 21.388222935342498
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.496420099506224,
            "upper_bound": 123.12674988878382
          },
          "point_estimate": 92.3895106221168,
          "standard_error": 21.88451817601591
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101705.58779337212,
            "upper_bound": 101980.40336352996
          },
          "point_estimate": 101839.22088137596,
          "standard_error": 70.39508897708888
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101633.86692415732,
            "upper_bound": 102055.2426966292
          },
          "point_estimate": 101787.43103932583,
          "standard_error": 134.5686465364946
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.0243679563439,
            "upper_bound": 402.7705114378913
          },
          "point_estimate": 260.2923127009907,
          "standard_error": 104.8759831433497
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101630.81665247607,
            "upper_bound": 101953.3600448891
          },
          "point_estimate": 101774.23232890703,
          "standard_error": 88.98231892558071
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146.79783799691106,
            "upper_bound": 280.19241296427083
          },
          "point_estimate": 234.14701719148383,
          "standard_error": 34.64377600808667
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318720.8959075225,
            "upper_bound": 319490.3467028985
          },
          "point_estimate": 319079.6567097999,
          "standard_error": 197.3311229866765
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318575.5005797101,
            "upper_bound": 319535.07608695654
          },
          "point_estimate": 318846.1966183575,
          "standard_error": 255.24572908002787
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126.60601595519032,
            "upper_bound": 1088.7078467585618
          },
          "point_estimate": 535.3743709299981,
          "standard_error": 247.63784758486
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318572.80561207864,
            "upper_bound": 319260.734437262
          },
          "point_estimate": 318916.0331564088,
          "standard_error": 185.7560065316965
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287.8482212400657,
            "upper_bound": 836.6803630041258
          },
          "point_estimate": 660.8536174055397,
          "standard_error": 138.4516266832249
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49875.40198288963,
            "upper_bound": 50027.50441653584
          },
          "point_estimate": 49941.33247835993,
          "standard_error": 39.3760696767939
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49859.04610233517,
            "upper_bound": 49989.79882914704
          },
          "point_estimate": 49893.08518009768,
          "standard_error": 33.903190769948374
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.931294664530444,
            "upper_bound": 165.91283972272194
          },
          "point_estimate": 59.50436604294407,
          "standard_error": 40.77519226489447
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49871.36831971447,
            "upper_bound": 49965.78754320098
          },
          "point_estimate": 49907.11915941202,
          "standard_error": 24.785970067558104
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.40046930191933,
            "upper_bound": 185.08581170178903
          },
          "point_estimate": 131.3767909289617,
          "standard_error": 41.94470708283049
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/regex_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19490.08147754663,
            "upper_bound": 19566.15854149075
          },
          "point_estimate": 19526.493052506332,
          "standard_error": 19.51841220418897
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19465.31192907039,
            "upper_bound": 19578.56985491671
          },
          "point_estimate": 19518.26429083186,
          "standard_error": 31.89872839747684
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.971675811791475,
            "upper_bound": 109.16412419037592
          },
          "point_estimate": 79.204454209099,
          "standard_error": 29.363262103985967
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19475.236469574367,
            "upper_bound": 19541.69002200558
          },
          "point_estimate": 19504.46203619057,
          "standard_error": 17.261569413535696
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.8075261300817,
            "upper_bound": 80.63635010148764
          },
          "point_estimate": 64.9024524875508,
          "standard_error": 11.34683763300187
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/regex_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1092689.220661502,
            "upper_bound": 1094550.5372794117
          },
          "point_estimate": 1093604.6537079834,
          "standard_error": 476.4830629567461
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1092171.5533088236,
            "upper_bound": 1095056.4485294118
          },
          "point_estimate": 1093342.9086134452,
          "standard_error": 761.439910217187
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230.8059311964323,
            "upper_bound": 2621.6619108091686
          },
          "point_estimate": 2098.957429648091,
          "standard_error": 598.2568349713337
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1091996.2923801115,
            "upper_bound": 1093897.005953557
          },
          "point_estimate": 1092683.4155844157,
          "standard_error": 486.15284764505657
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 956.6452020884612,
            "upper_bound": 1936.3542741918975
          },
          "point_estimate": 1582.545606278588,
          "standard_error": 251.11969533586205
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/regex_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2168.9081812743048,
            "upper_bound": 2171.2987606512543
          },
          "point_estimate": 2169.808250103272,
          "standard_error": 0.6748827349691624
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2168.827100551259,
            "upper_bound": 2169.8078223152856
          },
          "point_estimate": 2169.0173556685613,
          "standard_error": 0.2743543358369277
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.036183185717111446,
            "upper_bound": 1.0652190322404924
          },
          "point_estimate": 0.3714229398611547,
          "standard_error": 0.3160777657904285
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2168.8835650715005,
            "upper_bound": 2169.814873481404
          },
          "point_estimate": 2169.273902203091,
          "standard_error": 0.2565602579277621
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2037481789589207,
            "upper_bound": 3.4292589765626227
          },
          "point_estimate": 2.241938885957001,
          "standard_error": 1.0763369772358038
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52365.21937256988,
            "upper_bound": 52453.158204439416
          },
          "point_estimate": 52409.98338313664,
          "standard_error": 22.53918463428036
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52352.361281219986,
            "upper_bound": 52470.21902017291
          },
          "point_estimate": 52419.5719740634,
          "standard_error": 27.64786836148649
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.024348958515462,
            "upper_bound": 139.65922627415796
          },
          "point_estimate": 74.54966515102228,
          "standard_error": 32.27899760086487
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52351.9728796464,
            "upper_bound": 52444.45415602571
          },
          "point_estimate": 52397.42477637636,
          "standard_error": 23.280408923979433
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.86778408072116,
            "upper_bound": 93.59362252977728
          },
          "point_estimate": 75.16113542765919,
          "standard_error": 13.215289436033116
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48759.38941912422,
            "upper_bound": 49170.537466487935
          },
          "point_estimate": 48976.87672922253,
          "standard_error": 105.5287033007274
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48643.40013404826,
            "upper_bound": 49254.68900804289
          },
          "point_estimate": 49112.05522788204,
          "standard_error": 162.33039518077229
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.468290633236826,
            "upper_bound": 574.7511118470696
          },
          "point_estimate": 248.93320596929544,
          "standard_error": 137.620657717315
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48533.75599540729,
            "upper_bound": 49049.08884324105
          },
          "point_estimate": 48707.34763065353,
          "standard_error": 129.0304904462412
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149.40020394517407,
            "upper_bound": 417.498586479439
          },
          "point_estimate": 351.4683398808369,
          "standard_error": 62.18581784521829
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50833.96066798941,
            "upper_bound": 50920.148700369034
          },
          "point_estimate": 50874.33174775466,
          "standard_error": 22.034875916819747
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50814.20826330532,
            "upper_bound": 50924.54178338002
          },
          "point_estimate": 50865.76690676271,
          "standard_error": 23.71221242937479
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.542612628940883,
            "upper_bound": 122.82159123922588
          },
          "point_estimate": 64.85770112795649,
          "standard_error": 31.591025140046675
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50812.26059815622,
            "upper_bound": 50901.45041494859
          },
          "point_estimate": 50846.4417985376,
          "standard_error": 22.831472635254364
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.20484744521473,
            "upper_bound": 93.61413779211335
          },
          "point_estimate": 73.53555703656988,
          "standard_error": 15.617351046381945
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14145.6450829529,
            "upper_bound": 14167.174600740453
          },
          "point_estimate": 14155.60460274159,
          "standard_error": 5.525629032579806
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14142.507404520657,
            "upper_bound": 14171.573547241707
          },
          "point_estimate": 14148.10355194299,
          "standard_error": 6.603815582604397
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.1682673816142213,
            "upper_bound": 28.588827507513212
          },
          "point_estimate": 12.7273767135082,
          "standard_error": 6.526852520021304
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14140.778302805924,
            "upper_bound": 14154.810840247295
          },
          "point_estimate": 14146.79645210596,
          "standard_error": 3.6796512900552543
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.690057687723083,
            "upper_bound": 22.989082335542314
          },
          "point_estimate": 18.43149146642017,
          "standard_error": 4.199809917943125
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23317.564049529312,
            "upper_bound": 23362.26350836441
          },
          "point_estimate": 23335.002648669437,
          "standard_error": 12.269588579956054
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23311.502353444583,
            "upper_bound": 23335.525032092428
          },
          "point_estimate": 23326.474818907023,
          "standard_error": 6.695440868608895
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.3285240958065665,
            "upper_bound": 28.341840827602606
          },
          "point_estimate": 14.205172892865887,
          "standard_error": 6.605066335711996
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23311.884392591415,
            "upper_bound": 23329.861921546853
          },
          "point_estimate": 23319.97149525699,
          "standard_error": 4.70855214494343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.087124438247793,
            "upper_bound": 62.455110449311334
          },
          "point_estimate": 40.93242704646804,
          "standard_error": 18.51444411571624
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17857.865107941194,
            "upper_bound": 17889.684393753072
          },
          "point_estimate": 17874.653071131877,
          "standard_error": 8.17729372664467
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17854.057837350385,
            "upper_bound": 17892.23445646827
          },
          "point_estimate": 17882.452539350714,
          "standard_error": 8.746362877324971
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.1782826723001603,
            "upper_bound": 45.77086819871953
          },
          "point_estimate": 14.504019205117784,
          "standard_error": 11.354067814713463
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17843.259957721406,
            "upper_bound": 17884.994040480826
          },
          "point_estimate": 17866.60600864949,
          "standard_error": 11.10310325812136
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.648588252060732,
            "upper_bound": 34.80681587971085
          },
          "point_estimate": 27.33989184126949,
          "standard_error": 5.917468527474222
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17646.974477547985,
            "upper_bound": 17675.436581920792
          },
          "point_estimate": 17660.685430000387,
          "standard_error": 7.300717807508816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17635.11111111111,
            "upper_bound": 17680.715997566913
          },
          "point_estimate": 17656.688710462287,
          "standard_error": 10.720526992163352
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7190189020726582,
            "upper_bound": 41.99136762433627
          },
          "point_estimate": 30.24852651894354,
          "standard_error": 10.052742678821536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17645.544001377835,
            "upper_bound": 17674.30892756878
          },
          "point_estimate": 17657.910797231965,
          "standard_error": 7.3337878814071
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.248987365717175,
            "upper_bound": 30.06955123962579
          },
          "point_estimate": 24.27413159404397,
          "standard_error": 4.283371095357206
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17280.819831856563,
            "upper_bound": 17310.297014158852
          },
          "point_estimate": 17295.89036501518,
          "standard_error": 7.579769891112299
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17270.704201780616,
            "upper_bound": 17317.688094671743
          },
          "point_estimate": 17303.3566603235,
          "standard_error": 14.161707105898865
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.30701763013768596,
            "upper_bound": 40.77806764545537
          },
          "point_estimate": 27.53372241793402,
          "standard_error": 10.672944288239052
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17282.635741544967,
            "upper_bound": 17315.25103582222
          },
          "point_estimate": 17303.731766901034,
          "standard_error": 8.349627691653179
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.527169208062553,
            "upper_bound": 29.155404400124542
          },
          "point_estimate": 25.189164369055963,
          "standard_error": 3.44398917321589
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30708.60808892061,
            "upper_bound": 30764.962594565808
          },
          "point_estimate": 30735.672862279425,
          "standard_error": 14.455865968953924
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30704.12605752961,
            "upper_bound": 30787.616751269037
          },
          "point_estimate": 30719.288183869143,
          "standard_error": 19.40255069996499
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.404916083350765,
            "upper_bound": 83.51373972926196
          },
          "point_estimate": 26.62446008569434,
          "standard_error": 21.803127959488172
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30699.127014507227,
            "upper_bound": 30759.57092127527
          },
          "point_estimate": 30724.76096424726,
          "standard_error": 15.413249520110314
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.54311434265137,
            "upper_bound": 60.10714406908306
          },
          "point_estimate": 48.224904090052355,
          "standard_error": 8.811344991776066
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32163.61401879736,
            "upper_bound": 32259.57364307155
          },
          "point_estimate": 32208.02627940163,
          "standard_error": 24.54964304201328
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32143.905821277433,
            "upper_bound": 32253.69187333924
          },
          "point_estimate": 32192.89828757012,
          "standard_error": 32.64808258814069
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.302952748573947,
            "upper_bound": 139.8513749307812
          },
          "point_estimate": 78.21758853164562,
          "standard_error": 28.56482741600621
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32141.303390278397,
            "upper_bound": 32213.372876685367
          },
          "point_estimate": 32171.25762598783,
          "standard_error": 18.510573901819196
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.54367241502681,
            "upper_bound": 109.07602046236934
          },
          "point_estimate": 81.78461970313889,
          "standard_error": 19.184160578055163
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53258.23104530665,
            "upper_bound": 53379.09232678542
          },
          "point_estimate": 53318.69747478445,
          "standard_error": 30.894785096144947
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53255.763030746704,
            "upper_bound": 53413.51358386205
          },
          "point_estimate": 53298.92737920937,
          "standard_error": 35.848867111976084
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.375871063415218,
            "upper_bound": 189.89415179219617
          },
          "point_estimate": 84.87382870914675,
          "standard_error": 47.769008462773805
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53294.650961941465,
            "upper_bound": 53382.45238234679
          },
          "point_estimate": 53333.557019261854,
          "standard_error": 23.006145494590218
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.616664278014774,
            "upper_bound": 134.3944952225864
          },
          "point_estimate": 102.91616324966884,
          "standard_error": 20.47057571057136
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17775.596120134072,
            "upper_bound": 17814.467806350083
          },
          "point_estimate": 17794.907027351568,
          "standard_error": 9.976019682505068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17765.985624080433,
            "upper_bound": 17832.89823442864
          },
          "point_estimate": 17784.00756673439,
          "standard_error": 18.588073041516388
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.801429602639642,
            "upper_bound": 52.045916273973184
          },
          "point_estimate": 47.91327142335556,
          "standard_error": 15.37606325356035
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17762.907998913393,
            "upper_bound": 17814.57287744357
          },
          "point_estimate": 17787.60801640733,
          "standard_error": 14.01614176699587
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.042383360483363,
            "upper_bound": 38.846084679742745
          },
          "point_estimate": 33.28213123863181,
          "standard_error": 4.394862078295904
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17977.233312100816,
            "upper_bound": 18002.740746240186
          },
          "point_estimate": 17989.41678561597,
          "standard_error": 6.568501667407553
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17971.63847249752,
            "upper_bound": 18005.569761039533
          },
          "point_estimate": 17986.060802775028,
          "standard_error": 9.871116416203716
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.961298845680881,
            "upper_bound": 38.03480355996377
          },
          "point_estimate": 23.096710873403385,
          "standard_error": 8.434669336475881
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17983.8335643157,
            "upper_bound": 18006.819865352452
          },
          "point_estimate": 17995.63623492464,
          "standard_error": 5.79614823160651
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.737407096960531,
            "upper_bound": 27.717112901329067
          },
          "point_estimate": 21.879513364171988,
          "standard_error": 4.182231824712871
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86033.7812325096,
            "upper_bound": 86216.13028811215
          },
          "point_estimate": 86123.82687119159,
          "standard_error": 46.77245996988349
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85999.21998420221,
            "upper_bound": 86264.5625592417
          },
          "point_estimate": 86087.08089454976,
          "standard_error": 69.10110900043532
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.032703352767996,
            "upper_bound": 268.46849472425157
          },
          "point_estimate": 201.60294313883895,
          "standard_error": 63.16611042408984
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86008.19344436665,
            "upper_bound": 86115.36655704936
          },
          "point_estimate": 86065.01690158182,
          "standard_error": 26.875565594137207
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.15996932240682,
            "upper_bound": 189.12557886547967
          },
          "point_estimate": 156.17998291026063,
          "standard_error": 24.530963032299173
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32616.88319363438,
            "upper_bound": 32663.97137519271
          },
          "point_estimate": 32639.21080849606,
          "standard_error": 12.07073138551152
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32605.819694244605,
            "upper_bound": 32667.92315647482
          },
          "point_estimate": 32630.209907074342,
          "standard_error": 14.921779339330492
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.19509694130247,
            "upper_bound": 66.10875950978544
          },
          "point_estimate": 38.342230776125525,
          "standard_error": 14.417759541783877
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32617.8453052807,
            "upper_bound": 32663.418583837967
          },
          "point_estimate": 32639.917277866018,
          "standard_error": 11.429952104258604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.05692924762178,
            "upper_bound": 51.88741957892874
          },
          "point_estimate": 40.27649180272884,
          "standard_error": 8.515711720571677
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73877.13737974255,
            "upper_bound": 74071.64898631678
          },
          "point_estimate": 73972.10728747258,
          "standard_error": 49.60094338520906
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73847.59915311653,
            "upper_bound": 74057.64111498257
          },
          "point_estimate": 73977.82914126017,
          "standard_error": 53.040947051654925
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.28579327599726,
            "upper_bound": 290.6935743852344
          },
          "point_estimate": 135.90116803950298,
          "standard_error": 65.03721730766732
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73806.54457325437,
            "upper_bound": 73988.08588809846
          },
          "point_estimate": 73898.91207369867,
          "standard_error": 46.99101242175252
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.05131658407404,
            "upper_bound": 221.451591430143
          },
          "point_estimate": 165.72042530218667,
          "standard_error": 36.95217621400033
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18693.827343945297,
            "upper_bound": 18709.073608748135
          },
          "point_estimate": 18700.84213911967,
          "standard_error": 3.922689953611706
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18692.36730142391,
            "upper_bound": 18710.66396588486
          },
          "point_estimate": 18696.704291044774,
          "standard_error": 3.972022924577344
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4611869722587274,
            "upper_bound": 20.7004110709914
          },
          "point_estimate": 8.540557335512183,
          "standard_error": 4.616886923129283
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18692.197490510247,
            "upper_bound": 18703.374647279485
          },
          "point_estimate": 18696.024773579484,
          "standard_error": 2.8685393386947027
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.590857773690473,
            "upper_bound": 16.69601725625398
          },
          "point_estimate": 13.070189145569936,
          "standard_error": 3.0973028494340804
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22404.109980496556,
            "upper_bound": 22433.94822792368
          },
          "point_estimate": 22417.82941389305,
          "standard_error": 7.656888173044137
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22397.320388648983,
            "upper_bound": 22437.18445404072
          },
          "point_estimate": 22411.16566052701,
          "standard_error": 9.237616112151509
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.330307164263418,
            "upper_bound": 42.31405262940654
          },
          "point_estimate": 19.933800036811448,
          "standard_error": 9.37443256108795
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22399.396885942428,
            "upper_bound": 22428.330072760768
          },
          "point_estimate": 22409.5584575819,
          "standard_error": 7.456161763062579
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.337775426317409,
            "upper_bound": 32.751680898483016
          },
          "point_estimate": 25.49721393865354,
          "standard_error": 5.811975073301888
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35403.576884600625,
            "upper_bound": 35438.42061808561
          },
          "point_estimate": 35420.848532948534,
          "standard_error": 8.881332803367362
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35400.54534593244,
            "upper_bound": 35442.768849788205
          },
          "point_estimate": 35420.984359726295,
          "standard_error": 9.520775093698294
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.775557498961899,
            "upper_bound": 49.23057994709456
          },
          "point_estimate": 19.168164208409493,
          "standard_error": 12.25667822494455
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35416.60439975354,
            "upper_bound": 35441.3587542215
          },
          "point_estimate": 35427.77868758807,
          "standard_error": 6.204636004490241
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.657569503046853,
            "upper_bound": 38.799473583286385
          },
          "point_estimate": 29.663676491251927,
          "standard_error": 6.1362239176635835
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector-freq_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 380671.0190922619,
            "upper_bound": 381402.48652070935
          },
          "point_estimate": 381053.4019597388,
          "standard_error": 187.34634631025585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 380217.1805555555,
            "upper_bound": 381480.9057291667
          },
          "point_estimate": 381279.8165509259,
          "standard_error": 295.18861411631667
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.086676679420354,
            "upper_bound": 980.5094275229908
          },
          "point_estimate": 417.03992884605833,
          "standard_error": 252.8812618752864
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 380518.4017339135,
            "upper_bound": 381324.7658283908
          },
          "point_estimate": 380998.6194264069,
          "standard_error": 206.6501419204737
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291.48875354615984,
            "upper_bound": 738.1817375482099
          },
          "point_estimate": 623.3843031817883,
          "standard_error": 101.85322409951952
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector-repeated_ra"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2311951.0305357142,
            "upper_bound": 2315387.0607135417
          },
          "point_estimate": 2313535.8958928576,
          "standard_error": 887.4422005822504
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2311020.625,
            "upper_bound": 2315668.9296875
          },
          "point_estimate": 2312795.549479167,
          "standard_error": 1090.6953387693138
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 465.4878542360701,
            "upper_bound": 4799.34401041996
          },
          "point_estimate": 2506.965360492537,
          "standard_error": 1133.5465425723155
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2311364.247716895,
            "upper_bound": 2314734.379464286
          },
          "point_estimate": 2312906.952922078,
          "standard_error": 864.2373123390671
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1250.5311691865409,
            "upper_bound": 3835.318781639277
          },
          "point_estimate": 2951.713387514588,
          "standard_error": 680.7969494666559
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120182.55067106712,
            "upper_bound": 120331.1561574729
          },
          "point_estimate": 120255.2167971083,
          "standard_error": 38.09507650995771
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120144.98459845984,
            "upper_bound": 120357.6197469747
          },
          "point_estimate": 120237.9895489549,
          "standard_error": 50.02064819186883
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.688754217657443,
            "upper_bound": 225.8877250325832
          },
          "point_estimate": 148.77400263926302,
          "standard_error": 51.168149709132734
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120187.77481463172,
            "upper_bound": 120379.41252503628
          },
          "point_estimate": 120285.24105267668,
          "standard_error": 48.66520212533237
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.57234354523074,
            "upper_bound": 156.85082923393898
          },
          "point_estimate": 127.1933823342668,
          "standard_error": 21.71858293691032
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/sliceslice_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10650.39396406338,
            "upper_bound": 10678.364281534175
          },
          "point_estimate": 10663.913774858334,
          "standard_error": 7.1808392338003255
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10643.63090349076,
            "upper_bound": 10685.776725474583
          },
          "point_estimate": 10653.50941136208,
          "standard_error": 13.110405623496469
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6077904738944113,
            "upper_bound": 36.79202959321664
          },
          "point_estimate": 19.60364807085547,
          "standard_error": 9.950452307728296
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10645.689408730346,
            "upper_bound": 10676.232221921751
          },
          "point_estimate": 10661.312969107748,
          "standard_error": 7.766702297719198
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.871776399172088,
            "upper_bound": 28.24394410452053
          },
          "point_estimate": 23.939453206649983,
          "standard_error": 3.6716577356919897
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/sliceslice_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10570.587337973351,
            "upper_bound": 10601.712036021312
          },
          "point_estimate": 10587.448864222924,
          "standard_error": 8.038653078301014
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10567.040659021026,
            "upper_bound": 10603.986212914486
          },
          "point_estimate": 10597.676200633445,
          "standard_error": 8.443996862127927
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.003628141361516,
            "upper_bound": 40.336508411281464
          },
          "point_estimate": 11.617161257541454,
          "standard_error": 9.892837654742817
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10571.795093003308,
            "upper_bound": 10600.66174688018
          },
          "point_estimate": 10591.202371508654,
          "standard_error": 7.472898166475201
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.66393974889978,
            "upper_bound": 34.19256762308368
          },
          "point_estimate": 26.7332513655098,
          "standard_error": 6.48673637167515
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/sliceslice_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14083.44799097799,
            "upper_bound": 14113.34989415144
          },
          "point_estimate": 14097.90437610742,
          "standard_error": 7.672760707875729
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14076.0292248062,
            "upper_bound": 14120.855980066444
          },
          "point_estimate": 14091.818378552973,
          "standard_error": 10.40011006188275
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.467757076689474,
            "upper_bound": 42.30875678375202
          },
          "point_estimate": 23.86653617706098,
          "standard_error": 10.544813377104338
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14079.884323633814,
            "upper_bound": 14103.859384452682
          },
          "point_estimate": 14090.026351555422,
          "standard_error": 6.130878062774077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.622909740255622,
            "upper_bound": 31.216675471957743
          },
          "point_estimate": 25.54950905857931,
          "standard_error": 4.476688948360061
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/sliceslice_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.01550351931188,
            "upper_bound": 36.49193360216661
          },
          "point_estimate": 36.25249236635554,
          "standard_error": 0.12205361088017136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.92194586675334,
            "upper_bound": 36.71879296064003
          },
          "point_estimate": 36.18469445248489,
          "standard_error": 0.20754886541160417
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05402603138683029,
            "upper_bound": 0.6886348831502023
          },
          "point_estimate": 0.46079262194861814,
          "standard_error": 0.1653664526895796
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.93584340761193,
            "upper_bound": 36.563312959423264
          },
          "point_estimate": 36.18021514288873,
          "standard_error": 0.15664863386628405
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2555993363936404,
            "upper_bound": 0.4861256207566315
          },
          "point_estimate": 0.40694026805845696,
          "standard_error": 0.05865432497305443
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.177891595555302,
            "upper_bound": 15.226894911673416
          },
          "point_estimate": 15.20425367476816,
          "standard_error": 0.01261768134839166
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.166449082830372,
            "upper_bound": 15.23639925839095
          },
          "point_estimate": 15.217889929046445,
          "standard_error": 0.014812429935823436
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006251256580353884,
            "upper_bound": 0.06345026704006733
          },
          "point_estimate": 0.030736534231961852,
          "standard_error": 0.01455893856386041
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.156846355230854,
            "upper_bound": 15.22331455231832
          },
          "point_estimate": 15.195224081599635,
          "standard_error": 0.01788366328313437
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015360927533548872,
            "upper_bound": 0.052666096883524166
          },
          "point_estimate": 0.04202439600524998,
          "standard_error": 0.009669518512944964
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.662440333950563,
            "upper_bound": 16.713003991006183
          },
          "point_estimate": 16.68846246676341,
          "standard_error": 0.012918958685125662
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.65325457162364,
            "upper_bound": 16.722174450574798
          },
          "point_estimate": 16.697132624754737,
          "standard_error": 0.017094905443709424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009131854759177098,
            "upper_bound": 0.07261226904560107
          },
          "point_estimate": 0.04053891583777777,
          "standard_error": 0.015886993321958014
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.638765083904804,
            "upper_bound": 16.700441169594576
          },
          "point_estimate": 16.665241022885738,
          "standard_error": 0.015704343762484977
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023308090079477393,
            "upper_bound": 0.054790054436768605
          },
          "point_estimate": 0.04309647759596471,
          "standard_error": 0.008064620409466665
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.140836445309205,
            "upper_bound": 13.17216396940692
          },
          "point_estimate": 13.15846188591919,
          "standard_error": 0.00803376942843872
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.1553031193926,
            "upper_bound": 13.172158280086318
          },
          "point_estimate": 13.160963068003982,
          "standard_error": 0.0041024501229296835
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002585734626431839,
            "upper_bound": 0.03120202815425362
          },
          "point_estimate": 0.007882146585026754,
          "standard_error": 0.0072712006984264705
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.12513249096462,
            "upper_bound": 13.165138141396865
          },
          "point_estimate": 13.149521600290456,
          "standard_error": 0.011037340212488835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005295582900353391,
            "upper_bound": 0.03907155684893815
          },
          "point_estimate": 0.02669601749878239,
          "standard_error": 0.00950097025183184
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.920058289325176,
            "upper_bound": 11.958359307649967
          },
          "point_estimate": 11.938271364371282,
          "standard_error": 0.009522876769928605
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.917449266871555,
            "upper_bound": 11.971666918932158
          },
          "point_estimate": 11.921096928883786,
          "standard_error": 0.01352938622391041
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013556135975142738,
            "upper_bound": 0.046078377294043425
          },
          "point_estimate": 0.006383296877085961,
          "standard_error": 0.012598661888994663
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.9170877512388,
            "upper_bound": 11.928383629538349
          },
          "point_estimate": 11.920484758576936,
          "standard_error": 0.003016245813554603
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003368268357238558,
            "upper_bound": 0.03838647503184051
          },
          "point_estimate": 0.031869968662018584,
          "standard_error": 0.006943568750634403
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.331299433035907,
            "upper_bound": 13.365808724655247
          },
          "point_estimate": 13.350889699778426,
          "standard_error": 0.008900392710022647
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.333214737478851,
            "upper_bound": 13.366000141120644
          },
          "point_estimate": 13.364349572952548,
          "standard_error": 0.007520589789896097
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00048337335181036306,
            "upper_bound": 0.03720925957322379
          },
          "point_estimate": 0.003244288736615447,
          "standard_error": 0.008656309148437541
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.360285491250458,
            "upper_bound": 13.365949225113011
          },
          "point_estimate": 13.363907037691526,
          "standard_error": 0.0014588732388019152
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003827257921232843,
            "upper_bound": 0.03846538559146887
          },
          "point_estimate": 0.029714350015668176,
          "standard_error": 0.009135865150667633
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.09281771192512,
            "upper_bound": 26.11941170027666
          },
          "point_estimate": 26.10523309207573,
          "standard_error": 0.006848198844781982
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.08755653475702,
            "upper_bound": 26.122224265551008
          },
          "point_estimate": 26.095174533402677,
          "standard_error": 0.01030468359722722
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010570228156741885,
            "upper_bound": 0.036403145274091185
          },
          "point_estimate": 0.01289929001075902,
          "standard_error": 0.010883327297402516
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.087451861757895,
            "upper_bound": 26.122872850683684
          },
          "point_estimate": 26.100111780201775,
          "standard_error": 0.009338544402882835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00921721246702049,
            "upper_bound": 0.02869489078371508
          },
          "point_estimate": 0.022860383124777027,
          "standard_error": 0.004794016001196601
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.948899079779553,
            "upper_bound": 15.000454023693663
          },
          "point_estimate": 14.976130926512388,
          "standard_error": 0.01327058625387507
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.937611019809722,
            "upper_bound": 14.99941894119792
          },
          "point_estimate": 14.99575504187704,
          "standard_error": 0.016651941278092903
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001531810269814951,
            "upper_bound": 0.06987842902953885
          },
          "point_estimate": 0.012234858364562026,
          "standard_error": 0.02151916777789884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.96565038941286,
            "upper_bound": 14.996676627306906
          },
          "point_estimate": 14.987272105124598,
          "standard_error": 0.008137422427272969
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018887999033511127,
            "upper_bound": 0.05580026023422084
          },
          "point_estimate": 0.044262680389949446,
          "standard_error": 0.00908510598320698
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.191622813216057,
            "upper_bound": 18.206536647994636
          },
          "point_estimate": 18.19805525316609,
          "standard_error": 0.0038886678117584896
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.18866262831332,
            "upper_bound": 18.204580871512825
          },
          "point_estimate": 18.194191591313217,
          "standard_error": 0.0035959767370739994
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013579634354983763,
            "upper_bound": 0.016985864385574256
          },
          "point_estimate": 0.008388251629831096,
          "standard_error": 0.003909277062334562
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.19026219692036,
            "upper_bound": 18.19497371511096
          },
          "point_estimate": 18.19271433306403,
          "standard_error": 0.0012074016531032124
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003594488299518577,
            "upper_bound": 0.01855575545748655
          },
          "point_estimate": 0.012905050393719355,
          "standard_error": 0.004285490001497808
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.74190187473557,
            "upper_bound": 18.75290748532833
          },
          "point_estimate": 18.746974574747192,
          "standard_error": 0.0028339245389323634
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.739134756459748,
            "upper_bound": 18.75429404131464
          },
          "point_estimate": 18.743308521490455,
          "standard_error": 0.003945319291803509
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005262335596108632,
            "upper_bound": 0.01577714280899759
          },
          "point_estimate": 0.0071579545930495585,
          "standard_error": 0.0038784976113942367
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.74183357014998,
            "upper_bound": 18.753109005774704
          },
          "point_estimate": 18.74657575119606,
          "standard_error": 0.0030306684294132825
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004061249066095168,
            "upper_bound": 0.01217608167424173
          },
          "point_estimate": 0.009409004599440688,
          "standard_error": 0.002117998905394096
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.402578377823282,
            "upper_bound": 17.430176869543715
          },
          "point_estimate": 17.41762925225124,
          "standard_error": 0.007161716296489283
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.402349305221662,
            "upper_bound": 17.429634945353037
          },
          "point_estimate": 17.424674780245148,
          "standard_error": 0.005869498243443317
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013071580977674128,
            "upper_bound": 0.03527240007275674
          },
          "point_estimate": 0.008667454574893688,
          "standard_error": 0.008586254177984507
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.39043205183935,
            "upper_bound": 17.42635678850377
          },
          "point_estimate": 17.411903576740066,
          "standard_error": 0.010104744693822583
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005712885374462133,
            "upper_bound": 0.0311316557386283
          },
          "point_estimate": 0.02384843290018916,
          "standard_error": 0.006286561873456859
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.246457240951589,
            "upper_bound": 14.253591848702426
          },
          "point_estimate": 14.249738722980698,
          "standard_error": 0.0018301805887080997
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.244874464937489,
            "upper_bound": 14.253746855535958
          },
          "point_estimate": 14.248370100728463,
          "standard_error": 0.00232977862358693
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008638066265775831,
            "upper_bound": 0.009679757009614253
          },
          "point_estimate": 0.00536817505354298,
          "standard_error": 0.0023072544359604963
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.245996798877622,
            "upper_bound": 14.253163541445732
          },
          "point_estimate": 14.24940762101644,
          "standard_error": 0.0018752708277736956
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002687436913931438,
            "upper_bound": 0.008083976222977638
          },
          "point_estimate": 0.006123131060058915,
          "standard_error": 0.001454442548238617
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.689628014461285,
            "upper_bound": 22.750308372482863
          },
          "point_estimate": 22.721241693147512,
          "standard_error": 0.01555830463821464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.671988658456133,
            "upper_bound": 22.759754361653876
          },
          "point_estimate": 22.75081463516016,
          "standard_error": 0.02967749532129573
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003914984497521365,
            "upper_bound": 0.07556117032679488
          },
          "point_estimate": 0.029433165170594373,
          "standard_error": 0.022621543008322424
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.672481636241542,
            "upper_bound": 22.75869332816768
          },
          "point_estimate": 22.724088478217016,
          "standard_error": 0.023680502715573443
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02901522093894755,
            "upper_bound": 0.06215338179080761
          },
          "point_estimate": 0.05188441956489182,
          "standard_error": 0.00825289925012899
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52496.713502380946,
            "upper_bound": 52930.974333091785
          },
          "point_estimate": 52712.796152864044,
          "standard_error": 111.44747769056532
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52389.703191856446,
            "upper_bound": 53094.44347826087
          },
          "point_estimate": 52653.61344202899,
          "standard_error": 236.33201255421673
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.08122547899997,
            "upper_bound": 560.8787125641528
          },
          "point_estimate": 474.2570519425774,
          "standard_error": 156.9484524447807
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52344.4753808993,
            "upper_bound": 52650.79144144927
          },
          "point_estimate": 52448.05477131564,
          "standard_error": 78.89670442973689
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 250.7912292773053,
            "upper_bound": 421.49661442701154
          },
          "point_estimate": 371.2607917977032,
          "standard_error": 43.51952969414441
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48416.65372628597,
            "upper_bound": 48527.016774151714
          },
          "point_estimate": 48463.30913085688,
          "standard_error": 28.869037761004083
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48404.263941940415,
            "upper_bound": 48488.91574754902
          },
          "point_estimate": 48434.453152852046,
          "standard_error": 23.857210931672498
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.446506062451768,
            "upper_bound": 105.26598677086456
          },
          "point_estimate": 61.34357321240962,
          "standard_error": 22.65689476455294
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48408.16807588733,
            "upper_bound": 48475.47093719812
          },
          "point_estimate": 48444.647576220574,
          "standard_error": 17.25635132604394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.829961181530265,
            "upper_bound": 141.69234128075172
          },
          "point_estimate": 96.39148637298766,
          "standard_error": 34.91019744366283
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50395.19796208113,
            "upper_bound": 50825.278212191355
          },
          "point_estimate": 50594.11082363317,
          "standard_error": 110.9159936369544
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50334.8524691358,
            "upper_bound": 50924.36157407408
          },
          "point_estimate": 50384.27259424604,
          "standard_error": 154.71943786192605
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.670189234126056,
            "upper_bound": 609.251671072518
          },
          "point_estimate": 125.29775225470028,
          "standard_error": 167.2670835226788
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50346.79071314692,
            "upper_bound": 50640.22593289826
          },
          "point_estimate": 50474.04558080808,
          "standard_error": 84.3088571871714
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138.0725574115041,
            "upper_bound": 455.1866965342228
          },
          "point_estimate": 370.2762919824319,
          "standard_error": 81.63146522527181
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14114.46837404725,
            "upper_bound": 14135.328537296034
          },
          "point_estimate": 14124.236252821254,
          "standard_error": 5.361935028802724
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14110.658566433567,
            "upper_bound": 14138.46361046361
          },
          "point_estimate": 14118.240934990936,
          "standard_error": 6.774869613043431
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7344292871186826,
            "upper_bound": 29.229596718832447
          },
          "point_estimate": 13.504149158078553,
          "standard_error": 6.963874122840701
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14109.442426723566,
            "upper_bound": 14145.145727866904
          },
          "point_estimate": 14129.132572478027,
          "standard_error": 9.453578347624193
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.295572387577194,
            "upper_bound": 22.476199900717305
          },
          "point_estimate": 17.87101193874403,
          "standard_error": 3.676228147808759
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23146.265062075097,
            "upper_bound": 23188.520223780182
          },
          "point_estimate": 23166.132316585223,
          "standard_error": 10.856175689915736
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23141.944550669217,
            "upper_bound": 23194.857839388147
          },
          "point_estimate": 23155.66203526662,
          "standard_error": 14.089083303716276
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.103160174355828,
            "upper_bound": 60.29334386265761
          },
          "point_estimate": 21.23759104424043,
          "standard_error": 14.809148014725736
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23137.66108513718,
            "upper_bound": 23177.7167316993
          },
          "point_estimate": 23158.886393020617,
          "standard_error": 10.19597607584191
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.292284962328932,
            "upper_bound": 45.14330199451027
          },
          "point_estimate": 36.18613122105927,
          "standard_error": 7.299093943106008
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17642.462728227652,
            "upper_bound": 17681.020853424427
          },
          "point_estimate": 17659.76527744982,
          "standard_error": 9.951951716456986
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17637.80878301734,
            "upper_bound": 17682.390617403988
          },
          "point_estimate": 17648.85144628099,
          "standard_error": 10.218748861113625
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.801695910395764,
            "upper_bound": 50.4563655748083
          },
          "point_estimate": 17.912233033961034,
          "standard_error": 12.735290002935484
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17641.6357040045,
            "upper_bound": 17661.497783621337
          },
          "point_estimate": 17649.748621432045,
          "standard_error": 5.033305932916613
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.529733458737304,
            "upper_bound": 45.14177617167681
          },
          "point_estimate": 33.054708709971216,
          "standard_error": 9.0579196399641
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17631.876899249903,
            "upper_bound": 17664.212766445413
          },
          "point_estimate": 17644.918306351516,
          "standard_error": 8.659948511645924
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17628.39638983325,
            "upper_bound": 17649.76869839728
          },
          "point_estimate": 17635.172753763964,
          "standard_error": 5.973850570094343
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3353480578845451,
            "upper_bound": 25.327629540634216
          },
          "point_estimate": 10.444649192912026,
          "standard_error": 6.618137811662745
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17629.84198771125,
            "upper_bound": 17640.948763790173
          },
          "point_estimate": 17633.93112657134,
          "standard_error": 2.841884287120871
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.825767136954273,
            "upper_bound": 43.09921769401426
          },
          "point_estimate": 28.82297684183652,
          "standard_error": 11.679568905928209
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17421.50833578255,
            "upper_bound": 17448.209687040624
          },
          "point_estimate": 17434.35673759635,
          "standard_error": 6.872780530512092
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17417.804462571978,
            "upper_bound": 17452.136089784603
          },
          "point_estimate": 17431.66113243762,
          "standard_error": 7.489763725511867
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.0446260751857035,
            "upper_bound": 41.88107785530868
          },
          "point_estimate": 20.483646157456565,
          "standard_error": 9.936632984840678
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17421.42923359325,
            "upper_bound": 17440.744378747953
          },
          "point_estimate": 17431.07346511454,
          "standard_error": 4.779146509826164
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.936026939397282,
            "upper_bound": 29.20103785300772
          },
          "point_estimate": 22.90641140549065,
          "standard_error": 4.545597686804737
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30602.487583072787,
            "upper_bound": 30666.732581592216
          },
          "point_estimate": 30629.677770462564,
          "standard_error": 16.844895596174645
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30592.945906986533,
            "upper_bound": 30649.39393939394
          },
          "point_estimate": 30609.831790123455,
          "standard_error": 15.366343673255312
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.121037216402129,
            "upper_bound": 66.5142385013779
          },
          "point_estimate": 25.906162176855876,
          "standard_error": 16.21537985632417
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30595.958078289124,
            "upper_bound": 30624.130992429327
          },
          "point_estimate": 30605.601250601252,
          "standard_error": 7.280991399299889
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.427434156653902,
            "upper_bound": 81.46415725477199
          },
          "point_estimate": 55.89372217020199,
          "standard_error": 19.8292854565531
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32321.968683245777,
            "upper_bound": 32411.132438834524
          },
          "point_estimate": 32360.886676975653,
          "standard_error": 23.27926941749076
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32314.90569395018,
            "upper_bound": 32408.423709964412
          },
          "point_estimate": 32328.74945348246,
          "standard_error": 19.087328137447827
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.225959723353701,
            "upper_bound": 94.1222349837999
          },
          "point_estimate": 22.672560436744355,
          "standard_error": 19.93613398654328
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32317.84302869405,
            "upper_bound": 32337.91810879512
          },
          "point_estimate": 32326.202759162545,
          "standard_error": 5.161502182309196
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.814619826884368,
            "upper_bound": 103.25312718929084
          },
          "point_estimate": 77.58995337741865,
          "standard_error": 24.795323220835936
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51816.51754517356,
            "upper_bound": 51887.71357760173
          },
          "point_estimate": 51846.604081414305,
          "standard_error": 18.732417625796227
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51813.05896338564,
            "upper_bound": 51861.22238638679
          },
          "point_estimate": 51829.72434022825,
          "standard_error": 11.906498506406647
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.680656175901555,
            "upper_bound": 66.62227421307806
          },
          "point_estimate": 25.260572543681615,
          "standard_error": 15.490011158060184
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51818.05624910774,
            "upper_bound": 51862.318035459546
          },
          "point_estimate": 51834.17112103303,
          "standard_error": 11.432684017434962
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.04904957163408,
            "upper_bound": 91.9132833267768
          },
          "point_estimate": 62.578170353931355,
          "standard_error": 23.291680942172047
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17750.603682026896,
            "upper_bound": 17783.608015756476
          },
          "point_estimate": 17766.760517795956,
          "standard_error": 8.448834062499282
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17736.134524392233,
            "upper_bound": 17791.932513460597
          },
          "point_estimate": 17761.337502039485,
          "standard_error": 13.699474955902373
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9814972170369186,
            "upper_bound": 47.5285593250671
          },
          "point_estimate": 37.51626223165272,
          "standard_error": 11.67906770421315
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17750.310894415437,
            "upper_bound": 17781.014957006442
          },
          "point_estimate": 17764.55828136621,
          "standard_error": 7.678548048181202
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.69095220442483,
            "upper_bound": 34.222176527979386
          },
          "point_estimate": 28.11131319048595,
          "standard_error": 4.457471604688806
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18148.18207170968,
            "upper_bound": 18168.987347506805
          },
          "point_estimate": 18157.344595539533,
          "standard_error": 5.392524968012172
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18145.383344042264,
            "upper_bound": 18164.273163418293
          },
          "point_estimate": 18153.351740796268,
          "standard_error": 4.8755252105048115
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.994075617345916,
            "upper_bound": 23.44140473125505
          },
          "point_estimate": 10.850876833430569,
          "standard_error": 5.2750375705249475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18148.61043526945,
            "upper_bound": 18164.951908972173
          },
          "point_estimate": 18157.140789345587,
          "standard_error": 4.368403974213356
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.32513148621522,
            "upper_bound": 25.467528231333077
          },
          "point_estimate": 17.98523077724456,
          "standard_error": 5.582694857207152
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83701.37408838053,
            "upper_bound": 83858.88520700751
          },
          "point_estimate": 83774.31608770389,
          "standard_error": 40.418501567012875
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83663.23502304147,
            "upper_bound": 83878.14631336405
          },
          "point_estimate": 83742.87190037305,
          "standard_error": 51.59516464426265
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.79842506287882,
            "upper_bound": 227.9438409301436
          },
          "point_estimate": 127.7746838375348,
          "standard_error": 50.15242960828253
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83719.51661533654,
            "upper_bound": 83885.65101477152
          },
          "point_estimate": 83796.41456699983,
          "standard_error": 43.203572618498015
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.12026925494383,
            "upper_bound": 175.04167585849808
          },
          "point_estimate": 133.87441499998909,
          "standard_error": 29.77152886974781
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32778.97487125718,
            "upper_bound": 32858.47181010885
          },
          "point_estimate": 32818.85315087953,
          "standard_error": 20.42172601323984
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32764.284270113218,
            "upper_bound": 32878.00123985573
          },
          "point_estimate": 32812.5419554296,
          "standard_error": 28.741451008287044
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.322964388419194,
            "upper_bound": 115.46023954921148
          },
          "point_estimate": 71.77320807202648,
          "standard_error": 26.28538862215022
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32746.932369445814,
            "upper_bound": 32817.67889301305
          },
          "point_estimate": 32777.16300633542,
          "standard_error": 17.832957075764984
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.30469240345637,
            "upper_bound": 85.55025012652534
          },
          "point_estimate": 68.02557247136697,
          "standard_error": 11.875681378103218
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71954.77349174918,
            "upper_bound": 72070.18119094767
          },
          "point_estimate": 72007.22558038661,
          "standard_error": 29.655259555351975
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71937.15577557756,
            "upper_bound": 72037.4107260726
          },
          "point_estimate": 72013.65168316831,
          "standard_error": 26.17545989212661
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.627284587007612,
            "upper_bound": 136.34260573785917
          },
          "point_estimate": 68.92010323186756,
          "standard_error": 35.29025880543345
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71949.10362361044,
            "upper_bound": 72029.0923117902
          },
          "point_estimate": 71980.75994856628,
          "standard_error": 20.511243576501933
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.8633050731741,
            "upper_bound": 140.55642259001348
          },
          "point_estimate": 99.15220575431223,
          "standard_error": 29.56535546305519
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18524.21192615122,
            "upper_bound": 18609.25566215862
          },
          "point_estimate": 18558.51260190784,
          "standard_error": 22.80999875898183
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18512.32470071149,
            "upper_bound": 18566.600203977563
          },
          "point_estimate": 18542.598854750977,
          "standard_error": 16.38024584797961
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.137742517436369,
            "upper_bound": 68.34525043885007
          },
          "point_estimate": 38.241496478140526,
          "standard_error": 14.752626221212692
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18513.683483782595,
            "upper_bound": 18552.360536271244
          },
          "point_estimate": 18534.15600177487,
          "standard_error": 10.049029539201998
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.68317254012254,
            "upper_bound": 114.0701676867908
          },
          "point_estimate": 75.64831982100378,
          "standard_error": 31.249243100280747
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22420.086641446207,
            "upper_bound": 22445.84927579365
          },
          "point_estimate": 22430.55732929159,
          "standard_error": 6.849338503718397
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22417.042111992945,
            "upper_bound": 22434.754308127573
          },
          "point_estimate": 22425.257098765433,
          "standard_error": 4.799716156921304
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1163733752441722,
            "upper_bound": 20.32222053426917
          },
          "point_estimate": 11.35181955772556,
          "standard_error": 5.119887276112332
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22418.02842759126,
            "upper_bound": 22437.981785185184
          },
          "point_estimate": 22426.139491742822,
          "standard_error": 5.091780119222981
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.56707405320702,
            "upper_bound": 34.01736556718888
          },
          "point_estimate": 22.725904246594453,
          "standard_error": 9.08202984262021
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35672.137370905184,
            "upper_bound": 35718.38521123651
          },
          "point_estimate": 35691.68142273004,
          "standard_error": 12.126212673202732
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35666.05074419365,
            "upper_bound": 35704.4463526333
          },
          "point_estimate": 35675.39269592037,
          "standard_error": 11.005541742377144
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.350241878696548,
            "upper_bound": 46.720969484574006
          },
          "point_estimate": 18.12697750447756,
          "standard_error": 11.887175547025295
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35666.988635213456,
            "upper_bound": 35698.65479151927
          },
          "point_estimate": 35678.833947210784,
          "standard_error": 8.45691248375744
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.954514889948436,
            "upper_bound": 58.63064253203666
          },
          "point_estimate": 40.282439551787846,
          "standard_error": 14.229484864028066
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 423502.92633720936,
            "upper_bound": 424102.456839355
          },
          "point_estimate": 423789.2621594684,
          "standard_error": 153.84677597639654
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 423377.7511627907,
            "upper_bound": 424143.5863787376
          },
          "point_estimate": 423729.43846899224,
          "standard_error": 182.3427093027337
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146.8041802425532,
            "upper_bound": 882.9112703717038
          },
          "point_estimate": 494.5302719761387,
          "standard_error": 198.3963684260282
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 423305.9326056004,
            "upper_bound": 423836.6192763253
          },
          "point_estimate": 423558.5698882513,
          "standard_error": 141.15083258709782
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.4852099503167,
            "upper_bound": 650.0846141293322
          },
          "point_estimate": 510.8604849063077,
          "standard_error": 100.9818751447736
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2456420.4110000003,
            "upper_bound": 2458747.92
          },
          "point_estimate": 2457550.7724841265,
          "standard_error": 593.2584496484533
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2456291.222222222,
            "upper_bound": 2459119.811111111
          },
          "point_estimate": 2457473.2238095235,
          "standard_error": 589.7431375349582
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 178.3403035005087,
            "upper_bound": 3454.5897253355233
          },
          "point_estimate": 1336.5003362720208,
          "standard_error": 876.1982122120118
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2456060.4303661687,
            "upper_bound": 2458472.5231978446
          },
          "point_estimate": 2457346.461125541,
          "standard_error": 618.7655132391695
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 805.6032955265314,
            "upper_bound": 2566.7277318716706
          },
          "point_estimate": 1970.4198539023896,
          "standard_error": 423.1893802194197
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120155.14207999372,
            "upper_bound": 120370.5882633716
          },
          "point_estimate": 120265.01905285768,
          "standard_error": 55.178788288286974
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120132.099009901,
            "upper_bound": 120436.34455445544
          },
          "point_estimate": 120254.52621404998,
          "standard_error": 75.30728750583054
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.15805848026012,
            "upper_bound": 338.1263215218034
          },
          "point_estimate": 190.2212192411618,
          "standard_error": 76.52397253365102
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120183.06514784064,
            "upper_bound": 120301.58607903546
          },
          "point_estimate": 120242.59147914792,
          "standard_error": 30.290438846654496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.50157356787471,
            "upper_bound": 231.87932150626415
          },
          "point_estimate": 183.8597292540667,
          "standard_error": 32.97377204049382
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10838.479658254208,
            "upper_bound": 10901.140251017348
          },
          "point_estimate": 10868.877653232908,
          "standard_error": 16.02731692593503
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10827.08407254706,
            "upper_bound": 10928.810344827589
          },
          "point_estimate": 10843.00472263868,
          "standard_error": 30.208505491907772
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.229216450402385,
            "upper_bound": 81.7170706819177
          },
          "point_estimate": 52.09925460928833,
          "standard_error": 22.69539011090148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10816.938264860912,
            "upper_bound": 10877.536094079209
          },
          "point_estimate": 10837.02149963979,
          "standard_error": 15.671583823674304
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.56650547602057,
            "upper_bound": 61.963114981035794
          },
          "point_estimate": 53.461276055054526,
          "standard_error": 7.186261675069218
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9953.034112382386,
            "upper_bound": 10000.494414245914
          },
          "point_estimate": 9976.822888310346,
          "standard_error": 12.115002051252905
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9945.366892756008,
            "upper_bound": 10010.77249931488
          },
          "point_estimate": 9973.71610182394,
          "standard_error": 14.076871536685854
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.26844280789588,
            "upper_bound": 76.31875907029999
          },
          "point_estimate": 31.524566895339824,
          "standard_error": 17.545559543181916
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9941.157128628118,
            "upper_bound": 9984.700301838437
          },
          "point_estimate": 9967.145163414278,
          "standard_error": 11.114608328102996
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.264327063309768,
            "upper_bound": 51.14368181817952
          },
          "point_estimate": 40.44150416865347,
          "standard_error": 7.472558051696458
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14228.68950480384,
            "upper_bound": 14252.132158088714
          },
          "point_estimate": 14240.036127620911,
          "standard_error": 6.011551007489052
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14225.373858447489,
            "upper_bound": 14263.926223091976
          },
          "point_estimate": 14232.348330071756,
          "standard_error": 9.628757160250208
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4593502655355746,
            "upper_bound": 32.164866196081526
          },
          "point_estimate": 15.354005521933406,
          "standard_error": 8.518777854042797
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14222.450305966637,
            "upper_bound": 14240.13758823735
          },
          "point_estimate": 14231.414838234174,
          "standard_error": 4.581082118750439
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.684537253058224,
            "upper_bound": 23.919215183851257
          },
          "point_estimate": 20.083543067577473,
          "standard_error": 3.222298533122738
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.26505103713747,
            "upper_bound": 29.29189380737145
          },
          "point_estimate": 29.276018375155775,
          "standard_error": 0.00712857904353527
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.26359823817152,
            "upper_bound": 29.278644739066387
          },
          "point_estimate": 29.27130250133492,
          "standard_error": 0.003691963727084085
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001977653289046692,
            "upper_bound": 0.02026900577408647
          },
          "point_estimate": 0.009546489241029393,
          "standard_error": 0.004860434778042164
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.26604417009032,
            "upper_bound": 29.274639214327262
          },
          "point_estimate": 29.269645166750145,
          "standard_error": 0.0021583872589711825
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0052849963051279855,
            "upper_bound": 0.03574762468871503
          },
          "point_estimate": 0.023768845805437765,
          "standard_error": 0.0097040394563528
        }
      }
    },
    "memmem/sliceslice/prebuilt/sliceslice-haystack/words": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/sliceslice-haystack/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/sliceslice-haystack/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/sliceslice-haystack/words",
        "directory_name": "memmem/sliceslice_prebuilt_sliceslice-haystack_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171062.1777792682,
            "upper_bound": 171304.84877811314
          },
          "point_estimate": 171181.39314311798,
          "standard_error": 62.18420293936054
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170989.9650495566,
            "upper_bound": 171368.82658450704
          },
          "point_estimate": 171146.0408003577,
          "standard_error": 106.98885522761228
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.441441528960894,
            "upper_bound": 339.61830396664476
          },
          "point_estimate": 233.84229916616292,
          "standard_error": 78.28961078678464
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170966.26334387483,
            "upper_bound": 171279.08615395313
          },
          "point_estimate": 171084.71241997438,
          "standard_error": 82.20201326379592
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.74998701583877,
            "upper_bound": 248.89420366426535
          },
          "point_estimate": 207.7815543631584,
          "standard_error": 30.54147871084663
        }
      }
    },
    "memmem/sliceslice/prebuilt/sliceslice-i386/words": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/sliceslice_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26203885.09272619,
            "upper_bound": 26224945.600909226
          },
          "point_estimate": 26214340.450218253,
          "standard_error": 5385.541195349332
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26200899.872222222,
            "upper_bound": 26228673.666666668
          },
          "point_estimate": 26213341.839285716,
          "standard_error": 6559.315959996099
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4185.889369435608,
            "upper_bound": 30903.11617136075
          },
          "point_estimate": 18013.296248950985,
          "standard_error": 7086.892327986537
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26197895.894507244,
            "upper_bound": 26215485.06284916
          },
          "point_estimate": 26206850.04155844,
          "standard_error": 4536.746344748367
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9888.811732163502,
            "upper_bound": 22891.349009774553
          },
          "point_estimate": 17952.1973079357,
          "standard_error": 3337.4579573344704
        }
      }
    },
    "memmem/sliceslice/prebuilt/sliceslice-words/words": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/sliceslice_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83571961.06583333,
            "upper_bound": 83683155.65416667
          },
          "point_estimate": 83629538.66666667,
          "standard_error": 28496.180204639582
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83559181.0,
            "upper_bound": 83711896.33333333
          },
          "point_estimate": 83640507.16666667,
          "standard_error": 36564.42969072454
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20555.26023506382,
            "upper_bound": 160351.83565320057
          },
          "point_estimate": 113207.87459015478,
          "standard_error": 35703.370835999565
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50127.22047508378,
            "upper_bound": 122681.41618787564
          },
          "point_estimate": 94694.87834286584,
          "standard_error": 19169.49235060159
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.661808117736606,
            "upper_bound": 5.66527595518337
          },
          "point_estimate": 5.663397857534486,
          "standard_error": 0.0008872885777021432
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.661043522982322,
            "upper_bound": 5.664609084576841
          },
          "point_estimate": 5.663296522734328,
          "standard_error": 0.0008265078458372436
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00034362970006701556,
            "upper_bound": 0.004364178343425335
          },
          "point_estimate": 0.002308334470168291,
          "standard_error": 0.0010044578909892174
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.661802259067031,
            "upper_bound": 5.663712802430177
          },
          "point_estimate": 5.662755835210285,
          "standard_error": 0.000517521103575251
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00124389581847934,
            "upper_bound": 0.004128054550584064
          },
          "point_estimate": 0.0029624646285284165,
          "standard_error": 0.0008262426969669884
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.134905619848385,
            "upper_bound": 6.140363856434256
          },
          "point_estimate": 6.137279970955078,
          "standard_error": 0.0014126395929211835
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.134240003617593,
            "upper_bound": 6.139242943393311
          },
          "point_estimate": 6.13570927641295,
          "standard_error": 0.0011340243185148165
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002628322169263104,
            "upper_bound": 0.00608187810740479
          },
          "point_estimate": 0.002583047975201626,
          "standard_error": 0.001469100674685269
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.134444115263556,
            "upper_bound": 6.136307510898012
          },
          "point_estimate": 6.135314581969069,
          "standard_error": 0.00047675188884182905
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012104280174559988,
            "upper_bound": 0.006563854086418132
          },
          "point_estimate": 0.004692755733419079,
          "standard_error": 0.001472839966118317
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.134699340668588,
            "upper_bound": 6.13817791701994
          },
          "point_estimate": 6.13621759199969,
          "standard_error": 0.0008960291226109276
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.13478935217264,
            "upper_bound": 6.136900171287397
          },
          "point_estimate": 6.135780720959984,
          "standard_error": 0.000514690904353705
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00032430578571250525,
            "upper_bound": 0.0035948154526814317
          },
          "point_estimate": 0.001371753328882472,
          "standard_error": 0.000827861242685139
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.135428453500372,
            "upper_bound": 6.136685725695059
          },
          "point_estimate": 6.13610827144205,
          "standard_error": 0.00032308869741036445
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007719807407129517,
            "upper_bound": 0.004347781121604842
          },
          "point_estimate": 0.0029841989869026117,
          "standard_error": 0.0010341397170640983
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.135740074615334,
            "upper_bound": 6.140782208467312
          },
          "point_estimate": 6.137957333651867,
          "standard_error": 0.001300602784242478
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.134631974145421,
            "upper_bound": 6.139840350896437
          },
          "point_estimate": 6.137058844705742,
          "standard_error": 0.0014331409707979304
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00038618760984398057,
            "upper_bound": 0.005862367351685742
          },
          "point_estimate": 0.0032263597727224774,
          "standard_error": 0.001464993179536449
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.13529801634652,
            "upper_bound": 6.137649254326491
          },
          "point_estimate": 6.13625299034808,
          "standard_error": 0.0006013145002109144
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016282162773344395,
            "upper_bound": 0.006180143578820105
          },
          "point_estimate": 0.004347598636267686,
          "standard_error": 0.0013525094001773875
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.154042210438919,
            "upper_bound": 5.1565524755427266
          },
          "point_estimate": 5.155145955154101,
          "standard_error": 0.0006481586709483232
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.153732252421285,
            "upper_bound": 5.155988988736554
          },
          "point_estimate": 5.154567988657799,
          "standard_error": 0.0004684122464064877
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00006065925221670399,
            "upper_bound": 0.002771142671677613
          },
          "point_estimate": 0.0012116648413203798,
          "standard_error": 0.0007202148821462151
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.1533754924860675,
            "upper_bound": 5.155388819846623
          },
          "point_estimate": 5.154164387191798,
          "standard_error": 0.0005159787951834629
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006209416485971436,
            "upper_bound": 0.0030358957029372395
          },
          "point_estimate": 0.0021617903541522525,
          "standard_error": 0.0006784706228446888
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.571965309847728,
            "upper_bound": 14.612368203753231
          },
          "point_estimate": 14.594127369171904,
          "standard_error": 0.010449385332307331
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.567501715565124,
            "upper_bound": 14.616200944440854
          },
          "point_estimate": 14.609608555018344,
          "standard_error": 0.012066032997767494
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002987670297188297,
            "upper_bound": 0.057697528446441035
          },
          "point_estimate": 0.01071529266747458,
          "standard_error": 0.013526686705590032
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.597649649636828,
            "upper_bound": 14.617244297272922
          },
          "point_estimate": 14.610988066796356,
          "standard_error": 0.005113274436517599
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008256216047904438,
            "upper_bound": 0.04702983140636161
          },
          "point_estimate": 0.03487121461253863,
          "standard_error": 0.009441447220854842
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.872331669213158,
            "upper_bound": 6.876401409977521
          },
          "point_estimate": 6.8743286742550485,
          "standard_error": 0.001045224582559304
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.871454798179965,
            "upper_bound": 6.877505370254038
          },
          "point_estimate": 6.873530557825653,
          "standard_error": 0.001930837260283602
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00018095264714939797,
            "upper_bound": 0.005710888735754819
          },
          "point_estimate": 0.003972119535620175,
          "standard_error": 0.001433260269943699
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.87210157849479,
            "upper_bound": 6.875182483062425
          },
          "point_estimate": 6.8733853749871985,
          "standard_error": 0.000789038628341126
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002242703972275156,
            "upper_bound": 0.004130358891033818
          },
          "point_estimate": 0.0034981675903687946,
          "standard_error": 0.0004819836025162844
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.625353756828153,
            "upper_bound": 6.630978591125377
          },
          "point_estimate": 6.627719695234726,
          "standard_error": 0.0014800973213687366
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.624983132303894,
            "upper_bound": 6.629513433079241
          },
          "point_estimate": 6.626071634489884,
          "standard_error": 0.0010395486067528556
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0000945316676345842,
            "upper_bound": 0.005187501925539995
          },
          "point_estimate": 0.0016362682825378796,
          "standard_error": 0.001271124613796029
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.625032258696029,
            "upper_bound": 6.627996495097379
          },
          "point_estimate": 6.626172856563901,
          "standard_error": 0.0007636498268727092
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009680117128219736,
            "upper_bound": 0.007170330482655226
          },
          "point_estimate": 0.004953555851316284,
          "standard_error": 0.0018068894283777017
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.084940974102915,
            "upper_bound": 9.090727610119526
          },
          "point_estimate": 9.087362127873096,
          "standard_error": 0.0015299487011008206
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.084226556686833,
            "upper_bound": 9.088653292481204
          },
          "point_estimate": 9.08582720726518,
          "standard_error": 0.001055070395096687
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005002303910780008,
            "upper_bound": 0.005118501410559036
          },
          "point_estimate": 0.002347331689734806,
          "standard_error": 0.0012362247656648236
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.084547870613202,
            "upper_bound": 9.088445800049204
          },
          "point_estimate": 9.086624549038987,
          "standard_error": 0.0009865497526979105
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001338385482995101,
            "upper_bound": 0.007526817381416016
          },
          "point_estimate": 0.0050978469385122,
          "standard_error": 0.001917925383657857
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.135337977723592,
            "upper_bound": 6.1401220063708255
          },
          "point_estimate": 6.137317500692083,
          "standard_error": 0.0012672020918212715
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.134983323949468,
            "upper_bound": 6.137903210834516
          },
          "point_estimate": 6.1365296433928815,
          "standard_error": 0.0008337055440408777
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003935867697409297,
            "upper_bound": 0.003863334379782826
          },
          "point_estimate": 0.001876473084388844,
          "standard_error": 0.0008595950228846336
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.135482585678586,
            "upper_bound": 6.137358866638698
          },
          "point_estimate": 6.136645876067999,
          "standard_error": 0.0004813258591257423
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011171278810429431,
            "upper_bound": 0.006320018550616249
          },
          "point_estimate": 0.004233306190869874,
          "standard_error": 0.0016677751651903762
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.66482669972085,
            "upper_bound": 4.668276119258815
          },
          "point_estimate": 4.666295684229338,
          "standard_error": 0.0009006827405840439
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.664203642504137,
            "upper_bound": 4.6669589703417
          },
          "point_estimate": 4.6660115725280455,
          "standard_error": 0.0007434831959569749
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003760069474057016,
            "upper_bound": 0.0034009469603274328
          },
          "point_estimate": 0.002105208960394236,
          "standard_error": 0.0007701452112056117
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.664768507395943,
            "upper_bound": 4.666237311041736
          },
          "point_estimate": 4.665511247094884,
          "standard_error": 0.0003817206481735364
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000996512349875369,
            "upper_bound": 0.004443988524507573
          },
          "point_estimate": 0.0030217726554138524,
          "standard_error": 0.001084462143760019
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.446928690187537,
            "upper_bound": 16.454206814776615
          },
          "point_estimate": 16.449962364585296,
          "standard_error": 0.0019159729542150684
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.445596369764083,
            "upper_bound": 16.451828345004294
          },
          "point_estimate": 16.448694517680217,
          "standard_error": 0.0016214327402621265
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004861581860625071,
            "upper_bound": 0.007052846448935503
          },
          "point_estimate": 0.0046197631635518125,
          "standard_error": 0.001616237615489782
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.445898843835536,
            "upper_bound": 16.45062362189761
          },
          "point_estimate": 16.448323288806275,
          "standard_error": 0.001240512988929562
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001896425815880937,
            "upper_bound": 0.009425277400991706
          },
          "point_estimate": 0.006359150928038039,
          "standard_error": 0.002397959763684104
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1193809.4957072134,
            "upper_bound": 1197932.8280107528
          },
          "point_estimate": 1195796.7928968256,
          "standard_error": 1054.7588763230451
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1192721.0,
            "upper_bound": 1198420.5394265233
          },
          "point_estimate": 1195216.2157258063,
          "standard_error": 1879.6724146345616
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 509.13492752031664,
            "upper_bound": 6148.079646736885
          },
          "point_estimate": 4199.5053430783855,
          "standard_error": 1509.9293702895825
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1192951.8392579732,
            "upper_bound": 1197331.2122551682
          },
          "point_estimate": 1194822.134478425,
          "standard_error": 1140.9968103914714
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2113.1232064819137,
            "upper_bound": 4388.2604159252905
          },
          "point_estimate": 3513.392938668573,
          "standard_error": 609.120025963327
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1329273.1134587585,
            "upper_bound": 1331567.60377551
          },
          "point_estimate": 1330423.4914158164,
          "standard_error": 586.4082830029478
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1328339.892857143,
            "upper_bound": 1332170.206632653
          },
          "point_estimate": 1330744.9785714285,
          "standard_error": 878.9803575223921
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 345.5472813654738,
            "upper_bound": 3494.847441525371
          },
          "point_estimate": 2462.295984856922,
          "standard_error": 873.1378267473376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1328528.2795918367,
            "upper_bound": 1331484.521886447
          },
          "point_estimate": 1329894.5175324676,
          "standard_error": 746.9302727327542
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1175.47586061204,
            "upper_bound": 2360.8466210348693
          },
          "point_estimate": 1958.515758622751,
          "standard_error": 300.9276291446753
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1234504.0480115735,
            "upper_bound": 1238224.0398518518
          },
          "point_estimate": 1236004.9457050264,
          "standard_error": 1001.8604079651802
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1234039.4562499998,
            "upper_bound": 1236474.701904762
          },
          "point_estimate": 1235154.5305555556,
          "standard_error": 627.5007658450387
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 229.4357778711707,
            "upper_bound": 2699.6329335720243
          },
          "point_estimate": 1424.376007629114,
          "standard_error": 708.8970977367533
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1234132.878855721,
            "upper_bound": 1235705.9304536649
          },
          "point_estimate": 1234679.617056277,
          "standard_error": 399.63910423570985
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 707.8230992151746,
            "upper_bound": 5006.01416298457
          },
          "point_estimate": 3333.8729335028634,
          "standard_error": 1380.7101211483996
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/stud_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325496.3943069728,
            "upper_bound": 326509.46060228004
          },
          "point_estimate": 325984.6450584609,
          "standard_error": 260.5702465669143
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325206.3267857143,
            "upper_bound": 326774.60714285716
          },
          "point_estimate": 325913.42712053575,
          "standard_error": 370.1948731182992
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189.71808163181268,
            "upper_bound": 1580.0597419483
          },
          "point_estimate": 868.4280177966036,
          "standard_error": 365.77314117471343
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325425.9094387755,
            "upper_bound": 326062.64220383274
          },
          "point_estimate": 325767.5912569573,
          "standard_error": 161.36120873233858
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464.4785144253558,
            "upper_bound": 1072.9090755533468
          },
          "point_estimate": 871.7425021189866,
          "standard_error": 154.71072606240395
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/stud_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246304.53587730593,
            "upper_bound": 247051.51927689297
          },
          "point_estimate": 246674.1564103389,
          "standard_error": 191.71972201819833
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246088.36439564565,
            "upper_bound": 247309.14527027027
          },
          "point_estimate": 246621.48597972977,
          "standard_error": 331.27327619272705
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194.91013910721892,
            "upper_bound": 1047.6150754889643
          },
          "point_estimate": 847.0058588139972,
          "standard_error": 228.07849119004
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246131.6112623531,
            "upper_bound": 246706.36462231464
          },
          "point_estimate": 246399.59673569672,
          "standard_error": 145.0736042368214
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 407.0946014392924,
            "upper_bound": 760.1630154658114
          },
          "point_estimate": 640.3393504831845,
          "standard_error": 90.36982613533635
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 429253.799945845,
            "upper_bound": 430357.9611811157
          },
          "point_estimate": 429797.6448912231,
          "standard_error": 282.59087364624247
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 428779.75294117647,
            "upper_bound": 430608.2117647059
          },
          "point_estimate": 429753.4943137255,
          "standard_error": 399.5526159776998
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138.7232941646199,
            "upper_bound": 1626.4644981832944
          },
          "point_estimate": 1355.4365018185515,
          "standard_error": 432.4713284685705
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 429059.81907308375,
            "upper_bound": 430517.9555759804
          },
          "point_estimate": 429716.2528036669,
          "standard_error": 371.853732685184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 538.2547710194239,
            "upper_bound": 1151.683127427688
          },
          "point_estimate": 942.8066102532084,
          "standard_error": 154.68891998110175
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/stud_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237068.16964517624,
            "upper_bound": 237854.90254329005
          },
          "point_estimate": 237421.5127048547,
          "standard_error": 202.99136193338936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236929.76406926409,
            "upper_bound": 237746.3051948052
          },
          "point_estimate": 237300.0119047619,
          "standard_error": 187.96877674090143
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.74127986987932,
            "upper_bound": 948.75487406534
          },
          "point_estimate": 470.0796621089821,
          "standard_error": 220.4619537242416
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236948.55762367373,
            "upper_bound": 237705.60490061404
          },
          "point_estimate": 237275.6949401248,
          "standard_error": 201.84923200729872
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243.8505224631945,
            "upper_bound": 934.16296527205
          },
          "point_estimate": 674.8455773691129,
          "standard_error": 193.9050198369887
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/stud_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 529267.6251000691,
            "upper_bound": 529719.3273602484
          },
          "point_estimate": 529502.7320623419,
          "standard_error": 115.40538003053662
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 529197.4178743961,
            "upper_bound": 529841.0778985508
          },
          "point_estimate": 529579.4150563607,
          "standard_error": 168.17707294743988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.42542476939931,
            "upper_bound": 662.0158045512642
          },
          "point_estimate": 411.73198773867375,
          "standard_error": 146.34188798398236
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 529558.8774334118,
            "upper_bound": 529855.7347256485
          },
          "point_estimate": 529745.6739695087,
          "standard_error": 76.42208371289117
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195.77783337835908,
            "upper_bound": 466.6330102410394
          },
          "point_estimate": 383.82969274725326,
          "standard_error": 66.80612910130694
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112311.3227983539,
            "upper_bound": 112519.02239432688
          },
          "point_estimate": 112417.5714160788,
          "standard_error": 53.10268913198175
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112290.1383744856,
            "upper_bound": 112533.6899691358
          },
          "point_estimate": 112431.29756393298,
          "standard_error": 63.724850548100264
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.02150194708125,
            "upper_bound": 299.75517430790575
          },
          "point_estimate": 159.9484645214641,
          "standard_error": 76.59713393901592
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112209.45705935635,
            "upper_bound": 112479.05387110276
          },
          "point_estimate": 112334.9378387045,
          "standard_error": 70.62007680846217
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.44277259122846,
            "upper_bound": 226.41499206862363
          },
          "point_estimate": 177.2982144695504,
          "standard_error": 34.21597658165854
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118556.94060389846,
            "upper_bound": 118951.02355359084
          },
          "point_estimate": 118743.74012615687,
          "standard_error": 101.3035987055147
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118453.570497906,
            "upper_bound": 118980.0651465798
          },
          "point_estimate": 118659.4125407166,
          "standard_error": 145.65256912180743
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.543407462121834,
            "upper_bound": 560.4598148055417
          },
          "point_estimate": 339.0816715805423,
          "standard_error": 133.89701264580256
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118468.18183689758,
            "upper_bound": 118677.5998020715
          },
          "point_estimate": 118561.74915182537,
          "standard_error": 53.52324462934474
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162.83160561409494,
            "upper_bound": 425.04147302348787
          },
          "point_estimate": 338.157616703153,
          "standard_error": 66.29114509822327
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193659.7627135536,
            "upper_bound": 194091.2472639419
          },
          "point_estimate": 193868.10187208216,
          "standard_error": 110.66496068743017
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193629.2139037433,
            "upper_bound": 194191.47237076648
          },
          "point_estimate": 193805.59117647057,
          "standard_error": 126.14733811487874
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.069190624897423,
            "upper_bound": 655.1876204536588
          },
          "point_estimate": 331.5918529187568,
          "standard_error": 168.8673493544301
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193640.90082581848,
            "upper_bound": 193920.749925059
          },
          "point_estimate": 193773.9829710397,
          "standard_error": 70.82537203286763
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184.6404232396829,
            "upper_bound": 475.793715562145
          },
          "point_estimate": 369.921662418054,
          "standard_error": 73.41943166778076
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271678.29716796963,
            "upper_bound": 271866.4406832134
          },
          "point_estimate": 271771.27654732286,
          "standard_error": 48.25098369595185
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271629.9253731343,
            "upper_bound": 271897.0217661691
          },
          "point_estimate": 271800.33402185503,
          "standard_error": 63.683582562328894
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.99478057610939,
            "upper_bound": 293.87657799157574
          },
          "point_estimate": 201.27380205479952,
          "standard_error": 76.33243093858447
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271747.4248550677,
            "upper_bound": 271886.8775001075
          },
          "point_estimate": 271813.33500678424,
          "standard_error": 35.40179200138792
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.90126181813508,
            "upper_bound": 200.5069675431117
          },
          "point_estimate": 160.8475941354434,
          "standard_error": 27.69631427592549
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289006.8341880433,
            "upper_bound": 289693.8425088183
          },
          "point_estimate": 289313.4653303729,
          "standard_error": 177.40841537717728
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 288992.0582010582,
            "upper_bound": 289575.49823633156
          },
          "point_estimate": 289175.71395502647,
          "standard_error": 131.5313660672288
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.50232805840942,
            "upper_bound": 838.3208094223975
          },
          "point_estimate": 260.9150425900465,
          "standard_error": 191.15952067816008
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289050.4986641362,
            "upper_bound": 289578.64531919826
          },
          "point_estimate": 289279.4059163059,
          "standard_error": 131.7582338400463
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.07769810478152,
            "upper_bound": 839.0932666828223
          },
          "point_estimate": 594.8871454190196,
          "standard_error": 185.22705081970037
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316326.10636563145,
            "upper_bound": 317204.735731884
          },
          "point_estimate": 316751.8189409938,
          "standard_error": 225.21871311241972
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 315921.2,
            "upper_bound": 317231.43586956523
          },
          "point_estimate": 316813.1672463768,
          "standard_error": 342.76785273714705
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.39170948686516,
            "upper_bound": 1423.4079754903069
          },
          "point_estimate": 632.541609639708,
          "standard_error": 339.3787659917045
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316120.5455755899,
            "upper_bound": 316927.48172737955
          },
          "point_estimate": 316465.44004517223,
          "standard_error": 204.68305181902537
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 430.7338007581516,
            "upper_bound": 964.3313090200974
          },
          "point_estimate": 748.9340671685907,
          "standard_error": 144.94265859778355
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395339.3902250582,
            "upper_bound": 395830.0238252135
          },
          "point_estimate": 395569.82625819533,
          "standard_error": 125.9435576855031
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395302.7669836957,
            "upper_bound": 395897.7946859903
          },
          "point_estimate": 395422.27810559,
          "standard_error": 152.4576257730676
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.940776813003694,
            "upper_bound": 660.0006190344369
          },
          "point_estimate": 226.01088389511528,
          "standard_error": 173.77185465138814
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395305.23393491935,
            "upper_bound": 395768.48019071406
          },
          "point_estimate": 395528.35392433655,
          "standard_error": 118.49860784130892
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179.88976420371318,
            "upper_bound": 536.4169941957568
          },
          "point_estimate": 419.1010964619396,
          "standard_error": 89.02573885054704
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272696.2174583781,
            "upper_bound": 273611.79810776946
          },
          "point_estimate": 273134.4720703544,
          "standard_error": 234.2296451586516
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272369.6609022556,
            "upper_bound": 273754.86099624063
          },
          "point_estimate": 272929.96992481203,
          "standard_error": 425.45205387243544
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.83623508217723,
            "upper_bound": 1324.0966596504024
          },
          "point_estimate": 886.7354534302013,
          "standard_error": 340.72241803805287
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272456.6677874461,
            "upper_bound": 273361.22534146014
          },
          "point_estimate": 272807.4118933698,
          "standard_error": 236.8062286372812
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 466.0427833730069,
            "upper_bound": 967.617713476664
          },
          "point_estimate": 782.4793792058944,
          "standard_error": 132.5292856643809
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121595.09509541396,
            "upper_bound": 121937.87493218137
          },
          "point_estimate": 121767.7984343314,
          "standard_error": 87.5003225347123
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121556.23097826086,
            "upper_bound": 121950.84791898922
          },
          "point_estimate": 121782.01839464884,
          "standard_error": 100.174577721624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.944776705278734,
            "upper_bound": 501.693732464394
          },
          "point_estimate": 239.0907327118138,
          "standard_error": 112.9357196248524
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121505.05597654282,
            "upper_bound": 121923.63798869796
          },
          "point_estimate": 121693.85758589236,
          "standard_error": 106.16849851743608
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142.21538649619305,
            "upper_bound": 385.5789386340704
          },
          "point_estimate": 291.68194069342036,
          "standard_error": 61.22590067283669
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203477.92543052227,
            "upper_bound": 204067.21818768288
          },
          "point_estimate": 203768.4119983152,
          "standard_error": 151.3345396851991
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203306.67737430168,
            "upper_bound": 204243.0893854749
          },
          "point_estimate": 203755.7292498005,
          "standard_error": 235.81032452699563
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.22819857924716,
            "upper_bound": 887.3785329889046
          },
          "point_estimate": 582.4343610284365,
          "standard_error": 189.77441574518969
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203307.6370540767,
            "upper_bound": 203862.40435184856
          },
          "point_estimate": 203537.21181165203,
          "standard_error": 142.01826887070658
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306.14980792266897,
            "upper_bound": 607.7699685632416
          },
          "point_estimate": 504.50880189310993,
          "standard_error": 77.01169508321271
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150014.56188803623,
            "upper_bound": 150597.44621230158
          },
          "point_estimate": 150292.1756300013,
          "standard_error": 149.7881653492699
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149835.05681818182,
            "upper_bound": 150620.85399449035
          },
          "point_estimate": 150300.65531942804,
          "standard_error": 196.31259566211955
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.22397788824858,
            "upper_bound": 869.7008536712798
          },
          "point_estimate": 548.1802883532579,
          "standard_error": 191.9208019936829
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150002.17611690023,
            "upper_bound": 150550.00323282622
          },
          "point_estimate": 150305.27538907374,
          "standard_error": 142.39707048749264
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265.8649344173589,
            "upper_bound": 652.1032231795728
          },
          "point_estimate": 499.5771119258384,
          "standard_error": 104.98308741450155
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47581.68895947602,
            "upper_bound": 47631.11022648653
          },
          "point_estimate": 47604.240485799455,
          "standard_error": 12.697375895862582
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47577.063547120415,
            "upper_bound": 47632.74784966343
          },
          "point_estimate": 47595.34195935137,
          "standard_error": 12.924435143394392
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.2949250377904424,
            "upper_bound": 64.66827394681243
          },
          "point_estimate": 24.905752458142462,
          "standard_error": 15.973162207249915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47581.923560089526,
            "upper_bound": 47612.453832089515
          },
          "point_estimate": 47594.575052696,
          "standard_error": 7.783652746189555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.895531833283298,
            "upper_bound": 56.055804520821155
          },
          "point_estimate": 42.26005086286548,
          "standard_error": 10.898732670892404
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1115164.5425781626,
            "upper_bound": 1118661.5642255892
          },
          "point_estimate": 1116839.4931072632,
          "standard_error": 895.2702155916693
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1114456.2653198652,
            "upper_bound": 1120329.8409090908
          },
          "point_estimate": 1115671.2108585858,
          "standard_error": 1536.875308462753
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500.1630168779343,
            "upper_bound": 4750.595132478214
          },
          "point_estimate": 2820.839637192909,
          "standard_error": 1178.024875709984
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1114930.9087542088,
            "upper_bound": 1117173.031510547
          },
          "point_estimate": 1116048.7822117277,
          "standard_error": 589.9191347383618
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1641.432806963371,
            "upper_bound": 3476.890650215072
          },
          "point_estimate": 2983.519018887912,
          "standard_error": 452.1960534107056
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124983.52648740998,
            "upper_bound": 125170.23450563532
          },
          "point_estimate": 125063.33636297932,
          "standard_error": 48.69936636139965
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124957.454620084,
            "upper_bound": 125105.98453608248
          },
          "point_estimate": 125026.50171821304,
          "standard_error": 41.1549008183634
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.89202329520877,
            "upper_bound": 188.4958656540926
          },
          "point_estimate": 105.88139283671777,
          "standard_error": 39.76365722755451
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124944.09293398447,
            "upper_bound": 125027.79726636948
          },
          "point_estimate": 124976.06681840496,
          "standard_error": 21.44269126339602
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.06332078975712,
            "upper_bound": 237.79840758874568
          },
          "point_estimate": 162.19149437082706,
          "standard_error": 57.80807099191209
        }
      }
    },
    "memmem/stud/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/stud_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24118.07058761293,
            "upper_bound": 24241.6000672809
          },
          "point_estimate": 24174.55388136926,
          "standard_error": 31.82764581862736
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24090.46210106383,
            "upper_bound": 24260.699800531915
          },
          "point_estimate": 24137.21259604019,
          "standard_error": 44.00450966226428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.772149197120551,
            "upper_bound": 180.6845332727043
          },
          "point_estimate": 73.19835721997356,
          "standard_error": 43.59096577225417
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24099.248571877455,
            "upper_bound": 24211.86074477225
          },
          "point_estimate": 24146.87501554297,
          "standard_error": 29.31698536781629
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.69067584781342,
            "upper_bound": 137.8539933723623
          },
          "point_estimate": 105.6362699209983,
          "standard_error": 24.42410862304125
        }
      }
    },
    "memmem/stud/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/stud_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24514.48442444487,
            "upper_bound": 24576.39837745098
          },
          "point_estimate": 24542.77112216534,
          "standard_error": 15.94396489898634
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24510.823360378636,
            "upper_bound": 24581.03082037413
          },
          "point_estimate": 24528.10383517392,
          "standard_error": 14.78651920048428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.432324941574375,
            "upper_bound": 85.05342954477082
          },
          "point_estimate": 26.109057794300092,
          "standard_error": 19.061379299573144
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24502.558193307123,
            "upper_bound": 24532.67483894013
          },
          "point_estimate": 24518.515868040005,
          "standard_error": 7.636808667781016
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.357893667994254,
            "upper_bound": 69.32420866464213
          },
          "point_estimate": 52.957950429692666,
          "standard_error": 13.259217855897072
        }
      }
    },
    "memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/stud_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 607082.2591113094,
            "upper_bound": 609176.0029704366
          },
          "point_estimate": 608053.4087440476,
          "standard_error": 538.6861734342092
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 606718.526984127,
            "upper_bound": 609029.2
          },
          "point_estimate": 607759.7697916667,
          "standard_error": 665.2421348912354
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 385.6230764871908,
            "upper_bound": 2913.364898777309
          },
          "point_estimate": 1672.397480309004,
          "standard_error": 599.6722834391151
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 606801.5518333333,
            "upper_bound": 608550.6796626984
          },
          "point_estimate": 607662.7517748917,
          "standard_error": 450.9466738707279
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 856.1206101005278,
            "upper_bound": 2431.1165674260023
          },
          "point_estimate": 1795.0086908528808,
          "standard_error": 443.1918695234002
        }
      }
    },
    "memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/stud_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1331.9189672221205,
            "upper_bound": 1336.4142965122094
          },
          "point_estimate": 1334.102156509506,
          "standard_error": 1.147406495824583
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1330.9033876579383,
            "upper_bound": 1338.7986763282497
          },
          "point_estimate": 1332.147797106757,
          "standard_error": 2.411642072153944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03856949940673664,
            "upper_bound": 6.016505347620437
          },
          "point_estimate": 2.9200160774396764,
          "standard_error": 1.6783415664606254
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1332.0580204119017,
            "upper_bound": 1337.934545939133
          },
          "point_estimate": 1335.2387895276304,
          "standard_error": 1.6003296381658496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.465806044219871,
            "upper_bound": 4.224278525337995
          },
          "point_estimate": 3.8298276143264487,
          "standard_error": 0.4651045449795451
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.28997199968819,
            "upper_bound": 29.314996394426696
          },
          "point_estimate": 29.300618731968846,
          "standard_error": 0.006517171289393926
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.286221465379136,
            "upper_bound": 29.303584423200828
          },
          "point_estimate": 29.30056906708777,
          "standard_error": 0.005386200844269712
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013650278933235566,
            "upper_bound": 0.026671157328273888
          },
          "point_estimate": 0.009250627413819804,
          "standard_error": 0.006626028196793941
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.285918524460183,
            "upper_bound": 29.302556612209305
          },
          "point_estimate": 29.295052776755995,
          "standard_error": 0.004662864730619706
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007149110288883361,
            "upper_bound": 0.03182354364226784
          },
          "point_estimate": 0.02161505688167892,
          "standard_error": 0.007774408546045109
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.41152085816389,
            "upper_bound": 32.42497128679072
          },
          "point_estimate": 32.41767512169558,
          "standard_error": 0.0034649965375660777
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.4092670742685,
            "upper_bound": 32.42491860200437
          },
          "point_estimate": 32.414821094880324,
          "standard_error": 0.0035073134298407253
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002433208528916337,
            "upper_bound": 0.017185308333711784
          },
          "point_estimate": 0.007626061151941951,
          "standard_error": 0.0040224909820993585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.40786016381625,
            "upper_bound": 32.4240764940055
          },
          "point_estimate": 32.41508707297907,
          "standard_error": 0.004311983743958308
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004402438970391932,
            "upper_bound": 0.015584822140440326
          },
          "point_estimate": 0.011567667985922663,
          "standard_error": 0.00305626600568689
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.988506048011967,
            "upper_bound": 23.060912739789856
          },
          "point_estimate": 23.026418151097687,
          "standard_error": 0.018503150647080618
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.96639891833619,
            "upper_bound": 23.072819504121924
          },
          "point_estimate": 23.06754101210788,
          "standard_error": 0.033144302380272744
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001202306961277887,
            "upper_bound": 0.09731971725135909
          },
          "point_estimate": 0.0141372197201104,
          "standard_error": 0.031041095554892116
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.955116182394203,
            "upper_bound": 23.070261892149457
          },
          "point_estimate": 22.99417368896061,
          "standard_error": 0.02729100699546764
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03115921645570083,
            "upper_bound": 0.07168458719473421
          },
          "point_estimate": 0.0614725731822948,
          "standard_error": 0.010414948641207232
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.48769654227242,
            "upper_bound": 27.508949995498597
          },
          "point_estimate": 27.49693454693098,
          "standard_error": 0.005530313326414026
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.48499322623871,
            "upper_bound": 27.504917704062315
          },
          "point_estimate": 27.49089387349663,
          "standard_error": 0.0055360452186721506
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005208137697273186,
            "upper_bound": 0.02255799165475996
          },
          "point_estimate": 0.008950548475427674,
          "standard_error": 0.006511172036275361
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.48549626484175,
            "upper_bound": 27.49157915253351
          },
          "point_estimate": 27.487420023060707,
          "standard_error": 0.001584659866736321
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0055051538855835375,
            "upper_bound": 0.026053844171532805
          },
          "point_estimate": 0.01844561865084247,
          "standard_error": 0.005903908643546613
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.8789788964942,
            "upper_bound": 35.940453983019964
          },
          "point_estimate": 35.91174765565286,
          "standard_error": 0.015761503575778022
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.871410899282175,
            "upper_bound": 35.943572756570184
          },
          "point_estimate": 35.91959587139421,
          "standard_error": 0.015024896344978752
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005083522884595993,
            "upper_bound": 0.08835010668324898
          },
          "point_estimate": 0.029911237359069632,
          "standard_error": 0.021332992706984015
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.90200931855261,
            "upper_bound": 35.93280518468147
          },
          "point_estimate": 35.91948549057369,
          "standard_error": 0.007745309781081841
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018753491952825177,
            "upper_bound": 0.06803133206736722
          },
          "point_estimate": 0.052515329882723415,
          "standard_error": 0.011978526415148606
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.31506926061427,
            "upper_bound": 65.34870563044863
          },
          "point_estimate": 65.32937657674962,
          "standard_error": 0.008809942998626889
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.31384042174653,
            "upper_bound": 65.34232101481709
          },
          "point_estimate": 65.31869533617224,
          "standard_error": 0.007003571367398517
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018227898158826592,
            "upper_bound": 0.03545983121092514
          },
          "point_estimate": 0.00914850559958213,
          "standard_error": 0.00906129423659899
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.31470814028977,
            "upper_bound": 65.33308864688951
          },
          "point_estimate": 65.32186711075617,
          "standard_error": 0.004886625068723001
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007751679192174791,
            "upper_bound": 0.042806073570027595
          },
          "point_estimate": 0.0294349280668625,
          "standard_error": 0.010366595559219365
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.62412685659308,
            "upper_bound": 54.66423756974955
          },
          "point_estimate": 54.642874831552696,
          "standard_error": 0.010308131215363217
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.61680891615431,
            "upper_bound": 54.66684784350728
          },
          "point_estimate": 54.63662050976927,
          "standard_error": 0.013690688036930286
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035373227670311715,
            "upper_bound": 0.05199589728831091
          },
          "point_estimate": 0.026941858652747935,
          "standard_error": 0.014436952940561664
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.61190449203119,
            "upper_bound": 54.6465143465245
          },
          "point_estimate": 54.627712648493166,
          "standard_error": 0.0091059713891784
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0156534225722686,
            "upper_bound": 0.044568060130832705
          },
          "point_estimate": 0.03428779028024092,
          "standard_error": 0.007620244039591541
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.06699886922856,
            "upper_bound": 50.09770557088414
          },
          "point_estimate": 50.08034461376393,
          "standard_error": 0.00796964511697466
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.06398428181126,
            "upper_bound": 50.09065595073638
          },
          "point_estimate": 50.071906661936595,
          "standard_error": 0.006630541939217031
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030907366406620253,
            "upper_bound": 0.03397861896723181
          },
          "point_estimate": 0.015165216719056115,
          "standard_error": 0.008318243326761787
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.064238200588704,
            "upper_bound": 50.07977943577517
          },
          "point_estimate": 50.071729284690875,
          "standard_error": 0.003964870481441845
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008037861766566694,
            "upper_bound": 0.037423931028875104
          },
          "point_estimate": 0.026486582873098324,
          "standard_error": 0.008374117404695494
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.12886830154832,
            "upper_bound": 95.2437921849186
          },
          "point_estimate": 95.18257486960545,
          "standard_error": 0.029559509437081397
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.09730149864048,
            "upper_bound": 95.24743112787978
          },
          "point_estimate": 95.162049158238,
          "standard_error": 0.03658421562518205
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022187007199429815,
            "upper_bound": 0.160051453116194
          },
          "point_estimate": 0.1001009503390692,
          "standard_error": 0.034624751547658504
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.10374350416046,
            "upper_bound": 95.17845907537033
          },
          "point_estimate": 95.13755234921688,
          "standard_error": 0.019462917285048332
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04675869801721116,
            "upper_bound": 0.13047380716006768
          },
          "point_estimate": 0.09837467885522504,
          "standard_error": 0.022762372773127263
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.91428814822872,
            "upper_bound": 39.00808000049399
          },
          "point_estimate": 38.9697930323216,
          "standard_error": 0.024843644928179153
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.968143033508944,
            "upper_bound": 39.00102757740971
          },
          "point_estimate": 38.99204669833091,
          "standard_error": 0.010735546730154914
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0029773402220290134,
            "upper_bound": 0.06998872410370799
          },
          "point_estimate": 0.018341916354251617,
          "standard_error": 0.01604151845344385
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.9634406062204,
            "upper_bound": 38.99777047167984
          },
          "point_estimate": 38.98822174327745,
          "standard_error": 0.009234276995122014
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012818675209269517,
            "upper_bound": 0.12431585789483114
          },
          "point_estimate": 0.08274017215212258,
          "standard_error": 0.03508791164120669
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.71402829393736,
            "upper_bound": 43.881257614667824
          },
          "point_estimate": 43.79534538076771,
          "standard_error": 0.04312571604906664
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.66776341782435,
            "upper_bound": 43.94481569744571
          },
          "point_estimate": 43.74940129188025,
          "standard_error": 0.08760202547111456
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007124980114556485,
            "upper_bound": 0.2230210787316163
          },
          "point_estimate": 0.13411590817559602,
          "standard_error": 0.07191745427850149
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.676265196407634,
            "upper_bound": 43.81272809698975
          },
          "point_estimate": 43.72988780586489,
          "standard_error": 0.03729399346039002
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.091003514993446,
            "upper_bound": 0.16656679144569797
          },
          "point_estimate": 0.1439070753373472,
          "standard_error": 0.01954131095055565
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.42406632403592,
            "upper_bound": 79.58770585988233
          },
          "point_estimate": 79.51460467315252,
          "standard_error": 0.042196021309725135
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.44761401924968,
            "upper_bound": 79.58352713474419
          },
          "point_estimate": 79.54728643462886,
          "standard_error": 0.03026710195617204
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010204648324118527,
            "upper_bound": 0.2134469519749343
          },
          "point_estimate": 0.04951403176923411,
          "standard_error": 0.050167855474292616
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.31359869524506,
            "upper_bound": 79.55064898246525
          },
          "point_estimate": 79.44412057257216,
          "standard_error": 0.06932577590096593
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030156395778286795,
            "upper_bound": 0.1923513466015918
          },
          "point_estimate": 0.14022032884398994,
          "standard_error": 0.04122831624695852
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1077253.5084523808,
            "upper_bound": 1080726.982780112
          },
          "point_estimate": 1078787.093809524,
          "standard_error": 891.0667793586349
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1076879.649684874,
            "upper_bound": 1080233.6470588236
          },
          "point_estimate": 1077057.217647059,
          "standard_error": 1094.8128956442783
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.12210680537473,
            "upper_bound": 4371.060837104216
          },
          "point_estimate": 513.1566619192066,
          "standard_error": 1249.0372151950637
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1076991.4960402695,
            "upper_bound": 1078717.0745613212
          },
          "point_estimate": 1077673.9458365163,
          "standard_error": 471.3016125300529
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1005.0495401571094,
            "upper_bound": 4178.049154634199
          },
          "point_estimate": 2965.4865219842927,
          "standard_error": 916.875421096841
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1153648.321929315,
            "upper_bound": 1157176.032188988
          },
          "point_estimate": 1155307.5372433034,
          "standard_error": 902.1853889878035
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1152775.870833333,
            "upper_bound": 1157136.0416666667
          },
          "point_estimate": 1154978.2366071427,
          "standard_error": 1188.9586071638216
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 707.6924569673699,
            "upper_bound": 5162.651907329247
          },
          "point_estimate": 3116.288081393635,
          "standard_error": 1069.4839979017183
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1153272.7431162586,
            "upper_bound": 1156165.2269084938
          },
          "point_estimate": 1154752.4535714283,
          "standard_error": 742.1576785816281
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1524.5569737968572,
            "upper_bound": 4024.279122719772
          },
          "point_estimate": 3010.968579762493,
          "standard_error": 701.372732227999
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1313594.1425773809,
            "upper_bound": 1315869.6752257654
          },
          "point_estimate": 1314667.9727891155,
          "standard_error": 585.5283858311478
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1313378.1428571427,
            "upper_bound": 1316063.488095238
          },
          "point_estimate": 1314477.0288265306,
          "standard_error": 526.5485951089707
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203.8866188802791,
            "upper_bound": 3481.4208346208625
          },
          "point_estimate": 1056.800791237894,
          "standard_error": 910.8160985940316
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1313829.2618608328,
            "upper_bound": 1315486.6148634437
          },
          "point_estimate": 1314486.0165120594,
          "standard_error": 420.52674272871826
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 841.3534530993637,
            "upper_bound": 2540.0251775626616
          },
          "point_estimate": 1953.7248414945832,
          "standard_error": 442.58948138652926
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1285283.5589600438,
            "upper_bound": 1286065.550287356
          },
          "point_estimate": 1285641.264518336,
          "standard_error": 201.09371721289088
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1285158.4396551724,
            "upper_bound": 1286173.5229885057
          },
          "point_estimate": 1285438.7022988508,
          "standard_error": 227.2097146916381
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.62817800046165,
            "upper_bound": 1006.1499988039144
          },
          "point_estimate": 446.1471367347349,
          "standard_error": 222.45924647544317
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1285208.0910616624,
            "upper_bound": 1285851.2168784824
          },
          "point_estimate": 1285509.4645768024,
          "standard_error": 162.60356120139016
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230.0454944776523,
            "upper_bound": 847.1714525130024
          },
          "point_estimate": 670.6515695990324,
          "standard_error": 159.8397564505958
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1096981.8464705884,
            "upper_bound": 1097736.7665126051
          },
          "point_estimate": 1097406.6002054154,
          "standard_error": 195.02084923349412
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1097213.9444444445,
            "upper_bound": 1097769.962184874
          },
          "point_estimate": 1097550.6034313724,
          "standard_error": 192.22341090560337
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.550111328833975,
            "upper_bound": 781.5092817135255
          },
          "point_estimate": 393.292347429379,
          "standard_error": 208.5029857316547
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1097304.790037627,
            "upper_bound": 1097690.715123396
          },
          "point_estimate": 1097495.2013750954,
          "standard_error": 98.23293142835006
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224.9495465790072,
            "upper_bound": 937.2656402494026
          },
          "point_estimate": 648.9253277095194,
          "standard_error": 216.10317655477564
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 336058.32351851853,
            "upper_bound": 337436.6254166666
          },
          "point_estimate": 336677.59054894175,
          "standard_error": 356.8602559142752
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335798.8364197531,
            "upper_bound": 337256.3109567901
          },
          "point_estimate": 336291.71501322754,
          "standard_error": 388.0469436431296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135.00354019580118,
            "upper_bound": 1709.9895609379275
          },
          "point_estimate": 742.4332802451274,
          "standard_error": 424.0789887228855
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335920.73558325996,
            "upper_bound": 336746.87346876774
          },
          "point_estimate": 336237.0725829726,
          "standard_error": 210.0073822339119
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 386.44506790426914,
            "upper_bound": 1615.0350892272747
          },
          "point_estimate": 1188.5086574775114,
          "standard_error": 325.37109797357823
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 416976.89681818185,
            "upper_bound": 418171.091599026
          },
          "point_estimate": 417575.03640647547,
          "standard_error": 305.2401023882732
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 416816.1590909091,
            "upper_bound": 418457.7297979798
          },
          "point_estimate": 417592.3501420454,
          "standard_error": 402.503792848821
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231.33809461456693,
            "upper_bound": 1736.0590502394148
          },
          "point_estimate": 1216.8963435472808,
          "standard_error": 377.1836023534217
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 416951.0673527117,
            "upper_bound": 418356.5317842037
          },
          "point_estimate": 417837.84704840614,
          "standard_error": 360.11701003410116
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 582.3227403999646,
            "upper_bound": 1284.960206936943
          },
          "point_estimate": 1014.9510383165202,
          "standard_error": 180.35129695834257
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 641432.611971178,
            "upper_bound": 643318.3120260026
          },
          "point_estimate": 642325.3800459483,
          "standard_error": 482.1425456404055
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 641142.6766917293,
            "upper_bound": 643069.9502923976
          },
          "point_estimate": 642346.7479532163,
          "standard_error": 483.99812156063217
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 244.3162190836069,
            "upper_bound": 2780.8301314724313
          },
          "point_estimate": 1107.8097548062465,
          "standard_error": 643.4769105747728
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 642250.5625308207,
            "upper_bound": 642987.1939655172
          },
          "point_estimate": 642635.9545226704,
          "standard_error": 186.26532163450744
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 725.5007208041285,
            "upper_bound": 2190.285379485855
          },
          "point_estimate": 1603.4754398279176,
          "standard_error": 402.3667970015392
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 691188.8431415656,
            "upper_bound": 691532.7959366578
          },
          "point_estimate": 691335.9341442047,
          "standard_error": 89.6467867575548
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 691122.7421383648,
            "upper_bound": 691429.9896226416
          },
          "point_estimate": 691277.1383647799,
          "standard_error": 76.20304693907458
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.73070446558966,
            "upper_bound": 354.26388356904505
          },
          "point_estimate": 192.2770152184821,
          "standard_error": 79.66809963337451
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 691184.4061793181,
            "upper_bound": 691364.6860683701
          },
          "point_estimate": 691268.9790247488,
          "standard_error": 45.60090689676667
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.99211577098568,
            "upper_bound": 434.3785110248923
          },
          "point_estimate": 298.77403980659034,
          "standard_error": 102.60273131402904
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 590153.5105040964,
            "upper_bound": 591709.0336917563
          },
          "point_estimate": 590956.2322862262,
          "standard_error": 398.78795267946225
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 589948.6774193548,
            "upper_bound": 591970.0907258065
          },
          "point_estimate": 591118.3399193548,
          "standard_error": 516.0264043048018
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 436.19585782049495,
            "upper_bound": 2218.1190162657103
          },
          "point_estimate": 1447.3738765620894,
          "standard_error": 474.85593559127153
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 589699.7383955884,
            "upper_bound": 591649.2386944253
          },
          "point_estimate": 590730.3911604525,
          "standard_error": 504.5067232472149
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 719.5151476730006,
            "upper_bound": 1689.2317922262876
          },
          "point_estimate": 1326.131641451869,
          "standard_error": 250.4858115555684
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291721.0473028571,
            "upper_bound": 292030.27580714284
          },
          "point_estimate": 291861.2309676191,
          "standard_error": 79.49999846203391
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291678.8556,
            "upper_bound": 291988.05733333336
          },
          "point_estimate": 291835.6973333333,
          "standard_error": 78.7010197485079
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.90879027845995,
            "upper_bound": 381.4087272286219
          },
          "point_estimate": 223.5909020305077,
          "standard_error": 81.34611390063849
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291683.23647117795,
            "upper_bound": 291918.140979569
          },
          "point_estimate": 291797.16716883116,
          "standard_error": 61.46775025704076
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.691450145167,
            "upper_bound": 372.236669108324
          },
          "point_estimate": 265.7461567613843,
          "standard_error": 75.92258045933936
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527630.7518590263,
            "upper_bound": 528197.755952381
          },
          "point_estimate": 527893.7696762134,
          "standard_error": 145.3161959861971
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527534.1768115942,
            "upper_bound": 528366.695652174
          },
          "point_estimate": 527654.6585144927,
          "standard_error": 206.89892629023265
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.19448307076248,
            "upper_bound": 771.5260461578135
          },
          "point_estimate": 178.86252606363055,
          "standard_error": 185.19379029758775
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527542.276715632,
            "upper_bound": 527764.8710345288
          },
          "point_estimate": 527613.9057782796,
          "standard_error": 58.30495322210127
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.18276047606577,
            "upper_bound": 575.3870340150387
          },
          "point_estimate": 483.99960610942367,
          "standard_error": 98.4575678233764
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190087.3388806424,
            "upper_bound": 190263.64730324072
          },
          "point_estimate": 190180.98348048943,
          "standard_error": 45.05332954084901
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190104.2150173611,
            "upper_bound": 190266.40383184524
          },
          "point_estimate": 190203.8172164352,
          "standard_error": 45.84902909414146
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.358140146362675,
            "upper_bound": 232.5820128847479
          },
          "point_estimate": 87.73070847720071,
          "standard_error": 56.071979970361134
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190136.3627741228,
            "upper_bound": 190238.0324156746
          },
          "point_estimate": 190199.34587391777,
          "standard_error": 26.062549818950863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.95741721449095,
            "upper_bound": 209.07894666536993
          },
          "point_estimate": 150.83838980277142,
          "standard_error": 39.72768604829657
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/stud_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148846.2273602729,
            "upper_bound": 148927.46091950114
          },
          "point_estimate": 148885.04458973114,
          "standard_error": 20.77744665145067
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148832.4728862974,
            "upper_bound": 148946.27482993196
          },
          "point_estimate": 148860.35292517007,
          "standard_error": 29.486370176114
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.052232428468807,
            "upper_bound": 111.69056679259012
          },
          "point_estimate": 68.85920449179011,
          "standard_error": 27.859814745697136
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148841.62738609497,
            "upper_bound": 148906.41923883063
          },
          "point_estimate": 148869.47515504903,
          "standard_error": 16.930962564379534
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.59456608786331,
            "upper_bound": 87.52064487921753
          },
          "point_estimate": 69.23619422174009,
          "standard_error": 13.197937955998135
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/stud_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391509.6866581541,
            "upper_bound": 392769.53000416025
          },
          "point_estimate": 392161.3030798771,
          "standard_error": 322.7595286746419
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391205.3803763441,
            "upper_bound": 392995.32078853046
          },
          "point_estimate": 392312.3167434716,
          "standard_error": 514.5993213135831
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.47611980025003,
            "upper_bound": 1834.6251370525488
          },
          "point_estimate": 1020.4731823130268,
          "standard_error": 500.4127754912081
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390990.3284050179,
            "upper_bound": 392619.8716588393
          },
          "point_estimate": 391804.8011171624,
          "standard_error": 434.41490541471137
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 586.7667978129359,
            "upper_bound": 1318.4076055516646
          },
          "point_estimate": 1077.9385232052102,
          "standard_error": 183.6271662597345
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/stud_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829.2806542684084,
            "upper_bound": 829.9335072822859
          },
          "point_estimate": 829.5369047709597,
          "standard_error": 0.17763123747964185
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829.2936883899233,
            "upper_bound": 829.490562248996
          },
          "point_estimate": 829.4182934389071,
          "standard_error": 0.06881371698814485
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0221796990859508,
            "upper_bound": 0.38144877745575423
          },
          "point_estimate": 0.11308472884764734,
          "standard_error": 0.09257926803829236
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829.368228478176,
            "upper_bound": 829.4786650310164
          },
          "point_estimate": 829.4325645557436,
          "standard_error": 0.02837491879551448
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07148960495206128,
            "upper_bound": 0.9029615428370984
          },
          "point_estimate": 0.591711988737134,
          "standard_error": 0.2708895035601173
        }
      }
    },
    "memmem/stud/prebuilt/sliceslice-haystack/words": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/prebuilt/sliceslice-haystack/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/prebuilt/sliceslice-haystack/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/stud/prebuilt/sliceslice-haystack/words",
        "directory_name": "memmem/stud_prebuilt_sliceslice-haystack_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1612429.7275914424,
            "upper_bound": 1619179.2332278294
          },
          "point_estimate": 1615217.920873016,
          "standard_error": 1785.1856298738678
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1611771.2765700482,
            "upper_bound": 1616933.8608695653
          },
          "point_estimate": 1613180.4260869566,
          "standard_error": 1345.0351833478503
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570.8024223300491,
            "upper_bound": 6037.625714767317
          },
          "point_estimate": 2854.206773240878,
          "standard_error": 1399.2093419614298
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1612450.1015711294,
            "upper_bound": 1614675.271385303
          },
          "point_estimate": 1613324.1856578204,
          "standard_error": 562.3325464610646
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1608.5822185433522,
            "upper_bound": 8855.536053029815
          },
          "point_estimate": 5959.5624068586985,
          "standard_error": 2296.346814075372
        }
      }
    },
    "memmem/stud/prebuilt/sliceslice-i386/words": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/stud/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/stud_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 329461052.725,
            "upper_bound": 330105960.92749995
          },
          "point_estimate": 329790512.2,
          "standard_error": 165245.54822646198
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 329331114.0,
            "upper_bound": 330202484.0
          },
          "point_estimate": 329896407.5,
          "standard_error": 207710.11704910043
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 133812.80192434788,
            "upper_bound": 887915.0395363569
          },
          "point_estimate": 470509.0320467949,
          "standard_error": 216670.4033623234
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 284788.29284345446,
            "upper_bound": 700297.6167460652
          },
          "point_estimate": 550565.8635920361,
          "standard_error": 104188.98312055912
        }
      }
    },
    "memmem/stud/prebuilt/sliceslice-words/words": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/stud/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/stud/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/stud_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248128620.5,
            "upper_bound": 248870207.7
          },
          "point_estimate": 248470049.3,
          "standard_error": 191005.41859195865
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248032421.0,
            "upper_bound": 248950968.0
          },
          "point_estimate": 248230550.0,
          "standard_error": 192138.4979444656
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16329.356110095978,
            "upper_bound": 962477.2161126136
          },
          "point_estimate": 407525.2199649811,
          "standard_error": 243998.40431341884
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 212993.94487543535,
            "upper_bound": 809790.2520974799
          },
          "point_estimate": 636653.843908926,
          "standard_error": 152682.2152029075
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160156.61958178185,
            "upper_bound": 160523.5670739808
          },
          "point_estimate": 160335.0551026152,
          "standard_error": 94.08549098488416
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160016.86028949026,
            "upper_bound": 160590.13754282918
          },
          "point_estimate": 160313.64118942732,
          "standard_error": 155.53008576458754
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.226353018851608,
            "upper_bound": 521.2368309370785
          },
          "point_estimate": 424.97042035540943,
          "standard_error": 134.70176350557398
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160119.48715474442,
            "upper_bound": 160460.71488238717
          },
          "point_estimate": 160311.37164597516,
          "standard_error": 86.07203817388749
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 186.57064555149825,
            "upper_bound": 378.6889486862236
          },
          "point_estimate": 312.7698467860034,
          "standard_error": 49.07779032963506
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217891.2112275449,
            "upper_bound": 218583.58472198463
          },
          "point_estimate": 218208.36758126604,
          "standard_error": 176.81308896212255
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217710.3113772455,
            "upper_bound": 218415.9431137725
          },
          "point_estimate": 218210.28902908467,
          "standard_error": 194.15601345161355
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.45329103819952,
            "upper_bound": 988.8856480874568
          },
          "point_estimate": 325.7346018518263,
          "standard_error": 232.54536594226713
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217747.29889905723,
            "upper_bound": 218306.5652422255
          },
          "point_estimate": 217996.84550898205,
          "standard_error": 143.8428449779136
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 268.77940563114964,
            "upper_bound": 822.0058110142571
          },
          "point_estimate": 589.9474644576079,
          "standard_error": 162.47859098250944
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217832.1632710056,
            "upper_bound": 218315.54093812377
          },
          "point_estimate": 218065.3814162152,
          "standard_error": 123.94014565382766
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217712.60523952096,
            "upper_bound": 218321.83333333337
          },
          "point_estimate": 218106.377340557,
          "standard_error": 151.48325452190537
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.22097915264456,
            "upper_bound": 730.2905229828543
          },
          "point_estimate": 454.5615197642838,
          "standard_error": 173.39599817572895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217705.65823283012,
            "upper_bound": 218161.7606858888
          },
          "point_estimate": 217919.28524768644,
          "standard_error": 119.34372262536296
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.7136447194779,
            "upper_bound": 539.1641174398545
          },
          "point_estimate": 413.2650062371044,
          "standard_error": 85.20258267383241
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128107.97548122068,
            "upper_bound": 128457.56147857982
          },
          "point_estimate": 128247.0543544601,
          "standard_error": 95.10231755540316
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128078.90082159624,
            "upper_bound": 128295.83274647887
          },
          "point_estimate": 128109.93433098593,
          "standard_error": 61.74862807475343
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.35789633049578,
            "upper_bound": 231.76065433613783
          },
          "point_estimate": 77.34760605637615,
          "standard_error": 67.42086673857011
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128087.45667521335,
            "upper_bound": 128220.13311089244
          },
          "point_estimate": 128138.12346808122,
          "standard_error": 33.921130368656144
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.315722543434234,
            "upper_bound": 481.3606900144141
          },
          "point_estimate": 318.7051963381846,
          "standard_error": 137.3306375033076
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92720.30983512512,
            "upper_bound": 92865.20037738905
          },
          "point_estimate": 92793.67134373989,
          "standard_error": 37.01675409783456
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92707.7760770975,
            "upper_bound": 92878.23205174928
          },
          "point_estimate": 92806.58482142858,
          "standard_error": 43.66839294868367
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.387470404544177,
            "upper_bound": 205.06089317065383
          },
          "point_estimate": 125.8926150268696,
          "standard_error": 50.64869004303902
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92730.01924033884,
            "upper_bound": 92882.00545717128
          },
          "point_estimate": 92802.65434667372,
          "standard_error": 38.23134291277641
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.41225898533513,
            "upper_bound": 158.1658939244086
          },
          "point_estimate": 123.40189769465252,
          "standard_error": 23.35266865597741
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81385.47513826608,
            "upper_bound": 81580.3179180013
          },
          "point_estimate": 81472.16439346217,
          "standard_error": 50.203054425132905
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81336.14648729446,
            "upper_bound": 81563.81133888534
          },
          "point_estimate": 81431.70798766817,
          "standard_error": 49.038186303571315
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.576929823409728,
            "upper_bound": 238.1493615418127
          },
          "point_estimate": 118.80855887728106,
          "standard_error": 57.45620722257191
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81371.92810816504,
            "upper_bound": 81572.41825856337
          },
          "point_estimate": 81442.2362063945,
          "standard_error": 52.156740028838975
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.60610197389029,
            "upper_bound": 230.1785043084748
          },
          "point_estimate": 167.6090434439747,
          "standard_error": 47.60092347946793
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57717.73709611993,
            "upper_bound": 57778.75255286596
          },
          "point_estimate": 57742.87773104057,
          "standard_error": 16.33434702635093
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57714.23404761904,
            "upper_bound": 57756.60665784833
          },
          "point_estimate": 57724.34867724868,
          "standard_error": 8.841170700318173
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.916267003017378,
            "upper_bound": 48.66160478423087
          },
          "point_estimate": 8.14067013325354,
          "standard_error": 12.546148105547212
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57719.243130227,
            "upper_bound": 57728.51677191785
          },
          "point_estimate": 57724.030921459496,
          "standard_error": 2.3353398462672463
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.309489666014884,
            "upper_bound": 79.61683976307063
          },
          "point_estimate": 54.403025516942385,
          "standard_error": 21.28815974984493
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124023.57098639457,
            "upper_bound": 124149.35692568297
          },
          "point_estimate": 124090.71865497786,
          "standard_error": 32.23814984205899
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124034.55608465608,
            "upper_bound": 124179.69834183676
          },
          "point_estimate": 124085.99931972788,
          "standard_error": 42.12203701527447
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.425346190227703,
            "upper_bound": 175.42956040590994
          },
          "point_estimate": 122.2506426510424,
          "standard_error": 43.090376468692774
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124048.96519447472,
            "upper_bound": 124125.14500730578
          },
          "point_estimate": 124084.85898931,
          "standard_error": 19.41128809394978
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.04145092198375,
            "upper_bound": 145.0593014213045
          },
          "point_estimate": 107.50730000272485,
          "standard_error": 26.01164431697245
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76611.90131606765,
            "upper_bound": 76741.46614113307
          },
          "point_estimate": 76670.13119953689,
          "standard_error": 33.341467049242716
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76587.233615222,
            "upper_bound": 76730.58103292057
          },
          "point_estimate": 76643.54210711768,
          "standard_error": 35.088268934857844
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.32519727806963,
            "upper_bound": 161.17645744509275
          },
          "point_estimate": 84.53771467250714,
          "standard_error": 36.5529019555424
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76605.67137094661,
            "upper_bound": 76707.55290202665
          },
          "point_estimate": 76657.77186238709,
          "standard_error": 25.644507670528608
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.01109114254198,
            "upper_bound": 151.89886464395192
          },
          "point_estimate": 110.90848715135463,
          "standard_error": 30.631020119077217
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76194.04936327031,
            "upper_bound": 76327.73726890756
          },
          "point_estimate": 76261.21051820728,
          "standard_error": 34.208943643872736
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76158.24369747899,
            "upper_bound": 76386.4100140056
          },
          "point_estimate": 76251.13970588235,
          "standard_error": 62.87516430048635
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.727386556494148,
            "upper_bound": 196.9072898865483
          },
          "point_estimate": 154.73079872356797,
          "standard_error": 46.14724255142587
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76163.10079647726,
            "upper_bound": 76284.35061012067
          },
          "point_estimate": 76208.1968405544,
          "standard_error": 30.999732340409587
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.97899165452102,
            "upper_bound": 133.16548707859118
          },
          "point_estimate": 113.74772042929166,
          "standard_error": 15.154622129355158
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80597.05383106826,
            "upper_bound": 80739.21564750581
          },
          "point_estimate": 80666.71838355107,
          "standard_error": 36.32299854718464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80562.44303097345,
            "upper_bound": 80773.0993362832
          },
          "point_estimate": 80651.50479351034,
          "standard_error": 63.37976831494122
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.701343772367036,
            "upper_bound": 197.14424871238307
          },
          "point_estimate": 144.39363867914605,
          "standard_error": 46.705633647880255
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80553.2271229925,
            "upper_bound": 80701.75007383211
          },
          "point_estimate": 80622.72076198139,
          "standard_error": 38.78366162942107
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.73296231933404,
            "upper_bound": 143.28661254203803
          },
          "point_estimate": 120.97800577526972,
          "standard_error": 17.11012696707639
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72051.55445582561,
            "upper_bound": 72108.16248032408
          },
          "point_estimate": 72080.3403995024,
          "standard_error": 14.471321344594385
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72036.84484126983,
            "upper_bound": 72119.84254535148
          },
          "point_estimate": 72090.82906194885,
          "standard_error": 20.794876116891384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.334980105079074,
            "upper_bound": 84.19748496352729
          },
          "point_estimate": 54.56377635272378,
          "standard_error": 18.464220380903136
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72070.11064814815,
            "upper_bound": 72115.24096221058
          },
          "point_estimate": 72090.30846732632,
          "standard_error": 11.581769268010326
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.567909739090627,
            "upper_bound": 59.29868225269254
          },
          "point_estimate": 48.31538921933916,
          "standard_error": 8.036602155678809
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258556.56556752365,
            "upper_bound": 259272.83593493188
          },
          "point_estimate": 258901.2498533716,
          "standard_error": 183.37891481421772
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258336.8713272543,
            "upper_bound": 259318.5859929078
          },
          "point_estimate": 258991.31776989755,
          "standard_error": 260.4139213025908
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.85807541484589,
            "upper_bound": 1090.980654035488
          },
          "point_estimate": 791.3962250456565,
          "standard_error": 278.07798022552504
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258410.22805029817,
            "upper_bound": 259232.56052219472
          },
          "point_estimate": 258845.19528414847,
          "standard_error": 217.0910550769401
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 342.49020745410394,
            "upper_bound": 782.3860567916385
          },
          "point_estimate": 610.3401843106648,
          "standard_error": 118.03439886448872
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145780.77288391662,
            "upper_bound": 146249.18122969253
          },
          "point_estimate": 146008.09328281382,
          "standard_error": 119.44673658726173
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145660.23172690763,
            "upper_bound": 146310.7831325301
          },
          "point_estimate": 145934.15484158858,
          "standard_error": 200.88006239225052
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.4697480903045,
            "upper_bound": 693.8445071095344
          },
          "point_estimate": 425.7783001517535,
          "standard_error": 161.05067217669068
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145658.85701347532,
            "upper_bound": 146064.46312951337
          },
          "point_estimate": 145819.8735617796,
          "standard_error": 105.09923073716614
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.71932246226623,
            "upper_bound": 496.2504423625947
          },
          "point_estimate": 398.29559170342014,
          "standard_error": 68.00431804169796
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71811.0095111362,
            "upper_bound": 71948.79781472332
          },
          "point_estimate": 71877.45233162369,
          "standard_error": 35.36068155951763
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71766.80731225296,
            "upper_bound": 72010.11363636363
          },
          "point_estimate": 71837.52354601919,
          "standard_error": 58.944377519587334
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.248936180007252,
            "upper_bound": 191.2957892915612
          },
          "point_estimate": 111.76635572326155,
          "standard_error": 46.303937269511245
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71786.49076591499,
            "upper_bound": 71893.97103039389
          },
          "point_estimate": 71830.36657255788,
          "standard_error": 27.09342868402491
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.19186156526507,
            "upper_bound": 138.05061847861808
          },
          "point_estimate": 118.2139752313462,
          "standard_error": 17.749894674070987
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232798.95446886448,
            "upper_bound": 233813.97801806065
          },
          "point_estimate": 233295.0837138787,
          "standard_error": 260.13310716196736
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232545.86538461535,
            "upper_bound": 234091.45019332517
          },
          "point_estimate": 233296.28279914532,
          "standard_error": 355.0754356123294
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.288934434636886,
            "upper_bound": 1576.4203120129712
          },
          "point_estimate": 805.0956617964769,
          "standard_error": 394.2345616773569
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233131.95960400547,
            "upper_bound": 234157.2010912771
          },
          "point_estimate": 233627.1129037629,
          "standard_error": 269.64723543424446
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 485.43443514042264,
            "upper_bound": 1079.2925164094224
          },
          "point_estimate": 870.5843632466233,
          "standard_error": 152.4586800241375
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59249.9629728453,
            "upper_bound": 59356.43390934514
          },
          "point_estimate": 59302.48523207488,
          "standard_error": 27.37210367003891
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59208.0528548124,
            "upper_bound": 59385.23926046764
          },
          "point_estimate": 59306.31752990756,
          "standard_error": 48.83703294077616
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.750452959034432,
            "upper_bound": 158.8498244229264
          },
          "point_estimate": 128.64793964100366,
          "standard_error": 40.34432642215884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59222.975404693185,
            "upper_bound": 59347.12154547248
          },
          "point_estimate": 59279.896620834304,
          "standard_error": 32.8415860777712
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.13818676036292,
            "upper_bound": 109.2403422495906
          },
          "point_estimate": 91.29467190582504,
          "standard_error": 13.309184315956442
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65175.568491921,
            "upper_bound": 65302.04162285201
          },
          "point_estimate": 65235.2769908951,
          "standard_error": 32.541064772832264
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65157.24236983842,
            "upper_bound": 65335.9453545781
          },
          "point_estimate": 65200.60028212362,
          "standard_error": 41.74443172082557
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.432305794643714,
            "upper_bound": 188.206794863322
          },
          "point_estimate": 91.69605788733097,
          "standard_error": 42.53415657737115
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65168.13156735883,
            "upper_bound": 65313.84771529507
          },
          "point_estimate": 65238.9526172212,
          "standard_error": 36.92635898968331
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.16503183654821,
            "upper_bound": 134.96649771359452
          },
          "point_estimate": 108.51620683993944,
          "standard_error": 21.141105779538016
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68490.00831136221,
            "upper_bound": 68596.78365976145
          },
          "point_estimate": 68536.24031835709,
          "standard_error": 27.552475583357687
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68478.96583266075,
            "upper_bound": 68579.22033898305
          },
          "point_estimate": 68492.68848085374,
          "standard_error": 28.554142865289418
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.150881962357154,
            "upper_bound": 114.00844786294316
          },
          "point_estimate": 38.02589723452123,
          "standard_error": 32.12987726689907
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68483.76411983818,
            "upper_bound": 68520.65523734114
          },
          "point_estimate": 68496.51818426394,
          "standard_error": 9.5277658739876
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.942274797806448,
            "upper_bound": 130.36044534956295
          },
          "point_estimate": 91.8182439830654,
          "standard_error": 29.3776474160261
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92680.85662536442,
            "upper_bound": 92896.7008605594
          },
          "point_estimate": 92781.87271238258,
          "standard_error": 55.48683607111102
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92652.8408649174,
            "upper_bound": 92866.3975340136
          },
          "point_estimate": 92752.169678288,
          "standard_error": 50.90160962704891
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.14720293387642,
            "upper_bound": 283.7178196058613
          },
          "point_estimate": 133.65279459148104,
          "standard_error": 69.98722842006781
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92659.78170653508,
            "upper_bound": 92763.3512077752
          },
          "point_estimate": 92712.6548303737,
          "standard_error": 26.06339512324324
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.48713830352787,
            "upper_bound": 253.1187582829646
          },
          "point_estimate": 184.8555969719448,
          "standard_error": 47.5168657383654
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66305.76641612899,
            "upper_bound": 66443.02936206877
          },
          "point_estimate": 66367.42585831594,
          "standard_error": 35.23804306024515
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66299.73576642336,
            "upper_bound": 66431.17563868614
          },
          "point_estimate": 66314.78896745743,
          "standard_error": 34.5981457198737
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.342309852993529,
            "upper_bound": 172.4843642735681
          },
          "point_estimate": 26.912898276769905,
          "standard_error": 41.38611094820688
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66294.71617506865,
            "upper_bound": 66371.93568021951
          },
          "point_estimate": 66331.46937150441,
          "standard_error": 21.023770033463727
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.59623905770309,
            "upper_bound": 150.81021955064966
          },
          "point_estimate": 117.45529433086888,
          "standard_error": 32.09744876371189
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93408.67095408526,
            "upper_bound": 93470.8770528083
          },
          "point_estimate": 93436.89550010176,
          "standard_error": 16.00512203724896
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93392.84493284492,
            "upper_bound": 93459.80384615384
          },
          "point_estimate": 93436.52086894588,
          "standard_error": 18.735521743536097
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.470577335317945,
            "upper_bound": 82.90585006658482
          },
          "point_estimate": 53.211400080952195,
          "standard_error": 18.39349538831514
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93402.27426776124,
            "upper_bound": 93441.4460690285
          },
          "point_estimate": 93420.71043623044,
          "standard_error": 10.007606024035653
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.36942630922772,
            "upper_bound": 73.71208444044757
          },
          "point_estimate": 53.2567238916148,
          "standard_error": 14.479761056074532
        }
      }
    },
    "memmem/twoway/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/twoway_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19204.974034601764,
            "upper_bound": 19255.212634045347
          },
          "point_estimate": 19228.271794740696,
          "standard_error": 12.907795760327849
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19195.75462808665,
            "upper_bound": 19273.62380699894
          },
          "point_estimate": 19210.294518822902,
          "standard_error": 19.837147448407624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.021223741145888,
            "upper_bound": 67.32851936538948
          },
          "point_estimate": 24.422471146477207,
          "standard_error": 17.679448068973795
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19195.105497368255,
            "upper_bound": 19216.827212302145
          },
          "point_estimate": 19204.942892950105,
          "standard_error": 5.629152336839289
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.787573460463909,
            "upper_bound": 53.548507800860506
          },
          "point_estimate": 42.97667031547955,
          "standard_error": 8.727612341199327
        }
      }
    },
    "memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/twoway_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19194.760655612692,
            "upper_bound": 19237.48154197705
          },
          "point_estimate": 19215.767944301595,
          "standard_error": 10.929359185253247
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19193.310896417857,
            "upper_bound": 19241.5821201694
          },
          "point_estimate": 19206.409378860066,
          "standard_error": 13.118330940494666
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.949403755435962,
            "upper_bound": 63.12150046751208
          },
          "point_estimate": 32.85004767248542,
          "standard_error": 15.310552066569429
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19181.91495040746,
            "upper_bound": 19209.977151597952
          },
          "point_estimate": 19194.52691934852,
          "standard_error": 7.038633005491276
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.71889162717455,
            "upper_bound": 47.58368091974278
          },
          "point_estimate": 36.40169332370766,
          "standard_error": 7.443835884439785
        }
      }
    },
    "memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/twoway_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246967.8896713589,
            "upper_bound": 247171.6876539039
          },
          "point_estimate": 247067.10890551264,
          "standard_error": 52.12381727342163
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246924.58445945947,
            "upper_bound": 247263.1167953668
          },
          "point_estimate": 247029.15465465465,
          "standard_error": 79.19079838136875
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.48994264998491,
            "upper_bound": 299.34394698291163
          },
          "point_estimate": 162.11568439662375,
          "standard_error": 69.30720982745959
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246919.2997305743,
            "upper_bound": 247137.5819274857
          },
          "point_estimate": 247005.31998947,
          "standard_error": 55.214738130737835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.140603557316,
            "upper_bound": 209.5274381084569
          },
          "point_estimate": 173.41180121147383,
          "standard_error": 28.02814858764087
        }
      }
    },
    "memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/twoway_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 525.7759989201448,
            "upper_bound": 527.8347439087154
          },
          "point_estimate": 526.7907116518229,
          "standard_error": 0.5296339423778272
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 525.160647068716,
            "upper_bound": 528.5817395032482
          },
          "point_estimate": 526.4469026084232,
          "standard_error": 1.1585512680113028
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10709354342596011,
            "upper_bound": 2.4974975374957147
          },
          "point_estimate": 2.220505524268353,
          "standard_error": 0.7707437822452736
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 525.2364823713193,
            "upper_bound": 527.4167810621805
          },
          "point_estimate": 526.1979527150949,
          "standard_error": 0.6103898645651904
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.197823798813781,
            "upper_bound": 1.9702294363499648
          },
          "point_estimate": 1.7636081601412747,
          "standard_error": 0.19965016003779376
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.551597477962616,
            "upper_bound": 44.72554711535058
          },
          "point_estimate": 44.63938512422881,
          "standard_error": 0.04461533573525942
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.49899413359448,
            "upper_bound": 44.75286296579869
          },
          "point_estimate": 44.67773139862085,
          "standard_error": 0.08312819550953204
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009571487768072874,
            "upper_bound": 0.23573647731809175
          },
          "point_estimate": 0.14740359914995554,
          "standard_error": 0.067127757412872
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.47985342049457,
            "upper_bound": 44.62964258194335
          },
          "point_estimate": 44.530631568174314,
          "standard_error": 0.03865249498932667
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09499568541862265,
            "upper_bound": 0.17680122223904776
          },
          "point_estimate": 0.14912364233149283,
          "standard_error": 0.020893621559559636
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.68461783795023,
            "upper_bound": 47.71482329883109
          },
          "point_estimate": 47.70217264432559,
          "standard_error": 0.007906914574860185
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.699203515589346,
            "upper_bound": 47.714129967548985
          },
          "point_estimate": 47.706554294295856,
          "standard_error": 0.004566446473053044
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011653889848351005,
            "upper_bound": 0.02576143200186304
          },
          "point_estimate": 0.010648998473263695,
          "standard_error": 0.005767390094327378
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.70247866069388,
            "upper_bound": 47.71294540914404
          },
          "point_estimate": 47.7080050243027,
          "standard_error": 0.0026997988096584165
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005607613675374639,
            "upper_bound": 0.0391598391262304
          },
          "point_estimate": 0.026387137735931543,
          "standard_error": 0.010324829403722905
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.74898487801808,
            "upper_bound": 26.765969763445256
          },
          "point_estimate": 26.756582435879448,
          "standard_error": 0.004365973616117095
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.74803362796269,
            "upper_bound": 26.766326328682773
          },
          "point_estimate": 26.75171462516598,
          "standard_error": 0.0037081169667734495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006355030839816907,
            "upper_bound": 0.020196128042466777
          },
          "point_estimate": 0.00495261586766882,
          "standard_error": 0.004804674473802882
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.74732961470536,
            "upper_bound": 26.75407635540596
          },
          "point_estimate": 26.75068760464855,
          "standard_error": 0.0016890349455855622
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0036655349344444903,
            "upper_bound": 0.018744612477482776
          },
          "point_estimate": 0.014517772582330828,
          "standard_error": 0.003957318396880374
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.752307732227223,
            "upper_bound": 26.767343026435356
          },
          "point_estimate": 26.75852722769892,
          "standard_error": 0.0039727414631662815
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.75089400407125,
            "upper_bound": 26.760917740342315
          },
          "point_estimate": 26.75512440509085,
          "standard_error": 0.0025655864211738084
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009488575754627112,
            "upper_bound": 0.012334879161424773
          },
          "point_estimate": 0.00651033531714778,
          "standard_error": 0.00302715159446444
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.751429690587145,
            "upper_bound": 26.75707604454094
          },
          "point_estimate": 26.75390528155309,
          "standard_error": 0.001428274537688735
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035640644349952833,
            "upper_bound": 0.01966877052062745
          },
          "point_estimate": 0.013223948890365876,
          "standard_error": 0.005105259868654225
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.959930862725194,
            "upper_bound": 38.08312622557857
          },
          "point_estimate": 38.017890166346334,
          "standard_error": 0.031555154037846755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.928205350620416,
            "upper_bound": 38.120065099435664
          },
          "point_estimate": 37.974896545405585,
          "standard_error": 0.05026532942982183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005638195625302557,
            "upper_bound": 0.1829883789388355
          },
          "point_estimate": 0.07486255978481521,
          "standard_error": 0.049733712631779024
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.95853020048172,
            "upper_bound": 38.139728685284695
          },
          "point_estimate": 38.054272975391925,
          "standard_error": 0.04618783209547018
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04949002707950489,
            "upper_bound": 0.1272426351504167
          },
          "point_estimate": 0.10534997752800208,
          "standard_error": 0.01962448202167251
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.15712383260787,
            "upper_bound": 53.36425728670351
          },
          "point_estimate": 53.26202336202738,
          "standard_error": 0.05276273806882509
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.075185515076726,
            "upper_bound": 53.4181305128096
          },
          "point_estimate": 53.32538379646681,
          "standard_error": 0.12338209138226436
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00311876808343417,
            "upper_bound": 0.26575284884646017
          },
          "point_estimate": 0.15216408672290713,
          "standard_error": 0.0887001883758087
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.18299230202811,
            "upper_bound": 53.41118542188768
          },
          "point_estimate": 53.32166984979639,
          "standard_error": 0.06246459139850165
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11728158024935548,
            "upper_bound": 0.189505789463205
          },
          "point_estimate": 0.17494742433847388,
          "standard_error": 0.018963876291002657
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.8789585009739,
            "upper_bound": 69.11475653221419
          },
          "point_estimate": 69.00448195090107,
          "standard_error": 0.060490707857624906
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.87829883320255,
            "upper_bound": 69.11216028683386
          },
          "point_estimate": 69.10426495761617,
          "standard_error": 0.07075350552462185
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009380449310906388,
            "upper_bound": 0.3213248716125709
          },
          "point_estimate": 0.014684240214968287,
          "standard_error": 0.08413443737257108
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.72901553984579,
            "upper_bound": 69.10525386948436
          },
          "point_estimate": 68.85813057950081,
          "standard_error": 0.08928041971671553
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04871561967275032,
            "upper_bound": 0.2479140684758916
          },
          "point_estimate": 0.2013800688577025,
          "standard_error": 0.048290287318267226
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.98894317409818,
            "upper_bound": 46.053102001140175
          },
          "point_estimate": 46.022229831667445,
          "standard_error": 0.016451396220218437
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.97867369176373,
            "upper_bound": 46.05206216810645
          },
          "point_estimate": 46.04271990299078,
          "standard_error": 0.02043449703314625
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0040485931866681615,
            "upper_bound": 0.0928194202332078
          },
          "point_estimate": 0.024233892771507465,
          "standard_error": 0.02617368477854396
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.99981090333155,
            "upper_bound": 46.04886518178527
          },
          "point_estimate": 46.03370678062352,
          "standard_error": 0.013094299847718194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02414873587079766,
            "upper_bound": 0.06928367711855586
          },
          "point_estimate": 0.05486655165015278,
          "standard_error": 0.010746479723604657
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.45539718283429,
            "upper_bound": 64.55221463659004
          },
          "point_estimate": 64.50684574661082,
          "standard_error": 0.024800286738062095
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.44753286427265,
            "upper_bound": 64.57467208674493
          },
          "point_estimate": 64.5358185472651,
          "standard_error": 0.034808489961630815
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01523678707653155,
            "upper_bound": 0.13820130320510737
          },
          "point_estimate": 0.08591991508652615,
          "standard_error": 0.03161059584114102
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.39717326304654,
            "upper_bound": 64.54501080395394
          },
          "point_estimate": 64.45921064389317,
          "standard_error": 0.03825234365258805
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04137181921781108,
            "upper_bound": 0.10571127489600284
          },
          "point_estimate": 0.08264957483643767,
          "standard_error": 0.016909244225308213
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.53636422691307,
            "upper_bound": 53.655730467868935
          },
          "point_estimate": 53.60466375346283,
          "standard_error": 0.03113641533159107
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.56883952107297,
            "upper_bound": 53.67031361190152
          },
          "point_estimate": 53.64862501588867,
          "standard_error": 0.027219208943967763
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008655200895265792,
            "upper_bound": 0.11871900238225494
          },
          "point_estimate": 0.052377804178333466,
          "standard_error": 0.030416786764948047
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.630336273780834,
            "upper_bound": 53.673497476713905
          },
          "point_estimate": 53.65989526611041,
          "standard_error": 0.011320276351164811
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027920791023927066,
            "upper_bound": 0.1499014823737254
          },
          "point_estimate": 0.10404519908328327,
          "standard_error": 0.03551364403207848
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.6941571957641,
            "upper_bound": 39.83542321342955
          },
          "point_estimate": 39.761652218472015,
          "standard_error": 0.03549203804940153
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.67975229084263,
            "upper_bound": 39.92101834829827
          },
          "point_estimate": 39.69636669088864,
          "standard_error": 0.05827250076013754
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005270308824095981,
            "upper_bound": 0.18228205216941384
          },
          "point_estimate": 0.03215342684697352,
          "standard_error": 0.04822752970246688
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.69137045954047,
            "upper_bound": 39.86202508166742
          },
          "point_estimate": 39.75812895063495,
          "standard_error": 0.04227672373448677
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017544731705953415,
            "upper_bound": 0.1324831406025179
          },
          "point_estimate": 0.11805771848846631,
          "standard_error": 0.022846477821365913
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.49165625143619,
            "upper_bound": 62.69597123056907
          },
          "point_estimate": 62.60349638586879,
          "standard_error": 0.05271507966454675
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.483377481731694,
            "upper_bound": 62.70200980054231
          },
          "point_estimate": 62.66144737522404,
          "standard_error": 0.04478706999176022
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010515313399071935,
            "upper_bound": 0.2618393149433163
          },
          "point_estimate": 0.07241979247779214,
          "standard_error": 0.06104889935194331
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.46737827476433,
            "upper_bound": 62.667204681843785
          },
          "point_estimate": 62.59908803970055,
          "standard_error": 0.05151848398765834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.047209334532815034,
            "upper_bound": 0.2275116525029503
          },
          "point_estimate": 0.1760160347590043,
          "standard_error": 0.04628005637884748
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231917.94356872412,
            "upper_bound": 232223.00187746435
          },
          "point_estimate": 232044.7240501466,
          "standard_error": 80.87913084078635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231909.14467697908,
            "upper_bound": 232134.3315286624
          },
          "point_estimate": 231939.90286624205,
          "standard_error": 49.36055205496445
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.44135935102909,
            "upper_bound": 278.14504098550844
          },
          "point_estimate": 49.070634054512176,
          "standard_error": 64.6422640368046
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231916.4745627512,
            "upper_bound": 232040.0852894156
          },
          "point_estimate": 231964.95318057737,
          "standard_error": 31.369984788960576
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.075713434593744,
            "upper_bound": 388.5256099574205
          },
          "point_estimate": 269.68400797678754,
          "standard_error": 100.89153483301564
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169659.14712996953,
            "upper_bound": 170063.9492765919
          },
          "point_estimate": 169850.16084994463,
          "standard_error": 103.73742951146993
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169550.5111627907,
            "upper_bound": 170162.05174418603
          },
          "point_estimate": 169766.94122093022,
          "standard_error": 144.67066529799476
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.48684883810908,
            "upper_bound": 604.3037267133268
          },
          "point_estimate": 352.7030345405884,
          "standard_error": 139.06356034735998
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169575.62719241183,
            "upper_bound": 169848.71151846892
          },
          "point_estimate": 169675.56021745695,
          "standard_error": 69.66457365027492
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159.8851132487565,
            "upper_bound": 424.2887670486128
          },
          "point_estimate": 346.55481524206004,
          "standard_error": 66.89243328263811
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 303080.2522755952,
            "upper_bound": 303280.59114285716
          },
          "point_estimate": 303173.3543095238,
          "standard_error": 51.621543766181425
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 303037.1388888889,
            "upper_bound": 303315.61250000005
          },
          "point_estimate": 303106.5858333333,
          "standard_error": 71.34873098663711
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.083670166606607,
            "upper_bound": 287.2913982328909
          },
          "point_estimate": 118.94364205501024,
          "standard_error": 69.17836187646645
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 303053.79579831933,
            "upper_bound": 303146.16070319264
          },
          "point_estimate": 303096.49225108227,
          "standard_error": 23.479291824086346
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.33188870333281,
            "upper_bound": 209.9979888093244
          },
          "point_estimate": 171.83516354644507,
          "standard_error": 35.359014101321925
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 359673.33240431984,
            "upper_bound": 359999.0917079208
          },
          "point_estimate": 359825.42365354387,
          "standard_error": 83.79487336406517
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 359629.6002121641,
            "upper_bound": 360042.21782178216
          },
          "point_estimate": 359684.7712871287,
          "standard_error": 123.25433398454462
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.912909966306415,
            "upper_bound": 460.7917572648408
          },
          "point_estimate": 176.1106745713676,
          "standard_error": 123.92913569935396
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 359601.9646243694,
            "upper_bound": 359799.433159445
          },
          "point_estimate": 359668.23757232865,
          "standard_error": 50.85885707374979
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.38793396861243,
            "upper_bound": 355.34399400856785
          },
          "point_estimate": 278.77934150633286,
          "standard_error": 58.52965808970277
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500638.49616438366,
            "upper_bound": 501482.3147465754
          },
          "point_estimate": 501061.6552054795,
          "standard_error": 216.6995469188516
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500335.2109589041,
            "upper_bound": 501694.8287671233
          },
          "point_estimate": 501108.51027397264,
          "standard_error": 392.75226907853846
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.66037001678905,
            "upper_bound": 1190.46278818013
          },
          "point_estimate": 970.9669085153288,
          "standard_error": 265.6596820140616
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500302.65673567174,
            "upper_bound": 501180.07217760984
          },
          "point_estimate": 500672.73648816935,
          "standard_error": 225.89743488308372
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 468.0481853661128,
            "upper_bound": 848.1452379260135
          },
          "point_estimate": 721.9598394757892,
          "standard_error": 97.29103447201666
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103927.89361428571,
            "upper_bound": 104003.21635941042
          },
          "point_estimate": 103961.41570226758,
          "standard_error": 19.441961701691547
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103912.14285714286,
            "upper_bound": 103995.54414285714
          },
          "point_estimate": 103942.62526984129,
          "standard_error": 20.142678406638044
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.449907796723007,
            "upper_bound": 89.76041480642691
          },
          "point_estimate": 46.182636180100246,
          "standard_error": 20.446368331573172
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103927.3887929678,
            "upper_bound": 103965.51769516728
          },
          "point_estimate": 103948.63513172542,
          "standard_error": 9.732525574339943
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.061900278577745,
            "upper_bound": 90.54001142521818
          },
          "point_estimate": 64.96989171877492,
          "standard_error": 19.022229237310224
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160170.4227239354,
            "upper_bound": 160274.3210792952
          },
          "point_estimate": 160217.4443538913,
          "standard_error": 26.88902400989543
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160152.23810572686,
            "upper_bound": 160263.6387665198
          },
          "point_estimate": 160192.70723201177,
          "standard_error": 34.85956077284022
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.350441886743091,
            "upper_bound": 139.24574598602268
          },
          "point_estimate": 67.81751906033084,
          "standard_error": 32.72175926793019
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160161.36956832,
            "upper_bound": 160212.25505094847
          },
          "point_estimate": 160181.4319011385,
          "standard_error": 12.984096382274922
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.755878252182725,
            "upper_bound": 124.24120038918689
          },
          "point_estimate": 89.80578329021962,
          "standard_error": 24.810733589232377
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297077.71684483485,
            "upper_bound": 297274.87860498123
          },
          "point_estimate": 297165.3252242225,
          "standard_error": 50.44020067669755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297093.10917537747,
            "upper_bound": 297199.23414634145
          },
          "point_estimate": 297136.3355916892,
          "standard_error": 29.135605640004265
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.18629181802181,
            "upper_bound": 210.2045510648517
          },
          "point_estimate": 67.95401745882668,
          "standard_error": 42.703484833396935
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297096.11983836355,
            "upper_bound": 297172.96598459454
          },
          "point_estimate": 297137.88197656005,
          "standard_error": 20.102908754675063
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.338415813794555,
            "upper_bound": 246.6016703829703
          },
          "point_estimate": 168.69087381201695,
          "standard_error": 59.68364173657756
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271761.946682362,
            "upper_bound": 272430.19632048096
          },
          "point_estimate": 272081.4282145818,
          "standard_error": 169.1379310920091
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271867.30597014923,
            "upper_bound": 272282.85463308456
          },
          "point_estimate": 272029.2189647003,
          "standard_error": 119.70552565805446
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.444309605183015,
            "upper_bound": 845.6255127856845
          },
          "point_estimate": 247.2066310278874,
          "standard_error": 181.88930459897884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271804.9041696021,
            "upper_bound": 272186.57423943037
          },
          "point_estimate": 272002.1789687924,
          "standard_error": 95.53749720610462
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151.71215565351957,
            "upper_bound": 803.2914314947527
          },
          "point_estimate": 562.6979054953019,
          "standard_error": 171.56103663997877
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142090.70967447918,
            "upper_bound": 142162.41856460812
          },
          "point_estimate": 142126.3279075211,
          "standard_error": 18.367367308870673
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142073.3372124566,
            "upper_bound": 142176.244140625
          },
          "point_estimate": 142129.15108816966,
          "standard_error": 24.42888532641685
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.970353764752044,
            "upper_bound": 115.72677333606408
          },
          "point_estimate": 69.2869352933485,
          "standard_error": 26.471693508035216
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142083.7410705689,
            "upper_bound": 142154.65052682286
          },
          "point_estimate": 142116.603125,
          "standard_error": 18.238240566917533
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.167874380472355,
            "upper_bound": 75.34554434008348
          },
          "point_estimate": 61.36045944198352,
          "standard_error": 10.03689706486281
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152744.4716503518,
            "upper_bound": 152845.87820711615
          },
          "point_estimate": 152785.40608826865,
          "standard_error": 27.140287235298032
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152736.55714285714,
            "upper_bound": 152785.73374766574
          },
          "point_estimate": 152770.0780312125,
          "standard_error": 13.51116873858062
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.550526911251162,
            "upper_bound": 72.90923299483157
          },
          "point_estimate": 24.150326475985867,
          "standard_error": 17.9981611319269
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152750.7802371522,
            "upper_bound": 152782.66232898217
          },
          "point_estimate": 152771.83796791444,
          "standard_error": 8.16092352990887
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.982555133618433,
            "upper_bound": 137.0440952007553
          },
          "point_estimate": 90.49352647712476,
          "standard_error": 38.51064322711173
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151577.57334716435,
            "upper_bound": 151837.61099644098
          },
          "point_estimate": 151702.79167972883,
          "standard_error": 66.84188738049376
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151516.01666666666,
            "upper_bound": 151948.50595238095
          },
          "point_estimate": 151650.88729166664,
          "standard_error": 103.45207881330894
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.46410809675426,
            "upper_bound": 350.2133137269621
          },
          "point_estimate": 235.01765523381343,
          "standard_error": 87.48951464795473
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151578.60343112642,
            "upper_bound": 151824.86916666667
          },
          "point_estimate": 151689.50595238095,
          "standard_error": 61.647667452366655
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.02546064986409,
            "upper_bound": 262.3056654163608
          },
          "point_estimate": 222.38687159519975,
          "standard_error": 35.31436180209084
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85583.26686993464,
            "upper_bound": 85663.44197196078
          },
          "point_estimate": 85618.48333660132,
          "standard_error": 20.71450724981847
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85571.82980392157,
            "upper_bound": 85650.65892810457
          },
          "point_estimate": 85605.31568627451,
          "standard_error": 18.39929654766042
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.199815558462315,
            "upper_bound": 90.9081077664748
          },
          "point_estimate": 39.306399796286776,
          "standard_error": 22.01530650758716
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85568.52296767356,
            "upper_bound": 85620.16612062546
          },
          "point_estimate": 85591.07781818182,
          "standard_error": 13.139176568693886
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.264030294281103,
            "upper_bound": 97.29169167495633
          },
          "point_estimate": 69.00002281755205,
          "standard_error": 21.391327331178513
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/twoway_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28776.729075355943,
            "upper_bound": 28807.256534579148
          },
          "point_estimate": 28791.63193991648,
          "standard_error": 7.80283801524481
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28767.576730058103,
            "upper_bound": 28809.783379556266
          },
          "point_estimate": 28790.66550008804,
          "standard_error": 12.268089333165994
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.2025099272963424,
            "upper_bound": 46.479244844402515
          },
          "point_estimate": 28.48474989049159,
          "standard_error": 10.136869229118906
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28778.59382264753,
            "upper_bound": 28822.60927205093
          },
          "point_estimate": 28802.106493506493,
          "standard_error": 11.715094595155234
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.269581902970993,
            "upper_bound": 33.05804271111185
          },
          "point_estimate": 26.032929491675485,
          "standard_error": 4.722060079504316
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/twoway_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2042748.590482253,
            "upper_bound": 2043845.915442295
          },
          "point_estimate": 2043218.4159942684,
          "standard_error": 285.5601282025654
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2042621.2152777775,
            "upper_bound": 2043515.3194444445
          },
          "point_estimate": 2042945.1759259256,
          "standard_error": 248.9907607293878
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.84596859919748,
            "upper_bound": 1120.1431754837984
          },
          "point_estimate": 681.4733517903449,
          "standard_error": 243.7754620857956
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2042624.0123331717,
            "upper_bound": 2043223.7998661313
          },
          "point_estimate": 2042893.5857142855,
          "standard_error": 151.66399083385406
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322.1463508189679,
            "upper_bound": 1390.202530126209
          },
          "point_estimate": 953.472946059914,
          "standard_error": 328.85479000561605
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/twoway_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4090.3800731975034,
            "upper_bound": 4091.6764406293746
          },
          "point_estimate": 4090.997864226631,
          "standard_error": 0.33101585291510105
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4090.127437130951,
            "upper_bound": 4091.663325449563
          },
          "point_estimate": 4090.84803656568,
          "standard_error": 0.3552831829951465
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13390463778524636,
            "upper_bound": 1.8500150946098348
          },
          "point_estimate": 0.8799207002993565,
          "standard_error": 0.4481875631511115
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4090.337435128414,
            "upper_bound": 4091.369970728648
          },
          "point_estimate": 4090.873429352492,
          "standard_error": 0.260251140006331
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5183985994956879,
            "upper_bound": 1.4818760770668786
          },
          "point_estimate": 1.1057740042227149,
          "standard_error": 0.25545787599829195
        }
      }
    },
    "memrchr1/krate/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr1/krate/empty/never",
        "directory_name": "memrchr1/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4924917249018147,
            "upper_bound": 0.49842875302650375
          },
          "point_estimate": 0.49523875697850606,
          "standard_error": 0.0015280145456654894
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4911727046798296,
            "upper_bound": 0.4987549991134812
          },
          "point_estimate": 0.49353088351649,
          "standard_error": 0.0018611510048434775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00016802102328446889,
            "upper_bound": 0.007954089204834291
          },
          "point_estimate": 0.003553641993609803,
          "standard_error": 0.0019601588347263068
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4921599362262667,
            "upper_bound": 0.4991705110350454
          },
          "point_estimate": 0.4949884159502906,
          "standard_error": 0.0018215921154887736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014725553970930964,
            "upper_bound": 0.006389279503489075
          },
          "point_estimate": 0.005121637907317859,
          "standard_error": 0.001187913031414751
        }
      }
    },
    "memrchr1/krate/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/common",
        "directory_name": "memrchr1/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 222682.3406575616,
            "upper_bound": 223449.01609406952
          },
          "point_estimate": 223062.45956008375,
          "standard_error": 196.34997060332745
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 222505.775732788,
            "upper_bound": 223644.418404908
          },
          "point_estimate": 222981.90887014315,
          "standard_error": 266.620730062446
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142.57082315903892,
            "upper_bound": 1191.059688133604
          },
          "point_estimate": 725.3229905585305,
          "standard_error": 286.481115943743
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 222443.358257258,
            "upper_bound": 223110.10983333748
          },
          "point_estimate": 222735.3644490479,
          "standard_error": 169.3237764944872
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.32909602885826,
            "upper_bound": 799.8068490454147
          },
          "point_estimate": 652.727215621264,
          "standard_error": 107.29974293864704
        }
      }
    },
    "memrchr1/krate/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/never",
        "directory_name": "memrchr1/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9809.480451507035,
            "upper_bound": 9821.81512542383
          },
          "point_estimate": 9814.844431391964,
          "standard_error": 3.195211742023796
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9806.819335673776,
            "upper_bound": 9819.63769916284
          },
          "point_estimate": 9811.536423170402,
          "standard_error": 3.207247369565382
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6368901095649788,
            "upper_bound": 14.01787220229977
          },
          "point_estimate": 7.066200173857961,
          "standard_error": 3.0809581134014494
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9807.710440214778,
            "upper_bound": 9812.996751578012
          },
          "point_estimate": 9809.754855838191,
          "standard_error": 1.349799339262572
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.892124927818157,
            "upper_bound": 15.219018535739837
          },
          "point_estimate": 10.613001152645571,
          "standard_error": 3.402497655759901
        }
      }
    },
    "memrchr1/krate/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/rare",
        "directory_name": "memrchr1/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10849.930071280323,
            "upper_bound": 10878.97540803281
          },
          "point_estimate": 10862.445612114445,
          "standard_error": 7.549840500033521
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10844.809258152354,
            "upper_bound": 10870.140765092649
          },
          "point_estimate": 10858.064031679618,
          "standard_error": 7.613053053567389
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.314753637107453,
            "upper_bound": 32.15120637281701
          },
          "point_estimate": 18.77824576145964,
          "standard_error": 6.8729186287749195
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10845.35502997774,
            "upper_bound": 10866.408999535728
          },
          "point_estimate": 10854.078352908298,
          "standard_error": 5.429347817472942
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.309786666356615,
            "upper_bound": 36.48473588885603
          },
          "point_estimate": 25.082951067688818,
          "standard_error": 8.497972767900752
        }
      }
    },
    "memrchr1/krate/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/uncommon",
        "directory_name": "memrchr1/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77513.01464531085,
            "upper_bound": 77672.685376688
          },
          "point_estimate": 77585.12715444886,
          "standard_error": 40.99786050350601
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77492.14456289979,
            "upper_bound": 77634.87686567164
          },
          "point_estimate": 77570.28768360577,
          "standard_error": 38.74804732515056
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.81141527569401,
            "upper_bound": 189.0465123154032
          },
          "point_estimate": 105.80745416631616,
          "standard_error": 40.15442149729739
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77503.47854392411,
            "upper_bound": 77580.46042296564
          },
          "point_estimate": 77539.32623155096,
          "standard_error": 19.800915262210115
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.04751501984419,
            "upper_bound": 192.5936271134361
          },
          "point_estimate": 136.66703637520826,
          "standard_error": 40.14627094291122
        }
      }
    },
    "memrchr1/krate/huge/verycommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/verycommon",
        "directory_name": "memrchr1/krate_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462391.0365079993,
            "upper_bound": 463081.4887112342
          },
          "point_estimate": 462698.1689838256,
          "standard_error": 178.09396690447988
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462262.8101265823,
            "upper_bound": 463044.1693037975
          },
          "point_estimate": 462482.3738396624,
          "standard_error": 195.83537493005664
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.38299905527546,
            "upper_bound": 815.7760037132249
          },
          "point_estimate": 359.09063998561203,
          "standard_error": 220.84419980156952
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462279.51753262506,
            "upper_bound": 462685.3232331833
          },
          "point_estimate": 462459.23869801086,
          "standard_error": 111.26980629542074
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 191.8553050370925,
            "upper_bound": 818.4169368556513
          },
          "point_estimate": 595.9811148399461,
          "standard_error": 172.42328199392898
        }
      }
    },
    "memrchr1/krate/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/common",
        "directory_name": "memrchr1/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226.74739189027247,
            "upper_bound": 226.82726198358472
          },
          "point_estimate": 226.78880178488484,
          "standard_error": 0.020442902113107553
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226.72668808885007,
            "upper_bound": 226.83962375990515
          },
          "point_estimate": 226.79982030323828,
          "standard_error": 0.026336229454105065
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017704104914365953,
            "upper_bound": 0.11338716829333874
          },
          "point_estimate": 0.0751372391352715,
          "standard_error": 0.027534812979618496
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226.7219520417707,
            "upper_bound": 226.80889917755312
          },
          "point_estimate": 226.762440054486,
          "standard_error": 0.02202743193310481
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03550916668949443,
            "upper_bound": 0.08651188678218755
          },
          "point_estimate": 0.06836327335260396,
          "standard_error": 0.012849088887292338
        }
      }
    },
    "memrchr1/krate/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/never",
        "directory_name": "memrchr1/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.099193716461002,
            "upper_bound": 8.105622233831957
          },
          "point_estimate": 8.101823654914293,
          "standard_error": 0.0017061564522752178
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.09822325862914,
            "upper_bound": 8.102868863461126
          },
          "point_estimate": 8.100361350037872,
          "standard_error": 0.0012620787093396387
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006967401253437734,
            "upper_bound": 0.00538158393689804
          },
          "point_estimate": 0.0030813885812636783,
          "standard_error": 0.0011636748992481516
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.098619605653937,
            "upper_bound": 8.101242277864943
          },
          "point_estimate": 8.099775300172563,
          "standard_error": 0.0006748180887456857
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015321591818058466,
            "upper_bound": 0.008467396292655378
          },
          "point_estimate": 0.005668583845465091,
          "standard_error": 0.002240872826657921
        }
      }
    },
    "memrchr1/krate/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/rare",
        "directory_name": "memrchr1/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.10328677803787,
            "upper_bound": 13.11078906806503
          },
          "point_estimate": 13.106923666591678,
          "standard_error": 0.0019186096517783793
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.10177789969137,
            "upper_bound": 13.11130743163739
          },
          "point_estimate": 13.106351419508224,
          "standard_error": 0.0022669652257407595
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015828903001706075,
            "upper_bound": 0.011459958801683786
          },
          "point_estimate": 0.006122928392903996,
          "standard_error": 0.0027113794207501256
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.102828542817576,
            "upper_bound": 13.108643716682396
          },
          "point_estimate": 13.105273313747874,
          "standard_error": 0.0014562834813690213
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0033714729218143743,
            "upper_bound": 0.008122962361584196
          },
          "point_estimate": 0.006419967340436314,
          "standard_error": 0.0012102579648302825
        }
      }
    },
    "memrchr1/krate/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/uncommon",
        "directory_name": "memrchr1/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.83272070357937,
            "upper_bound": 43.9052163117272
          },
          "point_estimate": 43.8720490183328,
          "standard_error": 0.0186546575855832
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.848611233473974,
            "upper_bound": 43.89466027912411
          },
          "point_estimate": 43.88609708799184,
          "standard_error": 0.01085273905190671
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032534699552272625,
            "upper_bound": 0.10745755214177027
          },
          "point_estimate": 0.01370947531071484,
          "standard_error": 0.02466327537869301
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.77958773296987,
            "upper_bound": 43.889318413876474
          },
          "point_estimate": 43.83885953364418,
          "standard_error": 0.031275727510243156
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01042810974087592,
            "upper_bound": 0.08760365359234477
          },
          "point_estimate": 0.06237384989864227,
          "standard_error": 0.019020531527661012
        }
      }
    },
    "memrchr1/krate/small/verycommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/verycommon",
        "directory_name": "memrchr1/krate_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498.2799798508272,
            "upper_bound": 500.13543161583664
          },
          "point_estimate": 499.2581263130352,
          "standard_error": 0.46685893570815234
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 497.6087868128837,
            "upper_bound": 500.1770934393943
          },
          "point_estimate": 500.10897820539344,
          "standard_error": 0.6595547018424386
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02001903647469024,
            "upper_bound": 2.2397100947292374
          },
          "point_estimate": 0.10430444983458265,
          "standard_error": 0.6418063300706159
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499.7407672588442,
            "upper_bound": 500.3212980966233
          },
          "point_estimate": 500.1190572390572,
          "standard_error": 0.1501896474903985
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24620246177034696,
            "upper_bound": 1.8682835511225768
          },
          "point_estimate": 1.5592789678125127,
          "standard_error": 0.3348191700514815
        }
      }
    },
    "memrchr1/krate/tiny/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/common",
        "directory_name": "memrchr1/krate_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.85161987814429,
            "upper_bound": 49.902788061043466
          },
          "point_estimate": 49.87318946002879,
          "standard_error": 0.013397924490018963
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.84954162055181,
            "upper_bound": 49.88377274618471
          },
          "point_estimate": 49.858870276422394,
          "standard_error": 0.010691717319650877
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002429251907431308,
            "upper_bound": 0.04650531248195265
          },
          "point_estimate": 0.028490939550816153,
          "standard_error": 0.011787069692635486
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.84033717481935,
            "upper_bound": 49.87946326550551
          },
          "point_estimate": 49.86070572851009,
          "standard_error": 0.010605620778804689
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013500462892804169,
            "upper_bound": 0.06601552114065398
          },
          "point_estimate": 0.04476375778334552,
          "standard_error": 0.016544663972606635
        }
      }
    },
    "memrchr1/krate/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/never",
        "directory_name": "memrchr1/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.219285660965571,
            "upper_bound": 4.262341210855197
          },
          "point_estimate": 4.239676068363488,
          "standard_error": 0.01102516896784023
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.212565537157613,
            "upper_bound": 4.258356711642451
          },
          "point_estimate": 4.237752052281634,
          "standard_error": 0.014568975461051296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016945675948326284,
            "upper_bound": 0.06117244670014962
          },
          "point_estimate": 0.030907250322969985,
          "standard_error": 0.014403469398087535
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.213403561390683,
            "upper_bound": 4.287175776305519
          },
          "point_estimate": 4.247374011433531,
          "standard_error": 0.02072238624538815
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0178793682859596,
            "upper_bound": 0.04975889452823913
          },
          "point_estimate": 0.03661305827682967,
          "standard_error": 0.008845095128462705
        }
      }
    },
    "memrchr1/krate/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/rare",
        "directory_name": "memrchr1/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.833157932480225,
            "upper_bound": 6.905479198771662
          },
          "point_estimate": 6.866968909991459,
          "standard_error": 0.018740312589985215
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.828168828455366,
            "upper_bound": 6.927209916828403
          },
          "point_estimate": 6.836492953952661,
          "standard_error": 0.023688638000672536
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002187285543537385,
            "upper_bound": 0.10208140854293668
          },
          "point_estimate": 0.0410574090046139,
          "standard_error": 0.026691852298702422
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.823193110047229,
            "upper_bound": 6.873638646975943
          },
          "point_estimate": 6.847068477072504,
          "standard_error": 0.013227853793030877
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023925636542112733,
            "upper_bound": 0.07769510144500599
          },
          "point_estimate": 0.06243780211347895,
          "standard_error": 0.01380488089668904
        }
      }
    },
    "memrchr1/krate/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/uncommon",
        "directory_name": "memrchr1/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.172059383590938,
            "upper_bound": 27.184886839319994
          },
          "point_estimate": 27.177941723986976,
          "standard_error": 0.0032993153840495245
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.172057934585983,
            "upper_bound": 27.18339102931474
          },
          "point_estimate": 27.17500249376652,
          "standard_error": 0.003017008423762482
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000984470764793716,
            "upper_bound": 0.0162890086425425
          },
          "point_estimate": 0.006393514853199599,
          "standard_error": 0.003894979734533359
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.1720830764572,
            "upper_bound": 27.17778621391152
          },
          "point_estimate": 27.17448478922024,
          "standard_error": 0.001450833655390443
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0038826384730657926,
            "upper_bound": 0.015434189369371762
          },
          "point_estimate": 0.01102869149178384,
          "standard_error": 0.003184392632045713
        }
      }
    },
    "memrchr1/libc/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr1/libc/empty/never",
        "directory_name": "memrchr1/libc_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.49094283043505865,
            "upper_bound": 0.4927268045308411
          },
          "point_estimate": 0.4916163490161856,
          "standard_error": 0.0005013666406006905
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.49088091333319395,
            "upper_bound": 0.4915775099836235
          },
          "point_estimate": 0.4909949161252582,
          "standard_error": 0.000198965626603406
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0000288363561160716,
            "upper_bound": 0.0008516949427070545
          },
          "point_estimate": 0.0002637407598960882,
          "standard_error": 0.00023623280919465677
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4908712844313524,
            "upper_bound": 0.49196712922524327
          },
          "point_estimate": 0.4911745773014348,
          "standard_error": 0.0002971657449339443
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0001508581038929081,
            "upper_bound": 0.002566374844856056
          },
          "point_estimate": 0.001677280426146417,
          "standard_error": 0.000802862631391414
        }
      }
    },
    "memrchr1/libc/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/common",
        "directory_name": "memrchr1/libc_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262832.6724547101,
            "upper_bound": 263627.4811687299
          },
          "point_estimate": 263192.9907508051,
          "standard_error": 205.0743319758059
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262692.1992753623,
            "upper_bound": 263516.6696859903
          },
          "point_estimate": 263097.6508152174,
          "standard_error": 230.05133900261072
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.58968547952433,
            "upper_bound": 994.7386345138108
          },
          "point_estimate": 543.5842533627794,
          "standard_error": 229.48013905340784
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262620.279477512,
            "upper_bound": 263199.32353087154
          },
          "point_estimate": 262874.00239036325,
          "standard_error": 151.55474402819166
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 284.3799067491194,
            "upper_bound": 943.305207144571
          },
          "point_estimate": 683.0734258255662,
          "standard_error": 188.23740591301143
        }
      }
    },
    "memrchr1/libc/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/never",
        "directory_name": "memrchr1/libc_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9724.170365707272,
            "upper_bound": 9746.883742459251
          },
          "point_estimate": 9734.87033941055,
          "standard_error": 5.820954604098497
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9716.378640487876,
            "upper_bound": 9746.223400267738
          },
          "point_estimate": 9733.003770638108,
          "standard_error": 7.44795617099595
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.1382493338424777,
            "upper_bound": 33.73780506814794
          },
          "point_estimate": 19.865158132373686,
          "standard_error": 7.232567823512882
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9723.281796250983,
            "upper_bound": 9750.581146476552
          },
          "point_estimate": 9733.659364731651,
          "standard_error": 7.109890731230815
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.770903529059789,
            "upper_bound": 25.692618128832187
          },
          "point_estimate": 19.30550125429775,
          "standard_error": 4.425074008110968
        }
      }
    },
    "memrchr1/libc/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/rare",
        "directory_name": "memrchr1/libc_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9992.844374438091,
            "upper_bound": 10019.671687436718
          },
          "point_estimate": 10006.158552847495,
          "standard_error": 6.862728921761776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9988.19818531757,
            "upper_bound": 10024.093071212536
          },
          "point_estimate": 10007.378088298832,
          "standard_error": 7.053936880837651
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.35637120736429,
            "upper_bound": 42.152514450982835
          },
          "point_estimate": 17.360947092798682,
          "standard_error": 12.48202136410137
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9991.132624912709,
            "upper_bound": 10007.987171814288
          },
          "point_estimate": 9999.655624551418,
          "standard_error": 4.380661235134973
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.041752236630211,
            "upper_bound": 29.322209434019353
          },
          "point_estimate": 22.88979961267588,
          "standard_error": 4.475475634891567
        }
      }
    },
    "memrchr1/libc/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/uncommon",
        "directory_name": "memrchr1/libc_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77609.39074040158,
            "upper_bound": 77752.65302910052
          },
          "point_estimate": 77677.24615706825,
          "standard_error": 36.50901590931693
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77585.20237416905,
            "upper_bound": 77754.98005698006
          },
          "point_estimate": 77659.87034493285,
          "standard_error": 35.09536246708896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.899342683744576,
            "upper_bound": 195.1650299325597
          },
          "point_estimate": 88.63273565936575,
          "standard_error": 53.84860498744617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77613.54257246378,
            "upper_bound": 77688.30002970659
          },
          "point_estimate": 77653.83245088244,
          "standard_error": 18.82828484559064
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.57887431343524,
            "upper_bound": 165.2498838615351
          },
          "point_estimate": 121.73632857065095,
          "standard_error": 29.9519294260642
        }
      }
    },
    "memrchr1/libc/huge/verycommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/verycommon",
        "directory_name": "memrchr1/libc_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 548724.3240440653,
            "upper_bound": 550391.0976759061
          },
          "point_estimate": 549512.7041186922,
          "standard_error": 428.4470649763638
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 548196.7931769723,
            "upper_bound": 550558.4514925373
          },
          "point_estimate": 549292.2298507462,
          "standard_error": 602.5916943458175
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 260.9265311886086,
            "upper_bound": 2391.270007695723
          },
          "point_estimate": 1693.2267981480843,
          "standard_error": 546.0826578065904
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 548569.0621820475,
            "upper_bound": 549947.2281909419
          },
          "point_estimate": 549164.5146733864,
          "standard_error": 352.0397720145543
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 724.6905025344942,
            "upper_bound": 1799.5212995027784
          },
          "point_estimate": 1424.8418871623255,
          "standard_error": 275.1235436794994
        }
      }
    },
    "memrchr1/libc/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/common",
        "directory_name": "memrchr1/libc_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.19101211545157,
            "upper_bound": 239.3457535835396
          },
          "point_estimate": 239.27731277540525,
          "standard_error": 0.04013342921899747
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.22336272199215,
            "upper_bound": 239.37623920258005
          },
          "point_estimate": 239.3008564788909,
          "standard_error": 0.04766682303317229
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009560930655799157,
            "upper_bound": 0.19816925425289408
          },
          "point_estimate": 0.1059547449057568,
          "standard_error": 0.04322267286978895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.22897277321476,
            "upper_bound": 239.35883716357193
          },
          "point_estimate": 239.31155946837217,
          "standard_error": 0.033064247984944256
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.056092841220372405,
            "upper_bound": 0.1903965377692007
          },
          "point_estimate": 0.1339513390934064,
          "standard_error": 0.04063274158187474
        }
      }
    },
    "memrchr1/libc/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/never",
        "directory_name": "memrchr1/libc_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.050978903415658,
            "upper_bound": 8.069527333211141
          },
          "point_estimate": 8.059781371078277,
          "standard_error": 0.004771889033816627
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.046463010390418,
            "upper_bound": 8.073616042065712
          },
          "point_estimate": 8.055869465332353,
          "standard_error": 0.00670944599854213
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003960571561190305,
            "upper_bound": 0.025951715432823676
          },
          "point_estimate": 0.01573747787447918,
          "standard_error": 0.006186316864907608
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.046423112157886,
            "upper_bound": 8.063144888548536
          },
          "point_estimate": 8.05390025005377,
          "standard_error": 0.004313255321501355
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007562026239679728,
            "upper_bound": 0.01929837700749462
          },
          "point_estimate": 0.015931670559760126,
          "standard_error": 0.0028013693189996055
        }
      }
    },
    "memrchr1/libc/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/rare",
        "directory_name": "memrchr1/libc_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.318266127222213,
            "upper_bound": 14.362442009346632
          },
          "point_estimate": 14.33870791250411,
          "standard_error": 0.011435795788049322
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.311608543796812,
            "upper_bound": 14.383473129402493
          },
          "point_estimate": 14.316991322813871,
          "standard_error": 0.01827691041168946
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001798575048115372,
            "upper_bound": 0.0606158640746692
          },
          "point_estimate": 0.011793020637551626,
          "standard_error": 0.015816892496840575
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.314150024985585,
            "upper_bound": 14.333768522546242
          },
          "point_estimate": 14.320135394008483,
          "standard_error": 0.005196867758913096
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009605195953742669,
            "upper_bound": 0.0446199779779921
          },
          "point_estimate": 0.038042118462589605,
          "standard_error": 0.007400256519096339
        }
      }
    },
    "memrchr1/libc/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/uncommon",
        "directory_name": "memrchr1/libc_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.195040996856704,
            "upper_bound": 48.23482827234318
          },
          "point_estimate": 48.21753228077084,
          "standard_error": 0.010342556788643398
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.20597858560262,
            "upper_bound": 48.24333949899698
          },
          "point_estimate": 48.22237798180734,
          "standard_error": 0.0112736906173038
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003477111425096895,
            "upper_bound": 0.0459802970644841
          },
          "point_estimate": 0.028821684055891476,
          "standard_error": 0.010863363272486088
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.175312733436805,
            "upper_bound": 48.23126815371491
          },
          "point_estimate": 48.20607178876073,
          "standard_error": 0.014496675354080602
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013210862912268391,
            "upper_bound": 0.04977683044216
          },
          "point_estimate": 0.03449401084395688,
          "standard_error": 0.011338461529318573
        }
      }
    },
    "memrchr1/libc/small/verycommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/verycommon",
        "directory_name": "memrchr1/libc_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 583.0045400221702,
            "upper_bound": 583.2901658854083
          },
          "point_estimate": 583.140490479653,
          "standard_error": 0.07347478843061316
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 582.9319735383541,
            "upper_bound": 583.3354425303949
          },
          "point_estimate": 583.1101727991104,
          "standard_error": 0.10141455556402725
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03962572989682693,
            "upper_bound": 0.43371744586551786
          },
          "point_estimate": 0.27038021101156295,
          "standard_error": 0.09792464073873051
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 582.9537348459653,
            "upper_bound": 583.2971342549332
          },
          "point_estimate": 583.0992706392186,
          "standard_error": 0.08819493531700354
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11854911077463542,
            "upper_bound": 0.2996197091798965
          },
          "point_estimate": 0.2454670397131424,
          "standard_error": 0.0445980139759566
        }
      }
    },
    "memrchr1/libc/tiny/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/common",
        "directory_name": "memrchr1/libc_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.18094808600385,
            "upper_bound": 50.2047267891292
          },
          "point_estimate": 50.19188527012019,
          "standard_error": 0.006113390737002518
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.178496699606214,
            "upper_bound": 50.20222279865666
          },
          "point_estimate": 50.18838066841253,
          "standard_error": 0.005311194958886364
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000886067672696864,
            "upper_bound": 0.030196040046890422
          },
          "point_estimate": 0.01599149460683372,
          "standard_error": 0.008617848574375893
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.18428708404254,
            "upper_bound": 50.196569911457146
          },
          "point_estimate": 50.19009526710013,
          "standard_error": 0.003088304296304
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008603400904186527,
            "upper_bound": 0.027903210145173975
          },
          "point_estimate": 0.02033504311289655,
          "standard_error": 0.0053840820754418066
        }
      }
    },
    "memrchr1/libc/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/never",
        "directory_name": "memrchr1/libc_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9448814797202423,
            "upper_bound": 2.948959675593521
          },
          "point_estimate": 2.946363277890492,
          "standard_error": 0.0011873602576697156
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.944683842932073,
            "upper_bound": 2.945872559861103
          },
          "point_estimate": 2.945146585406466,
          "standard_error": 0.0004017601269359164
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00015095802512065347,
            "upper_bound": 0.001339252997644211
          },
          "point_estimate": 0.0005405385593351088,
          "standard_error": 0.0004443004140729098
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.944687795918276,
            "upper_bound": 2.945694454151296
          },
          "point_estimate": 2.9451180121624367,
          "standard_error": 0.00026128616481756
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00034424766248488406,
            "upper_bound": 0.006066477329561087
          },
          "point_estimate": 0.00394741356830572,
          "standard_error": 0.0020218637937239126
        }
      }
    },
    "memrchr1/libc/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/rare",
        "directory_name": "memrchr1/libc_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.871651672097323,
            "upper_bound": 5.8865243684908135
          },
          "point_estimate": 5.87934311434514,
          "standard_error": 0.0038193367568544463
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.868101661510562,
            "upper_bound": 5.891173153941638
          },
          "point_estimate": 5.881027284519525,
          "standard_error": 0.00637535111628016
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016682271810052355,
            "upper_bound": 0.02159502845960631
          },
          "point_estimate": 0.0154245693613884,
          "standard_error": 0.005680590448878014
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.872610321996645,
            "upper_bound": 5.887815455802035
          },
          "point_estimate": 5.879564647897608,
          "standard_error": 0.0038184387795854664
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006927912594215713,
            "upper_bound": 0.015085911197605596
          },
          "point_estimate": 0.012725423393103622,
          "standard_error": 0.002003417633975302
        }
      }
    },
    "memrchr1/libc/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr1/libc/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/uncommon",
        "directory_name": "memrchr1/libc_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.779677383840536,
            "upper_bound": 18.790638354672147
          },
          "point_estimate": 18.78460963047005,
          "standard_error": 0.002818500482298039
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.77753390503644,
            "upper_bound": 18.790611859101404
          },
          "point_estimate": 18.781683731555077,
          "standard_error": 0.0032876565891152973
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008540110580126251,
            "upper_bound": 0.014993901718193007
          },
          "point_estimate": 0.00634478174682687,
          "standard_error": 0.003526811053352851
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.77952474810559,
            "upper_bound": 18.78668468891672
          },
          "point_estimate": 18.782629362656103,
          "standard_error": 0.0017857620108860132
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028143097936921815,
            "upper_bound": 0.01269728074125752
          },
          "point_estimate": 0.009415945850103292,
          "standard_error": 0.0024753535802337695
        }
      }
    },
    "memrchr2/krate/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr2/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr2/krate/empty/never",
        "directory_name": "memrchr2/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9861420536382572,
            "upper_bound": 0.9868908355849912
          },
          "point_estimate": 0.9865138946125572,
          "standard_error": 0.0001925452111356344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9859831597698584,
            "upper_bound": 0.9870413771590424
          },
          "point_estimate": 0.986496433868094,
          "standard_error": 0.00033245294021825553
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000098223667100056,
            "upper_bound": 0.0010233995962923269
          },
          "point_estimate": 0.0007809951069071971,
          "standard_error": 0.00024544194318497196
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9861145507267924,
            "upper_bound": 0.9868845677571588
          },
          "point_estimate": 0.986497305979776,
          "standard_error": 0.00019977798280153763
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00040052890367335456,
            "upper_bound": 0.0007866522273236505
          },
          "point_estimate": 0.0006399371639894401,
          "standard_error": 0.00009971471717480606
        }
      }
    },
    "memrchr2/krate/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr2/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/common",
        "directory_name": "memrchr2/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 445021.4423968496,
            "upper_bound": 445385.6649070848
          },
          "point_estimate": 445189.7842499033,
          "standard_error": 93.45762270692435
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 444956.8506097561,
            "upper_bound": 445366.0835365853
          },
          "point_estimate": 445143.564701897,
          "standard_error": 103.94109700923528
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.72335955791982,
            "upper_bound": 496.01557351103014
          },
          "point_estimate": 269.04594065033814,
          "standard_error": 104.09107227174133
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 444979.05999813817,
            "upper_bound": 445198.021577803
          },
          "point_estimate": 445103.7134621476,
          "standard_error": 56.24181847397712
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142.22958828541246,
            "upper_bound": 419.9032488489289
          },
          "point_estimate": 311.25059029678164,
          "standard_error": 76.73174452824193
        }
      }
    },
    "memrchr2/krate/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr2/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/never",
        "directory_name": "memrchr2/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12216.626677996866,
            "upper_bound": 12238.726336936772
          },
          "point_estimate": 12227.913624156328,
          "standard_error": 5.653706813812004
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12213.78696812058,
            "upper_bound": 12242.070464985994
          },
          "point_estimate": 12229.813305322128,
          "standard_error": 7.112544728865595
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.447645409167965,
            "upper_bound": 31.301323420761697
          },
          "point_estimate": 17.31320951195536,
          "standard_error": 6.801654351938728
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12215.775180453684,
            "upper_bound": 12236.096227824462
          },
          "point_estimate": 12225.37821499509,
          "standard_error": 5.275125968314909
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.15627472562891,
            "upper_bound": 24.278930237466728
          },
          "point_estimate": 18.836979294031934,
          "standard_error": 3.6281612818536946
        }
      }
    },
    "memrchr2/krate/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr2/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/rare",
        "directory_name": "memrchr2/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17331.059294292245,
            "upper_bound": 17365.87313736877
          },
          "point_estimate": 17348.515867588227,
          "standard_error": 8.90351286864755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17321.569424517773,
            "upper_bound": 17371.892388012115
          },
          "point_estimate": 17348.033213773313,
          "standard_error": 14.225373879434713
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.260120665856246,
            "upper_bound": 49.06210000893905
          },
          "point_estimate": 36.349398069799435,
          "standard_error": 11.689371169636573
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17330.981075929885,
            "upper_bound": 17364.74212101919
          },
          "point_estimate": 17350.336535678573,
          "standard_error": 8.578211675035114
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.932257065430314,
            "upper_bound": 36.33538267895195
          },
          "point_estimate": 29.624503697637117,
          "standard_error": 4.688996559843993
        }
      }
    },
    "memrchr2/krate/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr2/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/uncommon",
        "directory_name": "memrchr2/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168250.1212877353,
            "upper_bound": 169529.28760871166
          },
          "point_estimate": 168864.58819010705,
          "standard_error": 327.54558416756123
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167960.72440014768,
            "upper_bound": 169687.5988372093
          },
          "point_estimate": 168934.25410852712,
          "standard_error": 414.16111010549594
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.17999525146904,
            "upper_bound": 1936.8866113498264
          },
          "point_estimate": 1226.4723890629532,
          "standard_error": 486.0405561322367
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167825.35082134785,
            "upper_bound": 168838.72621136464
          },
          "point_estimate": 168286.35045605557,
          "standard_error": 261.2676145970341
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 569.0308263784177,
            "upper_bound": 1366.0708575144545
          },
          "point_estimate": 1085.6950648813486,
          "standard_error": 205.34465969082495
        }
      }
    },
    "memrchr2/krate/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr2/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/common",
        "directory_name": "memrchr2/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 457.0781439481379,
            "upper_bound": 458.2640979767376
          },
          "point_estimate": 457.6240698781444,
          "standard_error": 0.3054010701077258
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 456.8902039788466,
            "upper_bound": 458.4284720473432
          },
          "point_estimate": 457.1365061627354,
          "standard_error": 0.419761520603528
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05308343338359075,
            "upper_bound": 1.6242748899495827
          },
          "point_estimate": 0.5075120808706404,
          "standard_error": 0.4562833694262397
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 456.91926628363245,
            "upper_bound": 457.79706216423233
          },
          "point_estimate": 457.3061296346564,
          "standard_error": 0.24199531308921995
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3912332484820244,
            "upper_bound": 1.2869352010332682
          },
          "point_estimate": 1.018173662363846,
          "standard_error": 0.23104818838564656
        }
      }
    },
    "memrchr2/krate/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr2/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/never",
        "directory_name": "memrchr2/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.998512679791595,
            "upper_bound": 13.020073384332882
          },
          "point_estimate": 13.008090543634534,
          "standard_error": 0.005437092455932462
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.002617190591764,
            "upper_bound": 13.009775228028976
          },
          "point_estimate": 13.006203356490136,
          "standard_error": 0.0018398605792179872
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005640474966734499,
            "upper_bound": 0.021861112065076804
          },
          "point_estimate": 0.00248072468505636,
          "standard_error": 0.00504176431540605
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.001600019026355,
            "upper_bound": 13.007226170368025
          },
          "point_estimate": 13.005436329922206,
          "standard_error": 0.001498074845447863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024558368292019643,
            "upper_bound": 0.026910936560330855
          },
          "point_estimate": 0.018083994655359755,
          "standard_error": 0.00659757281837081
        }
      }
    },
    "memrchr2/krate/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr2/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/rare",
        "directory_name": "memrchr2/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.86346450785819,
            "upper_bound": 23.889467055561983
          },
          "point_estimate": 23.875111160106975,
          "standard_error": 0.006707764497964706
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.860419980642313,
            "upper_bound": 23.89017670462959
          },
          "point_estimate": 23.868132817509565,
          "standard_error": 0.006485333342132566
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0034571374158032836,
            "upper_bound": 0.0302259055436395
          },
          "point_estimate": 0.01096753779601272,
          "standard_error": 0.007057186491939173
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.862112831381985,
            "upper_bound": 23.873051221904632
          },
          "point_estimate": 23.867707001857617,
          "standard_error": 0.002773807571943764
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006675833433476584,
            "upper_bound": 0.02955785240724151
          },
          "point_estimate": 0.022304435454893953,
          "standard_error": 0.006103287518856785
        }
      }
    },
    "memrchr2/krate/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr2/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/uncommon",
        "directory_name": "memrchr2/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.64215419616788,
            "upper_bound": 98.79578300850218
          },
          "point_estimate": 98.71875222032408,
          "standard_error": 0.039428588710127915
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.59482951118936,
            "upper_bound": 98.80836734804755
          },
          "point_estimate": 98.75441638279032,
          "standard_error": 0.06182851850388952
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01618053285700438,
            "upper_bound": 0.23995686595543797
          },
          "point_estimate": 0.1729084622864789,
          "standard_error": 0.05897900175072411
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.58479364662456,
            "upper_bound": 98.75685109535728
          },
          "point_estimate": 98.6575711077398,
          "standard_error": 0.04408730289583521
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0806944314724111,
            "upper_bound": 0.16360337250481916
          },
          "point_estimate": 0.1315563458635067,
          "standard_error": 0.02183280451698147
        }
      }
    },
    "memrchr2/krate/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr2/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/never",
        "directory_name": "memrchr2/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.953537267446194,
            "upper_bound": 4.979794749983385
          },
          "point_estimate": 4.966648507118053,
          "standard_error": 0.006746923717070925
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.946782629314586,
            "upper_bound": 4.98862729380191
          },
          "point_estimate": 4.964196571334268,
          "standard_error": 0.011699329965677187
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005790949193514343,
            "upper_bound": 0.036709554342231054
          },
          "point_estimate": 0.030793415208038192,
          "standard_error": 0.008367412622459897
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.945467726365313,
            "upper_bound": 4.979500322361928
          },
          "point_estimate": 4.959726784112119,
          "standard_error": 0.00870775558121316
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014464000967568382,
            "upper_bound": 0.02642137248138851
          },
          "point_estimate": 0.0224824533972699,
          "standard_error": 0.0030695381034232597
        }
      }
    },
    "memrchr2/krate/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr2/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/rare",
        "directory_name": "memrchr2/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.970732616157846,
            "upper_bound": 16.01885782480923
          },
          "point_estimate": 15.996171655960303,
          "standard_error": 0.0123588138343036
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.966644271660751,
            "upper_bound": 16.02139805635843
          },
          "point_estimate": 16.008815446655397,
          "standard_error": 0.014673503312373855
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005248607960971379,
            "upper_bound": 0.06662310306901478
          },
          "point_estimate": 0.01960977449018819,
          "standard_error": 0.01810833864234859
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.992170626404857,
            "upper_bound": 16.029706418429047
          },
          "point_estimate": 16.011708293595984,
          "standard_error": 0.00945464762544677
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01703052877692129,
            "upper_bound": 0.05190013584465886
          },
          "point_estimate": 0.04126960527239242,
          "standard_error": 0.008410604197981033
        }
      }
    },
    "memrchr2/krate/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr2/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/uncommon",
        "directory_name": "memrchr2/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.46856419486625,
            "upper_bound": 50.64795863224814
          },
          "point_estimate": 50.55791963612605,
          "standard_error": 0.045938289605145434
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.446679378597814,
            "upper_bound": 50.701113151011626
          },
          "point_estimate": 50.53488928636837,
          "standard_error": 0.07189949867406029
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029627474941926905,
            "upper_bound": 0.25477261164234793
          },
          "point_estimate": 0.1599143927280865,
          "standard_error": 0.060718514401076625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.40983332214601,
            "upper_bound": 50.663671556371746
          },
          "point_estimate": 50.52200324531834,
          "standard_error": 0.0668118533649803
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0906937722352295,
            "upper_bound": 0.19013524226320833
          },
          "point_estimate": 0.15273242997689468,
          "standard_error": 0.02554247705490303
        }
      }
    },
    "memrchr3/krate/empty/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr3/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr3/krate/empty/never",
        "directory_name": "memrchr3/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2270505405384629,
            "upper_bound": 1.2274716540195802
          },
          "point_estimate": 1.2272546910632696,
          "standard_error": 0.00010742080131033808
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2269097481132782,
            "upper_bound": 1.227565119777778
          },
          "point_estimate": 1.2271680483933154,
          "standard_error": 0.0001718524053808918
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00008365211125080047,
            "upper_bound": 0.0005866836673006828
          },
          "point_estimate": 0.00038851563712691294,
          "standard_error": 0.0001311588850988303
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2269835805068656,
            "upper_bound": 1.2274640181137018
          },
          "point_estimate": 1.227261785355294,
          "standard_error": 0.0001229309666523308
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00021103860096781016,
            "upper_bound": 0.0004433691368651206
          },
          "point_estimate": 0.00035967306196786196,
          "standard_error": 0.00005968499611501555
        }
      }
    },
    "memrchr3/krate/huge/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr3/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/common",
        "directory_name": "memrchr3/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 681374.0872308753,
            "upper_bound": 682605.6680901675
          },
          "point_estimate": 681963.5000110229,
          "standard_error": 315.77080536091154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 681179.8827160494,
            "upper_bound": 682736.2101851852
          },
          "point_estimate": 681779.4125,
          "standard_error": 398.1692231910466
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 212.57692307783893,
            "upper_bound": 1729.127520968578
          },
          "point_estimate": 908.562202666037,
          "standard_error": 412.47061231497423
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 681017.7968961692,
            "upper_bound": 682779.9224277218
          },
          "point_estimate": 681892.0846079846,
          "standard_error": 470.979387850123
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 505.4478561108889,
            "upper_bound": 1346.251969022538
          },
          "point_estimate": 1053.1601241547926,
          "standard_error": 211.8374879011492
        }
      }
    },
    "memrchr3/krate/huge/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr3/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/never",
        "directory_name": "memrchr3/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16710.51362064734,
            "upper_bound": 16737.335609924103
          },
          "point_estimate": 16722.49142258161,
          "standard_error": 6.930658425098334
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16707.782144500692,
            "upper_bound": 16735.071297087634
          },
          "point_estimate": 16710.312969780643,
          "standard_error": 7.961001968241109
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8847385208732673,
            "upper_bound": 32.95257053324607
          },
          "point_estimate": 7.588009132047943,
          "standard_error": 9.31756470280978
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16708.275366872287,
            "upper_bound": 16719.238954469405
          },
          "point_estimate": 16711.996479820227,
          "standard_error": 2.8301376794665227
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.610735566763826,
            "upper_bound": 31.860193596656597
          },
          "point_estimate": 23.107848246843787,
          "standard_error": 6.672369249915529
        }
      }
    },
    "memrchr3/krate/huge/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr3/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/rare",
        "directory_name": "memrchr3/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22270.09178559933,
            "upper_bound": 22313.298884530745
          },
          "point_estimate": 22290.70680644361,
          "standard_error": 11.057596308156857
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22265.926954322505,
            "upper_bound": 22324.389068932294
          },
          "point_estimate": 22280.36899652565,
          "standard_error": 13.11137556964028
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.3566814854705793,
            "upper_bound": 63.2273287487334
          },
          "point_estimate": 22.805155539334017,
          "standard_error": 17.447877377927995
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22262.06627278301,
            "upper_bound": 22296.666678904607
          },
          "point_estimate": 22276.31980220883,
          "standard_error": 8.885534296474821
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.527539821450826,
            "upper_bound": 46.86051445173009
          },
          "point_estimate": 36.76814893152072,
          "standard_error": 7.3061756629569645
        }
      }
    },
    "memrchr3/krate/huge/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr3/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/uncommon",
        "directory_name": "memrchr3/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226317.64200138027,
            "upper_bound": 226816.99100414076
          },
          "point_estimate": 226555.57136522728,
          "standard_error": 128.55234278724103
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226215.69210292812,
            "upper_bound": 227025.1801242236
          },
          "point_estimate": 226408.64796411316,
          "standard_error": 200.3319161046656
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.2384333580854,
            "upper_bound": 698.1945948509385
          },
          "point_estimate": 345.0421460481799,
          "standard_error": 163.72695491438822
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226338.2912936494,
            "upper_bound": 226918.71694906036
          },
          "point_estimate": 226575.79604743083,
          "standard_error": 152.87892662384263
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 214.43803642890543,
            "upper_bound": 520.6824651478099
          },
          "point_estimate": 427.71752174568866,
          "standard_error": 74.84028601242552
        }
      }
    },
    "memrchr3/krate/small/common": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr3/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/common",
        "directory_name": "memrchr3/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 721.1689559985454,
            "upper_bound": 723.0146518804637
          },
          "point_estimate": 722.1872490708076,
          "standard_error": 0.46972967780472064
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 721.317884062283,
            "upper_bound": 723.1309282951503
          },
          "point_estimate": 722.9655421902967,
          "standard_error": 0.4700468073956826
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06628176169144147,
            "upper_bound": 2.1397467645508015
          },
          "point_estimate": 0.2780047372048048,
          "standard_error": 0.5092030420253001
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 722.7382445700684,
            "upper_bound": 723.1245607495487
          },
          "point_estimate": 722.9999212767793,
          "standard_error": 0.10181370828720372
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13217445388171062,
            "upper_bound": 2.0053032658286165
          },
          "point_estimate": 1.5666161170392057,
          "standard_error": 0.449587236239894
        }
      }
    },
    "memrchr3/krate/small/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr3/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/never",
        "directory_name": "memrchr3/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.9507242397868,
            "upper_bound": 15.954618491520268
          },
          "point_estimate": 15.952631801303852,
          "standard_error": 0.00100099496819378
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.949276260165744,
            "upper_bound": 15.95643447700038
          },
          "point_estimate": 15.952094949913535,
          "standard_error": 0.00202264891221982
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00030272450611789974,
            "upper_bound": 0.005341633838355485
          },
          "point_estimate": 0.004187490888099938,
          "standard_error": 0.0015175582092284985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.950335548570576,
            "upper_bound": 15.955784170892022
          },
          "point_estimate": 15.953097445532192,
          "standard_error": 0.00140683430644234
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022286305961750955,
            "upper_bound": 0.0037830493603410657
          },
          "point_estimate": 0.003344870498235519,
          "standard_error": 0.0004109940432371874
        }
      }
    },
    "memrchr3/krate/small/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr3/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/rare",
        "directory_name": "memrchr3/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.208187625017246,
            "upper_bound": 35.28103071888117
          },
          "point_estimate": 35.24094684966476,
          "standard_error": 0.01881531197177915
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.199132002543166,
            "upper_bound": 35.27750441111923
          },
          "point_estimate": 35.22379163394214,
          "standard_error": 0.016276010290831793
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008928613504325222,
            "upper_bound": 0.09371193451349076
          },
          "point_estimate": 0.03184332774457146,
          "standard_error": 0.021581709731043126
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.190183111154646,
            "upper_bound": 35.25541007822286
          },
          "point_estimate": 35.21199866646122,
          "standard_error": 0.016863766371435565
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02096147123071033,
            "upper_bound": 0.08531219312246345
          },
          "point_estimate": 0.06253472803276627,
          "standard_error": 0.01762128617498077
        }
      }
    },
    "memrchr3/krate/small/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr3/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/uncommon",
        "directory_name": "memrchr3/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.09800821091247,
            "upper_bound": 155.6786360992131
          },
          "point_estimate": 155.39389026822397,
          "standard_error": 0.1494739524881178
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154.84079393580976,
            "upper_bound": 155.84090102091542
          },
          "point_estimate": 155.5958256731005,
          "standard_error": 0.32028266226400287
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.059195633752012373,
            "upper_bound": 0.7151326831058523
          },
          "point_estimate": 0.4765978969310749,
          "standard_error": 0.1989317499028506
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.33531509245714,
            "upper_bound": 155.80607093039742
          },
          "point_estimate": 155.63865637165514,
          "standard_error": 0.12105157919190124
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3199186786831383,
            "upper_bound": 0.5482347038508268
          },
          "point_estimate": 0.4965900282175307,
          "standard_error": 0.05819912081380639
        }
      }
    },
    "memrchr3/krate/tiny/never": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr3/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/never",
        "directory_name": "memrchr3/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.616553035664689,
            "upper_bound": 5.631340158128586
          },
          "point_estimate": 5.62370374440893,
          "standard_error": 0.003798225940110464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.611544583990407,
            "upper_bound": 5.6352492902546025
          },
          "point_estimate": 5.622297537683448,
          "standard_error": 0.006255189152666691
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011715851769543155,
            "upper_bound": 0.022237325417025423
          },
          "point_estimate": 0.015316654711151836,
          "standard_error": 0.005850693902875129
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.614079917897612,
            "upper_bound": 5.632505542569154
          },
          "point_estimate": 5.621883951351522,
          "standard_error": 0.004726888211463063
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007084132356395136,
            "upper_bound": 0.015165439741772351
          },
          "point_estimate": 0.012651322728128429,
          "standard_error": 0.002050863168025538
        }
      }
    },
    "memrchr3/krate/tiny/rare": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr3/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/rare",
        "directory_name": "memrchr3/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.242596537551073,
            "upper_bound": 16.48235995479728
          },
          "point_estimate": 16.37035548266951,
          "standard_error": 0.061273295263308186
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.28233876546215,
            "upper_bound": 16.532320364800665
          },
          "point_estimate": 16.377463488762437,
          "standard_error": 0.07129172112403598
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0281460782958573,
            "upper_bound": 0.33226514412522734
          },
          "point_estimate": 0.14847229577113985,
          "standard_error": 0.07608376663493487
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.321895087199888,
            "upper_bound": 16.52576396136321
          },
          "point_estimate": 16.405245146580572,
          "standard_error": 0.05250692962287269
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.093920418981517,
            "upper_bound": 0.2784253540109925
          },
          "point_estimate": 0.2038835543468727,
          "standard_error": 0.05128448939886934
        }
      }
    },
    "memrchr3/krate/tiny/uncommon": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrchr3/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/uncommon",
        "directory_name": "memrchr3/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.07530307852811,
            "upper_bound": 87.13917917571472
          },
          "point_estimate": 87.1055920675039,
          "standard_error": 0.01632850352853089
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.05120004471968,
            "upper_bound": 87.1342731454308
          },
          "point_estimate": 87.1138240120743,
          "standard_error": 0.025030354963580102
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0020065053607060492,
            "upper_bound": 0.10801310158848794
          },
          "point_estimate": 0.049855376916359315,
          "standard_error": 0.026228650569184688
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.06122942383054,
            "upper_bound": 87.12626017397095
          },
          "point_estimate": 87.08781878447607,
          "standard_error": 0.01653715425429937
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030415408894722464,
            "upper_bound": 0.07147949513762654
          },
          "point_estimate": 0.054472517758825896,
          "standard_error": 0.011565352369483436
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1382512.034723692,
            "upper_bound": 1384205.5475249854
          },
          "point_estimate": 1383352.6426352146,
          "standard_error": 431.154089208201
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1382519.6898148148,
            "upper_bound": 1384228.8827160494
          },
          "point_estimate": 1383340.4203703704,
          "standard_error": 497.7900574404275
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.29294494457994,
            "upper_bound": 2451.6355531414274
          },
          "point_estimate": 1152.668627591583,
          "standard_error": 549.8499916505522
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1382533.5041887125,
            "upper_bound": 1383529.7162479134
          },
          "point_estimate": 1383006.9736411737,
          "standard_error": 250.53252916235908
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 638.9117087793979,
            "upper_bound": 1935.429178075155
          },
          "point_estimate": 1438.226514540435,
          "standard_error": 339.4302515552439
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1539907.4367078373,
            "upper_bound": 1542954.11600897
          },
          "point_estimate": 1541084.274353505,
          "standard_error": 844.109327949719
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1539736.7777777778,
            "upper_bound": 1541141.6119791665
          },
          "point_estimate": 1540337.7337962962,
          "standard_error": 322.90370490306714
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.404418869291646,
            "upper_bound": 1765.1537320306156
          },
          "point_estimate": 508.44496277879296,
          "standard_error": 515.0455358241552
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1539771.5592850782,
            "upper_bound": 1540619.1941535776
          },
          "point_estimate": 1540151.1524891774,
          "standard_error": 213.42280142953
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397.10186889349905,
            "upper_bound": 4276.662643766643
          },
          "point_estimate": 2807.5473690382146,
          "standard_error": 1275.8030475293472
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1444430.657412317,
            "upper_bound": 1447260.8999136903
          },
          "point_estimate": 1445690.4029029305,
          "standard_error": 729.2982149857053
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1443816.4835164836,
            "upper_bound": 1446615.4871794872
          },
          "point_estimate": 1445497.814102564,
          "standard_error": 693.948419112209
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 362.06802049503153,
            "upper_bound": 3357.5624273142257
          },
          "point_estimate": 1883.7454329031111,
          "standard_error": 727.5676451813507
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1443613.5101389275,
            "upper_bound": 1446423.708701668
          },
          "point_estimate": 1445028.863136863,
          "standard_error": 753.2998963726882
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 962.5968630367036,
            "upper_bound": 3451.643141577856
          },
          "point_estimate": 2430.391994291532,
          "standard_error": 732.5019435260087
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 785841.5265197568,
            "upper_bound": 786332.2742863476
          },
          "point_estimate": 786072.5120314083,
          "standard_error": 125.98456892492752
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 785718.244680851,
            "upper_bound": 786307.7774822696
          },
          "point_estimate": 786074.9290780141,
          "standard_error": 155.5236278049976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.94189180720622,
            "upper_bound": 689.7323207335581
          },
          "point_estimate": 424.7827677776128,
          "standard_error": 151.20212297972557
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 785754.2558275519,
            "upper_bound": 786165.9461366181
          },
          "point_estimate": 785962.9680022106,
          "standard_error": 105.3115980525747
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 211.7272051679295,
            "upper_bound": 561.8512131317686
          },
          "point_estimate": 420.8137971594616,
          "standard_error": 96.61738378181046
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282475.6035852713,
            "upper_bound": 282773.82024492434
          },
          "point_estimate": 282620.9798544973,
          "standard_error": 75.81824856670413
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282492.7136089578,
            "upper_bound": 282730.1201550388
          },
          "point_estimate": 282635.2796234773,
          "standard_error": 64.46510702650308
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.204769550764652,
            "upper_bound": 423.4326595368607
          },
          "point_estimate": 184.5250823054229,
          "standard_error": 88.07889188141174
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282431.13178450515,
            "upper_bound": 282709.0910481807
          },
          "point_estimate": 282575.85764622974,
          "standard_error": 71.60252976714948
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.04425438421393,
            "upper_bound": 350.4892382361035
          },
          "point_estimate": 251.56243776367677,
          "standard_error": 69.19607108869015
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498088.0595962165,
            "upper_bound": 498816.0174679278
          },
          "point_estimate": 498480.9141547076,
          "standard_error": 186.74315319660687
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498113.7922374429,
            "upper_bound": 498961.86692759296
          },
          "point_estimate": 498577.78219178086,
          "standard_error": 193.5872647887595
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134.15778059768502,
            "upper_bound": 957.5451717672474
          },
          "point_estimate": 474.9007441373222,
          "standard_error": 229.41207988476788
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498450.25054395746,
            "upper_bound": 498947.7570175231
          },
          "point_estimate": 498706.0568582103,
          "standard_error": 127.20017293830438
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277.82188097810337,
            "upper_bound": 844.2931977354611
          },
          "point_estimate": 626.2233539487328,
          "standard_error": 157.19037145724582
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257481.25251027,
            "upper_bound": 257838.0898172368
          },
          "point_estimate": 257657.34249720545,
          "standard_error": 91.38030254193924
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257373.4647887324,
            "upper_bound": 257911.1552230047
          },
          "point_estimate": 257637.13399843505,
          "standard_error": 128.10521573193046
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.92657899092984,
            "upper_bound": 545.7445572711812
          },
          "point_estimate": 382.24455166452543,
          "standard_error": 120.87099095604994
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257387.6995504672,
            "upper_bound": 257891.7853057583
          },
          "point_estimate": 257619.64143040057,
          "standard_error": 131.23324040343238
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.2174700416341,
            "upper_bound": 373.18855574739314
          },
          "point_estimate": 305.2689489588857,
          "standard_error": 49.1307216598974
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 616440.571376463,
            "upper_bound": 616868.7019034168
          },
          "point_estimate": 616616.2983017218,
          "standard_error": 113.75783706199502
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 616401.6362523541,
            "upper_bound": 616673.1115819209
          },
          "point_estimate": 616535.2944915255,
          "standard_error": 65.68652854381723
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.083575660401134,
            "upper_bound": 351.27358444159483
          },
          "point_estimate": 128.02843094741073,
          "standard_error": 92.73558486907424
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 616356.8486842106,
            "upper_bound": 616587.6807771807
          },
          "point_estimate": 616444.9847677746,
          "standard_error": 59.73808870857322
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.29896433058184,
            "upper_bound": 565.1256830749162
          },
          "point_estimate": 378.5754480575676,
          "standard_error": 149.86199810811146
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2082.8965585497212,
            "upper_bound": 2087.851343731026
          },
          "point_estimate": 2085.949152092435,
          "standard_error": 1.378021324804478
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2086.4781872714575,
            "upper_bound": 2087.525781324897
          },
          "point_estimate": 2087.21846419327,
          "standard_error": 0.4187540241071643
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11553837681998694,
            "upper_bound": 2.469607042724509
          },
          "point_estimate": 0.5197556882126172,
          "standard_error": 0.6581939753374413
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2078.046498877441,
            "upper_bound": 2087.2942581355164
          },
          "point_estimate": 2083.467212143623,
          "standard_error": 2.8503837693761924
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4222337615105864,
            "upper_bound": 7.0293950338413875
          },
          "point_estimate": 4.591317555980501,
          "standard_error": 2.20010639539899
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 645.9913538149898,
            "upper_bound": 646.4292278214103
          },
          "point_estimate": 646.1914449611537,
          "standard_error": 0.11313961848448888
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 645.9236545110243,
            "upper_bound": 646.4331843704467
          },
          "point_estimate": 646.1014378028123,
          "standard_error": 0.11959560363713692
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04857434968310104,
            "upper_bound": 0.5547122238844828
          },
          "point_estimate": 0.23951599667172271,
          "standard_error": 0.13736631917665043
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 645.9458885653605,
            "upper_bound": 646.1518814338344
          },
          "point_estimate": 646.024745477345,
          "standard_error": 0.05273904698676769
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13502490387963922,
            "upper_bound": 0.5018014166071192
          },
          "point_estimate": 0.3773034902331279,
          "standard_error": 0.0972130342762596
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.93528251715202,
            "upper_bound": 125.0246377247533
          },
          "point_estimate": 124.97342329072724,
          "standard_error": 0.02324668334720337
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.93223040774444,
            "upper_bound": 124.98605730171116
          },
          "point_estimate": 124.95952972112312,
          "standard_error": 0.014889551301198728
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00868494624362793,
            "upper_bound": 0.08224958223550045
          },
          "point_estimate": 0.0327326483653968,
          "standard_error": 0.01834146452715683
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.9440107900806,
            "upper_bound": 124.96801235535096
          },
          "point_estimate": 124.9547042535076,
          "standard_error": 0.006100571460075657
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02001615731109714,
            "upper_bound": 0.11449390159829528
          },
          "point_estimate": 0.0777225686559139,
          "standard_error": 0.028517503618270032
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.84446034625418,
            "upper_bound": 48.86829315827818
          },
          "point_estimate": 48.85553435434021,
          "standard_error": 0.006082457978912323
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.84363357437336,
            "upper_bound": 48.86470883770165
          },
          "point_estimate": 48.8539836981174,
          "standard_error": 0.005165089610294797
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0037672654698574833,
            "upper_bound": 0.029820272826903423
          },
          "point_estimate": 0.015623092427892431,
          "standard_error": 0.006738347303595943
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.83960622039762,
            "upper_bound": 48.85618709376422
          },
          "point_estimate": 48.848624376766416,
          "standard_error": 0.004252978364784416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007743231786146887,
            "upper_bound": 0.028532340353092815
          },
          "point_estimate": 0.02028076408746656,
          "standard_error": 0.005690015759495621
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.46016463872596,
            "upper_bound": 77.51813331422383
          },
          "point_estimate": 77.48548510442967,
          "standard_error": 0.014750875798906712
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.45556583252704,
            "upper_bound": 77.5247228947245
          },
          "point_estimate": 77.46363745828236,
          "standard_error": 0.013839299453449291
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003050860218925872,
            "upper_bound": 0.06952422467175647
          },
          "point_estimate": 0.01821408378536571,
          "standard_error": 0.01354331322682556
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.45761130312012,
            "upper_bound": 77.48632793921509
          },
          "point_estimate": 77.46886152000191,
          "standard_error": 0.0073957312421832596
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008971832445765502,
            "upper_bound": 0.06210248328067806
          },
          "point_estimate": 0.049268544179659625,
          "standard_error": 0.014322356759050482
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300843.4441091598,
            "upper_bound": 301240.0391563114
          },
          "point_estimate": 301017.1359507412,
          "standard_error": 102.0659906335002
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300850.9276285583,
            "upper_bound": 301084.14462809917
          },
          "point_estimate": 300961.6838842975,
          "standard_error": 72.92264496576495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.798310038536483,
            "upper_bound": 399.06987704731984
          },
          "point_estimate": 168.78726791252416,
          "standard_error": 94.21550797327492
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300760.30363926344,
            "upper_bound": 300988.8883484948
          },
          "point_estimate": 300875.10415369755,
          "standard_error": 57.24831363585334
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.05983933166766,
            "upper_bound": 497.735933207191
          },
          "point_estimate": 342.2693838598981,
          "standard_error": 118.82734406224948
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.49419887317383,
            "upper_bound": 58.52045723882065
          },
          "point_estimate": 58.50592771109801,
          "standard_error": 0.006745715342095599
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.49087036924233,
            "upper_bound": 58.51412632646001
          },
          "point_estimate": 58.50449130353553,
          "standard_error": 0.00652725534549815
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003998915279603583,
            "upper_bound": 0.031052426562797532
          },
          "point_estimate": 0.015286469002468188,
          "standard_error": 0.006838174354882602
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.489629524987606,
            "upper_bound": 58.50760749138084
          },
          "point_estimate": 58.49852704911122,
          "standard_error": 0.004750277196769533
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008913083043086547,
            "upper_bound": 0.03178040129074152
          },
          "point_estimate": 0.02243971109931744,
          "standard_error": 0.0067427249101348905
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.48100067089764,
            "upper_bound": 96.57738659597663
          },
          "point_estimate": 96.51857405519338,
          "standard_error": 0.02650994653805288
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.4725236481286,
            "upper_bound": 96.5184419240712
          },
          "point_estimate": 96.49275200214755,
          "standard_error": 0.012575115065811358
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002655496404318364,
            "upper_bound": 0.05708549244549328
          },
          "point_estimate": 0.029354178741502004,
          "standard_error": 0.01569291479142614
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.48674368442394,
            "upper_bound": 96.50934539536787
          },
          "point_estimate": 96.49791777067917,
          "standard_error": 0.005865593820385603
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015863688354047865,
            "upper_bound": 0.13438301950563877
          },
          "point_estimate": 0.08805647584423619,
          "standard_error": 0.03972078110777883
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135061.87125295037,
            "upper_bound": 135316.20222251728
          },
          "point_estimate": 135195.81297427273,
          "standard_error": 65.14953501661032
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135038.27695167286,
            "upper_bound": 135359.87217206584
          },
          "point_estimate": 135269.85631970258,
          "standard_error": 99.5449130453284
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.66499277271375,
            "upper_bound": 377.3401569068218
          },
          "point_estimate": 216.73390383497517,
          "standard_error": 90.90324037586808
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135141.06263132373,
            "upper_bound": 135319.16145432042
          },
          "point_estimate": 135241.24009076424,
          "standard_error": 46.38514250145254
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.7634386428818,
            "upper_bound": 278.750556319199
          },
          "point_estimate": 216.67395351417423,
          "standard_error": 44.06422381575024
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.998821284907606,
            "upper_bound": 58.03247888682548
          },
          "point_estimate": 58.01358830121954,
          "standard_error": 0.00874763882243326
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.99878249472714,
            "upper_bound": 58.02459867197197
          },
          "point_estimate": 58.005101689443954,
          "standard_error": 0.006478283576497362
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002613353203104084,
            "upper_bound": 0.03731448678477118
          },
          "point_estimate": 0.012773446945128311,
          "standard_error": 0.009048230373303082
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.999878475643065,
            "upper_bound": 58.01718855568956
          },
          "point_estimate": 58.00612722695321,
          "standard_error": 0.004444811820342448
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008198418850872764,
            "upper_bound": 0.04169918068354284
          },
          "point_estimate": 0.029119733648319303,
          "standard_error": 0.009601286421454152
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.64603683629812,
            "upper_bound": 92.67450416406844
          },
          "point_estimate": 92.65977561541564,
          "standard_error": 0.007278344603246274
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.6410206426986,
            "upper_bound": 92.67738886686544
          },
          "point_estimate": 92.65770805568934,
          "standard_error": 0.009064217060863964
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00821554169630713,
            "upper_bound": 0.041444413300546
          },
          "point_estimate": 0.022623317139425284,
          "standard_error": 0.008853471719183617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.64914453178672,
            "upper_bound": 92.67169067214591
          },
          "point_estimate": 92.65955042462882,
          "standard_error": 0.00575753276847605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012838292952190886,
            "upper_bound": 0.031085659192085897
          },
          "point_estimate": 0.0242149020703767,
          "standard_error": 0.004760423086141999
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.94349540713188,
            "upper_bound": 240.15504976368973
          },
          "point_estimate": 240.03251932521135,
          "standard_error": 0.05575160366174776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.9147621751032,
            "upper_bound": 240.09720058444017
          },
          "point_estimate": 239.97310986638857,
          "standard_error": 0.03537516371996576
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00328536335739465,
            "upper_bound": 0.18150854333475905
          },
          "point_estimate": 0.060555447355126635,
          "standard_error": 0.04985959444375129
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.95355544241,
            "upper_bound": 240.03325933237153
          },
          "point_estimate": 239.9832164521597,
          "standard_error": 0.02043437861455149
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03218064899966733,
            "upper_bound": 0.26565471397285534
          },
          "point_estimate": 0.18558480066767324,
          "standard_error": 0.06693353333682214
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481.9508023638283,
            "upper_bound": 482.30694162069386
          },
          "point_estimate": 482.1083477287592,
          "standard_error": 0.09217336139309994
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481.8916849326227,
            "upper_bound": 482.2747189142475
          },
          "point_estimate": 482.0286356041216,
          "standard_error": 0.08535296363195154
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04953419853179948,
            "upper_bound": 0.4219377345290522
          },
          "point_estimate": 0.1831274948590965,
          "standard_error": 0.09736946177320312
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481.91683730502893,
            "upper_bound": 482.0629615059776
          },
          "point_estimate": 481.9899671720855,
          "standard_error": 0.03698692218023617
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.095908033424501,
            "upper_bound": 0.4224694707317505
          },
          "point_estimate": 0.30676126377589286,
          "standard_error": 0.09072444255192988
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.344826332069285,
            "upper_bound": 31.384445957558096
          },
          "point_estimate": 31.36055971458241,
          "standard_error": 0.01077015775548213
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.339881824817937,
            "upper_bound": 31.36208842015416
          },
          "point_estimate": 31.351461239574892,
          "standard_error": 0.00831698368420317
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013921824154071597,
            "upper_bound": 0.03029336527357651
          },
          "point_estimate": 0.015694717871255273,
          "standard_error": 0.008002499817930851
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.340967639219077,
            "upper_bound": 31.359201790024088
          },
          "point_estimate": 31.349836776090974,
          "standard_error": 0.004769353635833877
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008678008921198304,
            "upper_bound": 0.054547790969655216
          },
          "point_estimate": 0.03589627689206495,
          "standard_error": 0.015580041820710022
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/bstr_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33538.40214438784,
            "upper_bound": 33574.403213992584
          },
          "point_estimate": 33555.44784179014,
          "standard_error": 9.21789688969448
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33532.9621421976,
            "upper_bound": 33572.115856160875
          },
          "point_estimate": 33553.891331948296,
          "standard_error": 8.999690622698491
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.8863975240625805,
            "upper_bound": 49.401594013681034
          },
          "point_estimate": 23.3569046268852,
          "standard_error": 11.93055023086009
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33547.68179415114,
            "upper_bound": 33569.24553125603
          },
          "point_estimate": 33558.26606947992,
          "standard_error": 5.670273773495971
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.5055385866495,
            "upper_bound": 41.66730464129339
          },
          "point_estimate": 30.788895480569096,
          "standard_error": 7.484479532130191
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/bstr_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.71607034481332,
            "upper_bound": 127.5529935346666
          },
          "point_estimate": 126.52762253213872,
          "standard_error": 0.4760950381218031
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.4842716795768,
            "upper_bound": 127.52178253798316
          },
          "point_estimate": 125.97814957094508,
          "standard_error": 0.4063365019916261
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1203361913686537,
            "upper_bound": 2.1620695703230983
          },
          "point_estimate": 0.6437943814792474,
          "standard_error": 0.43696037161919143
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.68602728602616,
            "upper_bound": 127.71441325696124
          },
          "point_estimate": 126.49228901613182,
          "standard_error": 0.5802244420636015
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3006852891748899,
            "upper_bound": 2.065039835725059
          },
          "point_estimate": 1.587434651022522,
          "standard_error": 0.47471322374238273
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/bstr_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104935.2053679654,
            "upper_bound": 1105653.349901515
          },
          "point_estimate": 1105282.7292736892,
          "standard_error": 184.2559649676689
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104709.6536796538,
            "upper_bound": 1105980.4848484849
          },
          "point_estimate": 1105157.7727272727,
          "standard_error": 313.76993905073743
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.30058421979972,
            "upper_bound": 987.1179506568996
          },
          "point_estimate": 672.9078425987864,
          "standard_error": 254.7322265727695
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104805.01460387,
            "upper_bound": 1105539.4915217154
          },
          "point_estimate": 1105116.4559622195,
          "standard_error": 190.80543254435415
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.40070757999683,
            "upper_bound": 722.7506621347438
          },
          "point_estimate": 614.1937455025427,
          "standard_error": 93.16251236928204
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/bstr_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2231.7098487642934,
            "upper_bound": 2232.9356798009867
          },
          "point_estimate": 2232.242157275181,
          "standard_error": 0.3179849610926511
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2231.509498340096,
            "upper_bound": 2232.5953522685354
          },
          "point_estimate": 2232.043442939699,
          "standard_error": 0.2871361621700505
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.18220092723389197,
            "upper_bound": 1.2953608862610584
          },
          "point_estimate": 0.8049435028616118,
          "standard_error": 0.2741667703660767
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2231.420449559017,
            "upper_bound": 2232.407619192917
          },
          "point_estimate": 2231.9626907213037,
          "standard_error": 0.26094186657420204
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3712758255384915,
            "upper_bound": 1.530161448454958
          },
          "point_estimate": 1.056265781584051,
          "standard_error": 0.3561673761334794
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.72235927781886,
            "upper_bound": 41.74570903385057
          },
          "point_estimate": 41.732399617445026,
          "standard_error": 0.006087568045414105
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.72077134100848,
            "upper_bound": 41.74096142147128
          },
          "point_estimate": 41.72416711076713,
          "standard_error": 0.004966621054317431
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017019979725700033,
            "upper_bound": 0.024228681977087208
          },
          "point_estimate": 0.00838079707576231,
          "standard_error": 0.006067621251660261
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.72069798713903,
            "upper_bound": 41.73806912535749
          },
          "point_estimate": 41.7269962078417,
          "standard_error": 0.004738764486586831
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0049621886398670155,
            "upper_bound": 0.028886059586151383
          },
          "point_estimate": 0.020234639474874107,
          "standard_error": 0.006864554767376955
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.51456869395824,
            "upper_bound": 45.671615390245954
          },
          "point_estimate": 45.60446454500173,
          "standard_error": 0.039896885469570806
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.51402126549736,
            "upper_bound": 45.66908156756784
          },
          "point_estimate": 45.655432083109424,
          "standard_error": 0.03222551940382919
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0031597050518604457,
            "upper_bound": 0.202692901573304
          },
          "point_estimate": 0.01541674654088628,
          "standard_error": 0.03720016266211869
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.40884579488062,
            "upper_bound": 45.65713202354107
          },
          "point_estimate": 45.51284056331693,
          "standard_error": 0.06734166144191309
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010079520488075798,
            "upper_bound": 0.16961479410913108
          },
          "point_estimate": 0.13366794166374785,
          "standard_error": 0.041292618053759694
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.635497676328,
            "upper_bound": 33.65271369008897
          },
          "point_estimate": 33.642421328012034,
          "standard_error": 0.004632003370291015
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.63304638357869,
            "upper_bound": 33.64445187364987
          },
          "point_estimate": 33.63906042230935,
          "standard_error": 0.003337856969634223
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003204113938370542,
            "upper_bound": 0.012143446250535846
          },
          "point_estimate": 0.008566574633904629,
          "standard_error": 0.003790462045761497
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.63537135452004,
            "upper_bound": 33.64283364964309
          },
          "point_estimate": 33.63946758530839,
          "standard_error": 0.001879309114942868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00350086677777925,
            "upper_bound": 0.023310162204529136
          },
          "point_estimate": 0.015463910573350607,
          "standard_error": 0.006480904576104591
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.63324439033339,
            "upper_bound": 37.65204220845736
          },
          "point_estimate": 37.642128094044764,
          "standard_error": 0.004826597277149583
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.632179101033174,
            "upper_bound": 37.65391826657554
          },
          "point_estimate": 37.64077912282882,
          "standard_error": 0.004949910464617923
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009610593711427204,
            "upper_bound": 0.027586431532239764
          },
          "point_estimate": 0.012170896437057712,
          "standard_error": 0.006794464642520771
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.63409702523706,
            "upper_bound": 37.64263459040633
          },
          "point_estimate": 37.63803389469449,
          "standard_error": 0.0022202965788885895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005623085345083892,
            "upper_bound": 0.020880503380566517
          },
          "point_estimate": 0.016070909544654188,
          "standard_error": 0.003666883361683512
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.42600647169799,
            "upper_bound": 44.46466902003615
          },
          "point_estimate": 44.441858515172854,
          "standard_error": 0.010235748872777834
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.42215775593404,
            "upper_bound": 44.44635669091168
          },
          "point_estimate": 44.43345292406907,
          "standard_error": 0.007443678812691469
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010240936376655935,
            "upper_bound": 0.03309795834541913
          },
          "point_estimate": 0.017787387652172547,
          "standard_error": 0.007946509318473637
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.42525898112832,
            "upper_bound": 44.44324634669676
          },
          "point_estimate": 44.435804256148,
          "standard_error": 0.004627391506577015
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008948384057383164,
            "upper_bound": 0.051233344243140994
          },
          "point_estimate": 0.03420618914794738,
          "standard_error": 0.013571333762703494
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.40089614629497,
            "upper_bound": 70.52048209185033
          },
          "point_estimate": 70.45246851868097,
          "standard_error": 0.030867361374164497
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.38990455120468,
            "upper_bound": 70.48019360006819
          },
          "point_estimate": 70.43092193983429,
          "standard_error": 0.02565545334955914
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014240275767960818,
            "upper_bound": 0.11962247085634946
          },
          "point_estimate": 0.062203592206032186,
          "standard_error": 0.02646730706779173
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.40742289671383,
            "upper_bound": 70.45979939373117
          },
          "point_estimate": 70.43725778496068,
          "standard_error": 0.013417043923259816
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03333809850369911,
            "upper_bound": 0.14906759270531236
          },
          "point_estimate": 0.10264501660896154,
          "standard_error": 0.035316877271165795
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.04999481986059,
            "upper_bound": 78.10596253515533
          },
          "point_estimate": 78.07320130469587,
          "standard_error": 0.014847487048370394
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.05059934018907,
            "upper_bound": 78.08318462540343
          },
          "point_estimate": 78.05855020502801,
          "standard_error": 0.007966470485506513
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004232885152092618,
            "upper_bound": 0.05112496203701022
          },
          "point_estimate": 0.01481523065648212,
          "standard_error": 0.012215574897907151
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.04454758064702,
            "upper_bound": 78.07420643460122
          },
          "point_estimate": 78.05956936170165,
          "standard_error": 0.007403682314521837
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009242093915140278,
            "upper_bound": 0.07346734520814828
          },
          "point_estimate": 0.04961346680239224,
          "standard_error": 0.01931852828170113
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.49285633097277,
            "upper_bound": 58.5253857448037
          },
          "point_estimate": 58.50678121367923,
          "standard_error": 0.008500829489434689
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.489671654064075,
            "upper_bound": 58.52107306821492
          },
          "point_estimate": 58.49560053582249,
          "standard_error": 0.00809479129957904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002902164032672262,
            "upper_bound": 0.03714579478428631
          },
          "point_estimate": 0.013982976677662856,
          "standard_error": 0.008385120641122118
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.49362894165994,
            "upper_bound": 58.51373946223985
          },
          "point_estimate": 58.50347295121679,
          "standard_error": 0.005452616817485116
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008574922760511191,
            "upper_bound": 0.040749202910942826
          },
          "point_estimate": 0.02818829064623672,
          "standard_error": 0.009592447014904623
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.48482803154349,
            "upper_bound": 96.61520034504272
          },
          "point_estimate": 96.5343940627125,
          "standard_error": 0.03658608259881485
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.47844718382866,
            "upper_bound": 96.53251677322989
          },
          "point_estimate": 96.49720600063176,
          "standard_error": 0.014181435251397062
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007462842668787635,
            "upper_bound": 0.06694642487064847
          },
          "point_estimate": 0.02027877099163233,
          "standard_error": 0.017950257637164044
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.48665953634487,
            "upper_bound": 96.5084929879695
          },
          "point_estimate": 96.49609104446743,
          "standard_error": 0.0054654987562772865
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01484640109880489,
            "upper_bound": 0.18597031896802235
          },
          "point_estimate": 0.12168592811872277,
          "standard_error": 0.05779438712881071
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.341505921539465,
            "upper_bound": 59.431485826901536
          },
          "point_estimate": 59.37477763152212,
          "standard_error": 0.02578037307505049
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.341182417094274,
            "upper_bound": 59.36110746379265
          },
          "point_estimate": 59.35082658035043,
          "standard_error": 0.007097309561563772
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004090609661120262,
            "upper_bound": 0.03211135309676822
          },
          "point_estimate": 0.011580958505661631,
          "standard_error": 0.010975239899301737
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.344324076055855,
            "upper_bound": 59.36337328447076
          },
          "point_estimate": 59.354132867110025,
          "standard_error": 0.005079302190855989
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006815789263312795,
            "upper_bound": 0.13270031557180045
          },
          "point_estimate": 0.08643770615453077,
          "standard_error": 0.04367269224889275
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.94672716869462,
            "upper_bound": 58.01126163820733
          },
          "point_estimate": 57.98372523390393,
          "standard_error": 0.016857817237351774
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.96423774889951,
            "upper_bound": 58.01343881843074
          },
          "point_estimate": 57.99854334346058,
          "standard_error": 0.010349335003247985
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0056685864777784875,
            "upper_bound": 0.06713136481413128
          },
          "point_estimate": 0.018438401750544925,
          "standard_error": 0.01608958554231133
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.99566813043728,
            "upper_bound": 58.02021346903319
          },
          "point_estimate": 58.00847851605225,
          "standard_error": 0.006123059731531846
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012059796767373052,
            "upper_bound": 0.08116713469991854
          },
          "point_estimate": 0.05640680491442162,
          "standard_error": 0.019776349595737056
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.62762792412164,
            "upper_bound": 92.69578722908524
          },
          "point_estimate": 92.6572151106868,
          "standard_error": 0.01773567580470386
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.61560344585968,
            "upper_bound": 92.68373408481136
          },
          "point_estimate": 92.63225478230665,
          "standard_error": 0.01773797017537567
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0054413235935594695,
            "upper_bound": 0.0762136410392067
          },
          "point_estimate": 0.03085579830951956,
          "standard_error": 0.018682109439495102
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.61644322624784,
            "upper_bound": 92.65524834735795
          },
          "point_estimate": 92.63080832475768,
          "standard_error": 0.010177213695552537
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018755078729601315,
            "upper_bound": 0.08405600422672563
          },
          "point_estimate": 0.05901180706662169,
          "standard_error": 0.019033908665785932
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1160511.5398003473,
            "upper_bound": 1161913.2965711807
          },
          "point_estimate": 1161167.5105815972,
          "standard_error": 361.8745703297184
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1160416.4446614585,
            "upper_bound": 1162073.09375
          },
          "point_estimate": 1160766.87890625,
          "standard_error": 431.4562285610551
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.3474706438726,
            "upper_bound": 1998.790963628774
          },
          "point_estimate": 595.7654895532182,
          "standard_error": 509.1904592067416
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1160364.9167775058,
            "upper_bound": 1160999.1876233371
          },
          "point_estimate": 1160664.5081168832,
          "standard_error": 162.2360092281365
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 520.0266732218778,
            "upper_bound": 1593.6864064553026
          },
          "point_estimate": 1208.7197774276435,
          "standard_error": 279.20857352495915
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1633258.9034126983,
            "upper_bound": 1639326.441536836
          },
          "point_estimate": 1636157.0360938578,
          "standard_error": 1553.9580377346103
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1632339.7795031057,
            "upper_bound": 1640523.2053140097
          },
          "point_estimate": 1634186.683695652,
          "standard_error": 2053.619866935629
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 382.9587962448677,
            "upper_bound": 8540.28437301736
          },
          "point_estimate": 5090.0808516620455,
          "standard_error": 2156.6479605085733
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1632141.102766521,
            "upper_bound": 1637030.5443333858
          },
          "point_estimate": 1634514.850705816,
          "standard_error": 1234.6389767549758
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2552.632774882089,
            "upper_bound": 6423.66574451653
          },
          "point_estimate": 5177.466125050082,
          "standard_error": 966.6928064116028
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1435036.2396891024,
            "upper_bound": 1438493.0715384614
          },
          "point_estimate": 1436659.8781181318,
          "standard_error": 885.5301926659848
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1434150.5817307692,
            "upper_bound": 1438495.1
          },
          "point_estimate": 1436899.5183150182,
          "standard_error": 1226.1927903997232
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202.97363870422973,
            "upper_bound": 5250.833327452155
          },
          "point_estimate": 3266.7284689269427,
          "standard_error": 1214.7168110480236
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1435354.681572812,
            "upper_bound": 1440656.146606335
          },
          "point_estimate": 1438044.030569431,
          "standard_error": 1430.795847396691
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1548.1256669873046,
            "upper_bound": 3925.54758170405
          },
          "point_estimate": 2944.005644199573,
          "standard_error": 663.2790770180266
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398449.44438060734,
            "upper_bound": 399264.8583700397
          },
          "point_estimate": 398830.5237223947,
          "standard_error": 207.6177741685686
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398479.1044685991,
            "upper_bound": 399106.379981884
          },
          "point_estimate": 398737.301552795,
          "standard_error": 174.26160607966605
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.31627508022538,
            "upper_bound": 989.352634790476
          },
          "point_estimate": 423.5019774270423,
          "standard_error": 221.03445944182033
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398534.43264312414,
            "upper_bound": 399116.3518362233
          },
          "point_estimate": 398758.0285996612,
          "standard_error": 148.66562962439946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233.051872879325,
            "upper_bound": 985.5446450136452
          },
          "point_estimate": 687.7596905291367,
          "standard_error": 199.9081265531904
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 621422.0776862388,
            "upper_bound": 623589.9960451977
          },
          "point_estimate": 622293.8478450363,
          "standard_error": 582.3194614976236
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 621313.4548022598,
            "upper_bound": 622578.9491525424
          },
          "point_estimate": 621630.4915254237,
          "standard_error": 380.2775011625375
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.42022030440314,
            "upper_bound": 1599.8803326134253
          },
          "point_estimate": 760.2244959948479,
          "standard_error": 397.3589850753518
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 621116.6374573138,
            "upper_bound": 622186.4608828881
          },
          "point_estimate": 621564.2853620956,
          "standard_error": 270.11111525488354
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 450.09019660621385,
            "upper_bound": 2928.4158263681147
          },
          "point_estimate": 1940.7529236025723,
          "standard_error": 812.7473696053962
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 379518.7371527777,
            "upper_bound": 380385.9438004299
          },
          "point_estimate": 379940.1348073744,
          "standard_error": 221.1659388169904
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 379409.12916666665,
            "upper_bound": 380316.79530423286
          },
          "point_estimate": 379888.6032986111,
          "standard_error": 207.48284629245205
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.24746478631243,
            "upper_bound": 1224.7985626305135
          },
          "point_estimate": 507.1191168648947,
          "standard_error": 298.3547088556248
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 379672.432860262,
            "upper_bound": 380184.43060185184
          },
          "point_estimate": 379932.4306547619,
          "standard_error": 130.4744262109856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290.53315419570424,
            "upper_bound": 1007.742379727817
          },
          "point_estimate": 736.7127932236154,
          "standard_error": 178.29116072322068
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 471698.800708256,
            "upper_bound": 472410.11280612246
          },
          "point_estimate": 472009.8071660483,
          "standard_error": 183.59561887610184
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 471591.2987012987,
            "upper_bound": 472137.4285714286
          },
          "point_estimate": 471978.71688311687,
          "standard_error": 143.8745221456891
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.95202588882402,
            "upper_bound": 809.6921310796299
          },
          "point_estimate": 287.6109167121058,
          "standard_error": 188.47788711733168
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 471560.75842633133,
            "upper_bound": 472321.03284939687
          },
          "point_estimate": 471859.71634339687,
          "standard_error": 192.85615802073184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217.1847796947617,
            "upper_bound": 885.8430346457473
          },
          "point_estimate": 611.631579307363,
          "standard_error": 203.91543081022104
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 977779.4695833332,
            "upper_bound": 979100.2376436664
          },
          "point_estimate": 978438.12892335,
          "standard_error": 338.0408827210516
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 977205.7993421052,
            "upper_bound": 979520.2255639096
          },
          "point_estimate": 978386.4236842104,
          "standard_error": 610.1729892006258
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.99253709274163,
            "upper_bound": 1876.40063563459
          },
          "point_estimate": 1715.6841277641645,
          "standard_error": 468.6291482822099
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 977950.8341631144,
            "upper_bound": 979638.2475439914
          },
          "point_estimate": 979031.6455912509,
          "standard_error": 442.60549628957807
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 731.6467922794857,
            "upper_bound": 1303.4846019432998
          },
          "point_estimate": 1125.6643253409832,
          "standard_error": 146.67784337627467
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328436.29046975554,
            "upper_bound": 329198.1493822751
          },
          "point_estimate": 328798.9834198485,
          "standard_error": 195.1245251996692
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328352.1745495495,
            "upper_bound": 329311.618018018
          },
          "point_estimate": 328547.5618118118,
          "standard_error": 289.02541735927576
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.42937444305078,
            "upper_bound": 1112.649874300603
          },
          "point_estimate": 558.5294698138716,
          "standard_error": 282.14015791201257
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328378.14341480314,
            "upper_bound": 328926.9421482698
          },
          "point_estimate": 328562.31091611093,
          "standard_error": 141.7156584403598
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 342.7067650159944,
            "upper_bound": 835.6821306356729
          },
          "point_estimate": 652.8969881487877,
          "standard_error": 129.6977911205622
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 835169.7441982322,
            "upper_bound": 837357.3613467712
          },
          "point_estimate": 836082.4363519119,
          "standard_error": 572.8626378842262
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 835055.3328598484,
            "upper_bound": 836703.7878787878
          },
          "point_estimate": 835322.4585858586,
          "standard_error": 380.59772853050265
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.49447827038084,
            "upper_bound": 2047.958773338393
          },
          "point_estimate": 499.0629405338344,
          "standard_error": 470.34370514111663
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 834985.3363807246,
            "upper_bound": 835498.6964000528
          },
          "point_estimate": 835185.0546635183,
          "standard_error": 133.120481541145
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269.33152554416057,
            "upper_bound": 2748.973626039903
          },
          "point_estimate": 1914.6522733841252,
          "standard_error": 701.7034618491928
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389645.739905944,
            "upper_bound": 390535.69085313246
          },
          "point_estimate": 390047.1303638973,
          "standard_error": 229.26840097779436
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389495.8335106383,
            "upper_bound": 390421.6840425532
          },
          "point_estimate": 389889.9458797703,
          "standard_error": 279.7568515599523
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.0888303685886,
            "upper_bound": 1136.786571626474
          },
          "point_estimate": 568.94859110486,
          "standard_error": 256.8334202343513
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389585.1798247043,
            "upper_bound": 390191.0042294142
          },
          "point_estimate": 389788.7150594087,
          "standard_error": 156.73780660555877
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.53652089297697,
            "upper_bound": 1060.719816078799
          },
          "point_estimate": 764.6484160370062,
          "standard_error": 211.6499684257077
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174391.02351822736,
            "upper_bound": 174775.4073739842
          },
          "point_estimate": 174585.87279182806,
          "standard_error": 98.18746013979734
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174331.8293460925,
            "upper_bound": 174761.88769651402
          },
          "point_estimate": 174681.95600744284,
          "standard_error": 122.36837475032291
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.36392221801329,
            "upper_bound": 597.8133938842985
          },
          "point_estimate": 230.19028251617564,
          "standard_error": 150.6538011305309
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174543.28354803091,
            "upper_bound": 174779.28451683966
          },
          "point_estimate": 174678.42894426148,
          "standard_error": 59.347934674310366
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.4312370888792,
            "upper_bound": 426.3125619770168
          },
          "point_estimate": 327.6270653318014,
          "standard_error": 65.2373382898078
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230594.37213475737,
            "upper_bound": 230954.56303646776
          },
          "point_estimate": 230763.9716440627,
          "standard_error": 92.36047603324612
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230512.3283227848,
            "upper_bound": 230903.04694092827
          },
          "point_estimate": 230803.321880651,
          "standard_error": 100.8710581398206
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.966664380025852,
            "upper_bound": 497.9150927424831
          },
          "point_estimate": 281.9770652470621,
          "standard_error": 127.3637969258792
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230635.81139240507,
            "upper_bound": 230851.4150090416
          },
          "point_estimate": 230746.75497287523,
          "standard_error": 55.7813343110705
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148.8122113852502,
            "upper_bound": 418.72947420436225
          },
          "point_estimate": 308.4213190877216,
          "standard_error": 75.3823682788604
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172268.774933748,
            "upper_bound": 172651.11019070185
          },
          "point_estimate": 172445.24025069588,
          "standard_error": 97.98573700705208
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172215.9482622433,
            "upper_bound": 172648.11208530806
          },
          "point_estimate": 172382.7029028436,
          "standard_error": 95.58787522624976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.691179988792214,
            "upper_bound": 525.4353044157324
          },
          "point_estimate": 231.35899814374835,
          "standard_error": 120.11943254272155
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172186.76546775122,
            "upper_bound": 172515.25486290763
          },
          "point_estimate": 172354.77506001107,
          "standard_error": 84.66927689328861
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.85198403833934,
            "upper_bound": 431.49122952896874
          },
          "point_estimate": 326.0760911091369,
          "standard_error": 79.15110107634139
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 672559.3552169191,
            "upper_bound": 674251.0732035714
          },
          "point_estimate": 673294.5249105338,
          "standard_error": 439.668279371195
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 672404.1272727273,
            "upper_bound": 673856.8284848485
          },
          "point_estimate": 672945.2113636364,
          "standard_error": 357.77449548852945
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.267295952997486,
            "upper_bound": 1886.7711032302796
          },
          "point_estimate": 786.7955887588258,
          "standard_error": 441.9088043602747
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 672348.3906986829,
            "upper_bound": 673265.8933760836
          },
          "point_estimate": 672834.4595041323,
          "standard_error": 233.52014495399973
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 429.03783598474257,
            "upper_bound": 2092.448262038318
          },
          "point_estimate": 1468.3140152548458,
          "standard_error": 481.13090056746375
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1458.184010516048,
            "upper_bound": 1458.8075354993769
          },
          "point_estimate": 1458.4848042692852,
          "standard_error": 0.1599817427652445
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1458.0267658141586,
            "upper_bound": 1459.1300408883187
          },
          "point_estimate": 1458.218782570352,
          "standard_error": 0.3102892334505443
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06848392326095148,
            "upper_bound": 0.7984519961670014
          },
          "point_estimate": 0.36027211057665315,
          "standard_error": 0.2268266846450489
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1458.1696072783625,
            "upper_bound": 1458.8791642211986
          },
          "point_estimate": 1458.4961717829992,
          "standard_error": 0.18673848318037312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.32541240104979957,
            "upper_bound": 0.6020060647831125
          },
          "point_estimate": 0.5347523269499126,
          "standard_error": 0.07478772872822546
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1383555.793658179,
            "upper_bound": 1385089.2554984568
          },
          "point_estimate": 1384235.201574074,
          "standard_error": 394.2281957309489
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1383666.4166666665,
            "upper_bound": 1384490.6604938272
          },
          "point_estimate": 1383977.8689814815,
          "standard_error": 209.2399648409296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.29043182785396,
            "upper_bound": 1604.7268585474233
          },
          "point_estimate": 477.4887100413909,
          "standard_error": 430.1314468938508
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1382950.7332396084,
            "upper_bound": 1384307.950494866
          },
          "point_estimate": 1383638.327946128,
          "standard_error": 375.3863349429628
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 337.62145620444244,
            "upper_bound": 1909.6201408153236
          },
          "point_estimate": 1321.5389646596516,
          "standard_error": 444.2851061765512
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1539606.847372644,
            "upper_bound": 1541284.9966986608
          },
          "point_estimate": 1540345.0942146166,
          "standard_error": 432.09159987570285
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1539307.681547619,
            "upper_bound": 1540876.1805555555
          },
          "point_estimate": 1540196.8090277778,
          "standard_error": 468.5150633655294
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194.860409040717,
            "upper_bound": 1980.5433582410917
          },
          "point_estimate": 1097.7752655106078,
          "standard_error": 429.6742027515108
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1539592.8486080708,
            "upper_bound": 1540463.1950757576
          },
          "point_estimate": 1540050.6104978356,
          "standard_error": 223.85789518916937
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 577.8204442321585,
            "upper_bound": 2059.8738832712365
          },
          "point_estimate": 1439.7296083514934,
          "standard_error": 451.6310776494344
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1442176.4271794872,
            "upper_bound": 1445309.9201153847
          },
          "point_estimate": 1443836.8790064105,
          "standard_error": 804.7315753833341
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1441945.8596153846,
            "upper_bound": 1445396.4783653845
          },
          "point_estimate": 1444469.3230769231,
          "standard_error": 739.2983561294074
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399.15796743277355,
            "upper_bound": 4481.851963604038
          },
          "point_estimate": 1546.749033309086,
          "standard_error": 1055.8793005451682
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1443157.174905821,
            "upper_bound": 1444633.5310783896
          },
          "point_estimate": 1444058.79000999,
          "standard_error": 380.31784775387246
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 980.1749052251891,
            "upper_bound": 3568.923446285924
          },
          "point_estimate": 2680.2346605595035,
          "standard_error": 649.3028484040209
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 785959.1348024316,
            "upper_bound": 786618.2112929964
          },
          "point_estimate": 786255.175564843,
          "standard_error": 168.40232577020208
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 786051.8372340426,
            "upper_bound": 786345.6037234042
          },
          "point_estimate": 786176.0319148935,
          "standard_error": 80.72098573259338
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.239665438352095,
            "upper_bound": 716.4565795675541
          },
          "point_estimate": 202.9841030452021,
          "standard_error": 156.5257301268177
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 786114.2994417759,
            "upper_bound": 786259.5628783947
          },
          "point_estimate": 786189.4255871788,
          "standard_error": 36.3939310452436
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118.8117409698311,
            "upper_bound": 821.8639489555732
          },
          "point_estimate": 559.6561983201127,
          "standard_error": 193.5214861822899
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282584.50210538943,
            "upper_bound": 283129.6263307493
          },
          "point_estimate": 282821.1591177556,
          "standard_error": 141.15921560353365
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282525.61757105944,
            "upper_bound": 283058.8054263566
          },
          "point_estimate": 282645.272978959,
          "standard_error": 126.97797281860502
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.65562880661986,
            "upper_bound": 572.3558316991138
          },
          "point_estimate": 198.06099020466377,
          "standard_error": 140.22358484316877
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282549.30337107304,
            "upper_bound": 282731.4907276543
          },
          "point_estimate": 282643.56933454145,
          "standard_error": 47.65943603683059
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.70087533724124,
            "upper_bound": 654.6070235001786
          },
          "point_estimate": 470.6691662621578,
          "standard_error": 149.67083998686567
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498306.0606187215,
            "upper_bound": 499147.6115372906
          },
          "point_estimate": 498693.8833751903,
          "standard_error": 215.1339390744532
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498231.5867579909,
            "upper_bound": 499019.2313546423
          },
          "point_estimate": 498584.36187214614,
          "standard_error": 183.9825634124987
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120.7638607327316,
            "upper_bound": 1021.1696730349892
          },
          "point_estimate": 482.04484343969784,
          "standard_error": 238.96405284277267
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498241.2960115553,
            "upper_bound": 498839.75945709506
          },
          "point_estimate": 498556.76847536024,
          "standard_error": 151.746381553183
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 264.3546431598168,
            "upper_bound": 1009.9640425395054
          },
          "point_estimate": 717.3490992984406,
          "standard_error": 206.73817325293433
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257045.9826486698,
            "upper_bound": 257665.57063390064
          },
          "point_estimate": 257362.02905712053,
          "standard_error": 158.087249900761
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 256974.37470657277,
            "upper_bound": 257782.28841940532
          },
          "point_estimate": 257374.3806338028,
          "standard_error": 192.99018168574585
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 133.09867249619057,
            "upper_bound": 897.6797712695619
          },
          "point_estimate": 429.87394588934814,
          "standard_error": 202.89352020153208
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257265.60351380397,
            "upper_bound": 257594.6027709058
          },
          "point_estimate": 257429.3907993415,
          "standard_error": 83.02716737095768
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 285.7939631897824,
            "upper_bound": 675.548901083625
          },
          "point_estimate": 526.4018004209288,
          "standard_error": 101.06369369126692
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 615973.1729841271,
            "upper_bound": 616591.4863968253
          },
          "point_estimate": 616241.4537718253,
          "standard_error": 160.86210058666595
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 615956.5577777778,
            "upper_bound": 616450.138095238
          },
          "point_estimate": 616041.3440972222,
          "standard_error": 121.11118156437406
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.740405996598334,
            "upper_bound": 686.3095301489319
          },
          "point_estimate": 129.17100791503393,
          "standard_error": 173.35740432419806
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 615993.2450819672,
            "upper_bound": 616139.6929550774
          },
          "point_estimate": 616047.0953246753,
          "standard_error": 37.687957107393906
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.14809347247932,
            "upper_bound": 751.1486683700834
          },
          "point_estimate": 536.1911431523336,
          "standard_error": 173.14096604949944
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376.1746858426836,
            "upper_bound": 376.4833999067974
          },
          "point_estimate": 376.3063358505743,
          "standard_error": 0.08076802733137535
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376.1332756689755,
            "upper_bound": 376.3972683307117
          },
          "point_estimate": 376.2066821993306,
          "standard_error": 0.07382793271006388
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014342999217588377,
            "upper_bound": 0.3037144627966493
          },
          "point_estimate": 0.12467385375565336,
          "standard_error": 0.08343362710119108
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376.1504271878037,
            "upper_bound": 376.34609695151886
          },
          "point_estimate": 376.2438484522349,
          "standard_error": 0.05077703457226098
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07221136446789136,
            "upper_bound": 0.3893830430931623
          },
          "point_estimate": 0.2695714730457561,
          "standard_error": 0.09327493272165784
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.5832447085852,
            "upper_bound": 187.699669775419
          },
          "point_estimate": 187.63039622217303,
          "standard_error": 0.03104477123461199
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.56247334021327,
            "upper_bound": 187.6374441004472
          },
          "point_estimate": 187.62014744582044,
          "standard_error": 0.02396359400584298
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005086442118165235,
            "upper_bound": 0.09854368225466008
          },
          "point_estimate": 0.056676170820428334,
          "standard_error": 0.02547687825132916
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.5766291194095,
            "upper_bound": 187.62795774222
          },
          "point_estimate": 187.60303705788536,
          "standard_error": 0.013232104583825198
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02866179470020179,
            "upper_bound": 0.15623530553222642
          },
          "point_estimate": 0.10359602191772307,
          "standard_error": 0.042466519754817134
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.998056669281617,
            "upper_bound": 27.033142084396705
          },
          "point_estimate": 27.010824102420923,
          "standard_error": 0.01022706949211105
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.99571661037035,
            "upper_bound": 27.0051092895972
          },
          "point_estimate": 27.002262495516963,
          "standard_error": 0.003383018287519406
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007048922943322022,
            "upper_bound": 0.011989251355565276
          },
          "point_estimate": 0.005498813083510586,
          "standard_error": 0.004145144116895769
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.998350003198983,
            "upper_bound": 27.00341064150752
          },
          "point_estimate": 27.001118710536765,
          "standard_error": 0.0012904896913842625
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003394954036184402,
            "upper_bound": 0.0522672292119073
          },
          "point_estimate": 0.033959782374779666,
          "standard_error": 0.01740913467118793
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.16016047870302,
            "upper_bound": 18.171164373899117
          },
          "point_estimate": 18.164658095505143,
          "standard_error": 0.0029410649485927852
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.15901482197841,
            "upper_bound": 18.16552142229007
          },
          "point_estimate": 18.16257271514005,
          "standard_error": 0.0020002565753598905
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009063084910619876,
            "upper_bound": 0.008574649333523449
          },
          "point_estimate": 0.004625320851780859,
          "standard_error": 0.0018914927768944577
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.160589154088804,
            "upper_bound": 18.16444574416816
          },
          "point_estimate": 18.16251779842243,
          "standard_error": 0.0009846564222042806
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024998593361858284,
            "upper_bound": 0.014773199591977208
          },
          "point_estimate": 0.00982139564010631,
          "standard_error": 0.004033061464123933
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.730514962718257,
            "upper_bound": 26.754696907430223
          },
          "point_estimate": 26.740994162375717,
          "standard_error": 0.006303244689274309
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.72770961926971,
            "upper_bound": 26.749428037064373
          },
          "point_estimate": 26.732330011953877,
          "standard_error": 0.0071484153358468625
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012998732702248854,
            "upper_bound": 0.028942639800880493
          },
          "point_estimate": 0.011703350993271562,
          "standard_error": 0.006869073127470018
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.72861411154158,
            "upper_bound": 26.74051948749993
          },
          "point_estimate": 26.73291365324583,
          "standard_error": 0.0030637832047469603
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007572909247177239,
            "upper_bound": 0.03039801041149869
          },
          "point_estimate": 0.021046709076518556,
          "standard_error": 0.006991575623896938
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300429.1449687744,
            "upper_bound": 300721.83450819674
          },
          "point_estimate": 300588.89682930004,
          "standard_error": 75.3262184304621
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300432.1006147541,
            "upper_bound": 300768.10655737703
          },
          "point_estimate": 300652.25170765026,
          "standard_error": 71.55510963391212
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.26299809663144,
            "upper_bound": 382.4263336204191
          },
          "point_estimate": 161.67023565434567,
          "standard_error": 88.53355432302197
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300315.9003466955,
            "upper_bound": 300709.62583836383
          },
          "point_estimate": 300507.9703640622,
          "standard_error": 101.98342402658096
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.74038277314706,
            "upper_bound": 338.4735081256502
          },
          "point_estimate": 251.64080373321116,
          "standard_error": 66.79260244803385
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.201179882448535,
            "upper_bound": 16.20994343939251
          },
          "point_estimate": 16.205018816536736,
          "standard_error": 0.0022603366255781452
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.199192657802957,
            "upper_bound": 16.207234641538264
          },
          "point_estimate": 16.20429324282449,
          "standard_error": 0.002072405316619509
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007887352835840651,
            "upper_bound": 0.01005181902430668
          },
          "point_estimate": 0.005043313266727366,
          "standard_error": 0.0021970234813365698
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.202270006096178,
            "upper_bound": 16.205559837618626
          },
          "point_estimate": 16.20387716081493,
          "standard_error": 0.0008403431992984545
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028413950725944097,
            "upper_bound": 0.010891126739106322
          },
          "point_estimate": 0.00757113517092314,
          "standard_error": 0.002453429778731887
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.0168395691168,
            "upper_bound": 24.036952236085604
          },
          "point_estimate": 24.02557191999646,
          "standard_error": 0.005206094336094585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.014830583255364,
            "upper_bound": 24.03248608969568
          },
          "point_estimate": 24.018460718263658,
          "standard_error": 0.004379673175839115
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013321505128482927,
            "upper_bound": 0.021900892232780297
          },
          "point_estimate": 0.00559616691723087,
          "standard_error": 0.005486288180156703
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.01481937698844,
            "upper_bound": 24.020770941379336
          },
          "point_estimate": 24.017373132949476,
          "standard_error": 0.0015405255268032777
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00421779917225111,
            "upper_bound": 0.024385744177441677
          },
          "point_estimate": 0.017422757316934052,
          "standard_error": 0.005547967558475395
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134973.73677022482,
            "upper_bound": 135219.9843938861
          },
          "point_estimate": 135093.5813053933,
          "standard_error": 63.14474873581911
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134894.08443972384,
            "upper_bound": 135300.21982651798
          },
          "point_estimate": 135019.05115654686,
          "standard_error": 116.6447735838565
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.539581642682563,
            "upper_bound": 347.6668068388448
          },
          "point_estimate": 214.95294450472863,
          "standard_error": 81.50868198091464
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134993.79371795134,
            "upper_bound": 135267.47820127668
          },
          "point_estimate": 135109.05675662626,
          "standard_error": 70.55892786407706
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.15095019757405,
            "upper_bound": 243.52292753053737
          },
          "point_estimate": 210.76541825776053,
          "standard_error": 28.818576742735637
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.51854273723339,
            "upper_bound": 24.537583141537063
          },
          "point_estimate": 24.527305789807265,
          "standard_error": 0.004896566612637083
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.518775874214967,
            "upper_bound": 24.53477084176447
          },
          "point_estimate": 24.524563411766973,
          "standard_error": 0.003635435574562135
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002217499960630102,
            "upper_bound": 0.02545546938934696
          },
          "point_estimate": 0.00782894430545271,
          "standard_error": 0.005804304443733047
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.518357589425005,
            "upper_bound": 24.526258462304146
          },
          "point_estimate": 24.52232607511199,
          "standard_error": 0.001984669148586174
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004987255715983278,
            "upper_bound": 0.02261639600959059
          },
          "point_estimate": 0.01628255143918936,
          "standard_error": 0.0046435601202130215
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.35146287734291,
            "upper_bound": 21.364163264279032
          },
          "point_estimate": 21.357232557894484,
          "standard_error": 0.0032831242342537164
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.34971822897696,
            "upper_bound": 21.365809810276904
          },
          "point_estimate": 21.353246673220355,
          "standard_error": 0.003944963026225698
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012863927304985112,
            "upper_bound": 0.018327042011515342
          },
          "point_estimate": 0.005487998799081783,
          "standard_error": 0.004101875592134191
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.35064669253356,
            "upper_bound": 21.359214506744827
          },
          "point_estimate": 21.354272084704824,
          "standard_error": 0.0021743669912480963
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032098728006081352,
            "upper_bound": 0.014617295274877816
          },
          "point_estimate": 0.010942373312279268,
          "standard_error": 0.002821445345862631
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.32642301695039,
            "upper_bound": 47.53677760013682
          },
          "point_estimate": 47.44020238693035,
          "standard_error": 0.05388841867895243
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.32231405786517,
            "upper_bound": 47.56196306287568
          },
          "point_estimate": 47.48081207039145,
          "standard_error": 0.05243860833662311
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.034506886233121176,
            "upper_bound": 0.28224314340711126
          },
          "point_estimate": 0.10235901906397948,
          "standard_error": 0.06507706707651399
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.42118677026703,
            "upper_bound": 47.53115059503955
          },
          "point_estimate": 47.47707290468805,
          "standard_error": 0.02815121239366118
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06542303046949934,
            "upper_bound": 0.2372423721924726
          },
          "point_estimate": 0.17945871376575517,
          "standard_error": 0.04465654921034848
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.53138686263432,
            "upper_bound": 89.66016297284885
          },
          "point_estimate": 89.60775065079197,
          "standard_error": 0.0342339593149825
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.59109289469205,
            "upper_bound": 89.67541270974853
          },
          "point_estimate": 89.63010988233881,
          "standard_error": 0.025572845422082022
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009272779768912925,
            "upper_bound": 0.10841680724438647
          },
          "point_estimate": 0.05979397304788429,
          "standard_error": 0.023917531751877982
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.60514925984116,
            "upper_bound": 89.66999032391807
          },
          "point_estimate": 89.6394109244457,
          "standard_error": 0.017244866560603166
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03090508804028997,
            "upper_bound": 0.17131496206793365
          },
          "point_estimate": 0.11411701383155495,
          "standard_error": 0.04601090314056928
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.797355868906944,
            "upper_bound": 10.80405461363974
          },
          "point_estimate": 10.800414266945005,
          "standard_error": 0.0017099649246313277
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.797651488572589,
            "upper_bound": 10.802482966147574
          },
          "point_estimate": 10.799261320937912,
          "standard_error": 0.0013414164463634937
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007329891284174247,
            "upper_bound": 0.008039471094625583
          },
          "point_estimate": 0.0026861696860961295,
          "standard_error": 0.0017626796703594956
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.798092377643886,
            "upper_bound": 10.801339235434382
          },
          "point_estimate": 10.799582012959872,
          "standard_error": 0.0008410282992331327
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017634638330020042,
            "upper_bound": 0.008189121253288207
          },
          "point_estimate": 0.005677233105381859,
          "standard_error": 0.0017701628741651598
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/bstr_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33321.39367378694,
            "upper_bound": 33437.57119582186
          },
          "point_estimate": 33388.556828056906,
          "standard_error": 29.99455297327667
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33384.76728322183,
            "upper_bound": 33422.365449954086
          },
          "point_estimate": 33407.5302838996,
          "standard_error": 13.1394911332431
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.152795041674979,
            "upper_bound": 103.19142819553302
          },
          "point_estimate": 27.810756584620258,
          "standard_error": 21.488819248169992
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33369.894708407584,
            "upper_bound": 33410.717919541785
          },
          "point_estimate": 33396.17473197143,
          "standard_error": 10.624849837377864
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.526592159106483,
            "upper_bound": 148.25539700783995
          },
          "point_estimate": 100.1788415651611,
          "standard_error": 40.02525124349302
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/bstr_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.704127536561916,
            "upper_bound": 30.222241772700254
          },
          "point_estimate": 29.972948110877365,
          "standard_error": 0.13297603285820045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.42181504880761,
            "upper_bound": 30.374053121616505
          },
          "point_estimate": 30.05234704443373,
          "standard_error": 0.2301359200468062
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0416862441945814,
            "upper_bound": 0.6919979484414256
          },
          "point_estimate": 0.4779173919310076,
          "standard_error": 0.1906312512729848
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.49355801409724,
            "upper_bound": 30.247256751553135
          },
          "point_estimate": 29.839879656659463,
          "standard_error": 0.1989032072070641
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2135707987779492,
            "upper_bound": 0.5143777382273046
          },
          "point_estimate": 0.4429820689286217,
          "standard_error": 0.06753296120874736
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/bstr_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104849.845430255,
            "upper_bound": 1106528.8771780303
          },
          "point_estimate": 1105484.4759752282,
          "standard_error": 473.28666245875047
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104691.2477272726,
            "upper_bound": 1105438.181818182
          },
          "point_estimate": 1104991.4974747477,
          "standard_error": 253.70899594755497
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.063396462009383,
            "upper_bound": 917.1412108386272
          },
          "point_estimate": 470.7467786123202,
          "standard_error": 251.38471622153
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104748.1148983005,
            "upper_bound": 1105306.585393657
          },
          "point_estimate": 1105043.7919716646,
          "standard_error": 146.13662296741285
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 244.07637544694973,
            "upper_bound": 2428.040523915014
          },
          "point_estimate": 1584.1265238938684,
          "standard_error": 761.5952778972942
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/bstr_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2206.165654664011,
            "upper_bound": 2207.500021963267
          },
          "point_estimate": 2206.7713199778636,
          "standard_error": 0.3433260671898928
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2205.8733982972967,
            "upper_bound": 2207.369796091759
          },
          "point_estimate": 2206.4531091961,
          "standard_error": 0.4522872311407173
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09034212001249108,
            "upper_bound": 1.8617281901846816
          },
          "point_estimate": 0.9032219708563114,
          "standard_error": 0.43857851241462753
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2205.856809564123,
            "upper_bound": 2206.872683155805
          },
          "point_estimate": 2206.230572049628,
          "standard_error": 0.2671946128451838
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5039669709509793,
            "upper_bound": 1.5693887557251998
          },
          "point_estimate": 1.1469686268468895,
          "standard_error": 0.3042715688614037
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.607397071057677,
            "upper_bound": 7.611658745680954
          },
          "point_estimate": 7.609163260173173,
          "standard_error": 0.0011293804392907467
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.607045892244329,
            "upper_bound": 7.610156015247865
          },
          "point_estimate": 7.607648264441918,
          "standard_error": 0.0008131569120075222
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0001850355206661855,
            "upper_bound": 0.0035500177197020652
          },
          "point_estimate": 0.0015494445630739242,
          "standard_error": 0.000928255083870302
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.607353257324379,
            "upper_bound": 7.609309620695629
          },
          "point_estimate": 7.608205784276007,
          "standard_error": 0.0004936219985035283
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008200054997035658,
            "upper_bound": 0.005564355230597275
          },
          "point_estimate": 0.003769054084465151,
          "standard_error": 0.0014451166978311962
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.871033774702887,
            "upper_bound": 6.877895115779878
          },
          "point_estimate": 6.873740643296223,
          "standard_error": 0.0018630590057876024
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.870582988013145,
            "upper_bound": 6.873872477570911
          },
          "point_estimate": 6.872753528164628,
          "standard_error": 0.0009902553672608391
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00027518654175035667,
            "upper_bound": 0.004586956249599441
          },
          "point_estimate": 0.0027871250800732363,
          "standard_error": 0.0011713753556704051
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.87100504389426,
            "upper_bound": 6.873246685003446
          },
          "point_estimate": 6.872026577243985,
          "standard_error": 0.0005783646686855807
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001216947974007435,
            "upper_bound": 0.009385806330028112
          },
          "point_estimate": 0.0061753333228635795,
          "standard_error": 0.002702814539983021
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.099088932792014,
            "upper_bound": 8.104061607728474
          },
          "point_estimate": 8.1013939767798,
          "standard_error": 0.0012786409506891285
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.097813375314777,
            "upper_bound": 8.103801923296285
          },
          "point_estimate": 8.100946097442733,
          "standard_error": 0.0016416721381136284
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007520460658811268,
            "upper_bound": 0.006811983076362717
          },
          "point_estimate": 0.00421526109337328,
          "standard_error": 0.0015777225030090755
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.098526939763307,
            "upper_bound": 8.101702995276591
          },
          "point_estimate": 8.099997091585722,
          "standard_error": 0.0008163325903550884
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019778122562744208,
            "upper_bound": 0.005708997523313232
          },
          "point_estimate": 0.004261392631343506,
          "standard_error": 0.0010332665820803235
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.3881535719595,
            "upper_bound": 19.39906718352939
          },
          "point_estimate": 19.39304461462582,
          "standard_error": 0.0028275029862149604
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.387254019127933,
            "upper_bound": 19.39627495373077
          },
          "point_estimate": 19.39275979866188,
          "standard_error": 0.0024759056326540143
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00034611899565763644,
            "upper_bound": 0.012137458414280655
          },
          "point_estimate": 0.0067506501424042555,
          "standard_error": 0.0028128313220721637
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.38880248777144,
            "upper_bound": 19.39368577156777
          },
          "point_estimate": 19.39104291727341,
          "standard_error": 0.0012578946091164312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0033625333068121863,
            "upper_bound": 0.0134580162555954
          },
          "point_estimate": 0.009418372440780682,
          "standard_error": 0.002973020896623373
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.751219502883004,
            "upper_bound": 13.760545311628697
          },
          "point_estimate": 13.755379571136482,
          "standard_error": 0.0024068324349621286
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.749943435224862,
            "upper_bound": 13.758844220744365
          },
          "point_estimate": 13.754442613095492,
          "standard_error": 0.001999415827327431
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012832521109404718,
            "upper_bound": 0.010982010457849583
          },
          "point_estimate": 0.004649398300570211,
          "standard_error": 0.0027372340709024548
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.751401486226625,
            "upper_bound": 13.757422263554815
          },
          "point_estimate": 13.754560276317005,
          "standard_error": 0.0015109760374032405
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030452205848377407,
            "upper_bound": 0.011337129821383265
          },
          "point_estimate": 0.008058166511945094,
          "standard_error": 0.0024168289752260585
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.63327345031408,
            "upper_bound": 19.644206318896543
          },
          "point_estimate": 19.637928172869685,
          "standard_error": 0.00283504636546974
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.63346846703287,
            "upper_bound": 19.63921206339706
          },
          "point_estimate": 19.63681631794661,
          "standard_error": 0.0014932091995874574
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009487938000981444,
            "upper_bound": 0.01005760899209197
          },
          "point_estimate": 0.003587305496383227,
          "standard_error": 0.002327408746294093
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.633401671436975,
            "upper_bound": 19.638530910377455
          },
          "point_estimate": 19.63590431671196,
          "standard_error": 0.001352199412378871
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0021740005267526312,
            "upper_bound": 0.013856019621739638
          },
          "point_estimate": 0.009392521719706368,
          "standard_error": 0.0034884614407739434
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.798420926820995,
            "upper_bound": 10.80520031413122
          },
          "point_estimate": 10.801541208726745,
          "standard_error": 0.0017471954229465246
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.797758513848514,
            "upper_bound": 10.80543006131343
          },
          "point_estimate": 10.800139368284237,
          "standard_error": 0.0014945255256546975
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000501073406831625,
            "upper_bound": 0.009132266046812892
          },
          "point_estimate": 0.002710555942895559,
          "standard_error": 0.0022613933365861023
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.798257836417696,
            "upper_bound": 10.8018977276467
          },
          "point_estimate": 10.800079745868906,
          "standard_error": 0.0009166232991769018
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019990819633410803,
            "upper_bound": 0.007574082141963698
          },
          "point_estimate": 0.005814797633507129,
          "standard_error": 0.0014493085004476983
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.20885995005197,
            "upper_bound": 16.217558337851152
          },
          "point_estimate": 16.21264459166747,
          "standard_error": 0.0022583456790268575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.20903884206571,
            "upper_bound": 16.214776395271727
          },
          "point_estimate": 16.21039177434312,
          "standard_error": 0.0018262490431707075
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00033199292406854567,
            "upper_bound": 0.008630927415410132
          },
          "point_estimate": 0.0038456778225661615,
          "standard_error": 0.0022761763565373825
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.209037330892887,
            "upper_bound": 16.213209857891968
          },
          "point_estimate": 16.21070384396848,
          "standard_error": 0.0010975751556527016
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022560040421188963,
            "upper_bound": 0.011003469076003818
          },
          "point_estimate": 0.007539703224244227,
          "standard_error": 0.0026423828920083266
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.016970334800252,
            "upper_bound": 24.03506206609734
          },
          "point_estimate": 24.02455818417164,
          "standard_error": 0.004737340165493593
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.016028723382473,
            "upper_bound": 24.025921836604702
          },
          "point_estimate": 24.02326229853299,
          "standard_error": 0.0030923076965074492
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013211190768305677,
            "upper_bound": 0.014735437332888976
          },
          "point_estimate": 0.006524881818883638,
          "standard_error": 0.0037860475145746458
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.017667899611652,
            "upper_bound": 24.024732339717534
          },
          "point_estimate": 24.021652573133323,
          "standard_error": 0.001835560444061411
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003969679055911663,
            "upper_bound": 0.02358107054100749
          },
          "point_estimate": 0.01584653800102996,
          "standard_error": 0.006149195084074231
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.800150464143805,
            "upper_bound": 10.810345409636962
          },
          "point_estimate": 10.80426228642135,
          "standard_error": 0.002727837261533838
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.798668919980855,
            "upper_bound": 10.805293471073387
          },
          "point_estimate": 10.802299236123222,
          "standard_error": 0.0019766088000841477
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009879464326508958,
            "upper_bound": 0.008146258779921684
          },
          "point_estimate": 0.004585742343174756,
          "standard_error": 0.0017841841306740051
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.798851651332624,
            "upper_bound": 10.80293837394503
          },
          "point_estimate": 10.80102066362065,
          "standard_error": 0.001064178660723504
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00236471204502629,
            "upper_bound": 0.01376254999963669
          },
          "point_estimate": 0.009130390868289665,
          "standard_error": 0.003769119342139632
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.529354689778614,
            "upper_bound": 24.59278880232461
          },
          "point_estimate": 24.553467247647095,
          "standard_error": 0.017766686037360636
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.52843353962055,
            "upper_bound": 24.552795942988077
          },
          "point_estimate": 24.53182014620033,
          "standard_error": 0.007871637352606192
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010482871475021173,
            "upper_bound": 0.03319045501828919
          },
          "point_estimate": 0.010121577121336586,
          "standard_error": 0.00972030667778642
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.52902481372118,
            "upper_bound": 24.5410846478348
          },
          "point_estimate": 24.533423106787765,
          "standard_error": 0.0031083047326503992
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007425753512132705,
            "upper_bound": 0.0902842582750779
          },
          "point_estimate": 0.05901309378532084,
          "standard_error": 0.02781930062092451
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.351978145976155,
            "upper_bound": 21.37500761394028
          },
          "point_estimate": 21.361590532462184,
          "standard_error": 0.006216474652254629
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.350479612151233,
            "upper_bound": 21.36805787324714
          },
          "point_estimate": 21.35398341110007,
          "standard_error": 0.003643078396673482
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004522317789592812,
            "upper_bound": 0.020773026793370463
          },
          "point_estimate": 0.003390105343461359,
          "standard_error": 0.004824627801966994
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.351523511443556,
            "upper_bound": 21.366007758280805
          },
          "point_estimate": 21.356655679654683,
          "standard_error": 0.003884937242606335
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0020465550774802464,
            "upper_bound": 0.029951692180166085
          },
          "point_estimate": 0.02078306335768029,
          "standard_error": 0.008038480675642343
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1151622.235828745,
            "upper_bound": 1152634.806420015
          },
          "point_estimate": 1152166.8466617065,
          "standard_error": 258.7751224603155
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1151871.7859375,
            "upper_bound": 1152643.3671875
          },
          "point_estimate": 1152259.1678571429,
          "standard_error": 261.5586983123631
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.38174900978257,
            "upper_bound": 1218.6693796142245
          },
          "point_estimate": 534.6669544661839,
          "standard_error": 305.3239153357134
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1151908.974469564,
            "upper_bound": 1152440.4766757246
          },
          "point_estimate": 1152120.124107143,
          "standard_error": 135.54239285448676
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 320.75619443445356,
            "upper_bound": 1227.4839642602276
          },
          "point_estimate": 863.0180633759095,
          "standard_error": 252.36236760653756
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1551662.3912847224,
            "upper_bound": 1552997.3868402778
          },
          "point_estimate": 1552333.2089699076,
          "standard_error": 341.7635598900102
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1551628.0541666667,
            "upper_bound": 1553322.3578703704
          },
          "point_estimate": 1552087.0494791665,
          "standard_error": 443.72587123882727
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.87884712712876,
            "upper_bound": 2094.5945919800447
          },
          "point_estimate": 976.4087687763966,
          "standard_error": 493.848149638979
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1551580.7878787878,
            "upper_bound": 1552719.3898083623
          },
          "point_estimate": 1552150.3876623376,
          "standard_error": 287.0595539752286
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 617.0677821340751,
            "upper_bound": 1467.3576412825566
          },
          "point_estimate": 1137.6349339556564,
          "standard_error": 218.35760157509705
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1409970.9953846154,
            "upper_bound": 1411458.547286325
          },
          "point_estimate": 1410683.707039072,
          "standard_error": 381.2654941554816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1409398.2211538462,
            "upper_bound": 1411698.701923077
          },
          "point_estimate": 1410377.0598290598,
          "standard_error": 557.5141388144154
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139.9289259771388,
            "upper_bound": 2101.8743796072135
          },
          "point_estimate": 1491.4345905087605,
          "standard_error": 509.60808392130224
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1409965.4478095896,
            "upper_bound": 1411559.1662764924
          },
          "point_estimate": 1410663.0771228771,
          "standard_error": 403.79200665845303
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 713.9726713792652,
            "upper_bound": 1615.507090211743
          },
          "point_estimate": 1276.4540428320415,
          "standard_error": 237.0762848005236
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 375131.9649731529,
            "upper_bound": 376447.0508848797
          },
          "point_estimate": 375797.08112542954,
          "standard_error": 338.0993320821888
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 374800.7169243986,
            "upper_bound": 376859.7113402062
          },
          "point_estimate": 375873.6721649484,
          "standard_error": 689.031029497258
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161.93201465090257,
            "upper_bound": 1654.9932978087493
          },
          "point_estimate": 1465.9602090255592,
          "standard_error": 454.0760766263084
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 375474.8031018376,
            "upper_bound": 376793.7250178626
          },
          "point_estimate": 376353.3514258937,
          "standard_error": 336.6571999276158
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 758.5717578851209,
            "upper_bound": 1284.6383086700143
          },
          "point_estimate": 1125.5785385248785,
          "standard_error": 136.46944465582885
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527575.537068438,
            "upper_bound": 528597.4413848631
          },
          "point_estimate": 528027.3464331722,
          "standard_error": 265.46102160141805
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527461.7612318841,
            "upper_bound": 528542.731884058
          },
          "point_estimate": 527716.0756038647,
          "standard_error": 223.1318365184853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.855664973137635,
            "upper_bound": 1115.366406285192
          },
          "point_estimate": 312.9479664247094,
          "standard_error": 245.8726213426581
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527598.5075428195,
            "upper_bound": 527861.3309355055
          },
          "point_estimate": 527710.1178618483,
          "standard_error": 67.28483531710117
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194.00649924559187,
            "upper_bound": 1182.1875580444398
          },
          "point_estimate": 885.2871749828605,
          "standard_error": 265.7608353802615
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376143.0571620848,
            "upper_bound": 376311.3450478952
          },
          "point_estimate": 376217.78118843073,
          "standard_error": 43.47676152540838
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376111.62313860253,
            "upper_bound": 376279.3962628866
          },
          "point_estimate": 376189.4673539519,
          "standard_error": 37.37681416880359
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.51235654304747,
            "upper_bound": 194.13398418225668
          },
          "point_estimate": 97.7577681957217,
          "standard_error": 45.152578466611665
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376102.506274716,
            "upper_bound": 376228.9831818976
          },
          "point_estimate": 376154.6944437006,
          "standard_error": 32.854981779842305
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.93160423016668,
            "upper_bound": 204.31895275887177
          },
          "point_estimate": 144.91825819713003,
          "standard_error": 44.05060419036202
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464742.4584547292,
            "upper_bound": 465133.05568917014
          },
          "point_estimate": 464897.8790842877,
          "standard_error": 105.16899162572729
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464708.19529837254,
            "upper_bound": 464917.8007911393
          },
          "point_estimate": 464811.8212025316,
          "standard_error": 62.32038250763586
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.20444740519532,
            "upper_bound": 278.948470522359
          },
          "point_estimate": 146.11544048606157,
          "standard_error": 61.41384211037403
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464728.2880656243,
            "upper_bound": 464893.11525942414
          },
          "point_estimate": 464815.27446983394,
          "standard_error": 42.482997010004624
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.7916891105864,
            "upper_bound": 531.9750030718785
          },
          "point_estimate": 350.6029728641754,
          "standard_error": 150.22490863902564
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 981842.3780597328,
            "upper_bound": 982957.955482456
          },
          "point_estimate": 982331.4687740183,
          "standard_error": 288.8222914832206
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 981696.2744360904,
            "upper_bound": 982720.7662280702
          },
          "point_estimate": 982050.1842105264,
          "standard_error": 261.0101952097708
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.13672265410423,
            "upper_bound": 1257.434654518315
          },
          "point_estimate": 574.8697792676678,
          "standard_error": 296.1574865808423
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 981717.2994550952,
            "upper_bound": 982574.8146606464
          },
          "point_estimate": 982056.4928913192,
          "standard_error": 218.9816963432571
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 281.2658781157106,
            "upper_bound": 1352.482234712763
          },
          "point_estimate": 965.2807821811846,
          "standard_error": 297.6714453430434
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282029.3827075549,
            "upper_bound": 282383.19976495724
          },
          "point_estimate": 282182.5625946276,
          "standard_error": 92.27184380629316
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 281973.73824786325,
            "upper_bound": 282271.87019230775
          },
          "point_estimate": 282117.49384615384,
          "standard_error": 66.2929102568965
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.76329934141718,
            "upper_bound": 359.2148708919001
          },
          "point_estimate": 144.31416343792895,
          "standard_error": 90.22805089431132
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 281968.50706845894,
            "upper_bound": 282166.19066551427
          },
          "point_estimate": 282065.5761038961,
          "standard_error": 50.35430859009879
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.9769523796986,
            "upper_bound": 443.71329138898176
          },
          "point_estimate": 306.29904967093614,
          "standard_error": 104.14669773907836
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 795197.7213164251,
            "upper_bound": 796919.0461076603
          },
          "point_estimate": 796060.0943124568,
          "standard_error": 437.1280267370883
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 795170.8565217392,
            "upper_bound": 796975.213768116
          },
          "point_estimate": 796118.0597826086,
          "standard_error": 448.90487984698353
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247.0739043091878,
            "upper_bound": 2448.2252214810264
          },
          "point_estimate": 1135.547722883482,
          "standard_error": 552.703215320499
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 794523.7245211693,
            "upper_bound": 796253.7934027791
          },
          "point_estimate": 795313.7423489554,
          "standard_error": 434.62495495556306
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 672.9856573738022,
            "upper_bound": 1935.869418455471
          },
          "point_estimate": 1450.854551005856,
          "standard_error": 318.7898086610647
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387484.7450818135,
            "upper_bound": 389005.7744542173
          },
          "point_estimate": 388338.7904103344,
          "standard_error": 395.0841655322537
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387557.2375886525,
            "upper_bound": 389148.98176291795
          },
          "point_estimate": 388860.75106382975,
          "standard_error": 362.83297635988185
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.317014071664964,
            "upper_bound": 1805.1548445478577
          },
          "point_estimate": 460.6850827786566,
          "standard_error": 390.38082640996095
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388583.6050990462,
            "upper_bound": 389173.79264744173
          },
          "point_estimate": 388936.4723127936,
          "standard_error": 153.50598763222976
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 284.8330676650389,
            "upper_bound": 1775.6117560068296
          },
          "point_estimate": 1317.324874029255,
          "standard_error": 402.69321706484743
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159066.68446445902,
            "upper_bound": 159278.0328176163
          },
          "point_estimate": 159168.28141158936,
          "standard_error": 54.165047102093574
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159011.41004366812,
            "upper_bound": 159289.90642545227
          },
          "point_estimate": 159153.98190198932,
          "standard_error": 68.07999697590813
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.82382888345183,
            "upper_bound": 302.2931631921765
          },
          "point_estimate": 206.44936415138076,
          "standard_error": 64.18042542587614
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159049.44183458533,
            "upper_bound": 159254.72477631227
          },
          "point_estimate": 159144.2232178302,
          "standard_error": 52.88124687021013
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.77679713908324,
            "upper_bound": 234.7042762432349
          },
          "point_estimate": 180.77786556435188,
          "standard_error": 36.84159080214713
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230760.03824668477,
            "upper_bound": 231048.5552132309
          },
          "point_estimate": 230889.40538803497,
          "standard_error": 74.06389925999432
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230742.8036392405,
            "upper_bound": 230944.0730108499
          },
          "point_estimate": 230892.9974683544,
          "standard_error": 51.830474298515526
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.45459939913016,
            "upper_bound": 318.08713258066683
          },
          "point_estimate": 118.714735708835,
          "standard_error": 77.90470383069265
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230652.6960480796,
            "upper_bound": 230893.52971263364
          },
          "point_estimate": 230750.7116389939,
          "standard_error": 60.74062587891871
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.97715334127645,
            "upper_bound": 356.16852561763676
          },
          "point_estimate": 247.060137174949,
          "standard_error": 80.06999332076359
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171190.3063477089,
            "upper_bound": 171911.24119755634
          },
          "point_estimate": 171556.6410660003,
          "standard_error": 183.60378814008664
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170873.31839622642,
            "upper_bound": 172058.0653638814
          },
          "point_estimate": 171976.8749410377,
          "standard_error": 413.18487933215647
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.277184512696074,
            "upper_bound": 902.9136367884952
          },
          "point_estimate": 133.6501032836837,
          "standard_error": 305.21442123944325
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171707.1692563818,
            "upper_bound": 172026.29607043523
          },
          "point_estimate": 171937.40839255083,
          "standard_error": 85.96949241486796
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 366.10336128360126,
            "upper_bound": 653.4842685196408
          },
          "point_estimate": 612.374938010601,
          "standard_error": 75.00600191197154
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 650841.8569246033,
            "upper_bound": 651205.3347470238
          },
          "point_estimate": 651001.0855350058,
          "standard_error": 93.96534242077747
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 650776.9017857143,
            "upper_bound": 651130.8333333334
          },
          "point_estimate": 650947.0174319728,
          "standard_error": 85.27721889726935
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.94167834578977,
            "upper_bound": 401.2727428760299
          },
          "point_estimate": 260.0096046100677,
          "standard_error": 85.71530029758954
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 650827.9422682419,
            "upper_bound": 651043.4871473608
          },
          "point_estimate": 650941.4987476809,
          "standard_error": 55.13397848535891
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.0515337962472,
            "upper_bound": 450.8737747947107
          },
          "point_estimate": 313.5731685482054,
          "standard_error": 101.75271806382229
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1306.435843494961,
            "upper_bound": 1307.245295136643
          },
          "point_estimate": 1306.8121149461772,
          "standard_error": 0.20808491301293347
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1306.2526811524765,
            "upper_bound": 1307.295876455369
          },
          "point_estimate": 1306.6012581752193,
          "standard_error": 0.2738222759715435
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026014700798363984,
            "upper_bound": 1.101630172335004
          },
          "point_estimate": 0.523419098051487,
          "standard_error": 0.2900257900855544
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1306.344510909318,
            "upper_bound": 1306.7206464562648
          },
          "point_estimate": 1306.5357631013517,
          "standard_error": 0.09753545273670064
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2316016874852904,
            "upper_bound": 0.8672086964650818
          },
          "point_estimate": 0.6964062384702638,
          "standard_error": 0.15181751158528797
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1352546.629132716,
            "upper_bound": 1356099.9554791297
          },
          "point_estimate": 1354681.1898824223,
          "standard_error": 964.7438258494126
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1354538.2654320989,
            "upper_bound": 1356117.101058201
          },
          "point_estimate": 1355682.4388888888,
          "standard_error": 461.69750980333333
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.22615268217824,
            "upper_bound": 2504.971629602134
          },
          "point_estimate": 713.7627188096851,
          "standard_error": 628.5294939389626
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355520.071614277,
            "upper_bound": 1356115.747014115
          },
          "point_estimate": 1355846.635113035,
          "standard_error": 148.56008823457037
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 566.9468090081212,
            "upper_bound": 4871.923263498154
          },
          "point_estimate": 3216.7853609451545,
          "standard_error": 1393.187202286916
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1514413.974265873,
            "upper_bound": 1524377.2287698411
          },
          "point_estimate": 1519514.0455753969,
          "standard_error": 2434.269059044217
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1509218.0,
            "upper_bound": 1524804.3660714286
          },
          "point_estimate": 1524150.0381944445,
          "standard_error": 3911.944632389999
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.454932719377,
            "upper_bound": 12332.45499480532
          },
          "point_estimate": 1024.8091372225217,
          "standard_error": 3456.674117326646
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1522036.8541666667,
            "upper_bound": 1524644.3012881803
          },
          "point_estimate": 1523945.6386363637,
          "standard_error": 711.2952339421427
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 467.123437660011,
            "upper_bound": 9173.616984363203
          },
          "point_estimate": 8134.051107321588,
          "standard_error": 1662.1362196155706
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1409832.2642017703,
            "upper_bound": 1419837.3767693073
          },
          "point_estimate": 1414563.4229929794,
          "standard_error": 2583.389896634055
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1407547.138888889,
            "upper_bound": 1425126.6346153845
          },
          "point_estimate": 1410446.0637362637,
          "standard_error": 4553.655562119188
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570.303384330192,
            "upper_bound": 13074.543088072554
          },
          "point_estimate": 4995.839923870087,
          "standard_error": 3733.8293667161506
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1408062.2029352228,
            "upper_bound": 1412222.650378025
          },
          "point_estimate": 1409788.777922078,
          "standard_error": 1086.145822074871
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3655.042319605939,
            "upper_bound": 9965.091206003312
          },
          "point_estimate": 8618.93165575586,
          "standard_error": 1438.07305768913
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789181.1527220535,
            "upper_bound": 789599.8177777778
          },
          "point_estimate": 789346.1916050322,
          "standard_error": 114.02135762972134
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789161.192333671,
            "upper_bound": 789395.6524822694
          },
          "point_estimate": 789233.7216312056,
          "standard_error": 61.185581151329835
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.991933244303485,
            "upper_bound": 265.4149684261668
          },
          "point_estimate": 116.03042198250776,
          "standard_error": 70.44589983355147
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789180.9372848958,
            "upper_bound": 789339.8339846577
          },
          "point_estimate": 789244.3549046698,
          "standard_error": 41.134871261937384
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.93948089043033,
            "upper_bound": 576.2615953779267
          },
          "point_estimate": 379.36164100839505,
          "standard_error": 166.1452261266934
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278771.3413821338,
            "upper_bound": 278954.2035827502
          },
          "point_estimate": 278860.5344205137,
          "standard_error": 46.722443879951136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278754.88740458013,
            "upper_bound": 278936.0147900763
          },
          "point_estimate": 278853.3885920271,
          "standard_error": 39.42079821097961
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.873536347014532,
            "upper_bound": 268.9504257595401
          },
          "point_estimate": 92.42105713781388,
          "standard_error": 66.26069579295962
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278748.4354864796,
            "upper_bound": 278913.7881090098
          },
          "point_estimate": 278841.7474372955,
          "standard_error": 41.63953314000183
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.60836028238249,
            "upper_bound": 212.3582846412245
          },
          "point_estimate": 156.34645720325352,
          "standard_error": 36.87386928545219
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492222.5101369048,
            "upper_bound": 492838.9050482625
          },
          "point_estimate": 492472.8696428571,
          "standard_error": 164.69998497455666
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492248.9695945946,
            "upper_bound": 492527.6711711712
          },
          "point_estimate": 492298.50878378376,
          "standard_error": 74.81465433699927
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.491507459769313,
            "upper_bound": 516.0509770544622
          },
          "point_estimate": 90.93830804765966,
          "standard_error": 132.28926562913898
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492257.34938063065,
            "upper_bound": 492401.1137869387
          },
          "point_estimate": 492310.01969111967,
          "standard_error": 36.69411118375675
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.37025768426258,
            "upper_bound": 822.7634360182583
          },
          "point_estimate": 549.8404396694955,
          "standard_error": 226.333806081164
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252947.77100088185,
            "upper_bound": 253122.52469135803
          },
          "point_estimate": 253020.28883652991,
          "standard_error": 46.07445334451655
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252921.09429012344,
            "upper_bound": 253058.58333333337
          },
          "point_estimate": 252984.72321428577,
          "standard_error": 37.77470145569242
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.316962397060824,
            "upper_bound": 158.52565800739077
          },
          "point_estimate": 97.2327451488107,
          "standard_error": 37.40980095003279
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252935.81479124623,
            "upper_bound": 253017.78541136556
          },
          "point_estimate": 252967.42095959597,
          "standard_error": 20.989854325187896
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.68897952823525,
            "upper_bound": 228.9875772330442
          },
          "point_estimate": 154.13349238322633,
          "standard_error": 58.904429577227255
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632650.8711688218,
            "upper_bound": 633005.7055747126
          },
          "point_estimate": 632799.0069601806,
          "standard_error": 93.03067314696573
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632600.292025862,
            "upper_bound": 632861.4379310345
          },
          "point_estimate": 632738.7028325123,
          "standard_error": 73.12065217369306
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.59558472559124,
            "upper_bound": 333.69911045489926
          },
          "point_estimate": 181.88050797784527,
          "standard_error": 71.1512948453139
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632659.0973367783,
            "upper_bound": 632795.274539242
          },
          "point_estimate": 632717.8914464846,
          "standard_error": 34.43370761175077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.10984276660454,
            "upper_bound": 457.6112554121125
          },
          "point_estimate": 309.03695487722854,
          "standard_error": 114.87837145885165
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2074.510400851914,
            "upper_bound": 2088.8377763092367
          },
          "point_estimate": 2083.65666914312,
          "standard_error": 4.191368744240579
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2087.0651936559016,
            "upper_bound": 2088.1756594069648
          },
          "point_estimate": 2087.353972723442,
          "standard_error": 1.0473376917497637
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.16249217330171345,
            "upper_bound": 3.744185452433577
          },
          "point_estimate": 0.7338771893040013,
          "standard_error": 1.4210326962142263
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2059.3995746864043,
            "upper_bound": 2087.775335025015
          },
          "point_estimate": 2076.2447843731948,
          "standard_error": 8.876670991907396
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.42119391316093663,
            "upper_bound": 21.405819765931152
          },
          "point_estimate": 13.949873007778418,
          "standard_error": 7.393552958110292
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648.6831342987338,
            "upper_bound": 649.0478060668335
          },
          "point_estimate": 648.8497452666019,
          "standard_error": 0.09384295036121822
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648.636605313319,
            "upper_bound": 649.0330049737909
          },
          "point_estimate": 648.7676936012969,
          "standard_error": 0.09889074392879552
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027481390148292553,
            "upper_bound": 0.49160235662267954
          },
          "point_estimate": 0.2173296566133769,
          "standard_error": 0.11145872866808704
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648.6616259122254,
            "upper_bound": 648.8518846920173
          },
          "point_estimate": 648.7360842624138,
          "standard_error": 0.04862509774510212
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1255427255254032,
            "upper_bound": 0.4179027947829276
          },
          "point_estimate": 0.31123441392018575,
          "standard_error": 0.07852765042601552
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.73990765998238,
            "upper_bound": 124.83478202242112
          },
          "point_estimate": 124.78033895454412,
          "standard_error": 0.02483105317177932
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.73629123744264,
            "upper_bound": 124.80384785878934
          },
          "point_estimate": 124.75616874091467,
          "standard_error": 0.01642989108544452
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009704195582472536,
            "upper_bound": 0.09452004565792912
          },
          "point_estimate": 0.031843284424163336,
          "standard_error": 0.02240678837200422
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.7444022139017,
            "upper_bound": 124.79729880495118
          },
          "point_estimate": 124.76557608323348,
          "standard_error": 0.014019532393659363
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021015506210882287,
            "upper_bound": 0.12065438382826892
          },
          "point_estimate": 0.0829201234801516,
          "standard_error": 0.02956346862290485
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.83128749093723,
            "upper_bound": 49.87033305116887
          },
          "point_estimate": 49.84637827821036,
          "standard_error": 0.010817824482066814
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.82477689587103,
            "upper_bound": 49.84368687284093
          },
          "point_estimate": 49.840047629056926,
          "standard_error": 0.005248320718120009
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008553891318735891,
            "upper_bound": 0.024151482933805493
          },
          "point_estimate": 0.008409971525722178,
          "standard_error": 0.006490494190616468
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.83304624630912,
            "upper_bound": 49.84257454976851
          },
          "point_estimate": 49.83878910364187,
          "standard_error": 0.0024200877923101984
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0063163340149444466,
            "upper_bound": 0.0551163268826067
          },
          "point_estimate": 0.03608064259437297,
          "standard_error": 0.016742967007218078
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.4912572316308,
            "upper_bound": 78.53135099508816
          },
          "point_estimate": 78.510278905876,
          "standard_error": 0.01026557700665444
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.48622716758138,
            "upper_bound": 78.5353236082043
          },
          "point_estimate": 78.50317776882636,
          "standard_error": 0.012371832355747908
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0050611611948021395,
            "upper_bound": 0.057403984732274274
          },
          "point_estimate": 0.02829858529515807,
          "standard_error": 0.012984474820981204
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.48681128305238,
            "upper_bound": 78.51146444002275
          },
          "point_estimate": 78.49870544451024,
          "standard_error": 0.00615029975433224
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0159665676145503,
            "upper_bound": 0.0435205749610406
          },
          "point_estimate": 0.034181753409276726,
          "standard_error": 0.006943525074013114
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299245.30411816936,
            "upper_bound": 299433.32926844264
          },
          "point_estimate": 299325.27830601094,
          "standard_error": 48.94147037202239
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299229.3640710382,
            "upper_bound": 299370.8012295082
          },
          "point_estimate": 299294.6319672131,
          "standard_error": 32.58175453662595
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.06341476127212,
            "upper_bound": 189.7302630250175
          },
          "point_estimate": 73.86467000011278,
          "standard_error": 47.22397040687877
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299270.6210530566,
            "upper_bound": 299352.82779624866
          },
          "point_estimate": 299306.2375133064,
          "standard_error": 21.086862432232778
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.08074860717728,
            "upper_bound": 236.9607123726605
          },
          "point_estimate": 162.9033609093388,
          "standard_error": 57.323526637599294
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.24450043150729,
            "upper_bound": 59.26036560363623
          },
          "point_estimate": 59.25157316418913,
          "standard_error": 0.004091805378459923
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.24239059416991,
            "upper_bound": 59.25983907049647
          },
          "point_estimate": 59.24705152666056,
          "standard_error": 0.004017263219686678
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017921731926532104,
            "upper_bound": 0.019838647730795148
          },
          "point_estimate": 0.007483680542297272,
          "standard_error": 0.004382793622653583
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.24263804777914,
            "upper_bound": 59.2555128036198
          },
          "point_estimate": 59.24778060861365,
          "standard_error": 0.003257320526570496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003943136228685159,
            "upper_bound": 0.018043564276630227
          },
          "point_estimate": 0.013603134117584513,
          "standard_error": 0.0037244713606143015
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.25227856640402,
            "upper_bound": 97.32695448413962
          },
          "point_estimate": 97.28697649333022,
          "standard_error": 0.0192212167578975
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.24836202815364,
            "upper_bound": 97.32032972519482
          },
          "point_estimate": 97.2752652313164,
          "standard_error": 0.01936055996522646
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005591093442184911,
            "upper_bound": 0.09843311845865596
          },
          "point_estimate": 0.041047297919382726,
          "standard_error": 0.022782596526640542
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.25251227417448,
            "upper_bound": 97.28387093254668
          },
          "point_estimate": 97.27053049497596,
          "standard_error": 0.007903782159869727
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025993885544782172,
            "upper_bound": 0.08769874041595646
          },
          "point_estimate": 0.06403454959513409,
          "standard_error": 0.016776261566404212
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132751.85335994523,
            "upper_bound": 132922.96787807034
          },
          "point_estimate": 132819.63429585216,
          "standard_error": 46.72038039934301
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132724.65267639904,
            "upper_bound": 132826.99442127216
          },
          "point_estimate": 132776.05874898622,
          "standard_error": 24.62628152820679
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.989469419220141,
            "upper_bound": 108.85920737393944
          },
          "point_estimate": 68.17299495756546,
          "standard_error": 31.222656855360174
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132737.4553129347,
            "upper_bound": 132812.44585816647
          },
          "point_estimate": 132767.85700066356,
          "standard_error": 19.144719430958094
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.94997720577663,
            "upper_bound": 236.56019128714703
          },
          "point_estimate": 156.0907342862366,
          "standard_error": 67.78803952558172
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.68038972224061,
            "upper_bound": 58.81212687167712
          },
          "point_estimate": 58.729412763673096,
          "standard_error": 0.03769361883147175
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.67571627966814,
            "upper_bound": 58.71806252856264
          },
          "point_estimate": 58.68959502128256,
          "standard_error": 0.013483763998904705
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0056965400851912,
            "upper_bound": 0.054305957101483546
          },
          "point_estimate": 0.021672390189041026,
          "standard_error": 0.01604287990253999
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.68447495824067,
            "upper_bound": 58.70564283563735
          },
          "point_estimate": 58.69511466485982,
          "standard_error": 0.005487144528815578
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012075425447700858,
            "upper_bound": 0.1931885944791151
          },
          "point_estimate": 0.12596480050689557,
          "standard_error": 0.06230889632582042
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.20836236336608,
            "upper_bound": 93.27358081236792
          },
          "point_estimate": 93.2367793190366,
          "standard_error": 0.016894467720842528
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.19950421986916,
            "upper_bound": 93.25963671427216
          },
          "point_estimate": 93.2263032527143,
          "standard_error": 0.016469248766545763
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008746382092295123,
            "upper_bound": 0.07225552500195204
          },
          "point_estimate": 0.04126966170343037,
          "standard_error": 0.015414563375292487
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.2046932531882,
            "upper_bound": 93.2437456567039
          },
          "point_estimate": 93.223867373609,
          "standard_error": 0.010010557754655322
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020875225241228177,
            "upper_bound": 0.08097872132305098
          },
          "point_estimate": 0.05629912936623016,
          "standard_error": 0.01831488925588461
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.0335472913207,
            "upper_bound": 237.31284802462127
          },
          "point_estimate": 237.15400208224577,
          "standard_error": 0.07256268699938466
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236.99704702264887,
            "upper_bound": 237.26178076965715
          },
          "point_estimate": 237.06622877965833,
          "standard_error": 0.05999843262579763
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01930546769906718,
            "upper_bound": 0.3039028928836178
          },
          "point_estimate": 0.13204410547753037,
          "standard_error": 0.07064088315412267
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.01860503879743,
            "upper_bound": 237.2721262165348
          },
          "point_estimate": 237.1419345407659,
          "standard_error": 0.06985568019695992
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06291777936286846,
            "upper_bound": 0.3436189487071543
          },
          "point_estimate": 0.24187397302592317,
          "standard_error": 0.07978369711126294
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 487.4673653049459,
            "upper_bound": 487.7941980442142
          },
          "point_estimate": 487.6111113207309,
          "standard_error": 0.08385194651953716
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 487.46924620465586,
            "upper_bound": 487.6716009492271
          },
          "point_estimate": 487.583377800421,
          "standard_error": 0.04583699244406368
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013857569973934345,
            "upper_bound": 0.3473204281141831
          },
          "point_estimate": 0.11089397285258876,
          "standard_error": 0.08426353815373853
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 487.3587305128104,
            "upper_bound": 487.61083474346
          },
          "point_estimate": 487.4943786903431,
          "standard_error": 0.06776382218857725
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06910061624755318,
            "upper_bound": 0.4056974556324779
          },
          "point_estimate": 0.2797672976460156,
          "standard_error": 0.09615447777922588
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.36949128938734,
            "upper_bound": 32.38548024539428
          },
          "point_estimate": 32.37647805106229,
          "standard_error": 0.004128755584237312
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.36704393539877,
            "upper_bound": 32.379576651737885
          },
          "point_estimate": 32.37581276544595,
          "standard_error": 0.003364161542931916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001176112681792282,
            "upper_bound": 0.018641123757521454
          },
          "point_estimate": 0.005610397138865488,
          "standard_error": 0.0043899207455299835
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.3688491255732,
            "upper_bound": 32.37824824511936
          },
          "point_estimate": 32.37482590261018,
          "standard_error": 0.0023938311038206417
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004881942802489549,
            "upper_bound": 0.019980613270613738
          },
          "point_estimate": 0.013772550144514109,
          "standard_error": 0.0046084433008908255
        }
      }
    },
    "memrmem/krate/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33801.23152184015,
            "upper_bound": 33859.254226857076
          },
          "point_estimate": 33825.38019173747,
          "standard_error": 15.234761839478672
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33797.00089064436,
            "upper_bound": 33834.89079925651
          },
          "point_estimate": 33812.29259990706,
          "standard_error": 9.422686652855996
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.243130030706332,
            "upper_bound": 50.45304245000448
          },
          "point_estimate": 22.730142832987735,
          "standard_error": 12.07233596761444
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33801.37968951075,
            "upper_bound": 33818.546478059034
          },
          "point_estimate": 33810.68050016898,
          "standard_error": 4.274745820528216
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.57768209968702,
            "upper_bound": 75.5075799967007
          },
          "point_estimate": 50.848896960331224,
          "standard_error": 19.454521567491877
        }
      }
    },
    "memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.39514428082477,
            "upper_bound": 124.5725596591636
          },
          "point_estimate": 124.466807246429,
          "standard_error": 0.0476047377112476
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.38571477450414,
            "upper_bound": 124.49832149565783
          },
          "point_estimate": 124.41060447771692,
          "standard_error": 0.027400373084257613
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00922822175880596,
            "upper_bound": 0.1365556775303984
          },
          "point_estimate": 0.043657067419426536,
          "standard_error": 0.03348425142040356
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.3922231776864,
            "upper_bound": 124.43637698853328
          },
          "point_estimate": 124.40737406140684,
          "standard_error": 0.011425732008953917
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025757126764820393,
            "upper_bound": 0.23555342750583616
          },
          "point_estimate": 0.1580116495063712,
          "standard_error": 0.06459203326747506
        }
      }
    },
    "memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104759.3589036192,
            "upper_bound": 1106949.741534091
          },
          "point_estimate": 1105578.457382155,
          "standard_error": 622.6333825292438
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104700.753787879,
            "upper_bound": 1105376.343097643
          },
          "point_estimate": 1104984.5404040404,
          "standard_error": 213.2285999211876
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.22719772542041,
            "upper_bound": 965.351293568792
          },
          "point_estimate": 409.245327961585,
          "standard_error": 267.9981430837647
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104822.3477021134,
            "upper_bound": 1105218.6890769214
          },
          "point_estimate": 1104983.3778827232,
          "standard_error": 101.02851668432528
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.5904820720075,
            "upper_bound": 3186.931891447568
          },
          "point_estimate": 2075.6847657399667,
          "standard_error": 1018.9241929835327
        }
      }
    },
    "memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2233.62796756214,
            "upper_bound": 2235.185193950944
          },
          "point_estimate": 2234.3501581934797,
          "standard_error": 0.4016352423672068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2233.5234318968874,
            "upper_bound": 2235.425967193295
          },
          "point_estimate": 2233.994382799533,
          "standard_error": 0.368024562005108
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09949242000759186,
            "upper_bound": 2.0880876235301
          },
          "point_estimate": 0.7029445747213738,
          "standard_error": 0.49709407470925904
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2233.785394581169,
            "upper_bound": 2235.349828636004
          },
          "point_estimate": 2234.406203300603,
          "standard_error": 0.4249827176664924
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4033816944286929,
            "upper_bound": 1.729928135708385
          },
          "point_estimate": 1.342624920327233,
          "standard_error": 0.3230097643944569
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.90345280385636,
            "upper_bound": 24.9180476060586
          },
          "point_estimate": 24.910172189200768,
          "standard_error": 0.003759235842800598
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.90055172461019,
            "upper_bound": 24.919786623571383
          },
          "point_estimate": 24.905814268007916,
          "standard_error": 0.005230458573158715
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022247397978477222,
            "upper_bound": 0.022204278056555775
          },
          "point_estimate": 0.011068255060873024,
          "standard_error": 0.004819638115243487
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.90089033593447,
            "upper_bound": 24.913227368434654
          },
          "point_estimate": 24.905751916418826,
          "standard_error": 0.003200294566487358
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005937147643340724,
            "upper_bound": 0.016655530188548706
          },
          "point_estimate": 0.012478656064949387,
          "standard_error": 0.002988566926198443
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.371699662033773,
            "upper_bound": 25.38820505401273
          },
          "point_estimate": 25.37945368129784,
          "standard_error": 0.004234784535636592
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.367953851120063,
            "upper_bound": 25.39369428289839
          },
          "point_estimate": 25.375167967566085,
          "standard_error": 0.006120172468767133
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019624981189712235,
            "upper_bound": 0.022138144012853387
          },
          "point_estimate": 0.010784196737759507,
          "standard_error": 0.0052990614357200285
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.3698859152757,
            "upper_bound": 25.37845088474722
          },
          "point_estimate": 25.37343597614823,
          "standard_error": 0.0021951592702195563
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004825096162675066,
            "upper_bound": 0.016966828093587728
          },
          "point_estimate": 0.014120338693265,
          "standard_error": 0.0026483621508521507
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.023144467652084,
            "upper_bound": 25.08370227649474
          },
          "point_estimate": 25.052098453846316,
          "standard_error": 0.015602173869949484
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.00864644095276,
            "upper_bound": 25.090853239966194
          },
          "point_estimate": 25.044527052898463,
          "standard_error": 0.022618293823024077
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012735543746718592,
            "upper_bound": 0.09079542921646608
          },
          "point_estimate": 0.05950700687169728,
          "standard_error": 0.018768558496102247
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.02178577468646,
            "upper_bound": 25.06963147968801
          },
          "point_estimate": 25.0457861415564,
          "standard_error": 0.01214461047025958
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028910515697466208,
            "upper_bound": 0.06641224757103709
          },
          "point_estimate": 0.05186335929037505,
          "standard_error": 0.010004566133205987
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.951010739478026,
            "upper_bound": 31.218244627967948
          },
          "point_estimate": 31.084621317920785,
          "standard_error": 0.06836530907941209
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.92554904683745,
            "upper_bound": 31.249802682847513
          },
          "point_estimate": 31.07561650022159,
          "standard_error": 0.08014844308234208
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06206918288945202,
            "upper_bound": 0.39047746179179577
          },
          "point_estimate": 0.21825172629389175,
          "standard_error": 0.08810777052878317
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.98028054437696,
            "upper_bound": 31.279948235215677
          },
          "point_estimate": 31.10197587791236,
          "standard_error": 0.07799120504173596
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12377816492383314,
            "upper_bound": 0.2906903682297559
          },
          "point_estimate": 0.22765065212993155,
          "standard_error": 0.04291947115538825
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.650329066064536,
            "upper_bound": 19.66085854102009
          },
          "point_estimate": 19.65481549276999,
          "standard_error": 0.0027469538507861927
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.648443924884287,
            "upper_bound": 19.657267342534244
          },
          "point_estimate": 19.653906746709687,
          "standard_error": 0.002292838933827764
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008061964789014101,
            "upper_bound": 0.010312910565411922
          },
          "point_estimate": 0.006884462879867914,
          "standard_error": 0.0025535456711140805
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.649995105971687,
            "upper_bound": 19.65466700917195
          },
          "point_estimate": 19.65276727336087,
          "standard_error": 0.0011882624322860605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003041646433946273,
            "upper_bound": 0.013335960247669063
          },
          "point_estimate": 0.009123268964308078,
          "standard_error": 0.003210632965905179
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.861417321945215,
            "upper_bound": 21.876135881235367
          },
          "point_estimate": 21.867741999435964,
          "standard_error": 0.003848174726194179
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.861354493739743,
            "upper_bound": 21.87271716649898
          },
          "point_estimate": 21.863584460256263,
          "standard_error": 0.002448921897628976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008400177223341684,
            "upper_bound": 0.015624722378670358
          },
          "point_estimate": 0.0035978461617404496,
          "standard_error": 0.0038749485538956446
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.857714883649557,
            "upper_bound": 21.872089825421973
          },
          "point_estimate": 21.86404322221283,
          "standard_error": 0.003728505519686035
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002319309698939268,
            "upper_bound": 0.018186157392027988
          },
          "point_estimate": 0.012797424729702009,
          "standard_error": 0.0043377561384346855
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.49890364075214,
            "upper_bound": 35.51240391196059
          },
          "point_estimate": 35.50509198259646,
          "standard_error": 0.0034666931877380632
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.49673057364705,
            "upper_bound": 35.51220222378689
          },
          "point_estimate": 35.50143578144389,
          "standard_error": 0.0036806325546678313
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006574218404087428,
            "upper_bound": 0.017352070287882628
          },
          "point_estimate": 0.008638647976042265,
          "standard_error": 0.00435508123561276
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.49750996524452,
            "upper_bound": 35.50719941498897
          },
          "point_estimate": 35.50256180636043,
          "standard_error": 0.002526300002055561
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035175926620136275,
            "upper_bound": 0.014721202242109658
          },
          "point_estimate": 0.011600905021015124,
          "standard_error": 0.002764028619451298
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.084453071247296,
            "upper_bound": 26.09479926398367
          },
          "point_estimate": 26.089460533226845,
          "standard_error": 0.002665812762688582
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.08268347404156,
            "upper_bound": 26.09848722487158
          },
          "point_estimate": 26.085719632111307,
          "standard_error": 0.004143004345228715
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005474830431306249,
            "upper_bound": 0.014190565146489595
          },
          "point_estimate": 0.009731769128973104,
          "standard_error": 0.003812004482227469
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.085053232254648,
            "upper_bound": 26.09532949889573
          },
          "point_estimate": 26.089222402770066,
          "standard_error": 0.0026420197839644535
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005053412246120886,
            "upper_bound": 0.01090462107812201
          },
          "point_estimate": 0.008920485920073285,
          "standard_error": 0.001488232508488333
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.91893617503581,
            "upper_bound": 29.162201494319635
          },
          "point_estimate": 29.043593343506068,
          "standard_error": 0.06236246184679384
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.91672687882504,
            "upper_bound": 29.206752023831463
          },
          "point_estimate": 29.025539150146617,
          "standard_error": 0.08271238844909545
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04105746007842072,
            "upper_bound": 0.3772944903404984
          },
          "point_estimate": 0.197733376066548,
          "standard_error": 0.08037237147276201
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.950353887308296,
            "upper_bound": 29.13936471322987
          },
          "point_estimate": 29.025898877170462,
          "standard_error": 0.048456422599115104
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11076847632936863,
            "upper_bound": 0.27340478287158143
          },
          "point_estimate": 0.20729414748263447,
          "standard_error": 0.04290945019270117
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.12676680452635,
            "upper_bound": 28.15178620143672
          },
          "point_estimate": 28.138986277818464,
          "standard_error": 0.006324250580706493
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.129593491279337,
            "upper_bound": 28.148490784444306
          },
          "point_estimate": 28.13810493689589,
          "standard_error": 0.004260353841814132
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001584893638445754,
            "upper_bound": 0.033329805538023345
          },
          "point_estimate": 0.00887003120627955,
          "standard_error": 0.00782093477194836
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.133896119771116,
            "upper_bound": 28.14565822111703
          },
          "point_estimate": 28.13897516908716,
          "standard_error": 0.002982306731897933
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006557165355022045,
            "upper_bound": 0.029201739536258427
          },
          "point_estimate": 0.021076930606654384,
          "standard_error": 0.0057579126019339085
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.947049021448528,
            "upper_bound": 24.9718854165339
          },
          "point_estimate": 24.95728362662971,
          "standard_error": 0.006580172218210201
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.94431056615329,
            "upper_bound": 24.961968313991395
          },
          "point_estimate": 24.951462612637343,
          "standard_error": 0.003687793022050611
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018960610066869777,
            "upper_bound": 0.021511440868243848
          },
          "point_estimate": 0.006277248171116559,
          "standard_error": 0.005400538713408709
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.943690340949843,
            "upper_bound": 24.953573206744757
          },
          "point_estimate": 24.948753318590875,
          "standard_error": 0.0025793782137613825
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004786517991330587,
            "upper_bound": 0.03245479967987827
          },
          "point_estimate": 0.021889715741344712,
          "standard_error": 0.008523063872523922
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.754717176008455,
            "upper_bound": 27.839108580346913
          },
          "point_estimate": 27.78520249455075,
          "standard_error": 0.0246999435378413
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.752418558171208,
            "upper_bound": 27.773140483888916
          },
          "point_estimate": 27.75520582358444,
          "standard_error": 0.007963220867540019
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005530025659674943,
            "upper_bound": 0.025203416564537635
          },
          "point_estimate": 0.00894083381078955,
          "standard_error": 0.00918470460018204
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.75382676343432,
            "upper_bound": 27.7725082678572
          },
          "point_estimate": 27.762077599259595,
          "standard_error": 0.0050273318865690425
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005790741141923719,
            "upper_bound": 0.12664994547396907
          },
          "point_estimate": 0.0824408479201416,
          "standard_error": 0.04270774986811652
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1187096.835596518,
            "upper_bound": 1187833.6457067975
          },
          "point_estimate": 1187380.3293958013,
          "standard_error": 204.41300753569712
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1187028.2137096776,
            "upper_bound": 1187385.0333333332
          },
          "point_estimate": 1187201.0129032256,
          "standard_error": 98.42783135940284
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.66198504929145,
            "upper_bound": 402.2257859232736
          },
          "point_estimate": 209.5526198814456,
          "standard_error": 109.82216558541778
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1187090.4546117685,
            "upper_bound": 1187262.3251576712
          },
          "point_estimate": 1187173.5042312527,
          "standard_error": 44.127974090690785
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107.56819592726175,
            "upper_bound": 1041.058384010071
          },
          "point_estimate": 682.2142384216376,
          "standard_error": 314.7827430453316
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1642402.018972136,
            "upper_bound": 1644949.0925983435
          },
          "point_estimate": 1643832.7325500343,
          "standard_error": 649.0315286780734
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1643664.2560386474,
            "upper_bound": 1644698.0905797102
          },
          "point_estimate": 1643990.5489130437,
          "standard_error": 306.1284948469692
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.58479382746347,
            "upper_bound": 2530.6945572454174
          },
          "point_estimate": 663.5172056116568,
          "standard_error": 534.0773602775145
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1640376.3985533265,
            "upper_bound": 1644186.721047733
          },
          "point_estimate": 1642512.1305477133,
          "standard_error": 1144.5569176493098
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388.9281401464144,
            "upper_bound": 3193.0915574107803
          },
          "point_estimate": 2170.1675817426813,
          "standard_error": 798.6609027832129
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1459414.0376507142,
            "upper_bound": 1460633.579901667
          },
          "point_estimate": 1459940.468495238,
          "standard_error": 316.49487252674083
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1459227.1942857143,
            "upper_bound": 1460318.28
          },
          "point_estimate": 1459711.6883333337,
          "standard_error": 273.0061368838523
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.8595366647789,
            "upper_bound": 1237.6589260272244
          },
          "point_estimate": 719.930073218871,
          "standard_error": 274.606840340234
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1459273.2849840256,
            "upper_bound": 1460083.0225178148
          },
          "point_estimate": 1459738.8857142858,
          "standard_error": 208.98955192327884
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.01833318054935,
            "upper_bound": 1532.2652521991006
          },
          "point_estimate": 1056.8856711064102,
          "standard_error": 359.5903963609812
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397940.6338024069,
            "upper_bound": 399033.62933359214
          },
          "point_estimate": 398445.4163988095,
          "standard_error": 279.9594982892424
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397735.42391304346,
            "upper_bound": 399110.847826087
          },
          "point_estimate": 398275.63457556936,
          "standard_error": 324.9834595615566
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218.5384591636619,
            "upper_bound": 1507.3878634559196
          },
          "point_estimate": 941.654533825806,
          "standard_error": 332.05430720744425
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397981.17205209465,
            "upper_bound": 398827.66647354455
          },
          "point_estimate": 398420.4897233202,
          "standard_error": 227.64964236183587
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 423.7623166528342,
            "upper_bound": 1255.5845807118494
          },
          "point_estimate": 935.0385116940816,
          "standard_error": 229.12365207686224
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630626.0999504311,
            "upper_bound": 631923.9503103448
          },
          "point_estimate": 631224.7285344828,
          "standard_error": 333.19957738505747
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630582.4206896552,
            "upper_bound": 631781.4109195402
          },
          "point_estimate": 630957.5021551724,
          "standard_error": 346.5332327169727
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167.0385319482128,
            "upper_bound": 1650.2701017363418
          },
          "point_estimate": 874.4553579235923,
          "standard_error": 390.8538735196432
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630367.7963807352,
            "upper_bound": 631518.1614103285
          },
          "point_estimate": 630922.6806090461,
          "standard_error": 293.23858942767805
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 460.96643317090275,
            "upper_bound": 1547.8293159942718
          },
          "point_estimate": 1110.4840357734147,
          "standard_error": 304.51798254888195
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392495.7616799796,
            "upper_bound": 392854.6566056068
          },
          "point_estimate": 392658.9492899812,
          "standard_error": 92.49649681632776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392455.434203789,
            "upper_bound": 392844.7043010753
          },
          "point_estimate": 392611.95860215055,
          "standard_error": 87.29846645556212
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.42773838248778,
            "upper_bound": 472.4496119349228
          },
          "point_estimate": 215.0025412367333,
          "standard_error": 112.5672067363107
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392502.7737420237,
            "upper_bound": 392646.9926416224
          },
          "point_estimate": 392589.1786901271,
          "standard_error": 36.536711768802085
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.18442443724274,
            "upper_bound": 412.924114699647
          },
          "point_estimate": 309.5077438841254,
          "standard_error": 81.95904642509608
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482506.4290851609,
            "upper_bound": 482876.60223316104
          },
          "point_estimate": 482694.0948971387,
          "standard_error": 94.79944048574238
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482359.0350877193,
            "upper_bound": 483000.1513157895
          },
          "point_estimate": 482725.3184915414,
          "standard_error": 130.37282681086234
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.992153157475505,
            "upper_bound": 554.5605865032388
          },
          "point_estimate": 446.2049392822219,
          "standard_error": 164.05575191812093
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482397.5017821425,
            "upper_bound": 482762.0373480491
          },
          "point_estimate": 482600.9073820916,
          "standard_error": 98.39697229910615
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 183.8771380012614,
            "upper_bound": 383.7018379017732
          },
          "point_estimate": 315.61822240455183,
          "standard_error": 50.600265476282274
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1015586.492514192,
            "upper_bound": 1016989.2907142856
          },
          "point_estimate": 1016170.973067681,
          "standard_error": 368.7701461693237
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1015480.4319444444,
            "upper_bound": 1016364.523919753
          },
          "point_estimate": 1015924.715029762,
          "standard_error": 228.97359322160912
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.73967927813703,
            "upper_bound": 1207.4376552303384
          },
          "point_estimate": 494.0455537288806,
          "standard_error": 269.5981566236848
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1015387.5663768916,
            "upper_bound": 1016014.064178369
          },
          "point_estimate": 1015694.0536796536,
          "standard_error": 162.6935203806638
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322.83342472492166,
            "upper_bound": 1825.217425346168
          },
          "point_estimate": 1228.744567107597,
          "standard_error": 472.0209826232595
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332785.76119228,
            "upper_bound": 333103.0060815657
          },
          "point_estimate": 332922.5051857864,
          "standard_error": 82.80017606756537
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332742.44242424244,
            "upper_bound": 333016.875
          },
          "point_estimate": 332838.43454545457,
          "standard_error": 75.00069404504039
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.33823822841828,
            "upper_bound": 327.72559531809145
          },
          "point_estimate": 152.4724425658262,
          "standard_error": 77.37054583808066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332756.70113556896,
            "upper_bound": 333002.81494236103
          },
          "point_estimate": 332883.86460448644,
          "standard_error": 65.73494982077472
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.16787265774362,
            "upper_bound": 397.821163575065
          },
          "point_estimate": 275.97549053112834,
          "standard_error": 92.96855287787444
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 833223.5719418289,
            "upper_bound": 833609.7908549784
          },
          "point_estimate": 833413.0824458874,
          "standard_error": 98.94987921624296
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 833126.6984848485,
            "upper_bound": 833717.2386363635
          },
          "point_estimate": 833395.5151515151,
          "standard_error": 158.1958507706576
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.69740311280009,
            "upper_bound": 572.1796955236084
          },
          "point_estimate": 336.6536872050104,
          "standard_error": 123.19949294351284
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 833362.3696912649,
            "upper_bound": 833789.4466856525
          },
          "point_estimate": 833643.8268595041,
          "standard_error": 108.8026252382633
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199.44620447260235,
            "upper_bound": 399.1747231540409
          },
          "point_estimate": 329.4245034938025,
          "standard_error": 50.96973491612187
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400112.3091592207,
            "upper_bound": 400477.61863480066
          },
          "point_estimate": 400279.0891309088,
          "standard_error": 93.79052468010003
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400042.59584859584,
            "upper_bound": 400417.5710361068
          },
          "point_estimate": 400259.5578296703,
          "standard_error": 110.76307341837952
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.17874094943454,
            "upper_bound": 468.1243537296484
          },
          "point_estimate": 280.5641461087224,
          "standard_error": 105.00005277127973
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400070.6855852984,
            "upper_bound": 400292.1589672464
          },
          "point_estimate": 400150.5190238334,
          "standard_error": 56.40676770658894
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.38821945077797,
            "upper_bound": 431.0800010958471
          },
          "point_estimate": 311.7225688246426,
          "standard_error": 84.28402912283514
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174989.17358361947,
            "upper_bound": 175405.846654118
          },
          "point_estimate": 175186.7914978251,
          "standard_error": 106.99651059563388
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174913.92427884616,
            "upper_bound": 175449.92307692306
          },
          "point_estimate": 175140.71664663462,
          "standard_error": 129.46981834585907
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.55001419356282,
            "upper_bound": 577.8519313877177
          },
          "point_estimate": 396.1877779662479,
          "standard_error": 139.0057493647029
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174910.90450174824,
            "upper_bound": 175393.09149782936
          },
          "point_estimate": 175154.39702797204,
          "standard_error": 128.41980460280752
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.0026944751855,
            "upper_bound": 459.58944909833446
          },
          "point_estimate": 356.01155944778685,
          "standard_error": 73.71958925853511
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232696.1801982484,
            "upper_bound": 232890.84526387625
          },
          "point_estimate": 232807.3769846325,
          "standard_error": 50.60372263489967
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232776.94267515923,
            "upper_bound": 232916.32356687897
          },
          "point_estimate": 232810.70046001417,
          "standard_error": 41.851320571318354
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.483796062887624,
            "upper_bound": 196.61235138206465
          },
          "point_estimate": 93.31550337516614,
          "standard_error": 46.68147780173168
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232827.6126124628,
            "upper_bound": 232926.03183181817
          },
          "point_estimate": 232878.79703863015,
          "standard_error": 25.23664995592676
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.79225044592192,
            "upper_bound": 246.87089837886128
          },
          "point_estimate": 168.9026103094071,
          "standard_error": 59.68144138419477
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175459.65928651558,
            "upper_bound": 175584.8865771902
          },
          "point_estimate": 175518.82937652626,
          "standard_error": 32.14313400397096
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175442.5233173077,
            "upper_bound": 175607.4374332265
          },
          "point_estimate": 175494.78265224356,
          "standard_error": 35.509230436769066
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.1178123432936475,
            "upper_bound": 195.42610001606013
          },
          "point_estimate": 69.10830506320046,
          "standard_error": 44.42184825695838
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175461.6459769751,
            "upper_bound": 175531.67588312973
          },
          "point_estimate": 175493.6438811189,
          "standard_error": 17.805910689594292
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.06369131052812,
            "upper_bound": 136.58814403489689
          },
          "point_estimate": 107.03466122888766,
          "standard_error": 22.677386655478077
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 676063.9381481481,
            "upper_bound": 676413.7511213992
          },
          "point_estimate": 676221.890186655,
          "standard_error": 90.37817181782518
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 675950.7160493827,
            "upper_bound": 676363.7053571428
          },
          "point_estimate": 676188.2119341564,
          "standard_error": 82.60156813123062
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.51574706011508,
            "upper_bound": 426.35388270849927
          },
          "point_estimate": 227.41619299956977,
          "standard_error": 128.69049851287227
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 676037.6723163842,
            "upper_bound": 676233.1099514175
          },
          "point_estimate": 676143.2338143338,
          "standard_error": 49.992358862306816
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120.8601764848574,
            "upper_bound": 417.6088931030151
          },
          "point_estimate": 301.8481254740367,
          "standard_error": 85.0161949138545
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1401.5653201173704,
            "upper_bound": 1402.1567967334654
          },
          "point_estimate": 1401.886625507646,
          "standard_error": 0.15206832048355295
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1401.6335806094326,
            "upper_bound": 1402.2612046982458
          },
          "point_estimate": 1401.9889613867208,
          "standard_error": 0.17039238309523644
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11613470661599042,
            "upper_bound": 0.775674910372177
          },
          "point_estimate": 0.4652577287772963,
          "standard_error": 0.16158053285031182
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1401.802567892673,
            "upper_bound": 1402.126629651712
          },
          "point_estimate": 1401.939681792591,
          "standard_error": 0.0823461541913229
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2244669427185301,
            "upper_bound": 0.6987973994601924
          },
          "point_estimate": 0.5062533136636747,
          "standard_error": 0.13752882305260122
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355410.5772222222,
            "upper_bound": 1356082.0739197528
          },
          "point_estimate": 1355743.242798354,
          "standard_error": 171.96874837033852
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355286.5504115229,
            "upper_bound": 1356341.1975308645
          },
          "point_estimate": 1355540.4351851852,
          "standard_error": 340.0394484968838
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.00685933564084,
            "upper_bound": 876.4987665377179
          },
          "point_estimate": 677.8852811748146,
          "standard_error": 238.6661737301115
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355278.4255360623,
            "upper_bound": 1356051.3135894253
          },
          "point_estimate": 1355652.2315536316,
          "standard_error": 205.6177812700899
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 375.81719689667904,
            "upper_bound": 664.6413340063474
          },
          "point_estimate": 574.6667754848771,
          "standard_error": 73.88586125483523
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1519651.118922743,
            "upper_bound": 1524299.3413105984
          },
          "point_estimate": 1522328.9866286376,
          "standard_error": 1300.213494609221
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1520926.4675925926,
            "upper_bound": 1524421.129166667
          },
          "point_estimate": 1524174.0729166665,
          "standard_error": 778.9812193566718
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.11467437685292,
            "upper_bound": 4699.421932367431
          },
          "point_estimate": 365.11175445539664,
          "standard_error": 1014.8850198768034
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523519.676809749,
            "upper_bound": 1524328.121770026
          },
          "point_estimate": 1524062.3914502163,
          "standard_error": 214.1151350162081
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 186.54059094961323,
            "upper_bound": 6200.294771519753
          },
          "point_estimate": 4331.495227086839,
          "standard_error": 1711.339554457866
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1425663.6153311967,
            "upper_bound": 1426582.5722863246
          },
          "point_estimate": 1426143.5454960316,
          "standard_error": 236.28774051521043
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1425454.1217948718,
            "upper_bound": 1426736.3995726495
          },
          "point_estimate": 1426439.304029304,
          "standard_error": 382.00347581863326
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.11273786963838,
            "upper_bound": 1335.9615700315958
          },
          "point_estimate": 768.8047959662864,
          "standard_error": 329.6336135839895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1425819.1508517978,
            "upper_bound": 1426816.537943538
          },
          "point_estimate": 1426408.618981019,
          "standard_error": 257.5363477764194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 436.0632749102736,
            "upper_bound": 987.0808626433944
          },
          "point_estimate": 786.1092945474861,
          "standard_error": 144.77881335723686
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789195.740679669,
            "upper_bound": 790043.1253200355
          },
          "point_estimate": 789527.9525236406,
          "standard_error": 230.77698428589449
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789103.2411347518,
            "upper_bound": 789586.4255319149
          },
          "point_estimate": 789276.9230496454,
          "standard_error": 135.35345703299205
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.34154277616045,
            "upper_bound": 560.6050482033388
          },
          "point_estimate": 283.76214310052563,
          "standard_error": 128.9401403775993
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789179.4349827946,
            "upper_bound": 789435.0744478597
          },
          "point_estimate": 789288.8016579165,
          "standard_error": 64.5701280116046
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.51315439845817,
            "upper_bound": 1172.3475790136615
          },
          "point_estimate": 771.2123652775058,
          "standard_error": 339.7035727947684
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278765.56537134224,
            "upper_bound": 278926.16925209004
          },
          "point_estimate": 278846.1916185024,
          "standard_error": 41.09001450881553
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278720.1451472192,
            "upper_bound": 278943.2264631043
          },
          "point_estimate": 278871.2927480916,
          "standard_error": 65.87609278042015
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.023302959823642,
            "upper_bound": 217.19493421654343
          },
          "point_estimate": 132.05648925856096,
          "standard_error": 56.83977290028482
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278757.8101918095,
            "upper_bound": 278949.5385681654
          },
          "point_estimate": 278858.36476653116,
          "standard_error": 48.022127475277365
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.0702968888198,
            "upper_bound": 169.43493684470369
          },
          "point_estimate": 136.91699240981228,
          "standard_error": 22.45796003826793
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492245.5924137173,
            "upper_bound": 492497.78114114114
          },
          "point_estimate": 492360.8085800086,
          "standard_error": 64.71296541525209
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492188.6126126126,
            "upper_bound": 492469.27702702704
          },
          "point_estimate": 492327.25,
          "standard_error": 73.33363853092541
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.74998864986109,
            "upper_bound": 336.05833146620347
          },
          "point_estimate": 208.0565267116808,
          "standard_error": 68.83649178916198
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492196.5518924412,
            "upper_bound": 492663.25510962034
          },
          "point_estimate": 492423.52334152337,
          "standard_error": 127.3941270815698
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.98706756397571,
            "upper_bound": 295.89887173020327
          },
          "point_estimate": 215.674156159199,
          "standard_error": 56.94039272249793
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252892.54927779845,
            "upper_bound": 253043.15218584653
          },
          "point_estimate": 252968.66315228175,
          "standard_error": 38.655799008200304
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252883.27763310185,
            "upper_bound": 253078.0752314815
          },
          "point_estimate": 252969.396875,
          "standard_error": 57.8437361555175
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.87331907779607,
            "upper_bound": 218.08547980877472
          },
          "point_estimate": 122.24372623252188,
          "standard_error": 54.64063699968832
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252893.54571103584,
            "upper_bound": 253029.29858090807
          },
          "point_estimate": 252958.46401515152,
          "standard_error": 35.46618361852544
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.75364560176178,
            "upper_bound": 164.7382766272812
          },
          "point_estimate": 128.35694538270948,
          "standard_error": 23.811912308844914
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632595.2554262109,
            "upper_bound": 632947.6817705255
          },
          "point_estimate": 632730.9799876846,
          "standard_error": 97.99093451158453
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632584.3676108374,
            "upper_bound": 632720.41954023
          },
          "point_estimate": 632643.2823275862,
          "standard_error": 43.71687095798995
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.370447158662415,
            "upper_bound": 187.44044046547856
          },
          "point_estimate": 85.7230358057463,
          "standard_error": 47.632383344655715
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632600.3252483928,
            "upper_bound": 632708.4291918161
          },
          "point_estimate": 632666.6188087774,
          "standard_error": 27.44204101624413
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.69951489712923,
            "upper_bound": 500.5298750031873
          },
          "point_estimate": 326.79930745615025,
          "standard_error": 153.04405200764256
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.05535964305,
            "upper_bound": 378.2993459758322
          },
          "point_estimate": 378.1652515022549,
          "standard_error": 0.06276338161956044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.03148485405615,
            "upper_bound": 378.31553943657576
          },
          "point_estimate": 378.0915524086543,
          "standard_error": 0.06432881794858827
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01851032030493969,
            "upper_bound": 0.2969731701551315
          },
          "point_estimate": 0.09766059170540524,
          "standard_error": 0.06464993313308079
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.03649203314234,
            "upper_bound": 378.15050698472766
          },
          "point_estimate": 378.08478748567205,
          "standard_error": 0.028771357855657063
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.052353832636169315,
            "upper_bound": 0.2629655555867326
          },
          "point_estimate": 0.20955247138278857,
          "standard_error": 0.05452191571737984
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.05069617619125,
            "upper_bound": 187.29741090896843
          },
          "point_estimate": 187.14627267062585,
          "standard_error": 0.06794308206599634
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.03550263919692,
            "upper_bound": 187.14310008912472
          },
          "point_estimate": 187.09234641532117,
          "standard_error": 0.031816206409806945
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021433187693365035,
            "upper_bound": 0.14042661726223885
          },
          "point_estimate": 0.07305071487887851,
          "standard_error": 0.03328983072996227
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.04281867982812,
            "upper_bound": 187.12338335408705
          },
          "point_estimate": 187.08656566763185,
          "standard_error": 0.02086413161010042
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03924093758906898,
            "upper_bound": 0.34498144721811297
          },
          "point_estimate": 0.22585725428968736,
          "standard_error": 0.10333107010573082
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.508321528052768,
            "upper_bound": 26.528844528619707
          },
          "point_estimate": 26.517212365649556,
          "standard_error": 0.005359356361217584
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.505697448634937,
            "upper_bound": 26.523084653086645
          },
          "point_estimate": 26.51341018940063,
          "standard_error": 0.005085233295619757
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028151775178210372,
            "upper_bound": 0.022059461791723484
          },
          "point_estimate": 0.012631700873768815,
          "standard_error": 0.004576098072843418
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.50928342415823,
            "upper_bound": 26.519164355649856
          },
          "point_estimate": 26.514999104589847,
          "standard_error": 0.002501990834339754
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006351319861968032,
            "upper_bound": 0.025820994705904012
          },
          "point_estimate": 0.01780098738676106,
          "standard_error": 0.006026933671452958
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.689456648600956,
            "upper_bound": 17.705490966967155
          },
          "point_estimate": 17.69651363192549,
          "standard_error": 0.0041249986346765894
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.68692095455799,
            "upper_bound": 17.699767648899485
          },
          "point_estimate": 17.69533585108553,
          "standard_error": 0.002500445496608178
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009224172081427604,
            "upper_bound": 0.018783491786226507
          },
          "point_estimate": 0.0040088583808284475,
          "standard_error": 0.004841858495367254
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.690919612495854,
            "upper_bound": 17.69792843510963
          },
          "point_estimate": 17.694261227870786,
          "standard_error": 0.001753153880945808
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0041621278193167575,
            "upper_bound": 0.01986230906684942
          },
          "point_estimate": 0.01378272967851424,
          "standard_error": 0.0045573838494730925
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.260034536122873,
            "upper_bound": 26.28218745681602
          },
          "point_estimate": 26.27050574034252,
          "standard_error": 0.005657865326656043
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.2587898718633,
            "upper_bound": 26.29117016928843
          },
          "point_estimate": 26.263319778141305,
          "standard_error": 0.00797251891958038
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002050130645973348,
            "upper_bound": 0.029295551535163625
          },
          "point_estimate": 0.007596758266188493,
          "standard_error": 0.00806842933429882
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.26057142062377,
            "upper_bound": 26.28124469945424
          },
          "point_estimate": 26.2694996016479,
          "standard_error": 0.005547442307460698
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007302254074392567,
            "upper_bound": 0.02265333384495274
          },
          "point_estimate": 0.0188795072733508,
          "standard_error": 0.003474862454393215
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297292.98739473557,
            "upper_bound": 299143.1181175595
          },
          "point_estimate": 298294.500888954,
          "standard_error": 469.013704466039
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297314.88661202183,
            "upper_bound": 299247.98387978144
          },
          "point_estimate": 299109.4254488681,
          "standard_error": 528.0939171710296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.65606393248373,
            "upper_bound": 2217.933508328708
          },
          "point_estimate": 212.53718030457375,
          "standard_error": 548.7517561134429
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297261.7910201991,
            "upper_bound": 299193.79754377366
          },
          "point_estimate": 298563.3364487971,
          "standard_error": 501.1720591695654
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.46468000794609,
            "upper_bound": 1938.481006825527
          },
          "point_estimate": 1559.07405081688,
          "standard_error": 422.2107052627412
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.75744478630864,
            "upper_bound": 15.799256663001987
          },
          "point_estimate": 15.777872505872356,
          "standard_error": 0.01070339119423131
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.746713672346182,
            "upper_bound": 15.810378511755458
          },
          "point_estimate": 15.773729157241892,
          "standard_error": 0.01483242585714517
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007960736368210757,
            "upper_bound": 0.06401192342820812
          },
          "point_estimate": 0.04058784063698856,
          "standard_error": 0.013960126704055765
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.764273257386453,
            "upper_bound": 15.805089371493803
          },
          "point_estimate": 15.786962017677723,
          "standard_error": 0.01029749525209808
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020371572684071176,
            "upper_bound": 0.04369334752542044
          },
          "point_estimate": 0.03563541397669127,
          "standard_error": 0.00590033909893673
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.354705874716352,
            "upper_bound": 23.381191737405896
          },
          "point_estimate": 23.366111505266417,
          "standard_error": 0.006889830148383208
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.350440842039283,
            "upper_bound": 23.37312583128516
          },
          "point_estimate": 23.362034539564775,
          "standard_error": 0.005349929071898391
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030227388667619487,
            "upper_bound": 0.026525020125369746
          },
          "point_estimate": 0.010540705811989692,
          "standard_error": 0.006212623520515588
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.349443995666025,
            "upper_bound": 23.366780170826036
          },
          "point_estimate": 23.357116596649387,
          "standard_error": 0.004516682076747299
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007527396730939319,
            "upper_bound": 0.03339094569427947
          },
          "point_estimate": 0.022963089410855855,
          "standard_error": 0.007904053961974123
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132529.76934654155,
            "upper_bound": 132626.93254431695
          },
          "point_estimate": 132575.13414711502,
          "standard_error": 24.98596941670648
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132515.099756691,
            "upper_bound": 132654.51186131386
          },
          "point_estimate": 132534.75656934307,
          "standard_error": 36.58084537372544
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.618570239805631,
            "upper_bound": 131.0196999093338
          },
          "point_estimate": 45.84489506199969,
          "standard_error": 32.90155631675737
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132516.51146832883,
            "upper_bound": 132558.7693375149
          },
          "point_estimate": 132532.77865200493,
          "standard_error": 10.775961181809972
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.301698789291684,
            "upper_bound": 99.24535516868592
          },
          "point_estimate": 83.40244198418901,
          "standard_error": 15.960518058804908
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.052115488904505,
            "upper_bound": 24.06465163964383
          },
          "point_estimate": 24.05790971551287,
          "standard_error": 0.0032076204852601003
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.05151036683882,
            "upper_bound": 24.063709325117216
          },
          "point_estimate": 24.05421896086667,
          "standard_error": 0.003537324371232556
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010107171507554877,
            "upper_bound": 0.01627488682307524
          },
          "point_estimate": 0.008172596764902085,
          "standard_error": 0.00432732898316049
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.05092116740867,
            "upper_bound": 24.058547401087644
          },
          "point_estimate": 24.054464153283966,
          "standard_error": 0.0018957334909851227
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004568848213154525,
            "upper_bound": 0.014709378931101323
          },
          "point_estimate": 0.01068891889630188,
          "standard_error": 0.0028418970793366244
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.863434356254377,
            "upper_bound": 20.875047093583163
          },
          "point_estimate": 20.86813424037336,
          "standard_error": 0.0031231142815998808
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.8627717091154,
            "upper_bound": 20.86978668847952
          },
          "point_estimate": 20.864714280094436,
          "standard_error": 0.0020208672460743245
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00028754330506244305,
            "upper_bound": 0.008675470046873291
          },
          "point_estimate": 0.0032533576504045186,
          "standard_error": 0.002216247923954077
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.86358478900552,
            "upper_bound": 20.86817559719253
          },
          "point_estimate": 20.8656409388873,
          "standard_error": 0.001205959918525982
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002379596931332382,
            "upper_bound": 0.015732694853692837
          },
          "point_estimate": 0.010464476160250544,
          "standard_error": 0.004328621606886045
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.094663624189266,
            "upper_bound": 49.143943357810535
          },
          "point_estimate": 49.11848486677141,
          "standard_error": 0.012623871300568884
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.08774645872656,
            "upper_bound": 49.15276466885834
          },
          "point_estimate": 49.11484856946947,
          "standard_error": 0.013646758378256506
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006649136485148084,
            "upper_bound": 0.08058379698043139
          },
          "point_estimate": 0.02964270124229332,
          "standard_error": 0.01814829508160652
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.094975342835525,
            "upper_bound": 49.131683524017106
          },
          "point_estimate": 49.112912319656445,
          "standard_error": 0.009198704105055413
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020282566109148803,
            "upper_bound": 0.05368291755812171
          },
          "point_estimate": 0.04215870123221907,
          "standard_error": 0.008430063090763853
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.82739727009638,
            "upper_bound": 90.86387356596138
          },
          "point_estimate": 90.84529936106688,
          "standard_error": 0.00933416966006982
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.80983105144922,
            "upper_bound": 90.86575512080783
          },
          "point_estimate": 90.84504766801356,
          "standard_error": 0.01357707560880124
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025777235718078017,
            "upper_bound": 0.059114163790915865
          },
          "point_estimate": 0.031016705171241464,
          "standard_error": 0.014033036623515512
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.82057540526812,
            "upper_bound": 90.85652836584694
          },
          "point_estimate": 90.837340045188,
          "standard_error": 0.009461281019253186
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017922436319724094,
            "upper_bound": 0.03982366475734327
          },
          "point_estimate": 0.031124829110949097,
          "standard_error": 0.005914158737186156
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.808575180000645,
            "upper_bound": 10.813932496194148
          },
          "point_estimate": 10.81119561624718,
          "standard_error": 0.0013636761817761842
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.806921615103004,
            "upper_bound": 10.81362036098034
          },
          "point_estimate": 10.811646161859528,
          "standard_error": 0.0015420884528526825
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00023893488070946648,
            "upper_bound": 0.008271229797452309
          },
          "point_estimate": 0.004181234435109873,
          "standard_error": 0.0021429975528073288
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.808626984986589,
            "upper_bound": 10.811774668094872
          },
          "point_estimate": 10.810473364304578,
          "standard_error": 0.0008142263900868242
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024804064784798006,
            "upper_bound": 0.005947083809906878
          },
          "point_estimate": 0.0045718963587910695,
          "standard_error": 0.0009199188168087584
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33643.13707140288,
            "upper_bound": 33678.45198834853
          },
          "point_estimate": 33658.77314791566,
          "standard_error": 9.1157508660179
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33639.92865402405,
            "upper_bound": 33669.307971014496
          },
          "point_estimate": 33653.327124355754,
          "standard_error": 9.050401981686546
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.781990664410536,
            "upper_bound": 38.53719871452826
          },
          "point_estimate": 19.688080744364193,
          "standard_error": 9.123650519193715
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33638.34282319105,
            "upper_bound": 33656.204651108324
          },
          "point_estimate": 33645.89697129882,
          "standard_error": 4.418770589933344
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.29269862832897,
            "upper_bound": 43.49916487928608
          },
          "point_estimate": 30.456765879567342,
          "standard_error": 9.626152513567774
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.795524356801504,
            "upper_bound": 24.966708234813808
          },
          "point_estimate": 24.860874291353703,
          "standard_error": 0.04866788415263127
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.793119318444276,
            "upper_bound": 24.858552600531116
          },
          "point_estimate": 24.80023833076666,
          "standard_error": 0.018814518709366207
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017286472909292295,
            "upper_bound": 0.08688260235074165
          },
          "point_estimate": 0.012988947056788628,
          "standard_error": 0.023566647882483856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.79627654818463,
            "upper_bound": 24.894940013890725
          },
          "point_estimate": 24.84014515807484,
          "standard_error": 0.02677521806612792
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00814517636535397,
            "upper_bound": 0.24712267101368607
          },
          "point_estimate": 0.1620510346588694,
          "standard_error": 0.07707343783844038
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104967.8635497834,
            "upper_bound": 1105573.5975841752
          },
          "point_estimate": 1105250.4327501203,
          "standard_error": 155.52383109645646
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104873.6498316498,
            "upper_bound": 1105604.5462121214
          },
          "point_estimate": 1105044.423430736,
          "standard_error": 215.9464599921938
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.52976498259203,
            "upper_bound": 897.3488755840885
          },
          "point_estimate": 487.9913364122374,
          "standard_error": 200.81417488297785
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104966.5480275752,
            "upper_bound": 1105452.1724831122
          },
          "point_estimate": 1105198.9010625738,
          "standard_error": 130.35995441649973
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257.97550597892814,
            "upper_bound": 671.1884806668601
          },
          "point_estimate": 517.8259169923998,
          "standard_error": 111.04390238715564
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2206.285992581289,
            "upper_bound": 2207.4288826141487
          },
          "point_estimate": 2206.7318385753683,
          "standard_error": 0.31312051333621943
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2206.2938914851,
            "upper_bound": 2206.6622296014357
          },
          "point_estimate": 2206.5320480400287,
          "standard_error": 0.1444919525626224
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007582312179581091,
            "upper_bound": 0.6149688581116062
          },
          "point_estimate": 0.2825017580482662,
          "standard_error": 0.1761532672234517
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2206.339606261133,
            "upper_bound": 2206.6106370762436
          },
          "point_estimate": 2206.4924214706216,
          "standard_error": 0.06902752561671
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1539623166665394,
            "upper_bound": 1.590448278154713
          },
          "point_estimate": 1.0407319879498471,
          "standard_error": 0.477529755989774
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.348585274409826,
            "upper_bound": 8.354435234678228
          },
          "point_estimate": 8.351122584218464,
          "standard_error": 0.0015204274465037968
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.348359442423007,
            "upper_bound": 8.35283887114415
          },
          "point_estimate": 8.349504714876659,
          "standard_error": 0.0011334640051116535
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002091630463095845,
            "upper_bound": 0.005982943361144547
          },
          "point_estimate": 0.0025580818604930824,
          "standard_error": 0.0014657037546387329
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.348060848418404,
            "upper_bound": 8.350741789632993
          },
          "point_estimate": 8.349354852567881,
          "standard_error": 0.0006949449000101901
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015249996376955114,
            "upper_bound": 0.007269920213864164
          },
          "point_estimate": 0.0050590987280778035,
          "standard_error": 0.0016990700568952392
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.440529986579919,
            "upper_bound": 7.469787191415666
          },
          "point_estimate": 7.453658025836452,
          "standard_error": 0.007529083819570374
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.4345021839073615,
            "upper_bound": 7.464372042700312
          },
          "point_estimate": 7.452499629872312,
          "standard_error": 0.007364678983433509
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024400024844333655,
            "upper_bound": 0.034522811906797415
          },
          "point_estimate": 0.02223740434481185,
          "standard_error": 0.008900547078650217
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.44130999016126,
            "upper_bound": 7.457284052083465
          },
          "point_estimate": 7.450126719981037,
          "standard_error": 0.004063908630823706
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010257950747968227,
            "upper_bound": 0.03510290087650575
          },
          "point_estimate": 0.02510739934104876,
          "standard_error": 0.007260370161569513
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.940851668385566,
            "upper_bound": 8.994075255272351
          },
          "point_estimate": 8.967468433134993,
          "standard_error": 0.013637706737432772
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.93465796274536,
            "upper_bound": 9.00228708768061
          },
          "point_estimate": 8.966778898367128,
          "standard_error": 0.022233794761256535
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005901198914937315,
            "upper_bound": 0.07438492247566839
          },
          "point_estimate": 0.04529402276835292,
          "standard_error": 0.019291891961275263
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.948660484098962,
            "upper_bound": 8.996805303663646
          },
          "point_estimate": 8.98030958785613,
          "standard_error": 0.01220589857165758
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027406385995493228,
            "upper_bound": 0.05701247347996116
          },
          "point_estimate": 0.045432209656672995,
          "standard_error": 0.00769797557387527
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.398627748393285,
            "upper_bound": 19.40876742483279
          },
          "point_estimate": 19.40299821593273,
          "standard_error": 0.0026342801386464113
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.396387414327823,
            "upper_bound": 19.40547157697788
          },
          "point_estimate": 19.401799806427547,
          "standard_error": 0.0021955440895064155
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012156573018090716,
            "upper_bound": 0.010823846739329328
          },
          "point_estimate": 0.005857918790518266,
          "standard_error": 0.002529379050334962
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.39839946599812,
            "upper_bound": 19.404445075942125
          },
          "point_estimate": 19.401404211891386,
          "standard_error": 0.001588264211678282
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030870998892038636,
            "upper_bound": 0.01272167822873354
          },
          "point_estimate": 0.00876479745919725,
          "standard_error": 0.0029636997413304395
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.261186496970016,
            "upper_bound": 13.271373365446133
          },
          "point_estimate": 13.266083824972984,
          "standard_error": 0.002617728389048372
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.257770548983189,
            "upper_bound": 13.272387400077314
          },
          "point_estimate": 13.266643509401485,
          "standard_error": 0.0036697157030961087
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012142243559322484,
            "upper_bound": 0.015097195368924104
          },
          "point_estimate": 0.011213205055741514,
          "standard_error": 0.004017444876095131
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.25780751061382,
            "upper_bound": 13.26608919171372
          },
          "point_estimate": 13.26088136376045,
          "standard_error": 0.0021337987551603324
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004777288176879634,
            "upper_bound": 0.011031479848219316
          },
          "point_estimate": 0.008735425157004357,
          "standard_error": 0.0016535630821307404
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.142090024485583,
            "upper_bound": 19.154023926996594
          },
          "point_estimate": 19.14697900936317,
          "standard_error": 0.0031757089973830205
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.14070971700324,
            "upper_bound": 19.14850054698721
          },
          "point_estimate": 19.14484427823958,
          "standard_error": 0.0018608457948700785
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001178089619301246,
            "upper_bound": 0.009231214244273605
          },
          "point_estimate": 0.004393562831966758,
          "standard_error": 0.002256234849897477
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.143454495051436,
            "upper_bound": 19.148079442614325
          },
          "point_estimate": 19.145855209050463,
          "standard_error": 0.001196538638695522
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002520542016296531,
            "upper_bound": 0.015865875382642825
          },
          "point_estimate": 0.010600440304879713,
          "standard_error": 0.004286240420049336
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.847624990390244,
            "upper_bound": 10.909562888495516
          },
          "point_estimate": 10.876329257796584,
          "standard_error": 0.01580463837689742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.84412819679367,
            "upper_bound": 10.894078510205992
          },
          "point_estimate": 10.87629570018036,
          "standard_error": 0.013806426506363998
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005788403311548625,
            "upper_bound": 0.07826778790215333
          },
          "point_estimate": 0.02793516490911369,
          "standard_error": 0.018010360796875197
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.852935439205153,
            "upper_bound": 10.89405911024514
          },
          "point_estimate": 10.87296150365636,
          "standard_error": 0.010447716659121344
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02016734855627654,
            "upper_bound": 0.0744991788852343
          },
          "point_estimate": 0.052851120961445605,
          "standard_error": 0.015104328746274575
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.729168776945322,
            "upper_bound": 15.746913471611208
          },
          "point_estimate": 15.73816148342154,
          "standard_error": 0.004532465123603265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.728587259204268,
            "upper_bound": 15.746994134104437
          },
          "point_estimate": 15.739322666220202,
          "standard_error": 0.0037005112934228633
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018566731857373976,
            "upper_bound": 0.02628358440499673
          },
          "point_estimate": 0.006775669197038292,
          "standard_error": 0.0067680258209120246
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.735272958225558,
            "upper_bound": 15.748835350325876
          },
          "point_estimate": 15.742344055451015,
          "standard_error": 0.0035142647258578795
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00633338975167933,
            "upper_bound": 0.02027391339723774
          },
          "point_estimate": 0.015101933028320589,
          "standard_error": 0.003510022598823202
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.353547148831513,
            "upper_bound": 23.3881888888298
          },
          "point_estimate": 23.368573839394017,
          "standard_error": 0.009002007384126055
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.35080740487335,
            "upper_bound": 23.37779667390991
          },
          "point_estimate": 23.36279141281269,
          "standard_error": 0.0063386185349753495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002585679597241144,
            "upper_bound": 0.03617170513246558
          },
          "point_estimate": 0.011583881337273104,
          "standard_error": 0.008703178547939798
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.35733023112511,
            "upper_bound": 23.369143920450043
          },
          "point_estimate": 23.363076488297395,
          "standard_error": 0.0030765161606140967
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009334996071764478,
            "upper_bound": 0.04328998149679948
          },
          "point_estimate": 0.029969811552263137,
          "standard_error": 0.010182260229012412
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.070672183685552,
            "upper_bound": 11.128890014809569
          },
          "point_estimate": 11.098856067752392,
          "standard_error": 0.0148738311373151
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.060851587330404,
            "upper_bound": 11.134858588179242
          },
          "point_estimate": 11.089268938891538,
          "standard_error": 0.016967733615394238
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004067337653992886,
            "upper_bound": 0.08563958179601776
          },
          "point_estimate": 0.04486959773242788,
          "standard_error": 0.02070627691865474
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.056064538872338,
            "upper_bound": 11.141259183964564
          },
          "point_estimate": 11.096031754727054,
          "standard_error": 0.022267274329462895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02569549124687692,
            "upper_bound": 0.064138274582578
          },
          "point_estimate": 0.04967102926485567,
          "standard_error": 0.009904206186675105
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.048732587709527,
            "upper_bound": 24.063441668919456
          },
          "point_estimate": 24.055601483220887,
          "standard_error": 0.003771399582248523
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.04642732473459,
            "upper_bound": 24.060477431972327
          },
          "point_estimate": 24.055137908610448,
          "standard_error": 0.0033335775903979795
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002058217157365346,
            "upper_bound": 0.01879906993338689
          },
          "point_estimate": 0.009609630185497455,
          "standard_error": 0.0044038986659980794
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.049891638675888,
            "upper_bound": 24.058499876166188
          },
          "point_estimate": 24.05447478282674,
          "standard_error": 0.002180638037493303
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005072803999577354,
            "upper_bound": 0.017462250444786744
          },
          "point_estimate": 0.012510578109963762,
          "standard_error": 0.003419897027300381
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.880088402958336,
            "upper_bound": 20.88461290555339
          },
          "point_estimate": 20.882240365324872,
          "standard_error": 0.0011649362173081425
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.87934012076109,
            "upper_bound": 20.88530338136788
          },
          "point_estimate": 20.88099337909966,
          "standard_error": 0.0016677103253381314
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000249994913600857,
            "upper_bound": 0.006169103381521313
          },
          "point_estimate": 0.0025141755824500647,
          "standard_error": 0.0015762091123492292
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.88018821538938,
            "upper_bound": 20.88556947541257
          },
          "point_estimate": 20.882916595307098,
          "standard_error": 0.0014436038903077987
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00181769159333842,
            "upper_bound": 0.004777211536525085
          },
          "point_estimate": 0.0038909625915491417,
          "standard_error": 0.0007313453301148874
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1187066.4707680494,
            "upper_bound": 1187854.188215054
          },
          "point_estimate": 1187412.583671275,
          "standard_error": 203.9569899206425
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1186957.4838709678,
            "upper_bound": 1187810.182795699
          },
          "point_estimate": 1187201.9634408602,
          "standard_error": 162.8813907288424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.54841592997695,
            "upper_bound": 840.5225915291127
          },
          "point_estimate": 275.4663918836191,
          "standard_error": 200.36689119256008
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1187041.9032971738,
            "upper_bound": 1187322.113978495
          },
          "point_estimate": 1187178.7180561374,
          "standard_error": 70.57256205248382
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.28093406620178,
            "upper_bound": 907.6856992960836
          },
          "point_estimate": 680.3072087375826,
          "standard_error": 202.48159376809667
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1644700.6454261558,
            "upper_bound": 1646543.6535936422
          },
          "point_estimate": 1645503.5490976537,
          "standard_error": 474.4734105307319
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1644869.015942029,
            "upper_bound": 1645778.6739130437
          },
          "point_estimate": 1645084.860326087,
          "standard_error": 252.6287053015192
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.780536372500094,
            "upper_bound": 1859.1501778194104
          },
          "point_estimate": 407.2324571179574,
          "standard_error": 494.7721241545747
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1644519.0323229884,
            "upper_bound": 1645271.327166204
          },
          "point_estimate": 1644959.506606437,
          "standard_error": 191.74800989679844
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388.4639748791516,
            "upper_bound": 2296.35549759414
          },
          "point_estimate": 1583.5104480232048,
          "standard_error": 558.8935626210018
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1450005.4363985714,
            "upper_bound": 1457190.4597714285
          },
          "point_estimate": 1453782.5298571426,
          "standard_error": 1847.964185340468
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1447942.51,
            "upper_bound": 1458552.797
          },
          "point_estimate": 1457358.072285714,
          "standard_error": 3245.2193832099247
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 333.7091088757031,
            "upper_bound": 9490.733686305808
          },
          "point_estimate": 2443.2991288229578,
          "standard_error": 2953.855800915787
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1455413.053015826,
            "upper_bound": 1458253.8894858267
          },
          "point_estimate": 1457369.8017662338,
          "standard_error": 746.7804534228849
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3126.5562459455496,
            "upper_bound": 7202.761744257302
          },
          "point_estimate": 6137.308860476413,
          "standard_error": 1035.4636809813103
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400181.721161303,
            "upper_bound": 401104.1515879121
          },
          "point_estimate": 400639.08502354793,
          "standard_error": 235.9370864265948
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399945.7005494505,
            "upper_bound": 401260.856959707
          },
          "point_estimate": 400714.8104395605,
          "standard_error": 373.4675108394254
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.1013497450824,
            "upper_bound": 1357.1524651364882
          },
          "point_estimate": 1048.0413679320343,
          "standard_error": 301.33430989216413
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400030.10868031054,
            "upper_bound": 400802.82433470146
          },
          "point_estimate": 400343.84458398743,
          "standard_error": 195.93167830959283
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477.6656566074682,
            "upper_bound": 965.9402308375664
          },
          "point_estimate": 783.8966318675348,
          "standard_error": 126.4429461230912
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630570.341769294,
            "upper_bound": 631972.6211129925
          },
          "point_estimate": 631229.0523775315,
          "standard_error": 360.02990138079645
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630154.5754310344,
            "upper_bound": 631969.0344827587
          },
          "point_estimate": 631171.6889367816,
          "standard_error": 471.26238525114707
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179.86003947925184,
            "upper_bound": 1954.1466467724408
          },
          "point_estimate": 1303.3382844042708,
          "standard_error": 492.0812437301649
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630149.4947438341,
            "upper_bound": 631014.777718833
          },
          "point_estimate": 630417.9789520825,
          "standard_error": 224.62004368881196
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 592.0666356245829,
            "upper_bound": 1566.287037487259
          },
          "point_estimate": 1198.2139188478477,
          "standard_error": 264.43776418279816
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392741.8574193549,
            "upper_bound": 392922.2470754608
          },
          "point_estimate": 392824.86939836154,
          "standard_error": 46.351659648407825
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392711.53817204305,
            "upper_bound": 392906.6747311827
          },
          "point_estimate": 392778.63901689707,
          "standard_error": 55.32142206288616
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.439219832423223,
            "upper_bound": 233.8522474611356
          },
          "point_estimate": 164.48968449908364,
          "standard_error": 60.09905622964164
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392738.3557919854,
            "upper_bound": 392814.54206989246
          },
          "point_estimate": 392769.85172461945,
          "standard_error": 19.218471870367132
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.94500174953619,
            "upper_bound": 211.39724728230067
          },
          "point_estimate": 154.0273286966884,
          "standard_error": 40.08522747431293
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482218.1050838816,
            "upper_bound": 482575.78625
          },
          "point_estimate": 482392.278245614,
          "standard_error": 91.41041503679445
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482129.5032894737,
            "upper_bound": 482703.600877193
          },
          "point_estimate": 482349.3763157895,
          "standard_error": 123.16670088875452
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.2382395988912,
            "upper_bound": 529.3784146147941
          },
          "point_estimate": 330.7802465616573,
          "standard_error": 134.93065737481712
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482257.07542051,
            "upper_bound": 482597.8408059983
          },
          "point_estimate": 482441.4913875598,
          "standard_error": 85.81826915737187
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165.46918172808466,
            "upper_bound": 378.84173695717425
          },
          "point_estimate": 305.7263222266712,
          "standard_error": 53.21012221421608
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1014964.5294543653,
            "upper_bound": 1015646.1588194446
          },
          "point_estimate": 1015285.2155974428,
          "standard_error": 174.5517904519834
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1014817.314484127,
            "upper_bound": 1015564.3333333333
          },
          "point_estimate": 1015336.34375,
          "standard_error": 191.86087359128905
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.09684692067184,
            "upper_bound": 992.1122022937936
          },
          "point_estimate": 519.5247991099559,
          "standard_error": 236.77191283447553
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1014843.4424692424,
            "upper_bound": 1015375.259242242
          },
          "point_estimate": 1015097.9984848484,
          "standard_error": 135.71184619319837
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287.4973245967812,
            "upper_bound": 782.3725725093353
          },
          "point_estimate": 582.1881178099173,
          "standard_error": 137.14379886165426
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332773.344737013,
            "upper_bound": 333205.88807575754
          },
          "point_estimate": 332997.20452200575,
          "standard_error": 110.713166321553
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332759.70454545453,
            "upper_bound": 333263.2914141414
          },
          "point_estimate": 333038.89,
          "standard_error": 120.24578025072108
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.19651691662628,
            "upper_bound": 608.4170970771779
          },
          "point_estimate": 357.05808968369473,
          "standard_error": 155.02912068590257
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332641.7372991987,
            "upper_bound": 333155.8368236814
          },
          "point_estimate": 332888.2189846517,
          "standard_error": 131.80622466942293
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.47659505816765,
            "upper_bound": 478.40105634811863
          },
          "point_estimate": 367.0408516032282,
          "standard_error": 76.0612498905625
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 832651.0116061509,
            "upper_bound": 833109.8099242423
          },
          "point_estimate": 832879.7135569985,
          "standard_error": 117.3777231047359
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 832481.8295454546,
            "upper_bound": 833158.5511363636
          },
          "point_estimate": 832960.8897727273,
          "standard_error": 177.19505535297156
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.50532172420237,
            "upper_bound": 714.3374461815405
          },
          "point_estimate": 332.1177442552495,
          "standard_error": 166.5857127061659
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 832540.9464417686,
            "upper_bound": 832955.5250775093
          },
          "point_estimate": 832744.4940377804,
          "standard_error": 105.41033376742784
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233.10176094766132,
            "upper_bound": 491.8893573414894
          },
          "point_estimate": 392.0777479265461,
          "standard_error": 67.05559505133478
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400356.9248076923,
            "upper_bound": 400603.6769649398
          },
          "point_estimate": 400471.1206527996,
          "standard_error": 63.272581466930255
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400325.74267399264,
            "upper_bound": 400570.5434065934
          },
          "point_estimate": 400452.1751373626,
          "standard_error": 58.62808668446074
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.53623783063401,
            "upper_bound": 319.3690248795127
          },
          "point_estimate": 146.35693948957314,
          "standard_error": 76.48495356072773
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400286.3384493707,
            "upper_bound": 400483.7858255105
          },
          "point_estimate": 400368.95264735265,
          "standard_error": 49.64616551398019
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.78751472862795,
            "upper_bound": 290.14369329963637
          },
          "point_estimate": 210.83489952808404,
          "standard_error": 55.87382862149497
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175261.30905052845,
            "upper_bound": 175740.3912900641
          },
          "point_estimate": 175490.26823527165,
          "standard_error": 122.95748628734972
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175254.4119162088,
            "upper_bound": 175847.6015625
          },
          "point_estimate": 175366.72283653845,
          "standard_error": 146.27698814959737
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.03708273078185,
            "upper_bound": 669.2183639603436
          },
          "point_estimate": 236.761593152419,
          "standard_error": 179.30691877465495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175088.25147452418,
            "upper_bound": 175496.6170215288
          },
          "point_estimate": 175254.83878621378,
          "standard_error": 103.73132549957003
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.0112083378883,
            "upper_bound": 523.6040276040815
          },
          "point_estimate": 408.1449781683889,
          "standard_error": 84.81722245226892
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232978.45294762915,
            "upper_bound": 233164.8724508581
          },
          "point_estimate": 233048.28474345367,
          "standard_error": 52.80651740811089
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232962.0912951168,
            "upper_bound": 233035.93590764332
          },
          "point_estimate": 233001.7377388535,
          "standard_error": 21.530699689906857
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.431888905905601,
            "upper_bound": 81.0429751854173
          },
          "point_estimate": 44.82155214479257,
          "standard_error": 23.400274904527063
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232983.4135577798,
            "upper_bound": 233029.84998673035
          },
          "point_estimate": 233002.709620316,
          "standard_error": 12.013832113796688
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.277397171882612,
            "upper_bound": 270.7431818884675
          },
          "point_estimate": 176.3401351980466,
          "standard_error": 85.77209898052162
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175283.07243589743,
            "upper_bound": 175325.64170997404
          },
          "point_estimate": 175303.83668574484,
          "standard_error": 10.914190478930816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175271.4480769231,
            "upper_bound": 175348.49759615384
          },
          "point_estimate": 175299.6743742369,
          "standard_error": 18.093250969668293
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.461516387698056,
            "upper_bound": 62.967097984672584
          },
          "point_estimate": 48.37816263472644,
          "standard_error": 15.777521847046255
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175281.5866261659,
            "upper_bound": 175330.45668362576
          },
          "point_estimate": 175303.49892607392,
          "standard_error": 12.606752815824231
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.439296627518235,
            "upper_bound": 42.84438057726141
          },
          "point_estimate": 36.4960927998399,
          "standard_error": 5.376456093994536
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 676013.4192827748,
            "upper_bound": 676672.8279660493
          },
          "point_estimate": 676266.5223074661,
          "standard_error": 183.2491357604767
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 675979.6640211641,
            "upper_bound": 676259.5138888889
          },
          "point_estimate": 676102.2937242799,
          "standard_error": 76.40226445325511
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.676275807432431,
            "upper_bound": 341.0413385864908
          },
          "point_estimate": 207.45270326138257,
          "standard_error": 95.64099366192076
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 675924.9393838492,
            "upper_bound": 676201.0342312009
          },
          "point_estimate": 676054.0515632515,
          "standard_error": 73.10054106609707
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.8878624214884,
            "upper_bound": 932.370675612596
          },
          "point_estimate": 609.0773660762452,
          "standard_error": 284.3415871483245
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1365.8417840794389,
            "upper_bound": 1366.8754077855208
          },
          "point_estimate": 1366.264688828816,
          "standard_error": 0.2742499311303162
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1365.7888854587236,
            "upper_bound": 1366.2814792543595
          },
          "point_estimate": 1366.1278727262477,
          "standard_error": 0.11748630455959427
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.054382801199341045,
            "upper_bound": 0.8147123904517238
          },
          "point_estimate": 0.19418448993800635,
          "standard_error": 0.2075283219966353
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1365.8255983273905,
            "upper_bound": 1366.1785937287357
          },
          "point_estimate": 1366.030391699401,
          "standard_error": 0.08926454271632504
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.16150707979002307,
            "upper_bound": 1.37197492570845
          },
          "point_estimate": 0.9128890723547842,
          "standard_error": 0.3756919928435518
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355233.568659612,
            "upper_bound": 1356888.8548020285
          },
          "point_estimate": 1355880.0715696649,
          "standard_error": 453.0475028384729
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355024.1419753083,
            "upper_bound": 1355900.343209877
          },
          "point_estimate": 1355442.6175925925,
          "standard_error": 253.71584370271205
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.50927574496691,
            "upper_bound": 1048.6002091611433
          },
          "point_estimate": 648.1272727526957,
          "standard_error": 238.25770980791793
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355203.459791283,
            "upper_bound": 1355829.9077853365
          },
          "point_estimate": 1355562.022991823,
          "standard_error": 158.6149478771219
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 301.3685722574434,
            "upper_bound": 2311.974069916033
          },
          "point_estimate": 1516.3647687265589,
          "standard_error": 680.919484270224
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523850.176904762,
            "upper_bound": 1525964.190026414
          },
          "point_estimate": 1524688.221592262,
          "standard_error": 578.0543402335979
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523774.9375,
            "upper_bound": 1524963.2291666667
          },
          "point_estimate": 1524008.269270833,
          "standard_error": 281.6424210276773
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.60264624657547,
            "upper_bound": 1281.1169538183
          },
          "point_estimate": 379.77934544511646,
          "standard_error": 338.3909518785905
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523685.6097389555,
            "upper_bound": 1524203.7330538635
          },
          "point_estimate": 1523884.486038961,
          "standard_error": 131.76922567169385
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218.29691421235304,
            "upper_bound": 2900.9265164784456
          },
          "point_estimate": 1928.6763681308755,
          "standard_error": 835.1046596507335
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1424862.8402930405,
            "upper_bound": 1426105.032996795
          },
          "point_estimate": 1425495.8249084246,
          "standard_error": 318.9065103628834
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1424450.4615384615,
            "upper_bound": 1426406.987179487
          },
          "point_estimate": 1425780.964285714,
          "standard_error": 605.9941263759144
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.44836365477732,
            "upper_bound": 1638.629365139269
          },
          "point_estimate": 1281.807467627904,
          "standard_error": 432.6118607381273
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1425077.6822164296,
            "upper_bound": 1426448.6099028797
          },
          "point_estimate": 1425920.628171828,
          "standard_error": 353.7992290068648
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 687.8164814865381,
            "upper_bound": 1228.0354007476662
          },
          "point_estimate": 1061.870530960898,
          "standard_error": 138.19084069001093
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789275.1686989194,
            "upper_bound": 789698.7931408307
          },
          "point_estimate": 789471.5869056063,
          "standard_error": 108.253982492936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789185.5248226951,
            "upper_bound": 789744.3111702128
          },
          "point_estimate": 789370.1731678487,
          "standard_error": 161.2386172412888
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.92355606416926,
            "upper_bound": 641.410996538173
          },
          "point_estimate": 293.40445684782645,
          "standard_error": 149.14472604303214
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789175.8762720533,
            "upper_bound": 789567.760951189
          },
          "point_estimate": 789357.358662614,
          "standard_error": 101.62295635657054
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 178.88514475159997,
            "upper_bound": 472.5234990632915
          },
          "point_estimate": 359.991549326322,
          "standard_error": 81.06684197375907
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278734.09233805124,
            "upper_bound": 278940.6598182479
          },
          "point_estimate": 278835.6471589119,
          "standard_error": 52.95816965657609
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278719.1469465649,
            "upper_bound": 279000.28026172303
          },
          "point_estimate": 278793.87798982183,
          "standard_error": 74.15110012455003
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.57900505802278,
            "upper_bound": 309.09988380624395
          },
          "point_estimate": 144.98387897820658,
          "standard_error": 73.42061793669889
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278757.701493501,
            "upper_bound": 278915.30342232453
          },
          "point_estimate": 278833.96195102605,
          "standard_error": 40.01979316973325
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.64051208000058,
            "upper_bound": 222.513718036886
          },
          "point_estimate": 176.27692016782453,
          "standard_error": 31.359604132912256
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492196.40515588794,
            "upper_bound": 492515.55434362934
          },
          "point_estimate": 492351.1536679537,
          "standard_error": 81.7737145243857
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492160.8745173745,
            "upper_bound": 492549.96846846846
          },
          "point_estimate": 492305.31396396394,
          "standard_error": 110.31131651553405
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.99194439366214,
            "upper_bound": 448.77366230299486
          },
          "point_estimate": 242.93101798441543,
          "standard_error": 101.14009131994368
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492218.5668026629,
            "upper_bound": 492415.3285277228
          },
          "point_estimate": 492309.5069147069,
          "standard_error": 50.48381957618015
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.8530248420898,
            "upper_bound": 349.5279769339895
          },
          "point_estimate": 271.8929942265353,
          "standard_error": 53.035586828336726
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252441.20710978837,
            "upper_bound": 253106.47944782025
          },
          "point_estimate": 252875.03768463404,
          "standard_error": 202.23997287596345
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253029.625,
            "upper_bound": 253119.7216435185
          },
          "point_estimate": 253095.61000881833,
          "standard_error": 54.08505491507111
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.2431282903595,
            "upper_bound": 108.07250062761896
          },
          "point_estimate": 46.06613147381736,
          "standard_error": 66.34723283707136
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253039.18224201415,
            "upper_bound": 253116.76191626408
          },
          "point_estimate": 253086.68385642135,
          "standard_error": 20.19862962830948
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.13217747677601,
            "upper_bound": 1033.30543546
          },
          "point_estimate": 673.2563320821528,
          "standard_error": 368.2565743380564
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632539.058045977,
            "upper_bound": 633083.643065271
          },
          "point_estimate": 632783.1212972086,
          "standard_error": 140.60594691222755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632440.8793103448,
            "upper_bound": 633071.4507389162
          },
          "point_estimate": 632611.925862069,
          "standard_error": 157.32239916253755
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.5408147162353,
            "upper_bound": 682.3794189197788
          },
          "point_estimate": 385.466863846235,
          "standard_error": 161.38158818887928
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632531.096486457,
            "upper_bound": 632953.8868986166
          },
          "point_estimate": 632709.7367666815,
          "standard_error": 111.46053439789569
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 181.552323838458,
            "upper_bound": 638.5547731897519
          },
          "point_estimate": 468.1202410299671,
          "standard_error": 126.5926118668028
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2046.025305786964,
            "upper_bound": 2047.3613280578
          },
          "point_estimate": 2046.6509791723288,
          "standard_error": 0.34272618383670916
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2045.8351254046288,
            "upper_bound": 2047.2523428327463
          },
          "point_estimate": 2046.5710666591324,
          "standard_error": 0.36513033392002897
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.17172172097234786,
            "upper_bound": 1.8218689318865215
          },
          "point_estimate": 0.8068931184090653,
          "standard_error": 0.4063310539677083
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2045.9250413589216,
            "upper_bound": 2047.0652168620793
          },
          "point_estimate": 2046.4286618393755,
          "standard_error": 0.2993762084100542
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5262432063923719,
            "upper_bound": 1.5552995888403998
          },
          "point_estimate": 1.1427756238044458,
          "standard_error": 0.28272250742180927
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648.6305291371457,
            "upper_bound": 648.8618361107692
          },
          "point_estimate": 648.7367767995345,
          "standard_error": 0.05961584079575432
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648.5929780626167,
            "upper_bound": 648.9192543238752
          },
          "point_estimate": 648.6655895447365,
          "standard_error": 0.07634050308695149
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0287715526990783,
            "upper_bound": 0.3385278220333599
          },
          "point_estimate": 0.11432877283385148,
          "standard_error": 0.0730919789750528
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648.623037790741,
            "upper_bound": 648.8484849349874
          },
          "point_estimate": 648.7395759310343,
          "standard_error": 0.05727288939964342
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0652435918644377,
            "upper_bound": 0.2595540552904485
          },
          "point_estimate": 0.19876395003676972,
          "standard_error": 0.047154230821746614
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.56751070139784,
            "upper_bound": 125.64966645768638
          },
          "point_estimate": 125.6059622805097,
          "standard_error": 0.021137692517283972
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.55522252508187,
            "upper_bound": 125.6692310128295
          },
          "point_estimate": 125.5790701229518,
          "standard_error": 0.027585062274474294
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007903454532652609,
            "upper_bound": 0.1190166912122486
          },
          "point_estimate": 0.0574903462315627,
          "standard_error": 0.028462497217022535
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.55858093919876,
            "upper_bound": 125.62695823503498
          },
          "point_estimate": 125.58196507271796,
          "standard_error": 0.017684863464362505
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029993637318278032,
            "upper_bound": 0.08818999827263117
          },
          "point_estimate": 0.07048420580198922,
          "standard_error": 0.01474016494473945
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.31388591439545,
            "upper_bound": 50.34320730199096
          },
          "point_estimate": 50.32727143400852,
          "standard_error": 0.007532270748493895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.31020102415058,
            "upper_bound": 50.33890272876156
          },
          "point_estimate": 50.32313737803612,
          "standard_error": 0.00811395433504842
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003692637398980989,
            "upper_bound": 0.037222658504626686
          },
          "point_estimate": 0.02093526541962131,
          "standard_error": 0.008332304851497451
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.312372469299206,
            "upper_bound": 50.327674306276386
          },
          "point_estimate": 50.32001307059144,
          "standard_error": 0.003930491398845639
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010647410104335267,
            "upper_bound": 0.03469289514180458
          },
          "point_estimate": 0.025265512232514577,
          "standard_error": 0.0067329514104634445
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.90243048656116,
            "upper_bound": 78.94983526250597
          },
          "point_estimate": 78.92397863852797,
          "standard_error": 0.01218983203150635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.89281368931336,
            "upper_bound": 78.93654004775343
          },
          "point_estimate": 78.92540647255211,
          "standard_error": 0.012125515314473615
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004604523643032986,
            "upper_bound": 0.06033718735220746
          },
          "point_estimate": 0.02518242342963792,
          "standard_error": 0.014143295077480168
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.88743049969152,
            "upper_bound": 78.92174920063712
          },
          "point_estimate": 78.90005310946921,
          "standard_error": 0.008747223856911311
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016390716728046636,
            "upper_bound": 0.057591907660153645
          },
          "point_estimate": 0.04067105730925699,
          "standard_error": 0.012050021964073333
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299188.07049086,
            "upper_bound": 299365.3825450494
          },
          "point_estimate": 299268.8596148842,
          "standard_error": 45.61138310801304
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299156.53538251366,
            "upper_bound": 299369.05225409835
          },
          "point_estimate": 299220.1944672131,
          "standard_error": 46.783098634502
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.24653323211308,
            "upper_bound": 237.45843734161397
          },
          "point_estimate": 110.68709276164093,
          "standard_error": 54.845017721039945
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299193.5514222869,
            "upper_bound": 299275.34573636483
          },
          "point_estimate": 299231.73065786675,
          "standard_error": 21.157499561545123
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.201557168779026,
            "upper_bound": 202.49554091509677
          },
          "point_estimate": 152.4062817177746,
          "standard_error": 38.87552685301739
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.72421846670872,
            "upper_bound": 59.781167919956786
          },
          "point_estimate": 59.74850844833909,
          "standard_error": 0.014842514505282085
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.724660717926454,
            "upper_bound": 59.761250896092704
          },
          "point_estimate": 59.736035511852535,
          "standard_error": 0.009311825467645404
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010883522223905896,
            "upper_bound": 0.0536516651633504
          },
          "point_estimate": 0.025813980155414456,
          "standard_error": 0.012965800684284035
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.73032306901822,
            "upper_bound": 59.74486469512664
          },
          "point_estimate": 59.73750231576919,
          "standard_error": 0.0036937122775404273
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012991869086481034,
            "upper_bound": 0.07277379327852013
          },
          "point_estimate": 0.04954511554924822,
          "standard_error": 0.01811664908440376
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.76791646019772,
            "upper_bound": 97.8194567014956
          },
          "point_estimate": 97.7892782973216,
          "standard_error": 0.01360299554397253
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.76050683946556,
            "upper_bound": 97.79505287171516
          },
          "point_estimate": 97.78138100675082,
          "standard_error": 0.008202278663621545
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004834201178890844,
            "upper_bound": 0.04422343493086746
          },
          "point_estimate": 0.015727331740267667,
          "standard_error": 0.010498594958292695
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.76611059000425,
            "upper_bound": 97.78270665990892
          },
          "point_estimate": 97.77456974570472,
          "standard_error": 0.004182475645543239
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011683881941175178,
            "upper_bound": 0.06750054385183207
          },
          "point_estimate": 0.04514450688221359,
          "standard_error": 0.017759636220214307
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132691.0216788321,
            "upper_bound": 132759.88755877435
          },
          "point_estimate": 132724.36745235202,
          "standard_error": 17.62161428499704
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132677.92890815088,
            "upper_bound": 132775.52238442822
          },
          "point_estimate": 132711.80656934308,
          "standard_error": 21.174365018924668
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.69679138043182,
            "upper_bound": 106.18734526807864
          },
          "point_estimate": 50.24625100571555,
          "standard_error": 24.200436645598604
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132667.86714290656,
            "upper_bound": 132775.06831755835
          },
          "point_estimate": 132719.40786804436,
          "standard_error": 29.03671844291059
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.966472858451635,
            "upper_bound": 73.66489044653628
          },
          "point_estimate": 58.767203275396746,
          "standard_error": 11.250045418966693
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.40822292599952,
            "upper_bound": 59.445085093580694
          },
          "point_estimate": 59.424446329951294,
          "standard_error": 0.009544894192931449
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.397372437640534,
            "upper_bound": 59.43501117763949
          },
          "point_estimate": 59.42168159742195,
          "standard_error": 0.009652171059929354
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003219731254616639,
            "upper_bound": 0.04530761518815309
          },
          "point_estimate": 0.02062107642951403,
          "standard_error": 0.010076100716854185
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.405281761491395,
            "upper_bound": 59.42732936779379
          },
          "point_estimate": 59.41655571654722,
          "standard_error": 0.005535410508915939
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012670011282719247,
            "upper_bound": 0.045632951286255066
          },
          "point_estimate": 0.031782331237077556,
          "standard_error": 0.010088240152383578
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.63353515947188,
            "upper_bound": 93.66971291810503
          },
          "point_estimate": 93.65068110639687,
          "standard_error": 0.009304856776627887
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.62742581541204,
            "upper_bound": 93.67211069196004
          },
          "point_estimate": 93.64477160560632,
          "standard_error": 0.010375828369844305
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0036738610377322857,
            "upper_bound": 0.0523107149306717
          },
          "point_estimate": 0.020754801659948495,
          "standard_error": 0.011939799445170086
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.63018532183824,
            "upper_bound": 93.65473283686418
          },
          "point_estimate": 93.63972975329942,
          "standard_error": 0.006161533141280696
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01427673892627701,
            "upper_bound": 0.04036350008427947
          },
          "point_estimate": 0.03098628723876845,
          "standard_error": 0.0067877436308230975
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243.79239177905268,
            "upper_bound": 244.16084482677184
          },
          "point_estimate": 243.9566951580132,
          "standard_error": 0.0955239888330189
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243.80514651596707,
            "upper_bound": 244.134828555736
          },
          "point_estimate": 243.84633438951732,
          "standard_error": 0.07209588834575273
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010461982802378152,
            "upper_bound": 0.4456191570271177
          },
          "point_estimate": 0.07999210170146899,
          "standard_error": 0.10752862639305956
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243.81984855563047,
            "upper_bound": 244.0400122059704
          },
          "point_estimate": 243.89335357180775,
          "standard_error": 0.05851068464341414
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06201284670893344,
            "upper_bound": 0.4280576711527187
          },
          "point_estimate": 0.31906365608308207,
          "standard_error": 0.0927220726307243
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 487.7709886882042,
            "upper_bound": 488.7295817854474
          },
          "point_estimate": 488.14980694748886,
          "standard_error": 0.2620773015373694
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 487.6700675249359,
            "upper_bound": 488.2069243264287
          },
          "point_estimate": 487.94035232071286,
          "standard_error": 0.1492906505808642
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09401359728921792,
            "upper_bound": 0.640543571242136
          },
          "point_estimate": 0.3611184107130781,
          "standard_error": 0.14527977890078314
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 487.76426983711485,
            "upper_bound": 488.04589397037273
          },
          "point_estimate": 487.8920711244294,
          "standard_error": 0.07199429588742834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.18109840705658156,
            "upper_bound": 1.326636758724065
          },
          "point_estimate": 0.8721451712871904,
          "standard_error": 0.3857186105656208
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.12694822337748,
            "upper_bound": 33.15262397678714
          },
          "point_estimate": 33.137637369218915,
          "standard_error": 0.006801876174964934
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.123603200770276,
            "upper_bound": 33.14394762318315
          },
          "point_estimate": 33.13103919598926,
          "standard_error": 0.0045191215912136155
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002266852271808273,
            "upper_bound": 0.022609945823243337
          },
          "point_estimate": 0.008660230612750586,
          "standard_error": 0.005423569442791761
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.12611754855844,
            "upper_bound": 33.133155598541734
          },
          "point_estimate": 33.129148369962955,
          "standard_error": 0.001801749107121842
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00491916951200309,
            "upper_bound": 0.033256964725273536
          },
          "point_estimate": 0.022671258995843702,
          "standard_error": 0.008518545628206272
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33798.04914075131,
            "upper_bound": 33847.19941597333
          },
          "point_estimate": 33816.383384374814,
          "standard_error": 13.9889508763711
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33795.29349442379,
            "upper_bound": 33812.91821561338
          },
          "point_estimate": 33800.69395910781,
          "standard_error": 5.638620282606867
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9402139432565195,
            "upper_bound": 21.317197205188386
          },
          "point_estimate": 8.974207954062713,
          "standard_error": 6.054274202207193
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33795.926781833085,
            "upper_bound": 33803.60116054473
          },
          "point_estimate": 33799.950820740596,
          "standard_error": 1.969486762355963
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.5715698644384535,
            "upper_bound": 71.80376178539959
          },
          "point_estimate": 46.800120965673344,
          "standard_error": 23.01689776720335
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.15357352558154,
            "upper_bound": 125.2666514026374
          },
          "point_estimate": 125.1997931339833,
          "standard_error": 0.029993612508446184
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.14345159368176,
            "upper_bound": 125.22563569642452
          },
          "point_estimate": 125.15887736193488,
          "standard_error": 0.02278351925607502
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004750734741757235,
            "upper_bound": 0.10034212122173766
          },
          "point_estimate": 0.026047017403199577,
          "standard_error": 0.02543452889801821
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.15571270677594,
            "upper_bound": 125.20618278444438
          },
          "point_estimate": 125.17745289147054,
          "standard_error": 0.012661483693582404
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01610086198441582,
            "upper_bound": 0.1483088482505706
          },
          "point_estimate": 0.09993264289777848,
          "standard_error": 0.03911541816018873
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104916.2192011783,
            "upper_bound": 1105691.5284511785
          },
          "point_estimate": 1105271.1317845115,
          "standard_error": 200.36493705757505
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104914.4303030304,
            "upper_bound": 1105704.6666666667
          },
          "point_estimate": 1105118.4545454546,
          "standard_error": 163.2155249362007
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.86205305313177,
            "upper_bound": 1061.653649737968
          },
          "point_estimate": 275.9275796467461,
          "standard_error": 242.8424465438704
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104961.6138557154,
            "upper_bound": 1105303.1420171869
          },
          "point_estimate": 1105095.479338843,
          "standard_error": 87.88767934859958
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.88283376772404,
            "upper_bound": 883.8143724447486
          },
          "point_estimate": 667.7207045558638,
          "standard_error": 177.85632853932495
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2232.997326831797,
            "upper_bound": 2234.9232437692926
          },
          "point_estimate": 2233.8774267159197,
          "standard_error": 0.4953960670808105
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2232.677668970814,
            "upper_bound": 2235.0430476190477
          },
          "point_estimate": 2233.4553149001536,
          "standard_error": 0.5672959741239841
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06208570396725291,
            "upper_bound": 2.4990065201499214
          },
          "point_estimate": 1.164481910201907,
          "standard_error": 0.5848613237001193
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2232.859310517201,
            "upper_bound": 2235.467403398757
          },
          "point_estimate": 2234.003676102699,
          "standard_error": 0.7202047923943354
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5390499403512046,
            "upper_bound": 2.0808094662350736
          },
          "point_estimate": 1.646031994281988,
          "standard_error": 0.3978218945640315
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.47406980526126,
            "upper_bound": 43.49793429737747
          },
          "point_estimate": 43.48559178558616,
          "standard_error": 0.006134423141993375
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.467360809548246,
            "upper_bound": 43.504094476006365
          },
          "point_estimate": 43.4789439188777,
          "standard_error": 0.01180639666891531
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00022021317761034055,
            "upper_bound": 0.03488906277519936
          },
          "point_estimate": 0.017548691813803734,
          "standard_error": 0.010802646596799564
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.47027435781097,
            "upper_bound": 43.489819810955474
          },
          "point_estimate": 43.47900023911666,
          "standard_error": 0.004993142878926266
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0120839252277374,
            "upper_bound": 0.02401319579288949
          },
          "point_estimate": 0.02038325665680913,
          "standard_error": 0.0030534342100978413
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.38668839942029,
            "upper_bound": 47.43774182662656
          },
          "point_estimate": 47.40649627290549,
          "standard_error": 0.014065114869508503
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.381043593704845,
            "upper_bound": 47.40723653762166
          },
          "point_estimate": 47.393168849285246,
          "standard_error": 0.008280528787047058
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0034961363227942782,
            "upper_bound": 0.03231372559622221
          },
          "point_estimate": 0.01847132464865508,
          "standard_error": 0.007491872186219937
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.387027801882425,
            "upper_bound": 47.40209122245552
          },
          "point_estimate": 47.394440045805744,
          "standard_error": 0.003829911426001732
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009149611948539202,
            "upper_bound": 0.07162073750222217
          },
          "point_estimate": 0.046925614366300154,
          "standard_error": 0.021266209570815084
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.34808193798436,
            "upper_bound": 35.378697903454956
          },
          "point_estimate": 35.36154745990904,
          "standard_error": 0.007907255741253299
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.34540846304208,
            "upper_bound": 35.37678391876727
          },
          "point_estimate": 35.34817185197575,
          "standard_error": 0.008949941472978522
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009073405008750923,
            "upper_bound": 0.03845544008962353
          },
          "point_estimate": 0.011647707278513305,
          "standard_error": 0.010189357113555536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.34543911500338,
            "upper_bound": 35.364963373375424
          },
          "point_estimate": 35.35257519209427,
          "standard_error": 0.0052078063728610915
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009117534236284808,
            "upper_bound": 0.03674862084286369
          },
          "point_estimate": 0.026342926526494188,
          "standard_error": 0.007730596390294696
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.35871945248117,
            "upper_bound": 39.38181537264295
          },
          "point_estimate": 39.36911449598826,
          "standard_error": 0.005983380424736856
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.357072810287946,
            "upper_bound": 39.38185051473272
          },
          "point_estimate": 39.364395682852646,
          "standard_error": 0.00513152534993938
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023108874194611223,
            "upper_bound": 0.02775733362814118
          },
          "point_estimate": 0.00863028849938176,
          "standard_error": 0.006503513908905708
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.358893365780965,
            "upper_bound": 39.369338528042285
          },
          "point_estimate": 39.36417875051015,
          "standard_error": 0.002633431073240063
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005566563922685082,
            "upper_bound": 0.026246624975006367
          },
          "point_estimate": 0.019859322994157495,
          "standard_error": 0.005406674786480544
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.89804589895385,
            "upper_bound": 45.923749525491715
          },
          "point_estimate": 45.90918969913118,
          "standard_error": 0.006694116292045757
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.895884466597984,
            "upper_bound": 45.916163330168544
          },
          "point_estimate": 45.901108238109025,
          "standard_error": 0.0055298766998546625
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0020362051938377945,
            "upper_bound": 0.02818496989702545
          },
          "point_estimate": 0.00985925908492072,
          "standard_error": 0.006927046516155901
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.896753441965664,
            "upper_bound": 45.9151991158915
          },
          "point_estimate": 45.904426233776725,
          "standard_error": 0.004871135650264761
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005072413856620652,
            "upper_bound": 0.031733129967805085
          },
          "point_estimate": 0.0223616828062913,
          "standard_error": 0.007327118668614315
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.93114443838769,
            "upper_bound": 72.00986802230945
          },
          "point_estimate": 71.96373627338919,
          "standard_error": 0.020760243744053677
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.92944103308436,
            "upper_bound": 71.97029597536516
          },
          "point_estimate": 71.95094133076705,
          "standard_error": 0.011903502888514656
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008354717165325447,
            "upper_bound": 0.0630028504968837
          },
          "point_estimate": 0.03028576817507882,
          "standard_error": 0.014480514367387584
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.93770652925325,
            "upper_bound": 71.965528513017
          },
          "point_estimate": 71.9522335845742,
          "standard_error": 0.007062954818049883
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016265377933737887,
            "upper_bound": 0.1033463060735447
          },
          "point_estimate": 0.0690346678113264,
          "standard_error": 0.02738634273927835
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.13746179786789,
            "upper_bound": 81.18117905982578
          },
          "point_estimate": 81.15648604020389,
          "standard_error": 0.011320214421253078
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.13030429279402,
            "upper_bound": 81.17474857604543
          },
          "point_estimate": 81.1441479205353,
          "standard_error": 0.01045613044169927
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005205473177186874,
            "upper_bound": 0.04915367813049363
          },
          "point_estimate": 0.026315026120357264,
          "standard_error": 0.011782483506100104
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.13527125998266,
            "upper_bound": 81.15522556917097
          },
          "point_estimate": 81.14514214270179,
          "standard_error": 0.005380925797645374
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012570268386867724,
            "upper_bound": 0.053243138449069215
          },
          "point_estimate": 0.03774830666227165,
          "standard_error": 0.011701660613338856
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.72020037047582,
            "upper_bound": 60.74610935137295
          },
          "point_estimate": 60.73133417967398,
          "standard_error": 0.006745235404419831
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.71642503928776,
            "upper_bound": 60.740850753272866
          },
          "point_estimate": 60.72542991080301,
          "standard_error": 0.005789607670660173
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002946446680836767,
            "upper_bound": 0.026986157133432817
          },
          "point_estimate": 0.016444081785657396,
          "standard_error": 0.006382155114282052
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.717303481271784,
            "upper_bound": 60.73677283028401
          },
          "point_estimate": 60.72858382376024,
          "standard_error": 0.004995634070942171
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007270095667700695,
            "upper_bound": 0.03269290152964307
          },
          "point_estimate": 0.022526823478869928,
          "standard_error": 0.007702575081372493
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.12937217370175,
            "upper_bound": 98.20428758996748
          },
          "point_estimate": 98.15782201399884,
          "standard_error": 0.0209798467329068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.12575640177644,
            "upper_bound": 98.15480712553376
          },
          "point_estimate": 98.13200933212744,
          "standard_error": 0.008916638987712024
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000459437013499487,
            "upper_bound": 0.0366540610519864
          },
          "point_estimate": 0.00956228717784823,
          "standard_error": 0.010885744745871005
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.12931681324376,
            "upper_bound": 98.15448879977484
          },
          "point_estimate": 98.14110380056088,
          "standard_error": 0.0064263044623038635
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004426873869772102,
            "upper_bound": 0.10672531489552994
          },
          "point_estimate": 0.06988861147920497,
          "standard_error": 0.03308368793157206
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.07512824563842,
            "upper_bound": 62.11085657719458
          },
          "point_estimate": 62.09104293082189,
          "standard_error": 0.009181280577706287
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.07513850001452,
            "upper_bound": 62.10252855415039
          },
          "point_estimate": 62.08410596260835,
          "standard_error": 0.007317136532843297
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004292830500215566,
            "upper_bound": 0.04112934249018812
          },
          "point_estimate": 0.018439313234164187,
          "standard_error": 0.009040408399364252
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.07512390832834,
            "upper_bound": 62.095882116183496
          },
          "point_estimate": 62.084631035908075,
          "standard_error": 0.005305509442940892
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010111851286918045,
            "upper_bound": 0.043664059317041766
          },
          "point_estimate": 0.030609680169999227,
          "standard_error": 0.009652246662953258
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.210379680752126,
            "upper_bound": 60.24411761081398
          },
          "point_estimate": 60.22492949628539,
          "standard_error": 0.008737793660196676
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.20295625432867,
            "upper_bound": 60.23637124558572
          },
          "point_estimate": 60.216712832251375,
          "standard_error": 0.008491439801485765
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032953680842389603,
            "upper_bound": 0.036242344644215
          },
          "point_estimate": 0.02187909325004096,
          "standard_error": 0.008358482333734069
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.207607371293264,
            "upper_bound": 60.22343821281541
          },
          "point_estimate": 60.21401884889344,
          "standard_error": 0.004001040718753357
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010292755932333886,
            "upper_bound": 0.04168175674248302
          },
          "point_estimate": 0.029017170417992608,
          "standard_error": 0.009473725638996225
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.3557354175282,
            "upper_bound": 93.4386672471892
          },
          "point_estimate": 93.39538668691308,
          "standard_error": 0.021345348693751764
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.33719695011632,
            "upper_bound": 93.47337813388472
          },
          "point_estimate": 93.37184204574824,
          "standard_error": 0.03154615056760568
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007873318611834504,
            "upper_bound": 0.10735270166738238
          },
          "point_estimate": 0.05194234785013009,
          "standard_error": 0.028354178025198094
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.35231638743971,
            "upper_bound": 93.43263490644398
          },
          "point_estimate": 93.39245924816976,
          "standard_error": 0.020804028094006596
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03043292357061269,
            "upper_bound": 0.08419020680278322
          },
          "point_estimate": 0.07088479758042626,
          "standard_error": 0.012094094595243665
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1209721.977209421,
            "upper_bound": 1211038.574497408
          },
          "point_estimate": 1210213.4563223247,
          "standard_error": 374.7380023706014
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1209674.8270609318,
            "upper_bound": 1210099.7143817204
          },
          "point_estimate": 1209811.541935484,
          "standard_error": 140.3121994432902
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.684642483714537,
            "upper_bound": 561.9827084100444
          },
          "point_estimate": 238.67866834327032,
          "standard_error": 159.65185001498406
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1209736.7154725967,
            "upper_bound": 1210073.2392040165
          },
          "point_estimate": 1209888.2585672392,
          "standard_error": 85.95263497521273
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.28454087501171,
            "upper_bound": 1921.4115293550524
          },
          "point_estimate": 1251.9891758023623,
          "standard_error": 615.549755968778
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1535641.6219940477,
            "upper_bound": 1536499.7012074653
          },
          "point_estimate": 1535988.6500611776,
          "standard_error": 228.88396200779795
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1535578.956845238,
            "upper_bound": 1536113.3125
          },
          "point_estimate": 1535779.2416666667,
          "standard_error": 138.17869627144685
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.20641083455425,
            "upper_bound": 643.371169827821
          },
          "point_estimate": 321.03838148807404,
          "standard_error": 156.22398198835396
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1535636.4336419753,
            "upper_bound": 1536047.433841233
          },
          "point_estimate": 1535856.615909091,
          "standard_error": 108.09005629006396
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179.7222232702562,
            "upper_bound": 1143.9308425635463
          },
          "point_estimate": 761.1576198672524,
          "standard_error": 311.99806644980276
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1520832.0367622357,
            "upper_bound": 1521805.0190674602
          },
          "point_estimate": 1521268.0531531083,
          "standard_error": 251.17468655979383
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1520723.7972222222,
            "upper_bound": 1521754.7893518517
          },
          "point_estimate": 1520993.4404761903,
          "standard_error": 235.22596355407256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.8872918502321,
            "upper_bound": 1148.9042993945764
          },
          "point_estimate": 417.25025800918337,
          "standard_error": 263.0027998245828
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1520700.23966477,
            "upper_bound": 1521404.0910326086
          },
          "point_estimate": 1520993.137012987,
          "standard_error": 181.45896832570872
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206.3996887438362,
            "upper_bound": 1098.714235480502
          },
          "point_estimate": 838.699535358309,
          "standard_error": 233.4453787541732
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 664793.6452294372,
            "upper_bound": 666766.8631727274
          },
          "point_estimate": 665725.8805223665,
          "standard_error": 505.83488128090266
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 664221.4626262627,
            "upper_bound": 666927.7696969698
          },
          "point_estimate": 665308.023030303,
          "standard_error": 607.5172563677247
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153.19557909839384,
            "upper_bound": 2806.5662429009644
          },
          "point_estimate": 1635.9559217436863,
          "standard_error": 681.965354999823
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 664239.2089115691,
            "upper_bound": 665387.1064978688
          },
          "point_estimate": 664616.5108854781,
          "standard_error": 292.666680515121
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 739.8082508680369,
            "upper_bound": 2105.973431868192
          },
          "point_estimate": 1681.2118457176016,
          "standard_error": 338.20300905972556
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1429642.908735119,
            "upper_bound": 1431533.9447118433
          },
          "point_estimate": 1430524.9129151404,
          "standard_error": 486.5704244427184
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1429478.696153846,
            "upper_bound": 1431770.137973138
          },
          "point_estimate": 1430134.8798076925,
          "standard_error": 437.4989086306467
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.90442448966415,
            "upper_bound": 2649.0259991242824
          },
          "point_estimate": 847.4265838013795,
          "standard_error": 641.2627705183604
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1429750.038660176,
            "upper_bound": 1431918.254375557
          },
          "point_estimate": 1430672.7733266733,
          "standard_error": 588.044104578289
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 516.2352333559505,
            "upper_bound": 2099.6875329682935
          },
          "point_estimate": 1618.7775526063858,
          "standard_error": 388.73882518690624
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392137.72802270006,
            "upper_bound": 392381.4548835125
          },
          "point_estimate": 392263.409644564,
          "standard_error": 62.423561256674496
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392158.853046595,
            "upper_bound": 392408.2698924731
          },
          "point_estimate": 392262.7941308244,
          "standard_error": 57.2088614611174
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.42433282111649,
            "upper_bound": 342.1031686038631
          },
          "point_estimate": 152.0092748281723,
          "standard_error": 79.28620762638766
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392187.10037545016,
            "upper_bound": 392349.7444884965
          },
          "point_estimate": 392275.1608434576,
          "standard_error": 42.48037561069797
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.5650996623889,
            "upper_bound": 283.37095186350194
          },
          "point_estimate": 207.91551836760917,
          "standard_error": 50.25413853075574
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 583727.5871031748,
            "upper_bound": 584818.4441269841
          },
          "point_estimate": 584243.8396957673,
          "standard_error": 279.4705097068346
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 583516.4751984127,
            "upper_bound": 584970.7936507936
          },
          "point_estimate": 583745.6878306878,
          "standard_error": 487.30619977059575
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.09911300565669,
            "upper_bound": 1746.6086689913989
          },
          "point_estimate": 486.5163580293254,
          "standard_error": 456.98751274566473
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 583518.4108663577,
            "upper_bound": 583977.2836092621
          },
          "point_estimate": 583656.439703154,
          "standard_error": 120.32440988315024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 466.4528808525616,
            "upper_bound": 1188.1139147059182
          },
          "point_estimate": 928.3409199684628,
          "standard_error": 188.70903357262492
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1046990.9976695578,
            "upper_bound": 1047936.1177687076
          },
          "point_estimate": 1047416.9061020408,
          "standard_error": 243.28305945679745
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1047007.3666666668,
            "upper_bound": 1047769.0538095238
          },
          "point_estimate": 1047276.3576530612,
          "standard_error": 158.14739270888182
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.57099364059805,
            "upper_bound": 1225.1005982500558
          },
          "point_estimate": 287.31098132774827,
          "standard_error": 289.22686769618196
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1047022.7898135332,
            "upper_bound": 1047814.2053983536
          },
          "point_estimate": 1047451.0014100184,
          "standard_error": 210.8733700687327
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203.23977273144655,
            "upper_bound": 1142.231238084891
          },
          "point_estimate": 814.5005420795273,
          "standard_error": 247.41812365560796
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 721143.3354901961,
            "upper_bound": 722110.0298635622
          },
          "point_estimate": 721590.1768954248,
          "standard_error": 248.68015997035377
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 721018.4011437909,
            "upper_bound": 722372.0
          },
          "point_estimate": 721272.8088235294,
          "standard_error": 316.097770082685
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.56777461015776,
            "upper_bound": 1238.557234873966
          },
          "point_estimate": 395.4505963126608,
          "standard_error": 320.2161231133075
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 720940.088085592,
            "upper_bound": 721842.0328905756
          },
          "point_estimate": 721321.5876241405,
          "standard_error": 234.77543474178245
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246.2322142324135,
            "upper_bound": 1044.403493965841
          },
          "point_estimate": 827.4117554205772,
          "standard_error": 180.77029495838752
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 820803.659717284,
            "upper_bound": 821777.9685640653
          },
          "point_estimate": 821171.3026763669,
          "standard_error": 275.18330938170027
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 820777.5583333333,
            "upper_bound": 821080.6984126985
          },
          "point_estimate": 820934.7559259259,
          "standard_error": 102.26430472618073
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.03282976375149,
            "upper_bound": 440.49292484638727
          },
          "point_estimate": 194.0423642402629,
          "standard_error": 116.32808138400767
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 820702.7409961686,
            "upper_bound": 821214.4220496403
          },
          "point_estimate": 820901.4705339106,
          "standard_error": 132.4303423123717
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.47288305847428,
            "upper_bound": 1410.5183760561308
          },
          "point_estimate": 918.7562770735848,
          "standard_error": 446.1583041823836
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 457877.45738888887,
            "upper_bound": 458159.18270833336
          },
          "point_estimate": 457996.02665972215,
          "standard_error": 74.1669621798276
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 457836.5208333334,
            "upper_bound": 458066.803125
          },
          "point_estimate": 457937.7772916667,
          "standard_error": 57.404319706012934
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.858191420586644,
            "upper_bound": 258.57655490930705
          },
          "point_estimate": 143.19372674955306,
          "standard_error": 68.92146123109188
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 457857.40467891446,
            "upper_bound": 457950.1256061242
          },
          "point_estimate": 457899.7752272727,
          "standard_error": 23.638378735689574
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.04336116663208,
            "upper_bound": 360.2821127858396
          },
          "point_estimate": 246.9122759357216,
          "standard_error": 89.72567827159055
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326279.67584431695,
            "upper_bound": 326588.73082043655
          },
          "point_estimate": 326426.2620900652,
          "standard_error": 78.80510456259782
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326269.5799319728,
            "upper_bound": 326606.95089285716
          },
          "point_estimate": 326338.9616071428,
          "standard_error": 89.43643182715036
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.4516436531852115,
            "upper_bound": 414.8238847113249
          },
          "point_estimate": 159.48455669241193,
          "standard_error": 113.66639638943852
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326235.6027762213,
            "upper_bound": 326549.60538504465
          },
          "point_estimate": 326386.3812615955,
          "standard_error": 79.83931869330327
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.48163449566204,
            "upper_bound": 333.51172391299735
          },
          "point_estimate": 261.55842477969105,
          "standard_error": 54.19806635262739
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257200.95019121672,
            "upper_bound": 257343.40414245895
          },
          "point_estimate": 257261.13946205005,
          "standard_error": 37.22058644659282
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257183.95633802816,
            "upper_bound": 257290.59697769955
          },
          "point_estimate": 257221.8846830986,
          "standard_error": 23.644939188877498
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.826643998681526,
            "upper_bound": 141.54066113154147
          },
          "point_estimate": 47.96292121422448,
          "standard_error": 38.57537865253891
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257206.2646185799,
            "upper_bound": 257248.2907629828
          },
          "point_estimate": 257223.67148344617,
          "standard_error": 10.675046118043486
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.4948010174117,
            "upper_bound": 181.2862919389223
          },
          "point_estimate": 124.58063524354966,
          "standard_error": 44.56016928086317
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182284.0929529762,
            "upper_bound": 182436.15047916665
          },
          "point_estimate": 182346.18234742063,
          "standard_error": 40.54073493250713
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182274.9928125,
            "upper_bound": 182372.1025
          },
          "point_estimate": 182316.2563888889,
          "standard_error": 22.55507982194493
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.282242923979018,
            "upper_bound": 115.5576719984468
          },
          "point_estimate": 67.29071205535524,
          "standard_error": 32.15361459020074
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182259.63159709933,
            "upper_bound": 182340.0739356104
          },
          "point_estimate": 182296.4628181818,
          "standard_error": 21.29828370955138
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.32078289113685,
            "upper_bound": 202.83491364622705
          },
          "point_estimate": 135.0406021044213,
          "standard_error": 54.88806411484043
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-repeated-rare-huge_common-m"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2999506.8618925516,
            "upper_bound": 3000696.7645360185
          },
          "point_estimate": 3000033.8698565317,
          "standard_error": 307.3756457256436
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2999351.307692308,
            "upper_bound": 3000598.811965812
          },
          "point_estimate": 2999715.9203296704,
          "standard_error": 290.12524896939954
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.34357795802606,
            "upper_bound": 1481.2456756256563
          },
          "point_estimate": 569.3105152774575,
          "standard_error": 333.16892677115544
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2999528.6300780284,
            "upper_bound": 3000257.6153371274
          },
          "point_estimate": 2999835.5826173825,
          "standard_error": 187.32614872243403
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 314.9069905148271,
            "upper_bound": 1390.4758676161746
          },
          "point_estimate": 1024.2017701241596,
          "standard_error": 289.2568390714429
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-repeated-rare-small_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6041.656362956811,
            "upper_bound": 6046.936584005695
          },
          "point_estimate": 6044.16395983626,
          "standard_error": 1.3473561081671726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6040.568999762696,
            "upper_bound": 6045.997134551495
          },
          "point_estimate": 6044.843978405315,
          "standard_error": 1.4109807396795604
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2435699956755948,
            "upper_bound": 7.894780211583521
          },
          "point_estimate": 3.0395938164682907,
          "standard_error": 1.9276026988721973
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6039.49192071728,
            "upper_bound": 6044.3740383246095
          },
          "point_estimate": 6041.16996893472,
          "standard_error": 1.2315450029017276
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.0253993170804474,
            "upper_bound": 6.1227115290983525
          },
          "point_estimate": 4.481137160306639,
          "standard_error": 1.1162166292399096
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355364.8142901235,
            "upper_bound": 1355815.3741553496
          },
          "point_estimate": 1355590.2358230453,
          "standard_error": 115.50660778439072
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355303.9938271604,
            "upper_bound": 1355943.5185185184
          },
          "point_estimate": 1355568.295473251,
          "standard_error": 169.3454916516335
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.44786350594372,
            "upper_bound": 654.5709389965797
          },
          "point_estimate": 442.73333300423354,
          "standard_error": 138.7474266299428
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355367.4552845412,
            "upper_bound": 1355693.9151651652
          },
          "point_estimate": 1355511.7301587302,
          "standard_error": 82.1299565585223
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.93332517396755,
            "upper_bound": 478.59178821519254
          },
          "point_estimate": 385.0413091271733,
          "standard_error": 64.46139237479775
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523866.6430704363,
            "upper_bound": 1525576.7158235367
          },
          "point_estimate": 1524608.0705009922,
          "standard_error": 444.083947046413
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523749.861111111,
            "upper_bound": 1525560.6802083333
          },
          "point_estimate": 1524031.6397569445,
          "standard_error": 366.6603132439685
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.40963378556677,
            "upper_bound": 2026.8013108920677
          },
          "point_estimate": 368.1864579426808,
          "standard_error": 373.28506795385385
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523855.13157423,
            "upper_bound": 1524555.7152490758
          },
          "point_estimate": 1524066.2955627705,
          "standard_error": 184.28571056203828
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 186.7999377481217,
            "upper_bound": 1924.3011644646556
          },
          "point_estimate": 1482.497286253492,
          "standard_error": 473.3576451228674
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1424589.8076923075,
            "upper_bound": 1425684.2738369964
          },
          "point_estimate": 1425121.5348992674,
          "standard_error": 280.99587042129474
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1424405.033653846,
            "upper_bound": 1425893.8615384614
          },
          "point_estimate": 1424894.3305860804,
          "standard_error": 381.8512089261447
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 211.2372327881625,
            "upper_bound": 1588.081259498086
          },
          "point_estimate": 916.7172241097244,
          "standard_error": 363.9393482883553
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1424663.9816576038,
            "upper_bound": 1425983.5334962169
          },
          "point_estimate": 1425466.8877122875,
          "standard_error": 337.950259384665
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 501.38428624662953,
            "upper_bound": 1149.3930525474404
          },
          "point_estimate": 934.9361478607568,
          "standard_error": 161.71434050905748
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789278.8818245525,
            "upper_bound": 789990.4074510301
          },
          "point_estimate": 789565.6941599122,
          "standard_error": 192.5118100696632
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789267.1329787234,
            "upper_bound": 789664.7322695035
          },
          "point_estimate": 789362.3765619723,
          "standard_error": 91.4919765161767
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.863675055059623,
            "upper_bound": 506.3473218615615
          },
          "point_estimate": 124.64840986157115,
          "standard_error": 130.07569262266344
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789318.3727513469,
            "upper_bound": 789436.8640990099
          },
          "point_estimate": 789379.6271898315,
          "standard_error": 29.72956501277005
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.59508708990089,
            "upper_bound": 961.8822234577888
          },
          "point_estimate": 642.989055350754,
          "standard_error": 268.1706119035009
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 275457.7130240745,
            "upper_bound": 276935.8274427481
          },
          "point_estimate": 276071.23319671635,
          "standard_error": 389.83333967608127
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 275311.53880407126,
            "upper_bound": 276519.9110687023
          },
          "point_estimate": 275607.13007391256,
          "standard_error": 278.39891403927396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.37839848869845,
            "upper_bound": 1244.33301696718
          },
          "point_estimate": 441.8848195404418,
          "standard_error": 325.3713385671996
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 275406.59815622936,
            "upper_bound": 275793.1290076336
          },
          "point_estimate": 275567.87841776543,
          "standard_error": 99.43838912973511
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 229.34505028959407,
            "upper_bound": 1893.843402029536
          },
          "point_estimate": 1302.3462098917175,
          "standard_error": 487.3273543735921
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492186.5738738738,
            "upper_bound": 492468.4721231232
          },
          "point_estimate": 492317.1544894896,
          "standard_error": 72.60588972397335
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492093.1891891892,
            "upper_bound": 492475.2810810811
          },
          "point_estimate": 492293.2421171171,
          "standard_error": 70.34193223087885
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.56378624566715,
            "upper_bound": 369.536930376318
          },
          "point_estimate": 169.33695915582553,
          "standard_error": 118.37962410434316
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492130.3353696265,
            "upper_bound": 492329.6225225225
          },
          "point_estimate": 492222.8843804844,
          "standard_error": 51.58953327969471
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.1939497401707,
            "upper_bound": 317.0761103370343
          },
          "point_estimate": 241.9207775147669,
          "standard_error": 57.92185352568082
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252931.4439748677,
            "upper_bound": 253120.66480241404
          },
          "point_estimate": 253014.92149222884,
          "standard_error": 48.608387163169695
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252932.11574074073,
            "upper_bound": 253061.16547619048
          },
          "point_estimate": 252977.81886574073,
          "standard_error": 37.84646465693097
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.80063753120333,
            "upper_bound": 195.92588681671816
          },
          "point_estimate": 106.23309283620046,
          "standard_error": 45.917319120361
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252967.95046202003,
            "upper_bound": 253040.78669220163
          },
          "point_estimate": 253001.9139790765,
          "standard_error": 18.528649822219275
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.92142019811591,
            "upper_bound": 234.05801249792611
          },
          "point_estimate": 162.4826743174244,
          "standard_error": 53.66284813954366
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632576.1162787357,
            "upper_bound": 633056.8361459359
          },
          "point_estimate": 632764.8074035303,
          "standard_error": 131.06700317394274
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632586.2525862069,
            "upper_bound": 632723.9127155172
          },
          "point_estimate": 632674.7266009853,
          "standard_error": 41.76811037266381
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.017468876462319,
            "upper_bound": 277.9853648923674
          },
          "point_estimate": 60.82494202361945,
          "standard_error": 74.25254891661703
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632529.3161387738,
            "upper_bound": 632690.2435102673
          },
          "point_estimate": 632603.0664576802,
          "standard_error": 41.03424692324775
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.86112255556923,
            "upper_bound": 668.0481690337555
          },
          "point_estimate": 436.78976235173474,
          "standard_error": 200.5139996178329
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.0672189883559,
            "upper_bound": 378.389043373545
          },
          "point_estimate": 378.2053966673286,
          "standard_error": 0.08468582641822035
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.06148812227065,
            "upper_bound": 378.3528791952242
          },
          "point_estimate": 378.0842918950401,
          "standard_error": 0.062426354411945856
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010215416194963203,
            "upper_bound": 0.37120895015490424
          },
          "point_estimate": 0.04548828459768271,
          "standard_error": 0.06955232467471212
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.05948671947726,
            "upper_bound": 378.12708585016225
          },
          "point_estimate": 378.0884962465019,
          "standard_error": 0.016969858147676112
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02828040370331265,
            "upper_bound": 0.3797829477853364
          },
          "point_estimate": 0.2825301610800637,
          "standard_error": 0.09544255186953396
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.17496685820475,
            "upper_bound": 188.45446735639803
          },
          "point_estimate": 188.33596720872885,
          "standard_error": 0.07279594506583775
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.31164863548517,
            "upper_bound": 188.46931904902544
          },
          "point_estimate": 188.35221690843537,
          "standard_error": 0.04971915456453374
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004021669289591226,
            "upper_bound": 0.26933975060558063
          },
          "point_estimate": 0.07281471937940845,
          "standard_error": 0.06797440969930259
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.32288313753503,
            "upper_bound": 188.4087919199442
          },
          "point_estimate": 188.3528479758626,
          "standard_error": 0.02221085715247289
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06417176273895919,
            "upper_bound": 0.35810465746369896
          },
          "point_estimate": 0.2422565028051842,
          "standard_error": 0.09012662271563848
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.507721759336324,
            "upper_bound": 26.52865526919926
          },
          "point_estimate": 26.516834659809728,
          "standard_error": 0.005413029567713045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.505243099256656,
            "upper_bound": 26.524240743225405
          },
          "point_estimate": 26.510844858618334,
          "standard_error": 0.00447592768852833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022079494844176035,
            "upper_bound": 0.023472177523079684
          },
          "point_estimate": 0.009218297014943498,
          "standard_error": 0.00556710155185574
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.50795007329111,
            "upper_bound": 26.514223588591012
          },
          "point_estimate": 26.510768750476227,
          "standard_error": 0.001577041950050156
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004763308115571546,
            "upper_bound": 0.0253427030184741
          },
          "point_estimate": 0.01810879697721217,
          "standard_error": 0.005694532977535918
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.68778431912579,
            "upper_bound": 17.699359573086955
          },
          "point_estimate": 17.692872166331526,
          "standard_error": 0.0030083574374103407
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.68701703428747,
            "upper_bound": 17.69937485190532
          },
          "point_estimate": 17.688764119228466,
          "standard_error": 0.0029854111242623967
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007747191841329936,
            "upper_bound": 0.015248942154440923
          },
          "point_estimate": 0.0028656889500497397,
          "standard_error": 0.003547996474495837
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.687024566660536,
            "upper_bound": 17.69102509663792
          },
          "point_estimate": 17.688462710685744,
          "standard_error": 0.00102833329056683
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002275559749650765,
            "upper_bound": 0.014074819662380077
          },
          "point_estimate": 0.010013893898237836,
          "standard_error": 0.003106586857380471
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.26035380986195,
            "upper_bound": 26.29579061388697
          },
          "point_estimate": 26.27389286970132,
          "standard_error": 0.0099189068476609
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.25827621138384,
            "upper_bound": 26.273182698496516
          },
          "point_estimate": 26.262675495006157,
          "standard_error": 0.004492388900603524
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008203276098215208,
            "upper_bound": 0.018044192477698583
          },
          "point_estimate": 0.007127044218016297,
          "standard_error": 0.0051194919344226585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.25930913903528,
            "upper_bound": 26.268310069286144
          },
          "point_estimate": 26.2626415522066,
          "standard_error": 0.0023186966375676192
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003213496817795544,
            "upper_bound": 0.05067490124896149
          },
          "point_estimate": 0.03313353442258768,
          "standard_error": 0.015593145546396583
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298601.3428161593,
            "upper_bound": 299175.43890533113
          },
          "point_estimate": 298954.5593416601,
          "standard_error": 159.69459606936465
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298951.3838797814,
            "upper_bound": 299181.7951958106
          },
          "point_estimate": 299105.15519125684,
          "standard_error": 61.956226445255616
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4998620458381575,
            "upper_bound": 341.6078110801031
          },
          "point_estimate": 89.41898781140996,
          "standard_error": 89.71595472613573
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299107.8534734621,
            "upper_bound": 299191.6080454723
          },
          "point_estimate": 299151.3894613583,
          "standard_error": 21.195897910343348
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.5890807521686,
            "upper_bound": 812.0387644937663
          },
          "point_estimate": 532.4168089060436,
          "standard_error": 246.7888215763996
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.74278312383448,
            "upper_bound": 15.770220382986556
          },
          "point_estimate": 15.754993711667437,
          "standard_error": 0.007104337658096776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.73804518162259,
            "upper_bound": 15.76482064291208
          },
          "point_estimate": 15.750919151865144,
          "standard_error": 0.0061440273208753825
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023028336265039,
            "upper_bound": 0.031645795423184554
          },
          "point_estimate": 0.014618736152862188,
          "standard_error": 0.007602398101321387
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.736041533531624,
            "upper_bound": 15.755479849298698
          },
          "point_estimate": 15.74600876242904,
          "standard_error": 0.00539679566059935
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008587238309610414,
            "upper_bound": 0.03372987606238412
          },
          "point_estimate": 0.023681628639384528,
          "standard_error": 0.007367806999465093
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.348902965314835,
            "upper_bound": 23.38519232420664
          },
          "point_estimate": 23.364829643175632,
          "standard_error": 0.009396577161738034
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.344333647268517,
            "upper_bound": 23.375807063697593
          },
          "point_estimate": 23.35632275167099,
          "standard_error": 0.01096522957263846
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002813019239987615,
            "upper_bound": 0.04336434070789071
          },
          "point_estimate": 0.025802696250343066,
          "standard_error": 0.01027631709024178
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.349195419983747,
            "upper_bound": 23.370808168452996
          },
          "point_estimate": 23.3608108202206,
          "standard_error": 0.00561304374492613
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01244427881910562,
            "upper_bound": 0.04513045617628482
          },
          "point_estimate": 0.031429844065682284,
          "standard_error": 0.010067776150019806
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132625.55475669104,
            "upper_bound": 132773.88125666924
          },
          "point_estimate": 132691.95735922837,
          "standard_error": 38.374271616411896
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132596.8689781022,
            "upper_bound": 132756.50212895375
          },
          "point_estimate": 132653.79425182482,
          "standard_error": 41.108063889116494
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.89335329645033,
            "upper_bound": 180.6527197124557
          },
          "point_estimate": 89.67024366350782,
          "standard_error": 42.15681620740786
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132603.13205357254,
            "upper_bound": 132717.00335131926
          },
          "point_estimate": 132646.85719973457,
          "standard_error": 28.97081803842579
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.85951574572213,
            "upper_bound": 175.17990112479853
          },
          "point_estimate": 127.39358582361773,
          "standard_error": 35.86474287087554
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.05227918380407,
            "upper_bound": 24.06698562885878
          },
          "point_estimate": 24.05898159559168,
          "standard_error": 0.0037875051319836775
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.04883650193804,
            "upper_bound": 24.06704582095736
          },
          "point_estimate": 24.055236320276222,
          "standard_error": 0.004961103316087642
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009468798488349406,
            "upper_bound": 0.02005575236374079
          },
          "point_estimate": 0.009966028952924326,
          "standard_error": 0.005149426897695295
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.050447906390456,
            "upper_bound": 24.063785192724236
          },
          "point_estimate": 24.05748201771528,
          "standard_error": 0.003403667782406364
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005290513354942044,
            "upper_bound": 0.01696737778232247
          },
          "point_estimate": 0.012633453478049166,
          "standard_error": 0.0032107981416709995
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.865203562074335,
            "upper_bound": 20.883757510335126
          },
          "point_estimate": 20.8735317254855,
          "standard_error": 0.004780881610719999
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.861837366583583,
            "upper_bound": 20.881091348267784
          },
          "point_estimate": 20.869597989569726,
          "standard_error": 0.004898561066306533
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002980942825279005,
            "upper_bound": 0.02245540592083327
          },
          "point_estimate": 0.011650252849531997,
          "standard_error": 0.005189043352511627
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.86519085440733,
            "upper_bound": 20.873250719811335
          },
          "point_estimate": 20.86888659373979,
          "standard_error": 0.0020670886541838893
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005902849209322292,
            "upper_bound": 0.022082834327856397
          },
          "point_estimate": 0.01602419123014104,
          "standard_error": 0.004516537195021906
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-freq_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.095923208103066,
            "upper_bound": 49.1407840405585
          },
          "point_estimate": 49.11609555815479,
          "standard_error": 0.01153318672750587
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.09022946543395,
            "upper_bound": 49.14046596889759
          },
          "point_estimate": 49.10247204600881,
          "standard_error": 0.011453144895316295
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0029719927261649052,
            "upper_bound": 0.05632978907484729
          },
          "point_estimate": 0.01849320308627619,
          "standard_error": 0.012474045000206616
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.09615870795721,
            "upper_bound": 49.11454604532957
          },
          "point_estimate": 49.10316745545745,
          "standard_error": 0.004685099171899538
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009854729821142908,
            "upper_bound": 0.04983318676048627
          },
          "point_estimate": 0.03847509584920463,
          "standard_error": 0.010376103537069137
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-repeated_"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.84935081478484,
            "upper_bound": 90.95927037747208
          },
          "point_estimate": 90.8978472819939,
          "standard_error": 0.02855009765776995
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.8373561538058,
            "upper_bound": 90.94692041072852
          },
          "point_estimate": 90.8570946629782,
          "standard_error": 0.02602131324857933
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006314680925206435,
            "upper_bound": 0.1327349735410525
          },
          "point_estimate": 0.03834869943687702,
          "standard_error": 0.03002871759638456
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.83499949921986,
            "upper_bound": 90.89904290960727
          },
          "point_estimate": 90.85436482865806,
          "standard_error": 0.016825665731536203
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02327473581243229,
            "upper_bound": 0.1296594915063716
          },
          "point_estimate": 0.09485133793365644,
          "standard_error": 0.028606775419560596
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector_rare-alph"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.80664998502436,
            "upper_bound": 10.814817811473048
          },
          "point_estimate": 10.810312823942947,
          "standard_error": 0.0020959090832996688
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.80548434405762,
            "upper_bound": 10.81512324106444
          },
          "point_estimate": 10.808032546718394,
          "standard_error": 0.00203314553037048
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011160896409695051,
            "upper_bound": 0.009224615147966369
          },
          "point_estimate": 0.00381095179330328,
          "standard_error": 0.002125589092300347
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.80556004737581,
            "upper_bound": 10.81095964363087
          },
          "point_estimate": 10.807707085142614,
          "standard_error": 0.0013692693077284115
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001876065696767185,
            "upper_bound": 0.008995984618773373
          },
          "point_estimate": 0.006987629181254087,
          "standard_error": 0.001880353189375676
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33315.49358144557,
            "upper_bound": 33473.006615273924
          },
          "point_estimate": 33382.18100726105,
          "standard_error": 41.544078618167504
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33307.26098519889,
            "upper_bound": 33442.432284921364
          },
          "point_estimate": 33327.15878155148,
          "standard_error": 28.80468646677778
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.360407005443722,
            "upper_bound": 170.2648501070117
          },
          "point_estimate": 31.3882426781796,
          "standard_error": 33.39508886831247
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33311.163237398876,
            "upper_bound": 33336.412487240006
          },
          "point_estimate": 33323.00301788868,
          "standard_error": 6.529740245657075
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.672077788254391,
            "upper_bound": 192.75520168314716
          },
          "point_estimate": 138.61435795762986,
          "standard_error": 48.83205933956877
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.124606011486804,
            "upper_bound": 25.65532763955272
          },
          "point_estimate": 25.392636106780692,
          "standard_error": 0.13585431065997916
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.835390833582576,
            "upper_bound": 25.849080573154904
          },
          "point_estimate": 25.422185491490573,
          "standard_error": 0.2473931912453444
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013381314433753028,
            "upper_bound": 0.7441938290427299
          },
          "point_estimate": 0.6476715183597528,
          "standard_error": 0.20225250725998983
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.43331307747119,
            "upper_bound": 25.808695140634413
          },
          "point_estimate": 25.65964044434351,
          "standard_error": 0.09801151878712774
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2828742105042268,
            "upper_bound": 0.5252341981059836
          },
          "point_estimate": 0.45381743664337487,
          "standard_error": 0.06067646768536336
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-repeated-rare-huge_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104913.1892228536,
            "upper_bound": 1105806.2859690655
          },
          "point_estimate": 1105292.289368687,
          "standard_error": 233.4143571673876
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104850.106060606,
            "upper_bound": 1105499.21010101
          },
          "point_estimate": 1105009.7708333335,
          "standard_error": 176.75801819686134
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.85793335397396,
            "upper_bound": 886.0957171474257
          },
          "point_estimate": 314.26626714797305,
          "standard_error": 222.2049759824085
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104914.292078407,
            "upper_bound": 1105268.7139005717
          },
          "point_estimate": 1105054.329555293,
          "standard_error": 90.57023409056276
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201.73022986862603,
            "upper_bound": 1128.1781583960126
          },
          "point_estimate": 778.4677372968192,
          "standard_error": 271.56406003352794
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-repeated-rare-small_never-tric"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2206.5856591353813,
            "upper_bound": 2207.5408573161626
          },
          "point_estimate": 2207.0574584500746,
          "standard_error": 0.2459653071779569
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2206.2910240451756,
            "upper_bound": 2207.8452243609204
          },
          "point_estimate": 2207.0472048495153,
          "standard_error": 0.4121269755342443
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.18475297124784928,
            "upper_bound": 1.3921392508926067
          },
          "point_estimate": 1.1521286736072218,
          "standard_error": 0.328184861895381
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2206.5577319629256,
            "upper_bound": 2207.576386742345
          },
          "point_estimate": 2206.950650612366,
          "standard_error": 0.25951078695174756
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4991006508247856,
            "upper_bound": 0.9703895159712168
          },
          "point_estimate": 0.8211260374799112,
          "standard_error": 0.11796884578912302
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.348925945972544,
            "upper_bound": 8.361122124957253
          },
          "point_estimate": 8.35404192246244,
          "standard_error": 0.003204663475402335
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.347412470146892,
            "upper_bound": 8.3579347441108
          },
          "point_estimate": 8.349520128501002,
          "standard_error": 0.002633436081493228
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006989331919032109,
            "upper_bound": 0.011112396037476756
          },
          "point_estimate": 0.003823747412229461,
          "standard_error": 0.0030609936031104307
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.34766128241934,
            "upper_bound": 8.354215882223983
          },
          "point_estimate": 8.350193595842955,
          "standard_error": 0.001721048735976261
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00242112253898793,
            "upper_bound": 0.015568407758367766
          },
          "point_estimate": 0.010691308597361594,
          "standard_error": 0.003883571446054954
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.450801853552893,
            "upper_bound": 7.472939266108408
          },
          "point_estimate": 7.4617240197783,
          "standard_error": 0.00566417659102446
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.444766166800134,
            "upper_bound": 7.4815468224575
          },
          "point_estimate": 7.459406661524102,
          "standard_error": 0.00856646346101613
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004998886372125111,
            "upper_bound": 0.03263798611874173
          },
          "point_estimate": 0.025695884849451407,
          "standard_error": 0.007791224062378771
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.443354320047526,
            "upper_bound": 7.464696290148783
          },
          "point_estimate": 7.452720884042964,
          "standard_error": 0.005559646139885761
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011058893214227237,
            "upper_bound": 0.02263160376800563
          },
          "point_estimate": 0.01893291129793847,
          "standard_error": 0.002873315405276067
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.922663255502055,
            "upper_bound": 8.979159152312949
          },
          "point_estimate": 8.955726450893046,
          "standard_error": 0.014910725396320877
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.947606578184997,
            "upper_bound": 8.985373852824749
          },
          "point_estimate": 8.966889349553735,
          "standard_error": 0.010347338755471832
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006880993719924612,
            "upper_bound": 0.04556408760974285
          },
          "point_estimate": 0.0279968801934038,
          "standard_error": 0.01026942569541426
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.954783961845346,
            "upper_bound": 8.985058539316078
          },
          "point_estimate": 8.970116361370781,
          "standard_error": 0.007708036646212402
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013462180200856608,
            "upper_bound": 0.07415163031284491
          },
          "point_estimate": 0.04983153091061237,
          "standard_error": 0.01940439696782215
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.395451954535993,
            "upper_bound": 19.410473145897132
          },
          "point_estimate": 19.401486749635705,
          "standard_error": 0.004042908420616845
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.39373270177989,
            "upper_bound": 19.403590372811827
          },
          "point_estimate": 19.396660589408384,
          "standard_error": 0.002415011663994149
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008245338438496608,
            "upper_bound": 0.011098630535395162
          },
          "point_estimate": 0.004366662662833137,
          "standard_error": 0.0028121046352216545
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.393822084687475,
            "upper_bound": 19.399837471269343
          },
          "point_estimate": 19.396243886436825,
          "standard_error": 0.001540012721991513
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002412137526728958,
            "upper_bound": 0.02014608497249749
          },
          "point_estimate": 0.013454135663084796,
          "standard_error": 0.005527430460633593
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.261083967096738,
            "upper_bound": 13.271215593310089
          },
          "point_estimate": 13.265830145535116,
          "standard_error": 0.002598858582602234
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.259611463115986,
            "upper_bound": 13.272394563938809
          },
          "point_estimate": 13.26435736894619,
          "standard_error": 0.0032987024808430183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019252565781030784,
            "upper_bound": 0.014679876866854326
          },
          "point_estimate": 0.007957925532462074,
          "standard_error": 0.003116769302304967
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.258904606726045,
            "upper_bound": 13.26925143495076
          },
          "point_estimate": 13.263633843761385,
          "standard_error": 0.0026801308323508403
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004231134375689294,
            "upper_bound": 0.011457453556338097
          },
          "point_estimate": 0.008647785764650933,
          "standard_error": 0.001945765400903807
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.143978692821168,
            "upper_bound": 19.16168549553421
          },
          "point_estimate": 19.15135413712279,
          "standard_error": 0.004643063639859116
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.1416823169012,
            "upper_bound": 19.15621186199556
          },
          "point_estimate": 19.145752899607302,
          "standard_error": 0.004696387625764289
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003219062446267914,
            "upper_bound": 0.018296522919682767
          },
          "point_estimate": 0.008122431285719147,
          "standard_error": 0.004643798942220095
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.1414687176832,
            "upper_bound": 19.14877329787898
          },
          "point_estimate": 19.143816880804756,
          "standard_error": 0.0018592620795030196
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004919996497117809,
            "upper_bound": 0.022830836882190484
          },
          "point_estimate": 0.015434570686869676,
          "standard_error": 0.00569677600419265
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.848631263801538,
            "upper_bound": 10.905367712101857
          },
          "point_estimate": 10.873934911554407,
          "standard_error": 0.014528809040674058
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.842319256951413,
            "upper_bound": 10.89138915044836
          },
          "point_estimate": 10.860438382330932,
          "standard_error": 0.011601415786589672
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025937264039546898,
            "upper_bound": 0.06907636718685192
          },
          "point_estimate": 0.028781670427049898,
          "standard_error": 0.019829527542003105
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.855161513805822,
            "upper_bound": 10.876835628835194
          },
          "point_estimate": 10.862231584406604,
          "standard_error": 0.005610547075966173
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016727912142065343,
            "upper_bound": 0.06720010984924561
          },
          "point_estimate": 0.04859079091120667,
          "standard_error": 0.013995267428782867
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.73298663001129,
            "upper_bound": 15.750060308364798
          },
          "point_estimate": 15.741592966450408,
          "standard_error": 0.004384305749770551
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.731321088845778,
            "upper_bound": 15.753540672730256
          },
          "point_estimate": 15.739759974951262,
          "standard_error": 0.007101868737750909
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006024827030466405,
            "upper_bound": 0.025977696920419256
          },
          "point_estimate": 0.018696620853476387,
          "standard_error": 0.006057605772041852
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.73524111156474,
            "upper_bound": 15.75223892032523
          },
          "point_estimate": 15.742514067063285,
          "standard_error": 0.004322099144344711
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008697408106972383,
            "upper_bound": 0.018407928264039756
          },
          "point_estimate": 0.014608060536189814,
          "standard_error": 0.002532179838735818
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.350113132095196,
            "upper_bound": 23.4046241791064
          },
          "point_estimate": 23.372471883607172,
          "standard_error": 0.01448492940616066
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.34538907112394,
            "upper_bound": 23.38331722352615
          },
          "point_estimate": 23.353039815645538,
          "standard_error": 0.01148189749191172
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002860560686189454,
            "upper_bound": 0.045724582475503234
          },
          "point_estimate": 0.021488722164883577,
          "standard_error": 0.011595280279956788
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.343715977527253,
            "upper_bound": 23.368325352584808
          },
          "point_estimate": 23.354831977304,
          "standard_error": 0.0064014529365908745
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01242712484433798,
            "upper_bound": 0.07200715504269271
          },
          "point_estimate": 0.04824001390776404,
          "standard_error": 0.019062427391704818
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.9691301253255,
            "upper_bound": 11.01883212916733
          },
          "point_estimate": 10.995727269572528,
          "standard_error": 0.012729158538670814
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.975814250056809,
            "upper_bound": 11.022410946158352
          },
          "point_estimate": 10.996837708739289,
          "standard_error": 0.012964565932673653
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00961214991275409,
            "upper_bound": 0.06336220675672229
          },
          "point_estimate": 0.032836283532118565,
          "standard_error": 0.013994507788155182
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.985491960704442,
            "upper_bound": 11.019292192561526
          },
          "point_estimate": 11.002268141024452,
          "standard_error": 0.00863972581896189
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017333447152644763,
            "upper_bound": 0.05922003408590805
          },
          "point_estimate": 0.042344345378796146,
          "standard_error": 0.011506433160408523
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.052883019483403,
            "upper_bound": 24.065029164087314
          },
          "point_estimate": 24.058033204333256,
          "standard_error": 0.003172539242666856
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.05115148983662,
            "upper_bound": 24.06275250027512
          },
          "point_estimate": 24.05485349663605,
          "standard_error": 0.002677892759220435
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012877054535610753,
            "upper_bound": 0.012790917776195388
          },
          "point_estimate": 0.006132447910169465,
          "standard_error": 0.002975126074712957
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.053049340212393,
            "upper_bound": 24.05899573352837
          },
          "point_estimate": 24.055652376049316,
          "standard_error": 0.0015322629093691463
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003178211554714251,
            "upper_bound": 0.01538112015381731
          },
          "point_estimate": 0.010586150325658177,
          "standard_error": 0.0036990716042769
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.87624774973336,
            "upper_bound": 20.897540785635194
          },
          "point_estimate": 20.885229143233207,
          "standard_error": 0.0055526099115403875
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.87173032699889,
            "upper_bound": 20.887815289178626
          },
          "point_estimate": 20.885326150319784,
          "standard_error": 0.005265101430001755
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005120432132399593,
            "upper_bound": 0.022078387805877963
          },
          "point_estimate": 0.011141914051709318,
          "standard_error": 0.005972494520128115
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.873123572099143,
            "upper_bound": 20.886463935120737
          },
          "point_estimate": 20.878585835977425,
          "standard_error": 0.0034843199994497834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006289179331950959,
            "upper_bound": 0.027408193341587932
          },
          "point_estimate": 0.018445114993707955,
          "standard_error": 0.0068709922403373475
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1185980.4531317204,
            "upper_bound": 1187155.1064650535
          },
          "point_estimate": 1186683.8091487456,
          "standard_error": 313.4347528989875
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1186616.370967742,
            "upper_bound": 1187220.058467742
          },
          "point_estimate": 1186868.9684587815,
          "standard_error": 172.5096407155372
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.36818167460204,
            "upper_bound": 857.0079474463489
          },
          "point_estimate": 449.7079763974815,
          "standard_error": 201.8450522733969
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1186751.6553282372,
            "upper_bound": 1187051.909402883
          },
          "point_estimate": 1186902.2713028907,
          "standard_error": 75.24428745649499
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 222.42748647346096,
            "upper_bound": 1580.262668009938
          },
          "point_estimate": 1045.273830909062,
          "standard_error": 439.1439304500067
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1643084.2048196169,
            "upper_bound": 1644953.0495471011
          },
          "point_estimate": 1643814.5657039338,
          "standard_error": 513.6035258701714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1643026.5217391304,
            "upper_bound": 1643785.8173913043
          },
          "point_estimate": 1643334.963768116,
          "standard_error": 270.031676780666
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.82921624040286,
            "upper_bound": 1111.7727128709346
          },
          "point_estimate": 465.35375260793467,
          "standard_error": 298.74259395751324
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1643101.315691014,
            "upper_bound": 1643753.4355590062
          },
          "point_estimate": 1643392.2767927723,
          "standard_error": 181.1371295954721
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 309.2061136757698,
            "upper_bound": 2611.1491604922976
          },
          "point_estimate": 1713.1156380054003,
          "standard_error": 776.2253135580225
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1457284.6923859525,
            "upper_bound": 1459596.242247381
          },
          "point_estimate": 1458280.5659968257,
          "standard_error": 600.7093649093446
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1456954.5234285714,
            "upper_bound": 1458895.6172222225
          },
          "point_estimate": 1457649.716,
          "standard_error": 455.2508522281802
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.793227935154,
            "upper_bound": 2314.597166707764
          },
          "point_estimate": 1298.7116163433209,
          "standard_error": 637.6276611234342
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1457358.1708232511,
            "upper_bound": 1458786.1769844897
          },
          "point_estimate": 1458090.9026493507,
          "standard_error": 370.699921089027
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 643.1480337449356,
            "upper_bound": 2885.0566662704314
          },
          "point_estimate": 1996.5941373900696,
          "standard_error": 681.3164674252599
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398889.84429373697,
            "upper_bound": 399744.7735973085
          },
          "point_estimate": 399315.2952549172,
          "standard_error": 219.0437475412543
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398846.1564764493,
            "upper_bound": 400038.6956521739
          },
          "point_estimate": 399036.5516304348,
          "standard_error": 368.5432039299493
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.54691110549919,
            "upper_bound": 1258.017157013575
          },
          "point_estimate": 813.3823457497979,
          "standard_error": 344.05243141619457
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398634.765520485,
            "upper_bound": 399899.2651453776
          },
          "point_estimate": 399241.2169113495,
          "standard_error": 345.94869085710656
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 459.0129541360951,
            "upper_bound": 904.8402612504124
          },
          "point_estimate": 735.0008018642563,
          "standard_error": 116.0829544711038
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630218.6858924979,
            "upper_bound": 630442.595127617
          },
          "point_estimate": 630330.8643329227,
          "standard_error": 57.56082117749409
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630156.8132183908,
            "upper_bound": 630496.0229885058
          },
          "point_estimate": 630338.5183189656,
          "standard_error": 105.6313858621574
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.46510224555775,
            "upper_bound": 301.48755671654004
          },
          "point_estimate": 238.82171972564328,
          "standard_error": 69.31264428803853
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630186.4781915426,
            "upper_bound": 630449.6840316405
          },
          "point_estimate": 630304.6337662338,
          "standard_error": 67.580349282323
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.40311541516785,
            "upper_bound": 227.28422454340907
          },
          "point_estimate": 191.55986947722937,
          "standard_error": 26.285129718875677
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392573.26584811823,
            "upper_bound": 393023.9681660693
          },
          "point_estimate": 392755.03475593106,
          "standard_error": 121.25511245478708
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392598.91129032255,
            "upper_bound": 392765.6491636798
          },
          "point_estimate": 392629.0049923195,
          "standard_error": 54.69978643892205
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.953676382904112,
            "upper_bound": 352.5997372884728
          },
          "point_estimate": 55.08576289296739,
          "standard_error": 89.06505175494318
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392609.4935922488,
            "upper_bound": 392775.8767072924
          },
          "point_estimate": 392686.4881161849,
          "standard_error": 44.431717360210904
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.40754947516433,
            "upper_bound": 613.2118103239313
          },
          "point_estimate": 405.52998083989144,
          "standard_error": 173.44412885439908
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481900.56195191096,
            "upper_bound": 482540.06259816216
          },
          "point_estimate": 482285.3837113616,
          "standard_error": 172.8333112880372
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482246.50592105265,
            "upper_bound": 482587.8157894737
          },
          "point_estimate": 482442.2253550543,
          "standard_error": 108.49348719802838
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.09940475907977,
            "upper_bound": 443.1507732728136
          },
          "point_estimate": 229.64368144934664,
          "standard_error": 100.48376159116016
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482387.31632066605,
            "upper_bound": 482564.0111256038
          },
          "point_estimate": 482493.237012987,
          "standard_error": 43.72949641217514
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.4377544266776,
            "upper_bound": 874.57021199446
          },
          "point_estimate": 576.5486554682527,
          "standard_error": 248.35648865305
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1014381.839837963,
            "upper_bound": 1015258.276301422
          },
          "point_estimate": 1014831.7868992503,
          "standard_error": 222.48405837615115
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1014495.1222222224,
            "upper_bound": 1015297.8525132276
          },
          "point_estimate": 1014863.890625,
          "standard_error": 210.6384601307853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.1504764074339,
            "upper_bound": 1218.2032235115014
          },
          "point_estimate": 446.1742767084334,
          "standard_error": 267.1129673777484
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1014664.6435457516,
            "upper_bound": 1015152.7167943806
          },
          "point_estimate": 1014894.9776334777,
          "standard_error": 124.57862199118166
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292.01093237261387,
            "upper_bound": 1032.66200676239
          },
          "point_estimate": 743.8333208141274,
          "standard_error": 189.6433749825899
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332925.05852357286,
            "upper_bound": 333862.20004587155
          },
          "point_estimate": 333399.56403160037,
          "standard_error": 240.73728544605223
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332509.247706422,
            "upper_bound": 334124.63302752294
          },
          "point_estimate": 333431.7844036697,
          "standard_error": 365.1522929167082
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148.67485332384436,
            "upper_bound": 1392.032762381292
          },
          "point_estimate": 1083.663604981332,
          "standard_error": 342.1740662788066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332596.6924569392,
            "upper_bound": 333634.3884256927
          },
          "point_estimate": 333041.18746574526,
          "standard_error": 267.0591403315615
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481.1384325893674,
            "upper_bound": 970.9633218035258
          },
          "point_estimate": 802.2681361419998,
          "standard_error": 123.38176630318672
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 832130.1066125542,
            "upper_bound": 832650.1626010101
          },
          "point_estimate": 832376.052914863,
          "standard_error": 133.27804519120866
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 831997.6147727272,
            "upper_bound": 832764.6590909091
          },
          "point_estimate": 832284.7323232323,
          "standard_error": 177.9635289346852
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.149055608255,
            "upper_bound": 705.1076997545445
          },
          "point_estimate": 442.0569448034689,
          "standard_error": 173.61510622110129
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 832174.7439236111,
            "upper_bound": 832592.2457735247
          },
          "point_estimate": 832383.7163518299,
          "standard_error": 105.40191815740504
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203.2219985467524,
            "upper_bound": 556.2327864949302
          },
          "point_estimate": 445.3135931983962,
          "standard_error": 85.03794575833182
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400296.57969871786,
            "upper_bound": 400503.6356758242
          },
          "point_estimate": 400388.7680677655,
          "standard_error": 53.73631602290489
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400271.6236263736,
            "upper_bound": 400451.04505494505
          },
          "point_estimate": 400335.23099816847,
          "standard_error": 47.88561932839134
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.018030680069894,
            "upper_bound": 243.2012464515124
          },
          "point_estimate": 158.991400831232,
          "standard_error": 65.08123283156056
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400314.01796682406,
            "upper_bound": 400413.92528896494
          },
          "point_estimate": 400363.67852147855,
          "standard_error": 26.689768494898733
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.243295131773046,
            "upper_bound": 257.18694130264396
          },
          "point_estimate": 180.43361443477897,
          "standard_error": 56.89452708627197
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174777.44389880952,
            "upper_bound": 175011.13905677653
          },
          "point_estimate": 174896.86569940476,
          "standard_error": 59.68646783653258
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174743.5625,
            "upper_bound": 175036.5328525641
          },
          "point_estimate": 174945.57291666666,
          "standard_error": 81.04744546908849
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.118105358830014,
            "upper_bound": 336.7176993105324
          },
          "point_estimate": 232.35923349498796,
          "standard_error": 81.5471123643542
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174721.0756005869,
            "upper_bound": 174950.817511655
          },
          "point_estimate": 174835.497027972,
          "standard_error": 58.57763853348202
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.85649899834053,
            "upper_bound": 255.602121788178
          },
          "point_estimate": 198.64164738150643,
          "standard_error": 37.41371551407995
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232929.453995204,
            "upper_bound": 233154.90231604493
          },
          "point_estimate": 233013.61232408247,
          "standard_error": 64.20346660267712
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232923.03457688808,
            "upper_bound": 232980.62420382164
          },
          "point_estimate": 232967.01974522293,
          "standard_error": 21.956291181196573
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.293770668531799,
            "upper_bound": 87.67533355489293
          },
          "point_estimate": 35.81738606795167,
          "standard_error": 27.83423996524724
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232941.7228220518,
            "upper_bound": 232974.76510858117
          },
          "point_estimate": 232961.9732153197,
          "standard_error": 8.38272723829561
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.49139760831931,
            "upper_bound": 329.12883262988134
          },
          "point_estimate": 213.95181185509333,
          "standard_error": 106.49386296657582
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175264.81841346156,
            "upper_bound": 175328.13865155674
          },
          "point_estimate": 175296.00409340658,
          "standard_error": 16.181038979727624
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175249.1920673077,
            "upper_bound": 175341.4298878205
          },
          "point_estimate": 175296.4428228022,
          "standard_error": 19.8103217403208
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.322509209504704,
            "upper_bound": 106.87074810263618
          },
          "point_estimate": 46.09507860473587,
          "standard_error": 24.036175056017928
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175274.84233597285,
            "upper_bound": 175340.12284982606
          },
          "point_estimate": 175303.5378996004,
          "standard_error": 17.626858370038537
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.126876506484457,
            "upper_bound": 67.4847356730532
          },
          "point_estimate": 53.866995312332136,
          "standard_error": 9.549319969289796
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-repeated-rare-huge_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 675915.9591055812,
            "upper_bound": 676261.9638540379
          },
          "point_estimate": 676069.112653586,
          "standard_error": 89.63300230757498
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 675809.8941798941,
            "upper_bound": 676167.4340277778
          },
          "point_estimate": 676056.1612654321,
          "standard_error": 86.93963198864508
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.41498998636943,
            "upper_bound": 400.013709564927
          },
          "point_estimate": 249.3057062683457,
          "standard_error": 103.5173966974215
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 675896.2738772813,
            "upper_bound": 676146.0539062474
          },
          "point_estimate": 676023.012987013,
          "standard_error": 63.74970251599489
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119.48696004832505,
            "upper_bound": 422.9433179044325
          },
          "point_estimate": 298.9205822981374,
          "standard_error": 91.35947016374168
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-repeated-rare-small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1365.7986273774484,
            "upper_bound": 1367.299303699802
          },
          "point_estimate": 1366.4479601981816,
          "standard_error": 0.390455349997366
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1365.5025737366147,
            "upper_bound": 1367.0106049220365
          },
          "point_estimate": 1365.949377961467,
          "standard_error": 0.4136578822188231
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.045361328144664856,
            "upper_bound": 1.7444986578949326
          },
          "point_estimate": 0.7443177178897814,
          "standard_error": 0.41619766850103246
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1365.5878729539995,
            "upper_bound": 1366.6298359445927
          },
          "point_estimate": 1365.985246187215,
          "standard_error": 0.2757839400169568
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4407488034192552,
            "upper_bound": 1.866839342288365
          },
          "point_estimate": 1.2996988110658505,
          "standard_error": 0.42476754896739366
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1192662.937440956,
            "upper_bound": 1193845.3061658987
          },
          "point_estimate": 1193264.4315552996,
          "standard_error": 300.6870703586882
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1192725.2857142857,
            "upper_bound": 1193746.0612903226
          },
          "point_estimate": 1193319.0356182796,
          "standard_error": 262.98380574316684
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184.16123544021016,
            "upper_bound": 1717.0762233564346
          },
          "point_estimate": 735.1446063840779,
          "standard_error": 344.7180714707911
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1192548.6410810554,
            "upper_bound": 1193801.5024604923
          },
          "point_estimate": 1193288.5244239632,
          "standard_error": 313.70177024478323
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 372.9814926342436,
            "upper_bound": 1373.9653211039654
          },
          "point_estimate": 1000.9962783605332,
          "standard_error": 263.79258893913527
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1330909.2593863378,
            "upper_bound": 1334345.5819623724
          },
          "point_estimate": 1332486.2871782875,
          "standard_error": 884.023769850966
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1330381.9682539685,
            "upper_bound": 1334333.5720238094
          },
          "point_estimate": 1331337.4098214286,
          "standard_error": 1058.0691791145568
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 329.9524035172561,
            "upper_bound": 4357.904312274822
          },
          "point_estimate": 1524.708607514385,
          "standard_error": 1080.0458724070047
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1330502.9897381198,
            "upper_bound": 1333205.1595238096
          },
          "point_estimate": 1331529.4428571428,
          "standard_error": 692.6476979916204
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 783.289857797267,
            "upper_bound": 3702.721236583327
          },
          "point_estimate": 2949.5035035030555,
          "standard_error": 690.7578924745649
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1235006.492456349,
            "upper_bound": 1238372.7259166664
          },
          "point_estimate": 1236710.4799087304,
          "standard_error": 860.8886430282274
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1233946.1166666667,
            "upper_bound": 1239166.7034722222
          },
          "point_estimate": 1237509.0977777778,
          "standard_error": 1456.7211491331875
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 476.95858903220034,
            "upper_bound": 4607.062928207977
          },
          "point_estimate": 3969.495254777404,
          "standard_error": 1101.1809242955803
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1234336.7499001997,
            "upper_bound": 1238447.0558974303
          },
          "point_estimate": 1236340.1043290044,
          "standard_error": 1049.7966064325149
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1805.634588315537,
            "upper_bound": 3411.369018494876
          },
          "point_estimate": 2867.449040588839,
          "standard_error": 412.60600251417264
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325442.3395659722,
            "upper_bound": 326646.7595734127
          },
          "point_estimate": 326001.2959871032,
          "standard_error": 310.4266941842982
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325190.8053571429,
            "upper_bound": 326829.3794642857
          },
          "point_estimate": 325677.1555059524,
          "standard_error": 377.9606814682013
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189.9682703774745,
            "upper_bound": 1699.473572640861
          },
          "point_estimate": 841.4591019361792,
          "standard_error": 378.3133393230989
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325337.4178385089,
            "upper_bound": 326087.18375
          },
          "point_estimate": 325637.8042903525,
          "standard_error": 195.63369299234012
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 405.34132136881453,
            "upper_bound": 1285.8220974562714
          },
          "point_estimate": 1034.3833670369263,
          "standard_error": 223.0088667314404
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246358.08505884893,
            "upper_bound": 247328.36157936504
          },
          "point_estimate": 246817.22584116188,
          "standard_error": 249.06655347283987
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246104.5418367347,
            "upper_bound": 247325.3112244898
          },
          "point_estimate": 246754.77286470143,
          "standard_error": 319.766376000779
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154.3080639271492,
            "upper_bound": 1342.4829297376089
          },
          "point_estimate": 792.9642374526933,
          "standard_error": 310.92950469368617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246257.8280344779,
            "upper_bound": 246962.79420272508
          },
          "point_estimate": 246597.3873133669,
          "standard_error": 179.67688440284903
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409.0462215696901,
            "upper_bound": 1065.2446974516613
          },
          "point_estimate": 827.9880977751428,
          "standard_error": 171.39067779204356
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 429323.3847396359,
            "upper_bound": 431290.5744677872
          },
          "point_estimate": 430209.1902978525,
          "standard_error": 507.8671673437043
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 429034.50352941174,
            "upper_bound": 431109.1218487395
          },
          "point_estimate": 429663.22562091507,
          "standard_error": 611.5166625986705
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.52722322002673,
            "upper_bound": 2388.0136028983347
          },
          "point_estimate": 1262.9633775779423,
          "standard_error": 609.4750753619268
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 429177.1778358751,
            "upper_bound": 430330.0460419125
          },
          "point_estimate": 429673.09860962565,
          "standard_error": 295.2375582108667
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 685.0127484524289,
            "upper_bound": 2343.0624499673427
          },
          "point_estimate": 1691.6628731281437,
          "standard_error": 475.3489809192729
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236938.4041395524,
            "upper_bound": 237538.061753498
          },
          "point_estimate": 237231.9261799114,
          "standard_error": 153.98660218240732
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236736.48896103897,
            "upper_bound": 237658.6084505772
          },
          "point_estimate": 237167.64285714284,
          "standard_error": 308.93463236059387
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.16624351043529,
            "upper_bound": 853.4547035981625
          },
          "point_estimate": 676.6293851465108,
          "standard_error": 230.87814501333568
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236824.8131645985,
            "upper_bound": 237532.47040126572
          },
          "point_estimate": 237181.2308821049,
          "standard_error": 183.5732984884886
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.5435046236785,
            "upper_bound": 610.3510500544734
          },
          "point_estimate": 512.3626794220895,
          "standard_error": 72.33614097889729
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 529151.4872755349,
            "upper_bound": 530671.1859454798
          },
          "point_estimate": 529855.3242305038,
          "standard_error": 391.17243292455214
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 528889.0439613527,
            "upper_bound": 530838.096273292
          },
          "point_estimate": 529332.704710145,
          "standard_error": 495.6453069224752
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 212.05011464101727,
            "upper_bound": 2109.4912556650747
          },
          "point_estimate": 845.7790832453462,
          "standard_error": 493.57356868984994
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 529162.9992433502,
            "upper_bound": 531584.7546360176
          },
          "point_estimate": 530620.9134952005,
          "standard_error": 631.695633009314
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 476.6732406181368,
            "upper_bound": 1631.1194450322494
          },
          "point_estimate": 1305.9949797138536,
          "standard_error": 289.7944669471451
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112188.03058862434,
            "upper_bound": 112364.4548333517
          },
          "point_estimate": 112275.747154125,
          "standard_error": 45.25151815505277
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112146.50913065844,
            "upper_bound": 112391.72856040564
          },
          "point_estimate": 112281.8311042524,
          "standard_error": 71.51387245883757
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.13844535736052,
            "upper_bound": 254.43596739644747
          },
          "point_estimate": 164.2834914763924,
          "standard_error": 54.196041702433725
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112194.40272394488,
            "upper_bound": 112362.94421950664
          },
          "point_estimate": 112279.23187429854,
          "standard_error": 43.94675209605166
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.24091537979352,
            "upper_bound": 186.75531334435317
          },
          "point_estimate": 150.45418106883392,
          "standard_error": 24.61097863845614
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118504.59496923632,
            "upper_bound": 118717.96847448424
          },
          "point_estimate": 118601.08565353396,
          "standard_error": 54.97617325111414
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118461.59201954395,
            "upper_bound": 118668.98778501627
          },
          "point_estimate": 118605.92345276874,
          "standard_error": 56.7784080153012
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.25968333592203,
            "upper_bound": 261.99394114129854
          },
          "point_estimate": 145.22356501134377,
          "standard_error": 61.85977761335627
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118457.91456175964,
            "upper_bound": 118623.51493037034
          },
          "point_estimate": 118550.5978848513,
          "standard_error": 44.04861352045748
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.36053547150566,
            "upper_bound": 257.13776950140635
          },
          "point_estimate": 182.70745242063384,
          "standard_error": 53.343033958198255
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193709.23068784823,
            "upper_bound": 194125.59965591755
          },
          "point_estimate": 193905.12700713443,
          "standard_error": 106.62237551477922
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193589.2281323877,
            "upper_bound": 194107.4402070669
          },
          "point_estimate": 193907.35992907803,
          "standard_error": 121.90980127111484
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.71566943667635,
            "upper_bound": 588.3726363849477
          },
          "point_estimate": 330.5020648770948,
          "standard_error": 135.8986009922289
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193745.77000880777,
            "upper_bound": 194151.5439692634
          },
          "point_estimate": 193947.84709864605,
          "standard_error": 103.38351546504845
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.8595997258309,
            "upper_bound": 475.6299749145269
          },
          "point_estimate": 354.39826213722074,
          "standard_error": 83.13754900824723
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271988.0985803128,
            "upper_bound": 272384.2722239472
          },
          "point_estimate": 272178.0429646411,
          "standard_error": 101.68611799214898
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271899.5402985075,
            "upper_bound": 272437.61007462686
          },
          "point_estimate": 272096.92599502485,
          "standard_error": 118.68924397654054
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.192039655595254,
            "upper_bound": 578.2538668794849
          },
          "point_estimate": 333.53694392926514,
          "standard_error": 135.25583977116435
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271986.1109458312,
            "upper_bound": 272274.342254246
          },
          "point_estimate": 272114.8944950572,
          "standard_error": 72.1727471195564
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167.16156461919593,
            "upper_bound": 429.9997934395497
          },
          "point_estimate": 339.14302461531,
          "standard_error": 67.02727911358352
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289408.8320587679,
            "upper_bound": 290142.3848405848
          },
          "point_estimate": 289778.00705876795,
          "standard_error": 187.54464478656175
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289237.59837962966,
            "upper_bound": 290194.6521164021
          },
          "point_estimate": 289824.3111111111,
          "standard_error": 237.62285887790105
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.4222150550496,
            "upper_bound": 1006.031596722707
          },
          "point_estimate": 473.5261683789496,
          "standard_error": 245.9101890881932
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289232.3695084485,
            "upper_bound": 290003.7248429645
          },
          "point_estimate": 289690.2036487322,
          "standard_error": 198.38553752447663
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325.95478405379544,
            "upper_bound": 797.4305109719263
          },
          "point_estimate": 620.8823099287117,
          "standard_error": 119.48847778123913
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316323.3191537267,
            "upper_bound": 317093.24289984466
          },
          "point_estimate": 316700.984689441,
          "standard_error": 196.2789528038714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316102.3847826087,
            "upper_bound": 317250.401242236
          },
          "point_estimate": 316726.48695652175,
          "standard_error": 256.270391780217
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.931852383459596,
            "upper_bound": 1142.7687214509324
          },
          "point_estimate": 721.9368628352343,
          "standard_error": 271.61697659046615
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316114.65442801936,
            "upper_bound": 317108.7053195465
          },
          "point_estimate": 316590.2914737436,
          "standard_error": 254.50691903128185
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 370.3119018083674,
            "upper_bound": 820.1219015649554
          },
          "point_estimate": 653.880746453202,
          "standard_error": 115.42604975926874
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395717.30464596266,
            "upper_bound": 396642.3963258605
          },
          "point_estimate": 396183.2440221705,
          "standard_error": 236.86679869637572
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395424.25,
            "upper_bound": 396953.60217391304
          },
          "point_estimate": 396162.40670289856,
          "standard_error": 413.739041680282
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.56472730703635,
            "upper_bound": 1357.438096045572
          },
          "point_estimate": 1133.708746394381,
          "standard_error": 341.1917828767795
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395446.5024431592,
            "upper_bound": 396887.3761739554
          },
          "point_estimate": 396214.89971767366,
          "standard_error": 376.1975975551082
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 493.5753902040539,
            "upper_bound": 925.714711697067
          },
          "point_estimate": 791.3019041309536,
          "standard_error": 108.16561339085872
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272807.58290217066,
            "upper_bound": 273470.1761372158
          },
          "point_estimate": 273123.4340914475,
          "standard_error": 169.96661478134712
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272628.4642412935,
            "upper_bound": 273581.144278607
          },
          "point_estimate": 273070.2674307036,
          "standard_error": 270.20599771371354
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.1271430785349,
            "upper_bound": 975.2624115413312
          },
          "point_estimate": 663.120959682466,
          "standard_error": 220.84179480450243
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272719.19554255746,
            "upper_bound": 273194.6097823842
          },
          "point_estimate": 273001.0134328358,
          "standard_error": 121.69348043623096
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321.49958011634214,
            "upper_bound": 721.279564678781
          },
          "point_estimate": 567.078739607629,
          "standard_error": 107.22067700210928
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121543.80358506944,
            "upper_bound": 121821.97968256944
          },
          "point_estimate": 121679.70060833334,
          "standard_error": 71.19466645256453
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121476.61966666668,
            "upper_bound": 121846.42133333332
          },
          "point_estimate": 121675.94694444444,
          "standard_error": 86.15042095170652
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.65886217852155,
            "upper_bound": 402.7809000492184
          },
          "point_estimate": 215.7090916953996,
          "standard_error": 91.39901369404332
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121444.17010025065,
            "upper_bound": 121702.4077367681
          },
          "point_estimate": 121556.56774891776,
          "standard_error": 66.80822791067885
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126.90751078597734,
            "upper_bound": 306.5490838248348
          },
          "point_estimate": 237.19608647392505,
          "standard_error": 46.466520268264354
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203122.35607209365,
            "upper_bound": 203545.12188370136
          },
          "point_estimate": 203311.735506562,
          "standard_error": 108.84522773989536
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203065.20181564247,
            "upper_bound": 203543.8474860335
          },
          "point_estimate": 203155.69468830363,
          "standard_error": 107.38788016215308
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.63616696985134,
            "upper_bound": 553.4405931493086
          },
          "point_estimate": 252.51058822653425,
          "standard_error": 130.0916680839293
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203139.1782122905,
            "upper_bound": 203329.85689576337
          },
          "point_estimate": 203212.6752521222,
          "standard_error": 49.358523208936106
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.01577595678089,
            "upper_bound": 489.15988788704624
          },
          "point_estimate": 364.11969772288296,
          "standard_error": 100.02816242101234
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150446.47980767905,
            "upper_bound": 151024.82875404615
          },
          "point_estimate": 150710.79596303948,
          "standard_error": 148.8335004654634
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150432.65495867768,
            "upper_bound": 150977.03099173552
          },
          "point_estimate": 150538.24974173552,
          "standard_error": 130.70652398885986
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.765255203447346,
            "upper_bound": 757.7476228420642
          },
          "point_estimate": 216.28499244114772,
          "standard_error": 187.43692868448653
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150294.60138167284,
            "upper_bound": 151026.4016679651
          },
          "point_estimate": 150628.9037243748,
          "standard_error": 185.78978715381885
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166.7811140807588,
            "upper_bound": 677.4752767997383
          },
          "point_estimate": 498.1156246951238,
          "standard_error": 135.4889199841572
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47604.15705847167,
            "upper_bound": 47716.256412969466
          },
          "point_estimate": 47654.591243938135,
          "standard_error": 28.92815423842251
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47582.30658366981,
            "upper_bound": 47715.36137281292
          },
          "point_estimate": 47634.04445192164,
          "standard_error": 30.838990007112727
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.016369045082707,
            "upper_bound": 132.6359765315138
          },
          "point_estimate": 64.29286360733934,
          "standard_error": 34.77710980054512
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47592.60915616461,
            "upper_bound": 47654.67817826344
          },
          "point_estimate": 47621.91950848613,
          "standard_error": 15.71896413285866
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.81806410584714,
            "upper_bound": 128.97966078989302
          },
          "point_estimate": 96.34186562947498,
          "standard_error": 26.16334896225337
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1118387.3677386965,
            "upper_bound": 1121739.9208790285
          },
          "point_estimate": 1120082.3873689272,
          "standard_error": 857.4999149941918
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1118216.515993266,
            "upper_bound": 1122894.818181818
          },
          "point_estimate": 1119673.4515151514,
          "standard_error": 1099.5182155772563
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 259.8840508405578,
            "upper_bound": 5486.826199862209
          },
          "point_estimate": 2531.889602524683,
          "standard_error": 1320.9590675776708
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1118546.8251082252,
            "upper_bound": 1121233.1846453587
          },
          "point_estimate": 1119590.8562770565,
          "standard_error": 674.2620172065365
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1629.0699416297898,
            "upper_bound": 3649.39592610318
          },
          "point_estimate": 2856.571113700578,
          "standard_error": 536.0197916763601
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124656.3189749266,
            "upper_bound": 124870.93295279886
          },
          "point_estimate": 124768.20566658513,
          "standard_error": 55.08127476819895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124584.9549086758,
            "upper_bound": 124911.4216609589
          },
          "point_estimate": 124828.25718566537,
          "standard_error": 76.67984182267793
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.52597158890118,
            "upper_bound": 311.9421957461535
          },
          "point_estimate": 205.90389899858383,
          "standard_error": 77.16369930424088
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124557.97540884676,
            "upper_bound": 124815.1429725743
          },
          "point_estimate": 124673.88913004805,
          "standard_error": 66.88519356999986
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.16763794174048,
            "upper_bound": 226.0520237308239
          },
          "point_estimate": 183.11316628050685,
          "standard_error": 33.55602753677119
        }
      }
    },
    "memrmem/stud/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/stud_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24056.148031893394,
            "upper_bound": 24112.187827249385
          },
          "point_estimate": 24080.50403414593,
          "standard_error": 14.516180297031438
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24043.205172413793,
            "upper_bound": 24103.623342175066
          },
          "point_estimate": 24068.7679855585,
          "standard_error": 13.725089566360102
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.270966528740424,
            "upper_bound": 64.87287035579315
          },
          "point_estimate": 38.60700381502887,
          "standard_error": 15.158953086233293
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24045.53353180202,
            "upper_bound": 24082.73537483785
          },
          "point_estimate": 24061.24652244307,
          "standard_error": 9.673182747533446
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.742856768141873,
            "upper_bound": 68.7967685027385
          },
          "point_estimate": 48.21790253715129,
          "standard_error": 15.209324595662428
        }
      }
    },
    "memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/stud_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24516.07201746448,
            "upper_bound": 24591.19155014283
          },
          "point_estimate": 24549.1322756245,
          "standard_error": 19.535238029604443
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24514.01299536053,
            "upper_bound": 24590.3290576605
          },
          "point_estimate": 24527.504555705906,
          "standard_error": 15.243377647677857
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.3797737204431137,
            "upper_bound": 85.92873090003593
          },
          "point_estimate": 13.435004813911156,
          "standard_error": 18.381088625270035
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24519.16613570462,
            "upper_bound": 24556.103047684446
          },
          "point_estimate": 24530.8681053723,
          "standard_error": 9.66666642961664
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.565687416307265,
            "upper_bound": 84.98891704272927
          },
          "point_estimate": 65.13952787119095,
          "standard_error": 19.739720003363537
        }
      }
    },
    "memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/stud_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 606800.0976944445,
            "upper_bound": 607263.7485231482
          },
          "point_estimate": 607019.5877228837,
          "standard_error": 118.2949428120056
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 606657.6375,
            "upper_bound": 607232.6592592592
          },
          "point_estimate": 606995.3694940476,
          "standard_error": 132.8456435627768
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.40926135649882,
            "upper_bound": 677.1075263123024
          },
          "point_estimate": 390.8102643117941,
          "standard_error": 158.75697206279625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 606881.4792504222,
            "upper_bound": 607333.7953063538
          },
          "point_estimate": 607091.4656277057,
          "standard_error": 115.11894498371608
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198.86392168492344,
            "upper_bound": 527.9435381325078
          },
          "point_estimate": 394.0052675950545,
          "standard_error": 91.32704615044796
        }
      }
    },
    "memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/stud_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1333.1521722836624,
            "upper_bound": 1337.6542742128604
          },
          "point_estimate": 1335.5272232855507,
          "standard_error": 1.156910909396922
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1332.4682585128903,
            "upper_bound": 1338.8903204492897
          },
          "point_estimate": 1336.5609816181118,
          "standard_error": 1.5983570243874194
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7914589497192941,
            "upper_bound": 6.438265424490551
          },
          "point_estimate": 3.852453979373075,
          "standard_error": 1.4461764126662235
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1335.2375549297428,
            "upper_bound": 1338.5882528177276
          },
          "point_estimate": 1336.8056177423869,
          "standard_error": 0.8670334403374443
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8959887012270344,
            "upper_bound": 4.776852000559896
          },
          "point_estimate": 3.836076103615528,
          "standard_error": 0.7281160671978427
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.458132838406726,
            "upper_bound": 29.49318116156376
          },
          "point_estimate": 29.47944760889477,
          "standard_error": 0.009584228369536915
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.479578019637135,
            "upper_bound": 29.493831529250222
          },
          "point_estimate": 29.488289738936174,
          "standard_error": 0.004071877839389932
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000904344967624627,
            "upper_bound": 0.02222693643600342
          },
          "point_estimate": 0.008925204351133479,
          "standard_error": 0.005976580118382842
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.487073650143337,
            "upper_bound": 29.499154267938255
          },
          "point_estimate": 29.49276307703748,
          "standard_error": 0.0032600353435824365
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005529102472337123,
            "upper_bound": 0.04858031076056074
          },
          "point_estimate": 0.031887595944432606,
          "standard_error": 0.014262637095468753
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.40442650949865,
            "upper_bound": 32.440456001526464
          },
          "point_estimate": 32.418149320014074,
          "standard_error": 0.010065691598069726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.40143241793109,
            "upper_bound": 32.41758230658073
          },
          "point_estimate": 32.409222873647636,
          "standard_error": 0.005521249603642677
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009384207629646892,
            "upper_bound": 0.0203896411881642
          },
          "point_estimate": 0.011971912243431289,
          "standard_error": 0.005105034482371217
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.402254561287535,
            "upper_bound": 32.414072010849864
          },
          "point_estimate": 32.40882710228708,
          "standard_error": 0.003018194406071394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005780578170135527,
            "upper_bound": 0.051188954492993925
          },
          "point_estimate": 0.03342765919207387,
          "standard_error": 0.01572817335330322
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.935738299732712,
            "upper_bound": 23.0112664376504
          },
          "point_estimate": 22.97075191270265,
          "standard_error": 0.019454175684475196
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.9263116213796,
            "upper_bound": 23.017463927755735
          },
          "point_estimate": 22.94623619239176,
          "standard_error": 0.024340393028755836
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005134752049067221,
            "upper_bound": 0.1038636388241909
          },
          "point_estimate": 0.03403801564357062,
          "standard_error": 0.024582814644197627
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.94396658720551,
            "upper_bound": 23.049607467915955
          },
          "point_estimate": 22.999508895462128,
          "standard_error": 0.02722958653541007
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02117552171919531,
            "upper_bound": 0.07993129203591033
          },
          "point_estimate": 0.06470905475942232,
          "standard_error": 0.014392545561620932
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.491107234140127,
            "upper_bound": 27.506612932168537
          },
          "point_estimate": 27.49810075082081,
          "standard_error": 0.003996649369076964
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.48922927266908,
            "upper_bound": 27.50739920702724
          },
          "point_estimate": 27.492855190516632,
          "standard_error": 0.004520580678867053
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001579307277528833,
            "upper_bound": 0.02156295849580426
          },
          "point_estimate": 0.005624774413315881,
          "standard_error": 0.005067932348636893
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.49049028347694,
            "upper_bound": 27.500490064460863
          },
          "point_estimate": 27.49414612272334,
          "standard_error": 0.002581476144466813
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003677786468609344,
            "upper_bound": 0.017712833072471672
          },
          "point_estimate": 0.013272134596912037,
          "standard_error": 0.003458780211799386
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.88270187037851,
            "upper_bound": 35.93726611021364
          },
          "point_estimate": 35.91525086128864,
          "standard_error": 0.01453501498431686
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.914488599631454,
            "upper_bound": 35.9423040973421
          },
          "point_estimate": 35.924394411894525,
          "standard_error": 0.005909687642387567
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024034697437097124,
            "upper_bound": 0.04248083628534235
          },
          "point_estimate": 0.006486847723155956,
          "standard_error": 0.011143043532158984
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.92395546856825,
            "upper_bound": 35.93814820425599
          },
          "point_estimate": 35.92961376310689,
          "standard_error": 0.003611863670032096
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008195617807509503,
            "upper_bound": 0.07324954744722112
          },
          "point_estimate": 0.04851909529625842,
          "standard_error": 0.02059875550120907
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.04147841852185,
            "upper_bound": 65.30717938627083
          },
          "point_estimate": 65.18064680380182,
          "standard_error": 0.06858121689637406
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.9229639437804,
            "upper_bound": 65.35881429153474
          },
          "point_estimate": 65.30901195029747,
          "standard_error": 0.12104403417477448
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018341694562839423,
            "upper_bound": 0.3520666511786498
          },
          "point_estimate": 0.12986743750937307,
          "standard_error": 0.09908353203239505
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.1740366471822,
            "upper_bound": 65.33866054594077
          },
          "point_estimate": 65.28731805417702,
          "standard_error": 0.042409492753656006
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1214843105699296,
            "upper_bound": 0.26414921111463874
          },
          "point_estimate": 0.22916497583840056,
          "standard_error": 0.036015239619613955
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.61048139100573,
            "upper_bound": 54.63761120846468
          },
          "point_estimate": 54.62238343256202,
          "standard_error": 0.006981865483893727
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.60884461469645,
            "upper_bound": 54.62774413766395
          },
          "point_estimate": 54.62099633444511,
          "standard_error": 0.004637847134574334
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017251460472458011,
            "upper_bound": 0.028829328092715496
          },
          "point_estimate": 0.012177986340424805,
          "standard_error": 0.006770546921524443
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.60652743465741,
            "upper_bound": 54.62243632461395
          },
          "point_estimate": 54.61539166955212,
          "standard_error": 0.004066089067904552
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006735126573591481,
            "upper_bound": 0.03368588760629277
          },
          "point_estimate": 0.023259727401270795,
          "standard_error": 0.0078550308558896
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.05565547684124,
            "upper_bound": 50.09705681217341
          },
          "point_estimate": 50.07722386076516,
          "standard_error": 0.010611627625170962
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.060743678184664,
            "upper_bound": 50.09934750879512
          },
          "point_estimate": 50.07526752541004,
          "standard_error": 0.009156216820057309
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003304227313264335,
            "upper_bound": 0.0586086532888816
          },
          "point_estimate": 0.024897726217340585,
          "standard_error": 0.014376255659983442
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.06764864672608,
            "upper_bound": 50.091788832944424
          },
          "point_estimate": 50.07718522536045,
          "standard_error": 0.006225605038424023
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014888108920525498,
            "upper_bound": 0.04842492670009794
          },
          "point_estimate": 0.03531539226609882,
          "standard_error": 0.008800001488638699
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.69365429586068,
            "upper_bound": 95.87471752886285
          },
          "point_estimate": 95.78129234139632,
          "standard_error": 0.046210132945613186
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.6814836103106,
            "upper_bound": 95.8404050314681
          },
          "point_estimate": 95.78025182388411,
          "standard_error": 0.03454603024767345
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018985597442550923,
            "upper_bound": 0.2578124692221209
          },
          "point_estimate": 0.06976060620433734,
          "standard_error": 0.06178918519707613
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.6454243397676,
            "upper_bound": 95.7894449952246
          },
          "point_estimate": 95.73219577536942,
          "standard_error": 0.03707590472414642
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05334604323454854,
            "upper_bound": 0.2144138110029591
          },
          "point_estimate": 0.15361096668722338,
          "standard_error": 0.039670139962383874
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.90990015816876,
            "upper_bound": 39.01724625229931
          },
          "point_estimate": 38.96462275568954,
          "standard_error": 0.02737468633503866
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.89825506706624,
            "upper_bound": 39.01231168256492
          },
          "point_estimate": 38.98765600230327,
          "standard_error": 0.030292068721677185
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0091538958025828,
            "upper_bound": 0.16104967801499903
          },
          "point_estimate": 0.055823205566956906,
          "standard_error": 0.04066918381482914
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.8592519495043,
            "upper_bound": 38.98082764407521
          },
          "point_estimate": 38.92066788794059,
          "standard_error": 0.032805211210823776
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04448211746862677,
            "upper_bound": 0.11857189812495876
          },
          "point_estimate": 0.09073073251334252,
          "standard_error": 0.018702428900413265
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.894180572047354,
            "upper_bound": 43.95781158343467
          },
          "point_estimate": 43.93169042946253,
          "standard_error": 0.016914941135304496
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.916820268152705,
            "upper_bound": 43.959046193255915
          },
          "point_estimate": 43.95182891200892,
          "standard_error": 0.011393206436182636
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001795625001828584,
            "upper_bound": 0.05317090265590377
          },
          "point_estimate": 0.013061531185391244,
          "standard_error": 0.014501755457758812
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.92390592460619,
            "upper_bound": 43.96632183274425
          },
          "point_estimate": 43.94387309228642,
          "standard_error": 0.01095704073333274
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01313002407978432,
            "upper_bound": 0.08392162936623714
          },
          "point_estimate": 0.05632763374835945,
          "standard_error": 0.022315807297283614
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.55289238138047,
            "upper_bound": 79.60022305839043
          },
          "point_estimate": 79.57496604105782,
          "standard_error": 0.012056579275942713
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.54449080215844,
            "upper_bound": 79.59405108729013
          },
          "point_estimate": 79.57578306046877,
          "standard_error": 0.013072173356139943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0065638178091171106,
            "upper_bound": 0.06315489927282701
          },
          "point_estimate": 0.03479650224639155,
          "standard_error": 0.014030964937139389
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.55927412768587,
            "upper_bound": 79.58673915306848
          },
          "point_estimate": 79.5741363011782,
          "standard_error": 0.006920702521767275
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018607978053665663,
            "upper_bound": 0.054692117978349775
          },
          "point_estimate": 0.040077588181483464,
          "standard_error": 0.010129969138349328
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 912490.6318998015,
            "upper_bound": 913039.3010653524
          },
          "point_estimate": 912714.1427033732,
          "standard_error": 145.529074290123
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 912430.040625,
            "upper_bound": 912794.95625
          },
          "point_estimate": 912564.7638888888,
          "standard_error": 113.43765676616546
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.29687789014494,
            "upper_bound": 458.2138922400681
          },
          "point_estimate": 273.6459481417972,
          "standard_error": 100.42692231940572
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 912477.4809839636,
            "upper_bound": 912725.3692705026
          },
          "point_estimate": 912605.1213636365,
          "standard_error": 63.82923433039176
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 133.47659771730528,
            "upper_bound": 728.2621040977103
          },
          "point_estimate": 485.9792451921563,
          "standard_error": 193.72539875288885
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1233978.9730916666,
            "upper_bound": 1238301.2955238095
          },
          "point_estimate": 1236011.5176137567,
          "standard_error": 1106.905803950235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1232366.7866666666,
            "upper_bound": 1237502.3583333334
          },
          "point_estimate": 1236634.6092592594,
          "standard_error": 1423.8218293227717
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 343.2571939059338,
            "upper_bound": 7001.962616190471
          },
          "point_estimate": 2476.95712519196,
          "standard_error": 1701.190898278262
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1233117.0360653007,
            "upper_bound": 1237008.3389175255
          },
          "point_estimate": 1235034.7222510823,
          "standard_error": 1013.7424588509172
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1837.8184427872152,
            "upper_bound": 4988.9452371187845
          },
          "point_estimate": 3682.653662017441,
          "standard_error": 894.9971997531394
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1202147.7862724017,
            "upper_bound": 1206333.6700358423
          },
          "point_estimate": 1204128.4758474142,
          "standard_error": 1066.3592771879005
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1201514.275921659,
            "upper_bound": 1205460.6178315412
          },
          "point_estimate": 1204310.069892473,
          "standard_error": 964.2074094250798
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355.303888047073,
            "upper_bound": 5806.873012713838
          },
          "point_estimate": 2313.8895277375264,
          "standard_error": 1378.1626897397985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1201689.2919997445,
            "upper_bound": 1206240.1919263992
          },
          "point_estimate": 1203683.5738583996,
          "standard_error": 1164.1251046469044
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1531.8555713778417,
            "upper_bound": 4899.4623827819705
          },
          "point_estimate": 3566.483952759763,
          "standard_error": 920.148443179016
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1275352.1993322386,
            "upper_bound": 1276218.5681264368
          },
          "point_estimate": 1275732.4357033388,
          "standard_error": 220.53730294847657
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1275316.7955665025,
            "upper_bound": 1276264.2931034483
          },
          "point_estimate": 1275463.6180076627,
          "standard_error": 195.332080469254
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.126317855972072,
            "upper_bound": 1083.5837528317238
          },
          "point_estimate": 205.53266393724573,
          "standard_error": 225.63855501334024
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1275288.5015673982,
            "upper_bound": 1275603.5398515488
          },
          "point_estimate": 1275457.8802507836,
          "standard_error": 80.52341048352662
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146.06984120366525,
            "upper_bound": 924.815624817536
          },
          "point_estimate": 733.457966003282,
          "standard_error": 207.0083026653444
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1093335.3059640524,
            "upper_bound": 1094132.6851447246
          },
          "point_estimate": 1093763.0822525676,
          "standard_error": 205.16422880025908
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1093409.2491830066,
            "upper_bound": 1094312.6029411764
          },
          "point_estimate": 1093877.8858543418,
          "standard_error": 237.93085537115527
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131.7854530523778,
            "upper_bound": 1104.0258225565765
          },
          "point_estimate": 610.8212394009286,
          "standard_error": 242.9673032793829
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1093439.5433861366,
            "upper_bound": 1094018.4616658208
          },
          "point_estimate": 1093647.8179526357,
          "standard_error": 147.62382830544237
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318.4058281601287,
            "upper_bound": 932.1634814698928
          },
          "point_estimate": 683.7253059313604,
          "standard_error": 171.85686948678693
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 348836.3220047314,
            "upper_bound": 349502.83175366296
          },
          "point_estimate": 349110.95852983824,
          "standard_error": 177.99217343993337
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 348801.26923076925,
            "upper_bound": 349264.8214285714
          },
          "point_estimate": 348884.31176549144,
          "standard_error": 117.07686582631597
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.6431025900688,
            "upper_bound": 564.052962221697
          },
          "point_estimate": 217.1535466254975,
          "standard_error": 145.87942000030523
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 348848.72760322713,
            "upper_bound": 349048.9611271082
          },
          "point_estimate": 348954.5516233766,
          "standard_error": 52.360866480450305
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122.93848516563982,
            "upper_bound": 879.6921886403678
          },
          "point_estimate": 594.2080339183697,
          "standard_error": 231.69192942016113
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389955.640570331,
            "upper_bound": 390401.56913711585
          },
          "point_estimate": 390140.49187901046,
          "standard_error": 117.4643106855227
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389958.1893617021,
            "upper_bound": 390163.69215425535
          },
          "point_estimate": 390086.7674772036,
          "standard_error": 56.852214943480256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.24970773441922,
            "upper_bound": 356.6415266470731
          },
          "point_estimate": 145.67990539240492,
          "standard_error": 82.42121263362198
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389849.4984114104,
            "upper_bound": 390115.7267445364
          },
          "point_estimate": 390008.4618679193,
          "standard_error": 74.11941700949998
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.7123824194929,
            "upper_bound": 585.9099049161564
          },
          "point_estimate": 392.0657157417869,
          "standard_error": 158.0264313276518
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 893282.7552831735,
            "upper_bound": 896714.8917110192
          },
          "point_estimate": 894891.4424651569,
          "standard_error": 880.890869679773
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 892389.2996515679,
            "upper_bound": 897561.2357723578
          },
          "point_estimate": 894010.3146341464,
          "standard_error": 1128.023676639424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 496.075884851442,
            "upper_bound": 5262.214644137974
          },
          "point_estimate": 2602.412210871171,
          "standard_error": 1137.1466251713714
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 892310.5980080572,
            "upper_bound": 894616.8430343019
          },
          "point_estimate": 893086.5808045613,
          "standard_error": 596.4432468758239
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1286.3425632234455,
            "upper_bound": 3653.8062340847177
          },
          "point_estimate": 2936.037141746366,
          "standard_error": 601.8703401492294
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697665.7467767296,
            "upper_bound": 698059.886199461
          },
          "point_estimate": 697860.9839061096,
          "standard_error": 101.04728311963224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697649.5141509434,
            "upper_bound": 698153.7439353099
          },
          "point_estimate": 697812.6886792453,
          "standard_error": 122.01976948109092
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.25793273263903,
            "upper_bound": 593.9176606823161
          },
          "point_estimate": 285.9378208670237,
          "standard_error": 131.75515563941798
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697785.2611590986,
            "upper_bound": 698016.407917583
          },
          "point_estimate": 697893.1152168586,
          "standard_error": 58.40334698558774
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.80893439584565,
            "upper_bound": 441.1617480092904
          },
          "point_estimate": 337.3015090917176,
          "standard_error": 69.2602060043003
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 720258.4462044819,
            "upper_bound": 720977.8691726191
          },
          "point_estimate": 720590.7493744164,
          "standard_error": 184.73021646682625
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 720064.725490196,
            "upper_bound": 720934.9725490196
          },
          "point_estimate": 720492.4947478992,
          "standard_error": 214.0132873904068
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.755923591727296,
            "upper_bound": 987.0409324765204
          },
          "point_estimate": 617.759679228707,
          "standard_error": 237.6118593677453
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 720231.8182863243,
            "upper_bound": 720748.9810477756
          },
          "point_estimate": 720422.7418894831,
          "standard_error": 132.16684839196034
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 250.75220812848784,
            "upper_bound": 802.4547559796281
          },
          "point_estimate": 614.8151198793751,
          "standard_error": 142.43883912556868
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 310825.5074480651,
            "upper_bound": 311519.7484534663
          },
          "point_estimate": 311171.0391761634,
          "standard_error": 177.15706731507586
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 310740.0752136752,
            "upper_bound": 311722.2853751187
          },
          "point_estimate": 311025.7795940171,
          "standard_error": 291.13468815210354
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.2543645642319,
            "upper_bound": 1024.4763178162275
          },
          "point_estimate": 683.180800691601,
          "standard_error": 239.70347935427884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 310936.277339693,
            "upper_bound": 311571.6042016136
          },
          "point_estimate": 311256.51512931514,
          "standard_error": 163.04929858230682
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 360.2977900021993,
            "upper_bound": 729.4240391761977
          },
          "point_estimate": 590.6436939703973,
          "standard_error": 94.79037160826164
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 526340.5016241086,
            "upper_bound": 529730.7535829308
          },
          "point_estimate": 528003.2075994939,
          "standard_error": 860.641699104316
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 525650.9806763285,
            "upper_bound": 531075.304347826
          },
          "point_estimate": 526089.3859098229,
          "standard_error": 1875.83440318337
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.37866482010309,
            "upper_bound": 4077.020238487987
          },
          "point_estimate": 662.7303055288363,
          "standard_error": 1417.251316177968
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 525876.0225878052,
            "upper_bound": 527191.6706353835
          },
          "point_estimate": 526272.0228872576,
          "standard_error": 346.9840210010915
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1692.3454697215636,
            "upper_bound": 3171.368605649239
          },
          "point_estimate": 2867.874703646624,
          "standard_error": 377.58240786364195
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171515.40018386397,
            "upper_bound": 171645.56533559354
          },
          "point_estimate": 171572.57040749473,
          "standard_error": 33.69054782560539
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171497.76367924528,
            "upper_bound": 171645.63443396226
          },
          "point_estimate": 171522.59470649896,
          "standard_error": 36.48965825364785
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.31053426675512,
            "upper_bound": 170.65557134132544
          },
          "point_estimate": 40.15522567076964,
          "standard_error": 40.856765820605425
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171506.43795735927,
            "upper_bound": 171587.55050117924
          },
          "point_estimate": 171534.07006861063,
          "standard_error": 21.941917837528543
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.15817260104277,
            "upper_bound": 155.2696357134069
          },
          "point_estimate": 112.1116285704989,
          "standard_error": 33.331087434524086
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/stud_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146849.79079639495,
            "upper_bound": 146950.9030698285
          },
          "point_estimate": 146894.91483070917,
          "standard_error": 26.078924583339045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146833.05913978495,
            "upper_bound": 146932.05421146954
          },
          "point_estimate": 146880.93384216592,
          "standard_error": 24.283662745690215
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.081803744344636,
            "upper_bound": 117.8420377264354
          },
          "point_estimate": 73.38504533694336,
          "standard_error": 26.8112258899985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146864.53542282482,
            "upper_bound": 146911.46238260515
          },
          "point_estimate": 146889.59855467113,
          "standard_error": 11.898400992348131
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.97736149793394,
            "upper_bound": 122.40996735109388
          },
          "point_estimate": 87.00845089689119,
          "standard_error": 25.770751718671985
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/stud_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 442295.78549311525,
            "upper_bound": 442702.18037483265
          },
          "point_estimate": 442443.12718110543,
          "standard_error": 118.75331071710173
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 442283.0421686747,
            "upper_bound": 442387.1268406961
          },
          "point_estimate": 442323.81686746987,
          "standard_error": 38.139548575418445
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.544418228746224,
            "upper_bound": 122.90162613133596
          },
          "point_estimate": 49.935676824288585,
          "standard_error": 44.063337847314834
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 442300.27734124905,
            "upper_bound": 442387.8815366431
          },
          "point_estimate": 442344.034579878,
          "standard_error": 22.481906606267568
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.92151439295984,
            "upper_bound": 609.322361760873
          },
          "point_estimate": 396.52004196279546,
          "standard_error": 204.59457205514016
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/stud_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 927.5505158193484,
            "upper_bound": 928.0630273882332
          },
          "point_estimate": 927.7619120809328,
          "standard_error": 0.1361921901334032
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 927.5478641965858,
            "upper_bound": 927.8806573111098
          },
          "point_estimate": 927.6046696777156,
          "standard_error": 0.07424814253308071
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019908482639198855,
            "upper_bound": 0.42994100336499547
          },
          "point_estimate": 0.09988122016824608,
          "standard_error": 0.11113748025945792
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 927.586742318798,
            "upper_bound": 927.831419110264
          },
          "point_estimate": 927.6894603011116,
          "standard_error": 0.06351081869811731
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06143816208814878,
            "upper_bound": 0.6647399725540489
          },
          "point_estimate": 0.45320417182303974,
          "standard_error": 0.17487137435212596
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1381266.9970228178,
            "upper_bound": 1386430.6616275725
          },
          "point_estimate": 1383602.0093841858,
          "standard_error": 1329.2116261007195
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1380392.0833333335,
            "upper_bound": 1386472.619341564
          },
          "point_estimate": 1381612.3998015872,
          "standard_error": 1751.9895762242406
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.54519353956177,
            "upper_bound": 7131.826003631655
          },
          "point_estimate": 2377.7853069524263,
          "standard_error": 1811.281827807043
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1380794.5597720798,
            "upper_bound": 1384078.965568278
          },
          "point_estimate": 1382281.8358826358,
          "standard_error": 864.2031556945514
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1711.3026406794677,
            "upper_bound": 6038.2804827760765
          },
          "point_estimate": 4442.5043716542405,
          "standard_error": 1179.8388022711313
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1515164.8541507933,
            "upper_bound": 1518419.7928472222
          },
          "point_estimate": 1516779.1622718254,
          "standard_error": 833.2105696121967
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1514658.0395833333,
            "upper_bound": 1518489.3370535714
          },
          "point_estimate": 1516775.8819444445,
          "standard_error": 888.3466720226879
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.0740461523526,
            "upper_bound": 4482.5699791685665
          },
          "point_estimate": 1773.8907147571215,
          "standard_error": 1134.3701981018926
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1515559.555406613,
            "upper_bound": 1517641.5343333532
          },
          "point_estimate": 1516693.1906926406,
          "standard_error": 537.1847950941523
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1353.8073769859247,
            "upper_bound": 3636.032040957633
          },
          "point_estimate": 2769.740527404574,
          "standard_error": 577.3687739881544
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1410978.5375627668,
            "upper_bound": 1415999.4596314102
          },
          "point_estimate": 1413333.6117506102,
          "standard_error": 1293.387685873177
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1410260.1367521368,
            "upper_bound": 1417674.3846153845
          },
          "point_estimate": 1411336.8838141025,
          "standard_error": 1833.6931896331373
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 261.2892376688058,
            "upper_bound": 6546.877386654326
          },
          "point_estimate": 3173.16808340867,
          "standard_error": 1805.6775596896512
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1410125.999037444,
            "upper_bound": 1412961.6396761134
          },
          "point_estimate": 1411223.6023976023,
          "standard_error": 722.3091750520816
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2001.6663446076984,
            "upper_bound": 5459.84062625003
          },
          "point_estimate": 4316.199807057751,
          "standard_error": 879.690587421584
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 869235.4443310658,
            "upper_bound": 870829.6915684524
          },
          "point_estimate": 870084.4344501134,
          "standard_error": 407.6726826985027
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 868677.6714285715,
            "upper_bound": 870948.2619047619
          },
          "point_estimate": 870665.3117346938,
          "standard_error": 563.8715930051444
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119.81878787276828,
            "upper_bound": 2095.212047802549
          },
          "point_estimate": 472.9064012471253,
          "standard_error": 552.1342148125448
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 868500.1723224552,
            "upper_bound": 870698.6262012067
          },
          "point_estimate": 869571.2239950525,
          "standard_error": 559.1028503943053
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 439.6274387572775,
            "upper_bound": 1629.2958044888603
          },
          "point_estimate": 1356.3138093150865,
          "standard_error": 262.4289344881497
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308756.1782946429,
            "upper_bound": 309581.04471963283
          },
          "point_estimate": 309140.6030488297,
          "standard_error": 211.39907422975432
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308606.993220339,
            "upper_bound": 309634.89491525426
          },
          "point_estimate": 308982.9168684423,
          "standard_error": 269.72656885077794
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175.24415451594905,
            "upper_bound": 1162.2348293661928
          },
          "point_estimate": 636.2961898899658,
          "standard_error": 253.44475800884263
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308672.02219879575,
            "upper_bound": 309280.8552585139
          },
          "point_estimate": 308894.24125027517,
          "standard_error": 154.87434374079916
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325.94520526005635,
            "upper_bound": 915.1218457769912
          },
          "point_estimate": 705.9222002338604,
          "standard_error": 153.11191072031258
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 501986.75236681895,
            "upper_bound": 503623.8874096271
          },
          "point_estimate": 502661.5340791476,
          "standard_error": 435.1375089336063
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 501870.8892694064,
            "upper_bound": 503014.1232876713
          },
          "point_estimate": 502260.5143835617,
          "standard_error": 285.86049915549137
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.28764590364818,
            "upper_bound": 1344.7963116273884
          },
          "point_estimate": 740.4818489086312,
          "standard_error": 338.44903083978716
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 502038.075077331,
            "upper_bound": 502744.14673451666
          },
          "point_estimate": 502352.9244974204,
          "standard_error": 181.23762855813328
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 360.2045496120576,
            "upper_bound": 2158.258655554702
          },
          "point_estimate": 1450.4780504528417,
          "standard_error": 567.5572373320845
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277161.08520356234,
            "upper_bound": 278214.0591605476
          },
          "point_estimate": 277654.13258148555,
          "standard_error": 269.386462933363
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 276914.9306615776,
            "upper_bound": 278066.606870229
          },
          "point_estimate": 277672.5959651036,
          "standard_error": 283.17249130734757
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.52023771829488,
            "upper_bound": 1475.7453066247188
          },
          "point_estimate": 734.0798794611034,
          "standard_error": 329.89768236423015
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277222.6970486453,
            "upper_bound": 277828.4339052348
          },
          "point_estimate": 277519.893288391,
          "standard_error": 155.00659257380087
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 416.10324961201223,
            "upper_bound": 1226.464165174985
          },
          "point_estimate": 900.1811995973604,
          "standard_error": 225.0218465313782
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 646314.7459978592,
            "upper_bound": 647414.2219014203
          },
          "point_estimate": 646818.2845342524,
          "standard_error": 282.9551267403823
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 646149.0035087719,
            "upper_bound": 647645.2339181287
          },
          "point_estimate": 646353.5237573099,
          "standard_error": 356.60048979390143
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.8998316527263,
            "upper_bound": 1497.670147226819
          },
          "point_estimate": 318.0373106255779,
          "standard_error": 373.3221191723983
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 646193.7667482026,
            "upper_bound": 646493.5713656
          },
          "point_estimate": 646288.2283891547,
          "standard_error": 78.95333600540414
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159.66512427156613,
            "upper_bound": 1230.6976353726366
          },
          "point_estimate": 942.9796826546088,
          "standard_error": 236.62260791133627
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1743.0289440490462,
            "upper_bound": 1745.241199946709
          },
          "point_estimate": 1743.9261453599024,
          "standard_error": 0.5899297006856151
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1742.864204436451,
            "upper_bound": 1744.2408046895816
          },
          "point_estimate": 1743.2000711430856,
          "standard_error": 0.4282541893183686
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028167621802112575,
            "upper_bound": 1.8317425889029748
          },
          "point_estimate": 0.6603212803633116,
          "standard_error": 0.4592180771545915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1742.91912017849,
            "upper_bound": 1743.8600568283396
          },
          "point_estimate": 1743.3386816157465,
          "standard_error": 0.2492429272847583
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4501230079104145,
            "upper_bound": 2.945755633449975
          },
          "point_estimate": 1.9684795920281255,
          "standard_error": 0.7932567187979147
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 503.7448219405293,
            "upper_bound": 505.40356918226934
          },
          "point_estimate": 504.512994821005,
          "standard_error": 0.4260825211464272
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 503.5265699136101,
            "upper_bound": 505.6687259387018
          },
          "point_estimate": 503.8400132791479,
          "standard_error": 0.5296489450876465
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13872930757902338,
            "upper_bound": 2.2967614815862474
          },
          "point_estimate": 0.7252308566473501,
          "standard_error": 0.5300396496537867
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 503.543938697151,
            "upper_bound": 505.7434780281829
          },
          "point_estimate": 504.368833667413,
          "standard_error": 0.5660799207048023
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4215111778768379,
            "upper_bound": 1.7501563917017904
          },
          "point_estimate": 1.4237066978763335,
          "standard_error": 0.32654403455540726
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.63939834748116,
            "upper_bound": 99.68977823753738
          },
          "point_estimate": 99.66096683664384,
          "standard_error": 0.013142185846542654
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.63932917724864,
            "upper_bound": 99.67822612534373
          },
          "point_estimate": 99.64317579297564,
          "standard_error": 0.009219791750054836
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014930646270406018,
            "upper_bound": 0.05173274302893601
          },
          "point_estimate": 0.007396779587964454,
          "standard_error": 0.01260627274176718
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.63559791778408,
            "upper_bound": 99.65960782502457
          },
          "point_estimate": 99.64439344856542,
          "standard_error": 0.006117739999667441
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007021826708883904,
            "upper_bound": 0.06152350425536018
          },
          "point_estimate": 0.0438767867483175,
          "standard_error": 0.014933159896518677
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.63976507404826,
            "upper_bound": 38.795539911319835
          },
          "point_estimate": 38.72217845187641,
          "standard_error": 0.039337124231855536
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.55618555712523,
            "upper_bound": 38.80971613807516
          },
          "point_estimate": 38.78931135217372,
          "standard_error": 0.06186840951539017
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006482787332890387,
            "upper_bound": 0.20613832646681837
          },
          "point_estimate": 0.032046480783514034,
          "standard_error": 0.05343491614467073
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.72420327978926,
            "upper_bound": 38.80210459589545
          },
          "point_estimate": 38.77989817734188,
          "standard_error": 0.02049142303914484
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02404044230686874,
            "upper_bound": 0.149890410146825
          },
          "point_estimate": 0.1313265660006378,
          "standard_error": 0.02525738317826999
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.37890887851226,
            "upper_bound": 62.59314379422189
          },
          "point_estimate": 62.4934207127965,
          "standard_error": 0.054959894086743545
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.365241866069894,
            "upper_bound": 62.60837054570494
          },
          "point_estimate": 62.58870153593084,
          "standard_error": 0.07178781828007609
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007924597673045397,
            "upper_bound": 0.2871443173328893
          },
          "point_estimate": 0.03987999958463567,
          "standard_error": 0.07036133165002288
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.5444061825172,
            "upper_bound": 62.61175395342245
          },
          "point_estimate": 62.590779390690216,
          "standard_error": 0.017788211783242778
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03144966301336038,
            "upper_bound": 0.22123793138279527
          },
          "point_estimate": 0.182509615110542,
          "standard_error": 0.041918816738488754
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316337.3309634058,
            "upper_bound": 317609.8947128882
          },
          "point_estimate": 317004.9005817806,
          "standard_error": 323.68772217081107
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316593.8420289855,
            "upper_bound": 317527.5206521739
          },
          "point_estimate": 317170.4280538302,
          "standard_error": 204.255073154262
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.45802842738023,
            "upper_bound": 1628.1907385721331
          },
          "point_estimate": 451.27304329261017,
          "standard_error": 434.2279522551453
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 315513.32806792686,
            "upper_bound": 317191.632582368
          },
          "point_estimate": 316472.2445623941,
          "standard_error": 505.89265940578775
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 345.2648830931512,
            "upper_bound": 1520.9808879726647
          },
          "point_estimate": 1084.3442431653905,
          "standard_error": 304.1666376429515
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.40025781736768,
            "upper_bound": 46.43487201884917
          },
          "point_estimate": 46.41627322806924,
          "standard_error": 0.008882900767104566
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.39462168429649,
            "upper_bound": 46.436590420573346
          },
          "point_estimate": 46.40826354594411,
          "standard_error": 0.011948154541069789
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005349813231782084,
            "upper_bound": 0.05152322083582772
          },
          "point_estimate": 0.0277876773778366,
          "standard_error": 0.010572245916449594
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.39680102061681,
            "upper_bound": 46.417877274290106
          },
          "point_estimate": 46.40671884324378,
          "standard_error": 0.005338550890664788
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014783907804683045,
            "upper_bound": 0.03934325397772525
          },
          "point_estimate": 0.02957308053323723,
          "standard_error": 0.00687615782676952
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.05085047410759,
            "upper_bound": 78.13534275130549
          },
          "point_estimate": 78.08393327213011,
          "standard_error": 0.023069222431410658
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.0425085177203,
            "upper_bound": 78.08195991485962
          },
          "point_estimate": 78.07231956710999,
          "standard_error": 0.01314059481029791
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0026536564359350614,
            "upper_bound": 0.05132354270286575
          },
          "point_estimate": 0.03202578537982793,
          "standard_error": 0.014012762727614378
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.05598410547495,
            "upper_bound": 78.08002015398034
          },
          "point_estimate": 78.07066115651763,
          "standard_error": 0.006133104030778195
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015550489934745951,
            "upper_bound": 0.11744697849464374
          },
          "point_estimate": 0.0771265791304702,
          "standard_error": 0.03454407096150792
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141116.54502327295,
            "upper_bound": 141420.80844755186
          },
          "point_estimate": 141261.37930964734,
          "standard_error": 78.17616953531373
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141053.57155209684,
            "upper_bound": 141448.85427089123
          },
          "point_estimate": 141219.0644455253,
          "standard_error": 100.72464599940292
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.0907310124413,
            "upper_bound": 441.35948397366104
          },
          "point_estimate": 246.69961990685388,
          "standard_error": 91.23108483384718
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141053.54011264432,
            "upper_bound": 141409.96122753792
          },
          "point_estimate": 141184.76019000454,
          "standard_error": 91.41341420809192
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134.35368121411523,
            "upper_bound": 339.93720795900924
          },
          "point_estimate": 260.53888715444316,
          "standard_error": 54.59320712863005
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.22321617781574,
            "upper_bound": 51.248710807168194
          },
          "point_estimate": 51.234089500562746,
          "standard_error": 0.006665114291790666
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.21884087300643,
            "upper_bound": 51.23930119275647
          },
          "point_estimate": 51.2292768480247,
          "standard_error": 0.005256488124261075
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003970306278187645,
            "upper_bound": 0.024380301266123654
          },
          "point_estimate": 0.012667573513391955,
          "standard_error": 0.0054136393133361995
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.22313886150774,
            "upper_bound": 51.23469738280634
          },
          "point_estimate": 51.22885673350658,
          "standard_error": 0.003026622312167906
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006924377667322476,
            "upper_bound": 0.03249059219622229
          },
          "point_estimate": 0.02220424471268011,
          "standard_error": 0.007922960702251304
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.02682611341173,
            "upper_bound": 73.3518854407508
          },
          "point_estimate": 73.18775203786896,
          "standard_error": 0.08271074656530934
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.88928714906056,
            "upper_bound": 73.39812276654584
          },
          "point_estimate": 73.26152747860598,
          "standard_error": 0.17673119859794917
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01090281775230402,
            "upper_bound": 0.4879312445549148
          },
          "point_estimate": 0.3456675938515978,
          "standard_error": 0.137228247871003
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.07661664454933,
            "upper_bound": 73.36896097286446
          },
          "point_estimate": 73.2772556397532,
          "standard_error": 0.07544544322155806
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.18215212328510513,
            "upper_bound": 0.3217500091064525
          },
          "point_estimate": 0.2760601137318635,
          "standard_error": 0.03497633689922804
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166.90721842278538,
            "upper_bound": 169.34129315203987
          },
          "point_estimate": 168.1363057304064,
          "standard_error": 0.6258548733837517
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166.18936516054652,
            "upper_bound": 169.7743338570632
          },
          "point_estimate": 168.69473344299408,
          "standard_error": 1.1391648242135248
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09828775381909945,
            "upper_bound": 3.6191517542725142
          },
          "point_estimate": 2.7069320470615232,
          "standard_error": 0.8930138716783496
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165.99238607354613,
            "upper_bound": 168.36818957016789
          },
          "point_estimate": 166.80407262262648,
          "standard_error": 0.605076656742663
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3171660222185986,
            "upper_bound": 2.520476800285924
          },
          "point_estimate": 2.0862940464403557,
          "standard_error": 0.30922563237765927
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 416.2632921417184,
            "upper_bound": 417.50834594479136
          },
          "point_estimate": 416.8737832144085,
          "standard_error": 0.3199405034502128
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415.9240340627029,
            "upper_bound": 417.88559342421814
          },
          "point_estimate": 416.5808948619545,
          "standard_error": 0.6657758543570858
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0924684474361646,
            "upper_bound": 1.6640023393433123
          },
          "point_estimate": 1.2249395499393814,
          "standard_error": 0.4606066672186645
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415.95658412904726,
            "upper_bound": 416.80654502891247
          },
          "point_estimate": 416.2215192826419,
          "standard_error": 0.22221962546190632
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6930515920563225,
            "upper_bound": 1.2330402550221498
          },
          "point_estimate": 1.0710751813528272,
          "standard_error": 0.13547203178691594
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.81186281374696,
            "upper_bound": 22.831071363219536
          },
          "point_estimate": 22.81970564641833,
          "standard_error": 0.0051180653876582065
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.810055258571666,
            "upper_bound": 22.822411097901316
          },
          "point_estimate": 22.815509587906657,
          "standard_error": 0.0031969617408730343
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0026229866028241755,
            "upper_bound": 0.015111540541092466
          },
          "point_estimate": 0.007518326162765656,
          "standard_error": 0.003467043923641984
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.810692442089465,
            "upper_bound": 22.817963999986738
          },
          "point_estimate": 22.81483146891347,
          "standard_error": 0.0018296591589816555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004209791080766565,
            "upper_bound": 0.025527247304952948
          },
          "point_estimate": 0.017044648286889105,
          "standard_error": 0.006838942911897192
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/twoway_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40857.40815619415,
            "upper_bound": 40937.58756756757
          },
          "point_estimate": 40894.31203824361,
          "standard_error": 20.6133948119097
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40852.72809684685,
            "upper_bound": 40921.92445570571
          },
          "point_estimate": 40885.587998713,
          "standard_error": 12.82723871279004
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.298625311334408,
            "upper_bound": 104.36200338369764
          },
          "point_estimate": 18.39899007781935,
          "standard_error": 29.80581184397439
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40847.171076339495,
            "upper_bound": 40909.13333333333
          },
          "point_estimate": 40874.951395226395,
          "standard_error": 16.141374996319506
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.929254531280517,
            "upper_bound": 96.51068017870828
          },
          "point_estimate": 68.72186302425432,
          "standard_error": 19.72372099426328
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/twoway_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.30014375498294,
            "upper_bound": 99.48395324364176
          },
          "point_estimate": 99.39581838614792,
          "standard_error": 0.04663937793662928
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.3363589544853,
            "upper_bound": 99.4469298269596
          },
          "point_estimate": 99.4195473774191,
          "standard_error": 0.024957235163381723
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001098118617246548,
            "upper_bound": 0.2184709522361092
          },
          "point_estimate": 0.04914541880151416,
          "standard_error": 0.05526697077268483
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.24341060067292,
            "upper_bound": 99.43213550211328
          },
          "point_estimate": 99.35430784289582,
          "standard_error": 0.04931024026489392
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0312293390319917,
            "upper_bound": 0.22119852249612657
          },
          "point_estimate": 0.1554266981346358,
          "standard_error": 0.04766013385224665
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/twoway_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1343883.9993452383,
            "upper_bound": 1346426.853477891
          },
          "point_estimate": 1345093.6424659863,
          "standard_error": 651.252520821868
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1343529.519770408,
            "upper_bound": 1346389.7142857143
          },
          "point_estimate": 1345121.8303571427,
          "standard_error": 673.5153209320553
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287.2294761507232,
            "upper_bound": 3546.7718495322783
          },
          "point_estimate": 1883.5770790599704,
          "standard_error": 804.2228386082467
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1343563.933333333,
            "upper_bound": 1345269.8171704223
          },
          "point_estimate": 1344309.6525046383,
          "standard_error": 431.7770999373872
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1003.5219422409951,
            "upper_bound": 2946.8654031371393
          },
          "point_estimate": 2169.1929004189155,
          "standard_error": 526.2070677886534
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/twoway_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2763.7719054696036,
            "upper_bound": 2769.5770468954715
          },
          "point_estimate": 2766.7403507554236,
          "standard_error": 1.484343487785189
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2762.3541031552477,
            "upper_bound": 2770.0975687828673
          },
          "point_estimate": 2768.243401163529,
          "standard_error": 2.5067467748277648
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1096516823763988,
            "upper_bound": 7.8576131937060785
          },
          "point_estimate": 4.816590550682133,
          "standard_error": 2.3121550003558045
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2761.0297170320796,
            "upper_bound": 2768.42658528268
          },
          "point_estimate": 2764.1329038311737,
          "standard_error": 1.901779739182754
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8612942977398705,
            "upper_bound": 6.14141992558775
          },
          "point_estimate": 4.9531688844785045,
          "standard_error": 0.837320287467973
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.03262903207181,
            "upper_bound": 29.15979192641054
          },
          "point_estimate": 29.09425196374404,
          "standard_error": 0.03229048370295566
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.01083533630673,
            "upper_bound": 29.202068685920917
          },
          "point_estimate": 29.023949161964502,
          "standard_error": 0.0643184128763575
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004239359024795861,
            "upper_bound": 0.15172475212955178
          },
          "point_estimate": 0.03294427080945393,
          "standard_error": 0.05153551966545789
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.015889434973587,
            "upper_bound": 29.062207317866676
          },
          "point_estimate": 29.029708937737755,
          "standard_error": 0.012228034864878516
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06026723988372123,
            "upper_bound": 0.12762082048434528
          },
          "point_estimate": 0.10778267415199518,
          "standard_error": 0.01686291927714748
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.77368741265171,
            "upper_bound": 31.908086026176317
          },
          "point_estimate": 31.83474851625588,
          "standard_error": 0.034557676684838695
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.754361514948975,
            "upper_bound": 31.9231954294289
          },
          "point_estimate": 31.78111042716256,
          "standard_error": 0.04470769909189679
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010547425597722948,
            "upper_bound": 0.1922753301874741
          },
          "point_estimate": 0.051693747843878306,
          "standard_error": 0.04614267109895303
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.75487813457089,
            "upper_bound": 31.873657747756017
          },
          "point_estimate": 31.798633338819485,
          "standard_error": 0.030582328864033018
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03161474501481538,
            "upper_bound": 0.15026536088566286
          },
          "point_estimate": 0.11528589226791712,
          "standard_error": 0.02801524715371379
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.25809684142964,
            "upper_bound": 24.31611116011617
          },
          "point_estimate": 24.29002578043592,
          "standard_error": 0.01465932092954482
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.282852619313243,
            "upper_bound": 24.305901394854803
          },
          "point_estimate": 24.29513735890567,
          "standard_error": 0.005155873431398003
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00027219136461295476,
            "upper_bound": 0.05709026792131199
          },
          "point_estimate": 0.005506576348700961,
          "standard_error": 0.015066333226171614
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.228910047168583,
            "upper_bound": 24.29893033544181
          },
          "point_estimate": 24.273261958392595,
          "standard_error": 0.019528930256049327
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007938913528744205,
            "upper_bound": 0.07218500046390969
          },
          "point_estimate": 0.04889683771045524,
          "standard_error": 0.0174119742976297
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.66818169737798,
            "upper_bound": 33.67837626218892
          },
          "point_estimate": 33.67315095471522,
          "standard_error": 0.002615308112844428
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.6646251844141,
            "upper_bound": 33.68012534602244
          },
          "point_estimate": 33.67273396396088,
          "standard_error": 0.003894432701630107
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001624569347445316,
            "upper_bound": 0.01535255302345138
          },
          "point_estimate": 0.009788264884278116,
          "standard_error": 0.003516785313139545
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.66911435014252,
            "upper_bound": 33.68086355801003
          },
          "point_estimate": 33.674799321024864,
          "standard_error": 0.003036434240378946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005007573215921234,
            "upper_bound": 0.010727753654541311
          },
          "point_estimate": 0.008696351394813236,
          "standard_error": 0.0014731152528442825
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.44733053916751,
            "upper_bound": 35.56730981175604
          },
          "point_estimate": 35.50832632274449,
          "standard_error": 0.030732683395110004
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.4103170475381,
            "upper_bound": 35.58238219830859
          },
          "point_estimate": 35.52427932209145,
          "standard_error": 0.04655735059643679
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015055159004600964,
            "upper_bound": 0.17076036912918058
          },
          "point_estimate": 0.09333932144755624,
          "standard_error": 0.039794712881628544
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.456700925725315,
            "upper_bound": 35.56303100049031
          },
          "point_estimate": 35.52654274693816,
          "standard_error": 0.026934646308564777
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05927270785803607,
            "upper_bound": 0.1275112697809082
          },
          "point_estimate": 0.10241634470865651,
          "standard_error": 0.017216260948545416
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.73608520821816,
            "upper_bound": 55.796569310511906
          },
          "point_estimate": 55.7729459816828,
          "standard_error": 0.016595805202558135
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.77132355199312,
            "upper_bound": 55.800773268431506
          },
          "point_estimate": 55.79130431518675,
          "standard_error": 0.0089754908307182
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004153620250555151,
            "upper_bound": 0.03632294808371884
          },
          "point_estimate": 0.017451313875423553,
          "standard_error": 0.00901822409469743
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.78176240036835,
            "upper_bound": 55.79974808373858
          },
          "point_estimate": 55.79174292478604,
          "standard_error": 0.004516790177449895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009740435296026804,
            "upper_bound": 0.08416869887340284
          },
          "point_estimate": 0.05529659949532563,
          "standard_error": 0.024788006894096628
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.55641394039509,
            "upper_bound": 58.65290207836311
          },
          "point_estimate": 58.599122168182944,
          "standard_error": 0.024862825497581977
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.540933651428205,
            "upper_bound": 58.6575121703527
          },
          "point_estimate": 58.57035378587139,
          "standard_error": 0.021833900885574815
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0070137002537562585,
            "upper_bound": 0.10798715126809222
          },
          "point_estimate": 0.03353439623874096,
          "standard_error": 0.02382266551495832
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.5517910709842,
            "upper_bound": 58.582697896263575
          },
          "point_estimate": 58.56616854227081,
          "standard_error": 0.008146236207794524
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017616133670779955,
            "upper_bound": 0.10729458164562254
          },
          "point_estimate": 0.08305231387027132,
          "standard_error": 0.024004137146011795
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.47409113888301,
            "upper_bound": 46.6204813724155
          },
          "point_estimate": 46.54268457370178,
          "standard_error": 0.03808529229017751
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.447130504286335,
            "upper_bound": 46.69803855741156
          },
          "point_estimate": 46.47037310100306,
          "standard_error": 0.0657001545797531
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009140327030283136,
            "upper_bound": 0.19060080445868344
          },
          "point_estimate": 0.05196738255240787,
          "standard_error": 0.05345873738022309
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.45433231048313,
            "upper_bound": 46.60583468319911
          },
          "point_estimate": 46.50423569475224,
          "standard_error": 0.03901169883582688
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.049618627436826414,
            "upper_bound": 0.14587681368339084
          },
          "point_estimate": 0.12644837861000804,
          "standard_error": 0.02198110297451564
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.84659326512487,
            "upper_bound": 78.05455104841016
          },
          "point_estimate": 77.96103678964043,
          "standard_error": 0.05371331186894001
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.83880834143818,
            "upper_bound": 78.08130318690229
          },
          "point_estimate": 78.07126703051807,
          "standard_error": 0.07307502999760877
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00480112345508199,
            "upper_bound": 0.3009922632174635
          },
          "point_estimate": 0.028274924648988488,
          "standard_error": 0.0854060449967985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.9070758918573,
            "upper_bound": 78.07635372427609
          },
          "point_estimate": 78.0036353408942,
          "standard_error": 0.048272185695924674
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06427184100342309,
            "upper_bound": 0.24018169363711625
          },
          "point_estimate": 0.17909938062863465,
          "standard_error": 0.04651534195835826
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.708435814685856,
            "upper_bound": 43.72996624191059
          },
          "point_estimate": 43.71836529016888,
          "standard_error": 0.005512701977035492
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.704448939111614,
            "upper_bound": 43.72628976721421
          },
          "point_estimate": 43.71601473956977,
          "standard_error": 0.005416989392309323
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003650073050343835,
            "upper_bound": 0.0273733088726797
          },
          "point_estimate": 0.013358021987407756,
          "standard_error": 0.0066108346809892315
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.70358376042558,
            "upper_bound": 43.71898592632641
          },
          "point_estimate": 43.71037023564282,
          "standard_error": 0.004059270117705145
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00806314069275947,
            "upper_bound": 0.025135060047016485
          },
          "point_estimate": 0.01841203689842985,
          "standard_error": 0.004804215015288508
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.10055725331844,
            "upper_bound": 51.25197561536664
          },
          "point_estimate": 51.18551588809133,
          "standard_error": 0.039447408309693455
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.13999743717048,
            "upper_bound": 51.233611951428855
          },
          "point_estimate": 51.21964733232339,
          "standard_error": 0.020469366610568727
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035303823858603323,
            "upper_bound": 0.20660377235883612
          },
          "point_estimate": 0.01498563723026033,
          "standard_error": 0.048766887403864104
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.0192774669339,
            "upper_bound": 51.22356640422524
          },
          "point_estimate": 51.14256382359181,
          "standard_error": 0.05535131182554393
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01145510210818398,
            "upper_bound": 0.18639751451524547
          },
          "point_estimate": 0.13191689345550783,
          "standard_error": 0.04377562823465302
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.37585311516307,
            "upper_bound": 73.44627085696729
          },
          "point_estimate": 73.40378802778297,
          "standard_error": 0.01903608781319223
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.36562759104856,
            "upper_bound": 73.40514906843853
          },
          "point_estimate": 73.39079532744513,
          "standard_error": 0.010087419069589022
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00625953888144163,
            "upper_bound": 0.047673489572167424
          },
          "point_estimate": 0.02387978346284986,
          "standard_error": 0.011382190388789468
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.37010830511365,
            "upper_bound": 73.40045419767038
          },
          "point_estimate": 73.38412828595098,
          "standard_error": 0.007954512700970341
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01351452241298844,
            "upper_bound": 0.09637337553803782
          },
          "point_estimate": 0.06344244770905974,
          "standard_error": 0.027587599900001127
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1212969.3800133597,
            "upper_bound": 1213540.0682607472
          },
          "point_estimate": 1213241.7788955024,
          "standard_error": 145.4160000299138
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1212759.2388888889,
            "upper_bound": 1213495.4041666668
          },
          "point_estimate": 1213340.81875,
          "standard_error": 228.0789634195305
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.022149520121225,
            "upper_bound": 986.2913958233064
          },
          "point_estimate": 503.0633307910871,
          "standard_error": 243.6175038162742
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1212815.5114833133,
            "upper_bound": 1213278.665245478
          },
          "point_estimate": 1213002.8606926408,
          "standard_error": 118.98023386235312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277.0604188591049,
            "upper_bound": 634.0041825078315
          },
          "point_estimate": 483.6272811527607,
          "standard_error": 101.53859821445327
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1536904.586087963,
            "upper_bound": 1541756.93301918
          },
          "point_estimate": 1539073.95176918,
          "standard_error": 1248.9449150370756
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1536296.3739583334,
            "upper_bound": 1540631.9375
          },
          "point_estimate": 1538250.4351851852,
          "standard_error": 1345.1430370763485
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492.143766262603,
            "upper_bound": 5932.261283292338
          },
          "point_estimate": 3284.7433861286668,
          "standard_error": 1314.7876372299206
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1536707.8720462872,
            "upper_bound": 1539198.8064557614
          },
          "point_estimate": 1537911.1187229438,
          "standard_error": 630.6380099858194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1694.8437924242746,
            "upper_bound": 5872.483809943219
          },
          "point_estimate": 4153.226421332047,
          "standard_error": 1247.1276683801625
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1514476.0015343917,
            "upper_bound": 1515867.3732638888
          },
          "point_estimate": 1515210.557850529,
          "standard_error": 357.46008526536684
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1514229.6964285714,
            "upper_bound": 1516018.2928240742
          },
          "point_estimate": 1515567.8055555555,
          "standard_error": 487.5712851834301
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.75231388752544,
            "upper_bound": 1928.00271660426
          },
          "point_estimate": 917.3124024644494,
          "standard_error": 527.3000104405132
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1514656.9784196548,
            "upper_bound": 1515976.1108264828
          },
          "point_estimate": 1515514.2033549785,
          "standard_error": 345.4485355162317
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 542.8966151604758,
            "upper_bound": 1527.966644436851
          },
          "point_estimate": 1192.4431967212804,
          "standard_error": 251.31008217955804
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 368255.9897979798,
            "upper_bound": 369213.53670983645
          },
          "point_estimate": 368735.20970338304,
          "standard_error": 245.02293880883596
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 368065.0586419753,
            "upper_bound": 369380.9616161616
          },
          "point_estimate": 368701.69276094274,
          "standard_error": 327.60669403858714
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228.5512722050596,
            "upper_bound": 1455.1556504284931
          },
          "point_estimate": 851.8055651804534,
          "standard_error": 325.5985076152488
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 367793.7392715029,
            "upper_bound": 368902.96849987755
          },
          "point_estimate": 368188.20999606454,
          "standard_error": 281.00726968045245
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 475.21337670368104,
            "upper_bound": 1010.9333054068056
          },
          "point_estimate": 816.2585438539226,
          "standard_error": 137.4589465716146
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 584215.815696649,
            "upper_bound": 585181.9971384478
          },
          "point_estimate": 584591.4306834216,
          "standard_error": 264.9688202272731
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 584155.6555555556,
            "upper_bound": 584667.7232142857
          },
          "point_estimate": 584274.1455026455,
          "standard_error": 124.4770417436299
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.62737694998823,
            "upper_bound": 583.5183049181785
          },
          "point_estimate": 184.7752972751382,
          "standard_error": 147.06106483485635
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 584188.566588886,
            "upper_bound": 584405.6437936658
          },
          "point_estimate": 584294.0276231705,
          "standard_error": 56.5523854708152
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.3234790263743,
            "upper_bound": 1332.1794629727283
          },
          "point_estimate": 880.3369372393589,
          "standard_error": 392.0379901764879
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 437056.4360941044,
            "upper_bound": 438475.7722273951
          },
          "point_estimate": 437726.51431972784,
          "standard_error": 364.712008234232
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 437021.3273809524,
            "upper_bound": 438682.47321428574
          },
          "point_estimate": 437446.58551587304,
          "standard_error": 393.11689112244176
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163.73816459307184,
            "upper_bound": 2047.1204002635025
          },
          "point_estimate": 680.4292562533517,
          "standard_error": 520.1672259466453
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 436823.53173168213,
            "upper_bound": 437738.24741771707
          },
          "point_estimate": 437242.6716450217,
          "standard_error": 228.33846511173175
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 520.2079301672575,
            "upper_bound": 1629.8523153407448
          },
          "point_estimate": 1214.7394599030304,
          "standard_error": 291.95783570605835
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 549813.0457853589,
            "upper_bound": 551128.0405047382
          },
          "point_estimate": 550396.002360815,
          "standard_error": 341.24593510943396
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 549676.7440298507,
            "upper_bound": 551020.0321310116
          },
          "point_estimate": 549986.1691542289,
          "standard_error": 289.5082040769963
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.8127092447928,
            "upper_bound": 1488.0777257206073
          },
          "point_estimate": 522.4507914708998,
          "standard_error": 313.6282686038371
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 549871.2221386756,
            "upper_bound": 552113.8515513717
          },
          "point_estimate": 551157.0163209925,
          "standard_error": 597.2271852400444
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233.0087205154519,
            "upper_bound": 1531.3311329806695
          },
          "point_estimate": 1143.149876480558,
          "standard_error": 346.1136175685511
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1040279.4459306974,
            "upper_bound": 1043615.815595238
          },
          "point_estimate": 1041922.8307244896,
          "standard_error": 856.4317552442977
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1039142.6095238096,
            "upper_bound": 1044183.7314285716
          },
          "point_estimate": 1042128.5553571428,
          "standard_error": 1341.825040881835
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 699.0983333028394,
            "upper_bound": 5087.017604687389
          },
          "point_estimate": 3716.091826526103,
          "standard_error": 1168.428637523918
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1040177.2454767366,
            "upper_bound": 1042986.4769332778
          },
          "point_estimate": 1041626.4145454546,
          "standard_error": 715.3005544249831
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1726.7242646673594,
            "upper_bound": 3420.4774392653458
          },
          "point_estimate": 2851.21216498037,
          "standard_error": 430.5103058222345
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 311196.84422295995,
            "upper_bound": 312083.16033564816
          },
          "point_estimate": 311637.82180606434,
          "standard_error": 227.21811071507057
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 310997.4716880342,
            "upper_bound": 312255.6092796093
          },
          "point_estimate": 311679.8931623931,
          "standard_error": 312.0639955315066
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206.85437581479053,
            "upper_bound": 1418.096578990343
          },
          "point_estimate": 837.7319217084703,
          "standard_error": 321.3126804266408
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 311202.54987812025,
            "upper_bound": 311919.4840644078
          },
          "point_estimate": 311588.2152070152,
          "standard_error": 180.22306894629529
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 449.0039379212132,
            "upper_bound": 931.80288490011
          },
          "point_estimate": 759.7511768532561,
          "standard_error": 123.86665478326822
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 848063.7800342378,
            "upper_bound": 850880.661616279
          },
          "point_estimate": 849431.3338907346,
          "standard_error": 723.792960391307
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 847486.476744186,
            "upper_bound": 851188.4127906978
          },
          "point_estimate": 849350.4952934661,
          "standard_error": 938.611683240988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 302.98769345808574,
            "upper_bound": 4220.582855302267
          },
          "point_estimate": 2076.486998948891,
          "standard_error": 963.9553643108078
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 847398.7655117221,
            "upper_bound": 850188.0819947255
          },
          "point_estimate": 848521.0457867713,
          "standard_error": 706.1244704144597
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1317.631368985505,
            "upper_bound": 3096.423867234176
          },
          "point_estimate": 2415.1728636136913,
          "standard_error": 463.5802915871848
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 425961.1333910115,
            "upper_bound": 426944.9418346253
          },
          "point_estimate": 426405.2707442784,
          "standard_error": 252.94254595570365
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 425887.3787375415,
            "upper_bound": 427046.50058139535
          },
          "point_estimate": 426026.95712209307,
          "standard_error": 261.8437278434798
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.67037216544389,
            "upper_bound": 1197.0594554145882
          },
          "point_estimate": 359.84683907654437,
          "standard_error": 284.3679682298147
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 426046.5717402937,
            "upper_bound": 426679.2268230751
          },
          "point_estimate": 426327.88915735425,
          "standard_error": 159.39175228463895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221.5701334150833,
            "upper_bound": 1077.430696350981
          },
          "point_estimate": 842.9262384702888,
          "standard_error": 226.00276637641355
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162967.79791699589,
            "upper_bound": 163311.09440013528
          },
          "point_estimate": 163129.91493291335,
          "standard_error": 88.11006928085983
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162929.3933376041,
            "upper_bound": 163324.07249626308
          },
          "point_estimate": 163045.01601893373,
          "standard_error": 96.94243158734268
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.452829747123506,
            "upper_bound": 473.8735234256323
          },
          "point_estimate": 197.03431254081545,
          "standard_error": 126.71000760582976
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162976.98104793933,
            "upper_bound": 163130.39363355018
          },
          "point_estimate": 163047.14560596354,
          "standard_error": 38.082628714845505
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.01466300726868,
            "upper_bound": 374.33086640214543
          },
          "point_estimate": 294.4546412924455,
          "standard_error": 61.98304992893822
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 249018.1266086106,
            "upper_bound": 249204.55303614232
          },
          "point_estimate": 249094.72070640355,
          "standard_error": 49.544373413095094
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 249010.48630136985,
            "upper_bound": 249122.95547945204
          },
          "point_estimate": 249040.84941291585,
          "standard_error": 31.95007896919057
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.832898016712395,
            "upper_bound": 152.13506688813365
          },
          "point_estimate": 72.66072687783102,
          "standard_error": 37.19331300636278
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 249013.8694551107,
            "upper_bound": 249055.17701936705
          },
          "point_estimate": 249032.89507205124,
          "standard_error": 10.362778653131585
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.214077046986965,
            "upper_bound": 248.0107261968205
          },
          "point_estimate": 165.78294616066069,
          "standard_error": 66.20164886293557
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179508.05461629134,
            "upper_bound": 179631.38949409645
          },
          "point_estimate": 179576.41069395572,
          "standard_error": 31.479445130041007
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179503.09667487687,
            "upper_bound": 179644.8433497537
          },
          "point_estimate": 179623.33372429432,
          "standard_error": 31.88996828741344
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.349565650812656,
            "upper_bound": 143.1255515969752
          },
          "point_estimate": 32.24974323697714,
          "standard_error": 32.73172518223828
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179469.82054341133,
            "upper_bound": 179627.02091053082
          },
          "point_estimate": 179549.80175292687,
          "standard_error": 38.08754317760596
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.910907664428283,
            "upper_bound": 132.46090132736785
          },
          "point_estimate": 104.87550495053544,
          "standard_error": 28.494058184164892
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2260016.1409754907,
            "upper_bound": 2260780.2167810453
          },
          "point_estimate": 2260382.9580975724,
          "standard_error": 195.94068905609544
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2259857.194852941,
            "upper_bound": 2260943.226470588
          },
          "point_estimate": 2260217.370915033,
          "standard_error": 291.7654008354192
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.9328819603489,
            "upper_bound": 1125.3426096778649
          },
          "point_estimate": 662.5128800027998,
          "standard_error": 251.7423394184675
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2260018.572498628,
            "upper_bound": 2260616.497727273
          },
          "point_estimate": 2260307.730634072,
          "standard_error": 157.67899093522476
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 344.5662070248348,
            "upper_bound": 812.7401765961299
          },
          "point_estimate": 651.8437646295187,
          "standard_error": 119.33946322160917
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29_wasm-changes",
      "fullname": "2022-04-29_wasm-changes/memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4573.170708991367,
            "upper_bound": 4577.607059051907
          },
          "point_estimate": 4574.873075880636,
          "standard_error": 1.2369795525128575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4572.855811749842,
            "upper_bound": 4574.731124447252
          },
          "point_estimate": 4573.9784165087385,
          "standard_error": 0.5550117936905412
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.15618967505419112,
            "upper_bound": 2.2327355525905204
          },
          "point_estimate": 1.489612029775938,
          "standard_error": 0.6620625831982767
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4573.298339500045,
            "upper_bound": 4574.79300555563
          },
          "point_estimate": 4574.134760400686,
          "standard_error": 0.38860188683033087
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6513830707346886,
            "upper_bound": 6.307951214682964
          },
          "point_estimate": 4.120488863743916,
          "standard_error": 1.9325812650916907
        }
      }
    }
  }
}
