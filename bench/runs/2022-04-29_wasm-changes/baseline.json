{
  "name": "2022-04-29",
  "benchmarks": {
    "memchr1/fallback/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/fallback/empty/never",
        "directory_name": "memchr1/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3022621775336587,
            "upper_bound": 2.324399323706605
          },
          "point_estimate": 2.312201171545655,
          "standard_error": 0.005708661692715369
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.29600024792218,
            "upper_bound": 2.321225035024706
          },
          "point_estimate": 2.3076407751328736,
          "standard_error": 0.0059437690798970315
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009600639096516847,
            "upper_bound": 0.02616355544990102
          },
          "point_estimate": 0.01615699783503151,
          "standard_error": 0.006605255286701812
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.298738083310607,
            "upper_bound": 2.3141781185252053
          },
          "point_estimate": 2.30658804764698,
          "standard_error": 0.003948669542093631
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007147477053695542,
            "upper_bound": 0.026445477485545325
          },
          "point_estimate": 0.01915553384926536,
          "standard_error": 0.0054348147245587095
        }
      }
    },
    "memchr1/fallback/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/common",
        "directory_name": "memchr1/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 667934.4359287879,
            "upper_bound": 669321.1303896105
          },
          "point_estimate": 668556.8567330448,
          "standard_error": 357.8312440608518
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 667691.9494949495,
            "upper_bound": 669204.2405844156
          },
          "point_estimate": 668028.405909091,
          "standard_error": 410.56443282552453
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118.06437790389226,
            "upper_bound": 1745.4370941032598
          },
          "point_estimate": 617.0978696807564,
          "standard_error": 445.70268904636714
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 667706.5900864674,
            "upper_bound": 668861.4764979339
          },
          "point_estimate": 668150.6969539551,
          "standard_error": 300.1364078563075
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409.621310468539,
            "upper_bound": 1638.2686834870249
          },
          "point_estimate": 1199.7151218281665,
          "standard_error": 334.9524307189559
        }
      }
    },
    "memchr1/fallback/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/never",
        "directory_name": "memchr1/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42012.28433552366,
            "upper_bound": 42048.017008378694
          },
          "point_estimate": 42028.16999564685,
          "standard_error": 9.1982450197222
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42008.99505388761,
            "upper_bound": 42040.74587182448
          },
          "point_estimate": 42020.408152791526,
          "standard_error": 7.968570745909465
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.101070500713179,
            "upper_bound": 43.26136937822404
          },
          "point_estimate": 21.9810816815441,
          "standard_error": 10.52884097500206
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42007.77492900317,
            "upper_bound": 42025.33195219492
          },
          "point_estimate": 42015.02375753577,
          "standard_error": 4.571991859073969
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.91301851579116,
            "upper_bound": 42.28834054386659
          },
          "point_estimate": 30.59763553183963,
          "standard_error": 8.753948126952954
        }
      }
    },
    "memchr1/fallback/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/rare",
        "directory_name": "memchr1/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44439.31608058608,
            "upper_bound": 44473.63288737718
          },
          "point_estimate": 44452.66090223269,
          "standard_error": 9.416014195153958
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44436.80881155881,
            "upper_bound": 44453.47121489621
          },
          "point_estimate": 44444.39639295889,
          "standard_error": 4.3165199377502335
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9511223402123836,
            "upper_bound": 19.98109699569246
          },
          "point_estimate": 7.974373301650323,
          "standard_error": 5.350948270601415
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44435.66244647252,
            "upper_bound": 44448.174580480685
          },
          "point_estimate": 44440.74642183213,
          "standard_error": 3.2066870597888637
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.327992502160557,
            "upper_bound": 47.85552004433274
          },
          "point_estimate": 31.422885269738455,
          "standard_error": 14.174909376182187
        }
      }
    },
    "memchr1/fallback/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/uncommon",
        "directory_name": "memchr1/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164177.70708742223,
            "upper_bound": 164386.81255005003
          },
          "point_estimate": 164277.43359198485,
          "standard_error": 53.56229487844629
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164164.57404279278,
            "upper_bound": 164375.17736486485
          },
          "point_estimate": 164257.17402402405,
          "standard_error": 47.82342739891853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.466743980852966,
            "upper_bound": 300.0819448826713
          },
          "point_estimate": 132.2601401444065,
          "standard_error": 81.48299445169582
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164143.41307377737,
            "upper_bound": 164315.0775674457
          },
          "point_estimate": 164215.14207324208,
          "standard_error": 44.49236949031699
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.10359414124268,
            "upper_bound": 236.80584488678016
          },
          "point_estimate": 178.48929578477524,
          "standard_error": 39.82346666171529
        }
      }
    },
    "memchr1/fallback/huge/verycommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/verycommon",
        "directory_name": "memchr1/fallback_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1196461.028003072,
            "upper_bound": 1200923.9198387095
          },
          "point_estimate": 1198576.7497004608,
          "standard_error": 1142.599444376527
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1195186.930875576,
            "upper_bound": 1200513.070967742
          },
          "point_estimate": 1198419.4045698924,
          "standard_error": 1274.7403595140047
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 627.9751462704453,
            "upper_bound": 6551.137857887351
          },
          "point_estimate": 3853.076064658757,
          "standard_error": 1555.7395864414736
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1197128.1861912846,
            "upper_bound": 1200278.3601916102
          },
          "point_estimate": 1199079.8851277754,
          "standard_error": 799.8218378633458
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1939.116635073419,
            "upper_bound": 5054.640130825311
          },
          "point_estimate": 3792.7121008143736,
          "standard_error": 861.4166512143665
        }
      }
    },
    "memchr1/fallback/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/common",
        "directory_name": "memchr1/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274.71895108577775,
            "upper_bound": 275.8868216007683
          },
          "point_estimate": 275.2718512446265,
          "standard_error": 0.3006352220780168
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274.4684806896931,
            "upper_bound": 276.1973198723318
          },
          "point_estimate": 274.97452860085565,
          "standard_error": 0.3904972269690266
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1085167809393522,
            "upper_bound": 1.554244261742697
          },
          "point_estimate": 0.7752794091635345,
          "standard_error": 0.43459997677327167
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274.43784108318675,
            "upper_bound": 275.45146570280315
          },
          "point_estimate": 274.824888389259,
          "standard_error": 0.2586405540495006
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.41505787751234113,
            "upper_bound": 1.2165536601561346
          },
          "point_estimate": 1.00220915445168,
          "standard_error": 0.18460019517007997
        }
      }
    },
    "memchr1/fallback/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/never",
        "directory_name": "memchr1/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.71099974172925,
            "upper_bound": 56.93002490439224
          },
          "point_estimate": 56.826633370190656,
          "standard_error": 0.05611326525324408
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.71180549034665,
            "upper_bound": 56.90618498671825
          },
          "point_estimate": 56.85940092054112,
          "standard_error": 0.04087800180758191
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011122146221581418,
            "upper_bound": 0.3416545449749936
          },
          "point_estimate": 0.06055112357522186,
          "standard_error": 0.08154510518149692
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.74634960530352,
            "upper_bound": 56.88389041723525
          },
          "point_estimate": 56.83270112195636,
          "standard_error": 0.0351992610940063
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03580583712274609,
            "upper_bound": 0.2511539432133703
          },
          "point_estimate": 0.18644168851676077,
          "standard_error": 0.04828257620777507
        }
      }
    },
    "memchr1/fallback/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/rare",
        "directory_name": "memchr1/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.855561449860865,
            "upper_bound": 63.4354550298104
          },
          "point_estimate": 63.165731037170914,
          "standard_error": 0.14893622085054098
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.81749920369279,
            "upper_bound": 63.46018233563096
          },
          "point_estimate": 63.25932548356686,
          "standard_error": 0.14076298536541262
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07971415380508318,
            "upper_bound": 0.8178157897741857
          },
          "point_estimate": 0.27774171358933236,
          "standard_error": 0.1904135680283776
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.796976851419245,
            "upper_bound": 63.38152609185452
          },
          "point_estimate": 63.11032587963156,
          "standard_error": 0.15877485476036088
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.17273629935975082,
            "upper_bound": 0.6454420804325384
          },
          "point_estimate": 0.4953206261895606,
          "standard_error": 0.1169236114385552
        }
      }
    },
    "memchr1/fallback/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/uncommon",
        "directory_name": "memchr1/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.38838828545704,
            "upper_bound": 94.62306408227712
          },
          "point_estimate": 94.49544992035044,
          "standard_error": 0.06050368431547636
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.33460703340086,
            "upper_bound": 94.61992710335394
          },
          "point_estimate": 94.45896804062129,
          "standard_error": 0.07157384924953657
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015337117176161333,
            "upper_bound": 0.2994262531642098
          },
          "point_estimate": 0.16977881812499482,
          "standard_error": 0.07756738139426
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.38476567561956,
            "upper_bound": 94.57433230024428
          },
          "point_estimate": 94.47339522925778,
          "standard_error": 0.04852093712726109
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08153555310873685,
            "upper_bound": 0.2711081218529112
          },
          "point_estimate": 0.20199686688205737,
          "standard_error": 0.05194016753394176
        }
      }
    },
    "memchr1/fallback/small/verycommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/verycommon",
        "directory_name": "memchr1/fallback_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 541.9792567078764,
            "upper_bound": 542.3956201439092
          },
          "point_estimate": 542.1587448268616,
          "standard_error": 0.10769461657508908
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 541.9752928711068,
            "upper_bound": 542.2268710173261
          },
          "point_estimate": 542.0833001702922,
          "standard_error": 0.05790988467953106
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003334787307759437,
            "upper_bound": 0.3993590335363143
          },
          "point_estimate": 0.13435134773193372,
          "standard_error": 0.11792431753189957
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 542.0709899277986,
            "upper_bound": 542.321546121869
          },
          "point_estimate": 542.1875335606207,
          "standard_error": 0.06224736877846805
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0979334133091478,
            "upper_bound": 0.5246002213234472
          },
          "point_estimate": 0.3595198367340801,
          "standard_error": 0.12679138948718235
        }
      }
    },
    "memchr1/fallback/tiny/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/common",
        "directory_name": "memchr1/fallback_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.72502755722125,
            "upper_bound": 54.79625910998944
          },
          "point_estimate": 54.76348009287464,
          "standard_error": 0.01830314450657031
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.73471268402096,
            "upper_bound": 54.80643340900292
          },
          "point_estimate": 54.77159226995214,
          "standard_error": 0.015199242913350576
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006432109094047775,
            "upper_bound": 0.09649112807358834
          },
          "point_estimate": 0.04260213458100809,
          "standard_error": 0.023001278596166776
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.75392565334394,
            "upper_bound": 54.80531420577257
          },
          "point_estimate": 54.779442707443714,
          "standard_error": 0.013022956264154973
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023504839420443697,
            "upper_bound": 0.08379840822230925
          },
          "point_estimate": 0.06097486820086331,
          "standard_error": 0.016359324937105676
        }
      }
    },
    "memchr1/fallback/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/never",
        "directory_name": "memchr1/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.974041805733116,
            "upper_bound": 7.991856156716684
          },
          "point_estimate": 7.982780909363872,
          "standard_error": 0.004561361540875794
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.97195413355476,
            "upper_bound": 7.99178118842268
          },
          "point_estimate": 7.983024062299525,
          "standard_error": 0.004453254835433961
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002359992481915509,
            "upper_bound": 0.02699514149041282
          },
          "point_estimate": 0.011537821908488262,
          "standard_error": 0.006476825463133461
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.979571635237606,
            "upper_bound": 8.000875739692209
          },
          "point_estimate": 7.989938528041729,
          "standard_error": 0.005991575106484009
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007485894511631027,
            "upper_bound": 0.01996192832823638
          },
          "point_estimate": 0.015179770978775283,
          "standard_error": 0.003195089269044158
        }
      }
    },
    "memchr1/fallback/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/rare",
        "directory_name": "memchr1/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.295424126473028,
            "upper_bound": 11.30857091295196
          },
          "point_estimate": 11.30147420942128,
          "standard_error": 0.0033840345542064413
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.292673080714923,
            "upper_bound": 11.309990416607782
          },
          "point_estimate": 11.298203222908498,
          "standard_error": 0.004020112389771375
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013563047968971245,
            "upper_bound": 0.016743292983187812
          },
          "point_estimate": 0.0086621957981525,
          "standard_error": 0.004022374063082694
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.293322171613898,
            "upper_bound": 11.299811818667951
          },
          "point_estimate": 11.296242077836052,
          "standard_error": 0.0016830836313382995
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003828514761914184,
            "upper_bound": 0.0142175592786005
          },
          "point_estimate": 0.011340238116129235,
          "standard_error": 0.0026671718244581843
        }
      }
    },
    "memchr1/fallback/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/uncommon",
        "directory_name": "memchr1/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.21904520055282,
            "upper_bound": 40.30160167356625
          },
          "point_estimate": 40.25569618670766,
          "standard_error": 0.021207268339059452
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.219740400582495,
            "upper_bound": 40.27999290960859
          },
          "point_estimate": 40.23057000627267,
          "standard_error": 0.018965998044206046
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003772351838014733,
            "upper_bound": 0.09029862221468268
          },
          "point_estimate": 0.041522242005339605,
          "standard_error": 0.024988396162880757
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.19022988747593,
            "upper_bound": 40.247846054150365
          },
          "point_estimate": 40.21504421063354,
          "standard_error": 0.01517718733754321
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02465781566607964,
            "upper_bound": 0.101363378241399
          },
          "point_estimate": 0.07050065148003065,
          "standard_error": 0.02288406605691028
        }
      }
    },
    "memchr1/krate/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/krate/empty/never",
        "directory_name": "memchr1/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030676388470411747,
            "upper_bound": 0.030694842319988383
          },
          "point_estimate": 0.030684673784040377,
          "standard_error": 4.755738666627276e-6
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030673972996212683,
            "upper_bound": 0.030695087164083062
          },
          "point_estimate": 0.030677456661177833,
          "standard_error": 5.249118695620731e-6
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8074344737653744e-6,
            "upper_bound": 0.000023248774478375862
          },
          "point_estimate": 9.287435322552072e-6,
          "standard_error": 5.870891869530143e-6
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030673412545415987,
            "upper_bound": 0.030694893025809425
          },
          "point_estimate": 0.030683882054625564,
          "standard_error": 5.970171830163955e-6
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.268267057608476e-6,
            "upper_bound": 0.000021156180787778786
          },
          "point_estimate": 0.000015863634058511936,
          "standard_error": 4.238914448985523e-6
        }
      }
    },
    "memchr1/krate/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/common",
        "directory_name": "memchr1/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220234.34273761715,
            "upper_bound": 220738.74668746412
          },
          "point_estimate": 220476.64741418057,
          "standard_error": 129.10277632906548
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220093.0045180723,
            "upper_bound": 220776.84308663223
          },
          "point_estimate": 220447.16662483267,
          "standard_error": 148.94905948643714
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.61063652018322,
            "upper_bound": 730.6907067063731
          },
          "point_estimate": 403.7506008139066,
          "standard_error": 176.3106127380228
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220217.38449343937,
            "upper_bound": 220604.5688126957
          },
          "point_estimate": 220408.26563918008,
          "standard_error": 96.2490672635263
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223.95843596315575,
            "upper_bound": 564.9650825433021
          },
          "point_estimate": 432.6387075918505,
          "standard_error": 90.04632874321746
        }
      }
    },
    "memchr1/krate/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/never",
        "directory_name": "memchr1/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9147.266254149765,
            "upper_bound": 9163.70861032456
          },
          "point_estimate": 9155.044210249456,
          "standard_error": 4.209439081016646
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9143.255166330646,
            "upper_bound": 9166.712600806452
          },
          "point_estimate": 9149.613373655911,
          "standard_error": 6.818046626332026
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24433328699453996,
            "upper_bound": 23.478445627361285
          },
          "point_estimate": 9.68316569124367,
          "standard_error": 7.0646995982397565
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9144.901940145222,
            "upper_bound": 9159.215120767729
          },
          "point_estimate": 9153.02792207792,
          "standard_error": 3.6936814926015487
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.574796791655649,
            "upper_bound": 17.10806477903772
          },
          "point_estimate": 14.073425851525746,
          "standard_error": 2.6514142957046944
        }
      }
    },
    "memchr1/krate/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/rare",
        "directory_name": "memchr1/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10141.192458172562,
            "upper_bound": 10158.054374062163
          },
          "point_estimate": 10149.227706859592,
          "standard_error": 4.288104383203381
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10137.526366559485,
            "upper_bound": 10157.366881028938
          },
          "point_estimate": 10149.486454983922,
          "standard_error": 5.558022977906597
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.153532429474895,
            "upper_bound": 25.537510432469688
          },
          "point_estimate": 13.95578664934138,
          "standard_error": 5.206447069605155
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10139.597336481907,
            "upper_bound": 10165.483452999644
          },
          "point_estimate": 10153.714941328768,
          "standard_error": 6.5759328187577015
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.607485059651992,
            "upper_bound": 18.898615998435087
          },
          "point_estimate": 14.27231912861834,
          "standard_error": 3.0994312255036167
        }
      }
    },
    "memchr1/krate/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/uncommon",
        "directory_name": "memchr1/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76274.4614784247,
            "upper_bound": 76452.5351538428
          },
          "point_estimate": 76361.71925328465,
          "standard_error": 45.65983497862516
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76220.35032679739,
            "upper_bound": 76519.68487394958
          },
          "point_estimate": 76354.68259803922,
          "standard_error": 78.86316423166514
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.60842013988541,
            "upper_bound": 251.64455906182783
          },
          "point_estimate": 216.63972997740197,
          "standard_error": 56.75315595769054
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76223.16044369066,
            "upper_bound": 76436.93589688814
          },
          "point_estimate": 76302.8144439594,
          "standard_error": 54.479599858079105
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.81126733602504,
            "upper_bound": 181.57288918085632
          },
          "point_estimate": 152.36547318966777,
          "standard_error": 22.02259812325556
        }
      }
    },
    "memchr1/krate/huge/verycommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/verycommon",
        "directory_name": "memchr1/krate_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 458920.75661644334,
            "upper_bound": 460705.1490476191
          },
          "point_estimate": 459677.66054761905,
          "standard_error": 466.63950751830697
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 458634.5958333333,
            "upper_bound": 460086.03375
          },
          "point_estimate": 459286.23333333334,
          "standard_error": 465.4958581747738
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.89197363144785,
            "upper_bound": 1897.139206444003
          },
          "point_estimate": 1105.2376633155964,
          "standard_error": 415.9421864821337
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 458890.4894192158,
            "upper_bound": 459847.24474331265
          },
          "point_estimate": 459420.12246753246,
          "standard_error": 248.67654699663996
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 543.3430402361699,
            "upper_bound": 2273.487790803162
          },
          "point_estimate": 1553.3996476350264,
          "standard_error": 549.5857439465256
        }
      }
    },
    "memchr1/krate/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/common",
        "directory_name": "memchr1/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.2533406011882,
            "upper_bound": 204.73780783783923
          },
          "point_estimate": 204.54432729221136,
          "standard_error": 0.13077691452842785
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.55262940001097,
            "upper_bound": 204.77513509920308
          },
          "point_estimate": 204.62319602972573,
          "standard_error": 0.06180526628350075
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023344436274674783,
            "upper_bound": 0.3295199238728431
          },
          "point_estimate": 0.11343889445920172,
          "standard_error": 0.08427407655501155
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203.8796479470792,
            "upper_bound": 204.68542718533396
          },
          "point_estimate": 204.3628400052895,
          "standard_error": 0.23228688674314335
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0835049450100671,
            "upper_bound": 0.6642854103627007
          },
          "point_estimate": 0.43771967076645096,
          "standard_error": 0.19100550793295093
        }
      }
    },
    "memchr1/krate/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/never",
        "directory_name": "memchr1/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.79527695651857,
            "upper_bound": 6.810949761769807
          },
          "point_estimate": 6.801840420713053,
          "standard_error": 0.004100013014418468
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.794127412143943,
            "upper_bound": 6.805288372740009
          },
          "point_estimate": 6.798594949421842,
          "standard_error": 0.002428649445382276
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000669605278490641,
            "upper_bound": 0.015191212508032922
          },
          "point_estimate": 0.004305739463310745,
          "standard_error": 0.00377905169958869
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.793354886745625,
            "upper_bound": 6.799956143648884
          },
          "point_estimate": 6.796786566068964,
          "standard_error": 0.0017109250364943163
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032781229594376435,
            "upper_bound": 0.0199762343541688
          },
          "point_estimate": 0.013668693949175234,
          "standard_error": 0.004982764278642583
        }
      }
    },
    "memchr1/krate/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/rare",
        "directory_name": "memchr1/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.325334413248736,
            "upper_bound": 11.38231514061738
          },
          "point_estimate": 11.351412553653152,
          "standard_error": 0.014716841320320338
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.318111132306296,
            "upper_bound": 11.392761558588743
          },
          "point_estimate": 11.337891134262348,
          "standard_error": 0.0135491732583915
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003928189945708658,
            "upper_bound": 0.06920860744462035
          },
          "point_estimate": 0.01463298740710896,
          "standard_error": 0.017151540469615752
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.3135852691963,
            "upper_bound": 11.357912196935375
          },
          "point_estimate": 11.32913261114158,
          "standard_error": 0.011374385788082591
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01485931798879147,
            "upper_bound": 0.0623271115716939
          },
          "point_estimate": 0.04917853504198979,
          "standard_error": 0.012396395617675708
        }
      }
    },
    "memchr1/krate/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/uncommon",
        "directory_name": "memchr1/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.81892999232866,
            "upper_bound": 45.94104327941024
          },
          "point_estimate": 45.88066671102735,
          "standard_error": 0.031225045009951137
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.79501603815506,
            "upper_bound": 45.9696365475795
          },
          "point_estimate": 45.88956995322414,
          "standard_error": 0.04066335271487825
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017406730085125013,
            "upper_bound": 0.19355492048582776
          },
          "point_estimate": 0.08425166499892343,
          "standard_error": 0.04241678867054051
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.82210673535332,
            "upper_bound": 45.96198881368908
          },
          "point_estimate": 45.89773129280649,
          "standard_error": 0.03702096327437725
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05873226578640898,
            "upper_bound": 0.12992576000313188
          },
          "point_estimate": 0.10385274492179929,
          "standard_error": 0.018396251156167463
        }
      }
    },
    "memchr1/krate/small/verycommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/verycommon",
        "directory_name": "memchr1/krate_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495.77359207733986,
            "upper_bound": 495.96505657824446
          },
          "point_estimate": 495.86465365484025,
          "standard_error": 0.048860806175284016
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495.77859813849386,
            "upper_bound": 495.9529869621785
          },
          "point_estimate": 495.8357374375631,
          "standard_error": 0.0424749999780861
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016920889988099146,
            "upper_bound": 0.27125222107613556
          },
          "point_estimate": 0.08637135283515238,
          "standard_error": 0.06040032403318587
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495.8105559469821,
            "upper_bound": 495.9998377908359
          },
          "point_estimate": 495.8883168991393,
          "standard_error": 0.04916230508515405
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05739946661168005,
            "upper_bound": 0.22407115400640812
          },
          "point_estimate": 0.16218668242680415,
          "standard_error": 0.042602508897924635
        }
      }
    },
    "memchr1/krate/tiny/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/common",
        "directory_name": "memchr1/krate_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.264496387609874,
            "upper_bound": 50.36387774019249
          },
          "point_estimate": 50.315220355553386,
          "standard_error": 0.025061436688375797
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.291267593356174,
            "upper_bound": 50.34588097438788
          },
          "point_estimate": 50.311531863830325,
          "standard_error": 0.013891075373446792
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004078495664499134,
            "upper_bound": 0.1329354812070062
          },
          "point_estimate": 0.03780205184673836,
          "standard_error": 0.02904775906690589
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.21207904236269,
            "upper_bound": 50.327278783528946
          },
          "point_estimate": 50.273170082868496,
          "standard_error": 0.03126817920441958
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021343380999795984,
            "upper_bound": 0.11768357066277677
          },
          "point_estimate": 0.08364687900573449,
          "standard_error": 0.02538298199143517
        }
      }
    },
    "memchr1/krate/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/never",
        "directory_name": "memchr1/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.778141122391803,
            "upper_bound": 3.836411958461509
          },
          "point_estimate": 3.802470043651864,
          "standard_error": 0.0153531366201961
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.770892696862454,
            "upper_bound": 3.816798534650255
          },
          "point_estimate": 3.78843558882878,
          "standard_error": 0.010188538191444349
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006257359983402225,
            "upper_bound": 0.05309035384922323
          },
          "point_estimate": 0.022429807586731635,
          "standard_error": 0.012853588402199106
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.7789121412322695,
            "upper_bound": 3.797081254486693
          },
          "point_estimate": 3.788591908335807,
          "standard_error": 0.0045653732555248384
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011852843019041282,
            "upper_bound": 0.07508253860838685
          },
          "point_estimate": 0.0511372721129018,
          "standard_error": 0.01906734698153144
        }
      }
    },
    "memchr1/krate/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/rare",
        "directory_name": "memchr1/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.614801704621006,
            "upper_bound": 5.678406921833271
          },
          "point_estimate": 5.644680675409415,
          "standard_error": 0.016309257711335397
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.591247520100339,
            "upper_bound": 5.684092101231775
          },
          "point_estimate": 5.645388327713534,
          "standard_error": 0.02334964779170028
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028917743099999395,
            "upper_bound": 0.09926347947504802
          },
          "point_estimate": 0.05924710087425139,
          "standard_error": 0.02332804648383993
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.596457272272112,
            "upper_bound": 5.6545470148545505
          },
          "point_estimate": 5.6157024174215735,
          "standard_error": 0.014541130480400148
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02872737860883272,
            "upper_bound": 0.07144072855056696
          },
          "point_estimate": 0.05421709515468162,
          "standard_error": 0.01174712328743935
        }
      }
    },
    "memchr1/krate/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/uncommon",
        "directory_name": "memchr1/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.752026315116158,
            "upper_bound": 17.771175209594563
          },
          "point_estimate": 17.76044917851072,
          "standard_error": 0.004970274387278621
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.748577589441627,
            "upper_bound": 17.769693011596004
          },
          "point_estimate": 17.75580380623733,
          "standard_error": 0.004120849025409833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016442076057990274,
            "upper_bound": 0.019706380291454835
          },
          "point_estimate": 0.007702917485689219,
          "standard_error": 0.004833631079802877
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.750043764054748,
            "upper_bound": 17.757847474432822
          },
          "point_estimate": 17.753733859705374,
          "standard_error": 0.001996631224608869
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004039641361811455,
            "upper_bound": 0.02247088198706745
          },
          "point_estimate": 0.016525387572750672,
          "standard_error": 0.005037770094818219
        }
      }
    },
    "memchr1/libc/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/libc/empty/never",
        "directory_name": "memchr1/libc_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4529812006819443,
            "upper_bound": 2.515878718251975
          },
          "point_estimate": 2.482296917251649,
          "standard_error": 0.01626341621718595
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4376433254642813,
            "upper_bound": 2.514425494513508
          },
          "point_estimate": 2.4696943172061467,
          "standard_error": 0.01938003352731184
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012022725406982388,
            "upper_bound": 0.0862256676298013
          },
          "point_estimate": 0.04838333540965691,
          "standard_error": 0.019608632950951544
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4452571103652576,
            "upper_bound": 2.4748586287933834
          },
          "point_estimate": 2.4574207193825237,
          "standard_error": 0.0075905523484521105
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023640410754616873,
            "upper_bound": 0.07118744778924488
          },
          "point_estimate": 0.05426538913627423,
          "standard_error": 0.012475559091110295
        }
      }
    },
    "memchr1/libc/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/common",
        "directory_name": "memchr1/libc_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255958.9853568494,
            "upper_bound": 256794.0070422535
          },
          "point_estimate": 256348.2311183769,
          "standard_error": 214.0383366126655
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255723.463028169,
            "upper_bound": 256723.73063380283
          },
          "point_estimate": 256357.0407444668,
          "standard_error": 306.08950850831314
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.02224483183268,
            "upper_bound": 1293.0480009170733
          },
          "point_estimate": 692.8179236155355,
          "standard_error": 292.60942618425537
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255829.4362896702,
            "upper_bound": 256420.7768433748
          },
          "point_estimate": 256069.6306932504,
          "standard_error": 152.22914972873605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 374.33879206880584,
            "upper_bound": 959.792494332923
          },
          "point_estimate": 715.5807272292401,
          "standard_error": 166.6070394356744
        }
      }
    },
    "memchr1/libc/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/never",
        "directory_name": "memchr1/libc_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9127.244196723144,
            "upper_bound": 9155.43768697501
          },
          "point_estimate": 9138.181872179086,
          "standard_error": 7.767988557134005
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9124.074668679752,
            "upper_bound": 9137.05072455085
          },
          "point_estimate": 9133.385804604932,
          "standard_error": 3.7410088911051886
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9798068220624452,
            "upper_bound": 16.605306242689707
          },
          "point_estimate": 8.428257104184656,
          "standard_error": 4.240021666400876
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9127.716633068843,
            "upper_bound": 9136.209066235057
          },
          "point_estimate": 9133.140270197844,
          "standard_error": 2.153486003677383
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.514125630009176,
            "upper_bound": 39.56765574749786
          },
          "point_estimate": 25.891160700533067,
          "standard_error": 11.924109778331264
        }
      }
    },
    "memchr1/libc/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/rare",
        "directory_name": "memchr1/libc_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10237.258680084746,
            "upper_bound": 10256.08082095776
          },
          "point_estimate": 10244.821960552865,
          "standard_error": 5.053607293440408
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10233.53945386064,
            "upper_bound": 10246.525
          },
          "point_estimate": 10240.288450363198,
          "standard_error": 3.2234569223150107
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5759583796072892,
            "upper_bound": 14.50991618822954
          },
          "point_estimate": 9.626185182208577,
          "standard_error": 3.6145582446875233
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10235.762377989236,
            "upper_bound": 10244.452256253748
          },
          "point_estimate": 10240.000181231198,
          "standard_error": 2.2109017758589844
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.1163646644497724,
            "upper_bound": 25.44459050487038
          },
          "point_estimate": 16.83144188442669,
          "standard_error": 7.057571043962636
        }
      }
    },
    "memchr1/libc/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/uncommon",
        "directory_name": "memchr1/libc_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70706.27455969912,
            "upper_bound": 70874.42612458774
          },
          "point_estimate": 70794.14925712746,
          "standard_error": 42.853284477161495
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70709.41764216367,
            "upper_bound": 70881.61711165048
          },
          "point_estimate": 70822.76660194175,
          "standard_error": 44.22325793004209
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.99297216101415,
            "upper_bound": 245.1270083908344
          },
          "point_estimate": 126.0918490882954,
          "standard_error": 53.93058220124303
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70670.81672473867,
            "upper_bound": 70805.29722989223
          },
          "point_estimate": 70741.08590089522,
          "standard_error": 33.7534103786647
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.358102189263526,
            "upper_bound": 193.8563289965919
          },
          "point_estimate": 141.99551385145642,
          "standard_error": 34.293641599358445
        }
      }
    },
    "memchr1/libc/huge/verycommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/verycommon",
        "directory_name": "memchr1/libc_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 531999.3657943984,
            "upper_bound": 532547.554658385
          },
          "point_estimate": 532251.9613411549,
          "standard_error": 141.07285787899207
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 531908.9473429951,
            "upper_bound": 532621.0937198068
          },
          "point_estimate": 532062.6952495974,
          "standard_error": 167.55100080761088
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.99744546437144,
            "upper_bound": 741.5961998775082
          },
          "point_estimate": 322.07872906461324,
          "standard_error": 176.00021680898905
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 531931.908412992,
            "upper_bound": 532291.3698957348
          },
          "point_estimate": 532061.0445699228,
          "standard_error": 91.0248872412834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176.29334601030774,
            "upper_bound": 592.9405588624694
          },
          "point_estimate": 470.775773535472,
          "standard_error": 106.21847050136137
        }
      }
    },
    "memchr1/libc/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/common",
        "directory_name": "memchr1/libc_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221.5227900266558,
            "upper_bound": 222.15634422577756
          },
          "point_estimate": 221.8881443550359,
          "standard_error": 0.16092015298084167
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221.5262340650372,
            "upper_bound": 222.1790495970959
          },
          "point_estimate": 222.1227135777354,
          "standard_error": 0.1366242895892161
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015729874342351683,
            "upper_bound": 0.8441507715594632
          },
          "point_estimate": 0.09733993693789676,
          "standard_error": 0.14586662164576275
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 222.06909740482925,
            "upper_bound": 222.1907997860305
          },
          "point_estimate": 222.14613987512035,
          "standard_error": 0.031644206094522134
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05421862611987066,
            "upper_bound": 0.6766067718343464
          },
          "point_estimate": 0.5358041471436354,
          "standard_error": 0.1708646124063571
        }
      }
    },
    "memchr1/libc/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/never",
        "directory_name": "memchr1/libc_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.894974520964934,
            "upper_bound": 6.900339142882044
          },
          "point_estimate": 6.897769186299411,
          "standard_error": 0.0013826291396234972
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.892722050694131,
            "upper_bound": 6.901115285374498
          },
          "point_estimate": 6.8992546584798475,
          "standard_error": 0.00196620789262353
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006635886056951799,
            "upper_bound": 0.007375291012015093
          },
          "point_estimate": 0.0029215540441254444,
          "standard_error": 0.001934606160912996
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.892796757234788,
            "upper_bound": 6.899822649062707
          },
          "point_estimate": 6.8965156120628075,
          "standard_error": 0.0018345329058700897
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0021706760817657156,
            "upper_bound": 0.005571452313145514
          },
          "point_estimate": 0.004603668343916085,
          "standard_error": 0.0008002813661392137
        }
      }
    },
    "memchr1/libc/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/rare",
        "directory_name": "memchr1/libc_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.138954952314638,
            "upper_bound": 11.156604714057773
          },
          "point_estimate": 11.146884764430832,
          "standard_error": 0.0045487258956823816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.13662248067085,
            "upper_bound": 11.155829416742092
          },
          "point_estimate": 11.141412085677455,
          "standard_error": 0.0048608636353014885
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022030061728874947,
            "upper_bound": 0.02249946374701087
          },
          "point_estimate": 0.010664779257554269,
          "standard_error": 0.00532004768352489
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.137313344684747,
            "upper_bound": 11.145434349599142
          },
          "point_estimate": 11.140653524089997,
          "standard_error": 0.0020781597724875618
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005564078513390148,
            "upper_bound": 0.020742494524127256
          },
          "point_estimate": 0.015193585915126168,
          "standard_error": 0.004190643916433088
        }
      }
    },
    "memchr1/libc/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/uncommon",
        "directory_name": "memchr1/libc_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.57391871365866,
            "upper_bound": 47.60309498507724
          },
          "point_estimate": 47.58721087801049,
          "standard_error": 0.007480895250610946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.56531692467959,
            "upper_bound": 47.60049469381986
          },
          "point_estimate": 47.58138433129305,
          "standard_error": 0.009045557680429497
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019923233275250017,
            "upper_bound": 0.03990552555806835
          },
          "point_estimate": 0.024042973252305228,
          "standard_error": 0.00910301303165293
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.571506448888634,
            "upper_bound": 47.587696779042915
          },
          "point_estimate": 47.578809424976136,
          "standard_error": 0.004103870907181816
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010803540710951512,
            "upper_bound": 0.03361810958956825
          },
          "point_estimate": 0.024938859574882316,
          "standard_error": 0.006320651534248239
        }
      }
    },
    "memchr1/libc/small/verycommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/verycommon",
        "directory_name": "memchr1/libc_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 575.7133630254157,
            "upper_bound": 578.0062266306688
          },
          "point_estimate": 576.7783112811792,
          "standard_error": 0.5930807579074099
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 575.3706587301588,
            "upper_bound": 579.3430793650793
          },
          "point_estimate": 575.6784285714286,
          "standard_error": 1.000713746127618
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10214996151990624,
            "upper_bound": 3.0938001984073433
          },
          "point_estimate": 0.7156408095171989,
          "standard_error": 0.7820121368292292
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 575.4081501500681,
            "upper_bound": 576.1317227465682
          },
          "point_estimate": 575.6160512471655,
          "standard_error": 0.19223773857311935
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5061736625740655,
            "upper_bound": 2.242481121359213
          },
          "point_estimate": 1.977264074542784,
          "standard_error": 0.3614022125054363
        }
      }
    },
    "memchr1/libc/tiny/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/common",
        "directory_name": "memchr1/libc_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.50946446570938,
            "upper_bound": 51.57590588191787
          },
          "point_estimate": 51.54440436949918,
          "standard_error": 0.017168302947199315
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.50672010112343,
            "upper_bound": 51.56851889277223
          },
          "point_estimate": 51.55748465546701,
          "standard_error": 0.012890668971900268
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004815645868820906,
            "upper_bound": 0.10652153625079332
          },
          "point_estimate": 0.017836482078994527,
          "standard_error": 0.025449223557021262
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.47527778183718,
            "upper_bound": 51.56036718839517
          },
          "point_estimate": 51.517830192154406,
          "standard_error": 0.022494103480144707
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012151125047468283,
            "upper_bound": 0.07687950008279178
          },
          "point_estimate": 0.05725666547298665,
          "standard_error": 0.014492949963018647
        }
      }
    },
    "memchr1/libc/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/never",
        "directory_name": "memchr1/libc_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.169997845938538,
            "upper_bound": 3.172084688610498
          },
          "point_estimate": 3.1710289010648873,
          "standard_error": 0.0005337004894617084
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.1698731886514366,
            "upper_bound": 3.172370560265097
          },
          "point_estimate": 3.170810018296281,
          "standard_error": 0.000658495867284197
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002420788958433906,
            "upper_bound": 0.003193796361418142
          },
          "point_estimate": 0.001604607607486889,
          "standard_error": 0.0006873009838665038
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.169985493894668,
            "upper_bound": 3.1719559484571227
          },
          "point_estimate": 3.1709724433668427,
          "standard_error": 0.0005012902958946945
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009537405336098746,
            "upper_bound": 0.002296755932595117
          },
          "point_estimate": 0.0017781131188095165,
          "standard_error": 0.0003442380748327958
        }
      }
    },
    "memchr1/libc/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/rare",
        "directory_name": "memchr1/libc_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.216055452893878,
            "upper_bound": 5.218369448219202
          },
          "point_estimate": 5.21719846798776,
          "standard_error": 0.0005936123401474086
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.215577634032303,
            "upper_bound": 5.218944989475556
          },
          "point_estimate": 5.21718035997478,
          "standard_error": 0.0008805144870079269
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004844527685582207,
            "upper_bound": 0.0034578632727272577
          },
          "point_estimate": 0.002013269052515284,
          "standard_error": 0.000770282160852537
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.216051481356941,
            "upper_bound": 5.218682216901375
          },
          "point_estimate": 5.217039958818136,
          "standard_error": 0.0006719389694340958
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001141506638643518,
            "upper_bound": 0.002444981723528639
          },
          "point_estimate": 0.00197677332041367,
          "standard_error": 0.00032734146837889197
        }
      }
    },
    "memchr1/libc/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/libc/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/uncommon",
        "directory_name": "memchr1/libc_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.27017176117858,
            "upper_bound": 19.344857293685152
          },
          "point_estimate": 19.30680985529507,
          "standard_error": 0.01927547802375246
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.245611742631272,
            "upper_bound": 19.37198950430119
          },
          "point_estimate": 19.28992323235968,
          "standard_error": 0.042891750218462414
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013127980900513724,
            "upper_bound": 0.10005333351951425
          },
          "point_estimate": 0.06833204883493704,
          "standard_error": 0.030767143816358818
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.25237929602172,
            "upper_bound": 19.3031516785903
          },
          "point_estimate": 19.269445646806545,
          "standard_error": 0.013089912333413934
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04280760186632284,
            "upper_bound": 0.07170172501167882
          },
          "point_estimate": 0.06446475723824266,
          "standard_error": 0.00739250855714863
        }
      }
    },
    "memchr1/naive/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/naive/empty/never",
        "directory_name": "memchr1/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4940944360019251,
            "upper_bound": 0.5052996603080385
          },
          "point_estimate": 0.4993769770653623,
          "standard_error": 0.0028821176592890877
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4908837332849979,
            "upper_bound": 0.5065781739741061
          },
          "point_estimate": 0.4963974734022972,
          "standard_error": 0.004808229386386902
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003091245229668144,
            "upper_bound": 0.017936275292543984
          },
          "point_estimate": 0.008555577558757152,
          "standard_error": 0.0046102724064676645
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.49612970145131496,
            "upper_bound": 0.5060399154226699
          },
          "point_estimate": 0.5012850660861234,
          "standard_error": 0.0025126359250376906
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00495841647046345,
            "upper_bound": 0.01236268110680728
          },
          "point_estimate": 0.00961846484784721,
          "standard_error": 0.0019673280047019254
        }
      }
    },
    "memchr1/naive/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/common",
        "directory_name": "memchr1/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 453900.54632142856,
            "upper_bound": 454819.9156964285
          },
          "point_estimate": 454381.66048809513,
          "standard_error": 235.4585377635671
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 454007.1732142857,
            "upper_bound": 454993.1125
          },
          "point_estimate": 454339.479375,
          "standard_error": 236.80520781118776
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.0548052378016,
            "upper_bound": 1275.6351948529598
          },
          "point_estimate": 593.4802245886405,
          "standard_error": 285.7716646532051
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 454074.0539340702,
            "upper_bound": 454596.45169374783
          },
          "point_estimate": 454284.5292207792,
          "standard_error": 132.42555664765308
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 345.18281950964695,
            "upper_bound": 1082.1306655214796
          },
          "point_estimate": 784.5404922882027,
          "standard_error": 196.4139413197781
        }
      }
    },
    "memchr1/naive/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/never",
        "directory_name": "memchr1/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146054.8283568799,
            "upper_bound": 146134.87651220357
          },
          "point_estimate": 146089.47413574936,
          "standard_error": 20.80938287478883
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146042.31762765348,
            "upper_bound": 146118.5626004016
          },
          "point_estimate": 146071.44310575636,
          "standard_error": 19.66693844959323
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.4833555367902,
            "upper_bound": 83.93650036055955
          },
          "point_estimate": 39.53606545604911,
          "standard_error": 20.410334245505343
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146050.83700042334,
            "upper_bound": 146117.75993743568
          },
          "point_estimate": 146085.0718719032,
          "standard_error": 17.905413190485287
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.72661978166416,
            "upper_bound": 99.59995722274854
          },
          "point_estimate": 69.40689667753614,
          "standard_error": 22.675188626701473
        }
      }
    },
    "memchr1/naive/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/rare",
        "directory_name": "memchr1/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147407.5712582096,
            "upper_bound": 147487.41079059825
          },
          "point_estimate": 147440.1370186685,
          "standard_error": 21.381863058481077
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147400.3821862348,
            "upper_bound": 147449.92768780928
          },
          "point_estimate": 147420.2863697706,
          "standard_error": 14.476061183810264
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.527252396907666,
            "upper_bound": 61.74686251710288
          },
          "point_estimate": 30.281704300474857,
          "standard_error": 14.517276863996404
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147408.81458571,
            "upper_bound": 147432.9819842407
          },
          "point_estimate": 147420.96116515063,
          "standard_error": 6.115421737374004
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.990183794496176,
            "upper_bound": 107.33141906500953
          },
          "point_estimate": 71.55007546865878,
          "standard_error": 29.08464285311646
        }
      }
    },
    "memchr1/naive/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/uncommon",
        "directory_name": "memchr1/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198839.10142342135,
            "upper_bound": 199145.01228695028
          },
          "point_estimate": 198980.844847992,
          "standard_error": 78.58405247056214
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198757.2151639344,
            "upper_bound": 199149.0480874317
          },
          "point_estimate": 198951.6023072253,
          "standard_error": 106.8547514489206
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.27630215110091,
            "upper_bound": 452.85021494758945
          },
          "point_estimate": 290.4657410317338,
          "standard_error": 97.55843482055796
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198837.1793365496,
            "upper_bound": 199017.9898533483
          },
          "point_estimate": 198926.38654460295,
          "standard_error": 45.88543901257861
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.062637109318,
            "upper_bound": 349.4113666153922
          },
          "point_estimate": 262.34723967382905,
          "standard_error": 61.114905770134776
        }
      }
    },
    "memchr1/naive/huge/verycommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/verycommon",
        "directory_name": "memchr1/naive_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 917859.3169345238,
            "upper_bound": 919562.6795625
          },
          "point_estimate": 918759.716705357,
          "standard_error": 434.98504182107627
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 918164.2010416668,
            "upper_bound": 919679.971875
          },
          "point_estimate": 918632.36875,
          "standard_error": 410.3221319873663
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.84724445968736,
            "upper_bound": 2214.7991132419693
          },
          "point_estimate": 1144.7407674267636,
          "standard_error": 538.9747949844827
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 918439.122887324,
            "upper_bound": 919256.5664357684
          },
          "point_estimate": 918792.6764285716,
          "standard_error": 209.23200046982103
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 575.4505430269014,
            "upper_bound": 2025.335724920965
          },
          "point_estimate": 1447.4947365342357,
          "standard_error": 387.82274777402125
        }
      }
    },
    "memchr1/naive/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/common",
        "directory_name": "memchr1/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240.04647657579284,
            "upper_bound": 242.53752498470791
          },
          "point_estimate": 241.2625747837928,
          "standard_error": 0.6381223487430303
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.48400899775473,
            "upper_bound": 243.53806973010063
          },
          "point_estimate": 240.63067314931408,
          "standard_error": 1.1350646936802566
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.445386411346206,
            "upper_bound": 3.388673855811092
          },
          "point_estimate": 2.6025544903827296,
          "standard_error": 0.7873747495333709
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.3812807617676,
            "upper_bound": 242.9186846006418
          },
          "point_estimate": 240.9485229287712,
          "standard_error": 0.9174121848177804
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3276666792773897,
            "upper_bound": 2.496389785988512
          },
          "point_estimate": 2.125709604600634,
          "standard_error": 0.29930863835980337
        }
      }
    },
    "memchr1/naive/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/never",
        "directory_name": "memchr1/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.78891873139554,
            "upper_bound": 172.96432713519195
          },
          "point_estimate": 172.8678679046744,
          "standard_error": 0.04498323146418611
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.76545469663134,
            "upper_bound": 172.98327885209295
          },
          "point_estimate": 172.8228238849031,
          "standard_error": 0.04620508220171064
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022960329188021503,
            "upper_bound": 0.20849031311331517
          },
          "point_estimate": 0.08034615800247608,
          "standard_error": 0.043679271981101206
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.7695061283651,
            "upper_bound": 172.8479258745832
          },
          "point_estimate": 172.80547084786738,
          "standard_error": 0.020345251994422216
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04010749947476009,
            "upper_bound": 0.18887174253998995
          },
          "point_estimate": 0.14997553426674806,
          "standard_error": 0.03957517152757909
        }
      }
    },
    "memchr1/naive/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/rare",
        "directory_name": "memchr1/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.7679562914549,
            "upper_bound": 180.9200055162759
          },
          "point_estimate": 180.8385424699973,
          "standard_error": 0.03910808647390776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.74129632760216,
            "upper_bound": 180.93794313899028
          },
          "point_estimate": 180.773369397679,
          "standard_error": 0.05894827511196276
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004908462438117077,
            "upper_bound": 0.2146977664638017
          },
          "point_estimate": 0.06275227425442348,
          "standard_error": 0.060188302128772915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.7418514729613,
            "upper_bound": 180.82662584816853
          },
          "point_estimate": 180.77280984843856,
          "standard_error": 0.022064453024868785
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05317001430628496,
            "upper_bound": 0.1615981902750895
          },
          "point_estimate": 0.13047840576861772,
          "standard_error": 0.026733908071762697
        }
      }
    },
    "memchr1/naive/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/uncommon",
        "directory_name": "memchr1/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199.304346834022,
            "upper_bound": 205.611247594704
          },
          "point_estimate": 202.44464332720204,
          "standard_error": 1.6144676738952273
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198.45588003911035,
            "upper_bound": 204.8203171534857
          },
          "point_estimate": 203.323813171897,
          "standard_error": 1.4558477972428474
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5943456240923358,
            "upper_bound": 9.128274053907406
          },
          "point_estimate": 2.240390141054296,
          "standard_error": 2.409944494434398
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199.2670264669577,
            "upper_bound": 203.9882610665809
          },
          "point_estimate": 202.29599136407933,
          "standard_error": 1.2365691646854748
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.705269139025827,
            "upper_bound": 7.326059449509823
          },
          "point_estimate": 5.39746462055317,
          "standard_error": 1.3026240846829444
        }
      }
    },
    "memchr1/naive/small/verycommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/verycommon",
        "directory_name": "memchr1/naive_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 333.55901239116326,
            "upper_bound": 340.26474390747416
          },
          "point_estimate": 336.9250431982184,
          "standard_error": 1.7084422287787002
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332.69716468864874,
            "upper_bound": 343.04424860768677
          },
          "point_estimate": 335.6543813326268,
          "standard_error": 3.4679107869624612
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.30088165856882254,
            "upper_bound": 9.437301246455508
          },
          "point_estimate": 6.623581759317049,
          "standard_error": 2.6603638539489274
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.0987271520993,
            "upper_bound": 340.88575805342475
          },
          "point_estimate": 337.4167865208348,
          "standard_error": 1.7697691043320347
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.74612602220777,
            "upper_bound": 6.7162531880192855
          },
          "point_estimate": 5.699726593767227,
          "standard_error": 0.7531936083247858
        }
      }
    },
    "memchr1/naive/tiny/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/common",
        "directory_name": "memchr1/naive_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.34600402264136,
            "upper_bound": 36.52510491241848
          },
          "point_estimate": 36.42976299274784,
          "standard_error": 0.04621081397850891
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.31871702300747,
            "upper_bound": 36.572623179980205
          },
          "point_estimate": 36.332712206533465,
          "standard_error": 0.0745977480046369
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004363923925721718,
            "upper_bound": 0.24902401443680153
          },
          "point_estimate": 0.032317998319214225,
          "standard_error": 0.07382239276095524
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.324571036022775,
            "upper_bound": 36.37506061670239
          },
          "point_estimate": 36.339454160823,
          "standard_error": 0.01338154934014855
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06008881029817904,
            "upper_bound": 0.18217187725611456
          },
          "point_estimate": 0.15432901234230584,
          "standard_error": 0.029439347788781784
        }
      }
    },
    "memchr1/naive/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/never",
        "directory_name": "memchr1/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.622075309881364,
            "upper_bound": 27.83246141044504
          },
          "point_estimate": 27.721336965210945,
          "standard_error": 0.05424285042624598
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.574518948045956,
            "upper_bound": 27.877914039043336
          },
          "point_estimate": 27.703176207922525,
          "standard_error": 0.07203325035626337
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020371121266303485,
            "upper_bound": 0.3100349699573077
          },
          "point_estimate": 0.18210875703738233,
          "standard_error": 0.07593165893806378
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.622778578260185,
            "upper_bound": 27.907741522993632
          },
          "point_estimate": 27.76422704409611,
          "standard_error": 0.07832841226724868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08271698199844667,
            "upper_bound": 0.2255323588195452
          },
          "point_estimate": 0.1808986529470668,
          "standard_error": 0.0367112500503772
        }
      }
    },
    "memchr1/naive/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/rare",
        "directory_name": "memchr1/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.48286047214819,
            "upper_bound": 28.93049979392172
          },
          "point_estimate": 28.696312002433796,
          "standard_error": 0.11440878147855056
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.397310767007397,
            "upper_bound": 28.96018363203642
          },
          "point_estimate": 28.66409177924116,
          "standard_error": 0.13422558507319474
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08825997921105312,
            "upper_bound": 0.6419613998392041
          },
          "point_estimate": 0.29793321952325047,
          "standard_error": 0.14249823627060532
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.662041857627116,
            "upper_bound": 29.20997045869156
          },
          "point_estimate": 28.969086955386583,
          "standard_error": 0.14604028013087691
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1926216755236131,
            "upper_bound": 0.49839339417206496
          },
          "point_estimate": 0.38140730720730504,
          "standard_error": 0.08120149149106148
        }
      }
    },
    "memchr1/naive/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr1/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/uncommon",
        "directory_name": "memchr1/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.20306259566524,
            "upper_bound": 25.474043307151593
          },
          "point_estimate": 25.35060067390386,
          "standard_error": 0.06927861046312057
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.220764906728498,
            "upper_bound": 25.508710552947157
          },
          "point_estimate": 25.440294119279105,
          "standard_error": 0.07362999323173078
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010208729509844763,
            "upper_bound": 0.3537256839035135
          },
          "point_estimate": 0.1047329511422962,
          "standard_error": 0.08558684486920307
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.29402639026787,
            "upper_bound": 25.46514289616256
          },
          "point_estimate": 25.38437786408316,
          "standard_error": 0.04602813081222773
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06056963426505782,
            "upper_bound": 0.29391762827502077
          },
          "point_estimate": 0.23043301647247696,
          "standard_error": 0.05726167488236191
        }
      }
    },
    "memchr2/fallback/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/fallback/empty/never",
        "directory_name": "memchr2/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.70016827825123,
            "upper_bound": 2.7020728013142197
          },
          "point_estimate": 2.7011303611282536,
          "standard_error": 0.0004886711839636413
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.699338550313426,
            "upper_bound": 2.7025168021759223
          },
          "point_estimate": 2.70147278725727,
          "standard_error": 0.0008469816039381572
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00030681916572934746,
            "upper_bound": 0.0026814954646567587
          },
          "point_estimate": 0.0018528529019125235,
          "standard_error": 0.0006333925944249653
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.700452927805122,
            "upper_bound": 2.7019294831843235
          },
          "point_estimate": 2.70124117626116,
          "standard_error": 0.0003689649097474334
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010211489978319158,
            "upper_bound": 0.0019273309908717693
          },
          "point_estimate": 0.0016314875169503543,
          "standard_error": 0.0002307430324918222
        }
      }
    },
    "memchr2/fallback/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/common",
        "directory_name": "memchr2/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1242061.4691362104,
            "upper_bound": 1243169.736085913
          },
          "point_estimate": 1242570.63840873,
          "standard_error": 284.24659309523463
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1241860.170238095,
            "upper_bound": 1243085.7066666668
          },
          "point_estimate": 1242435.3250000002,
          "standard_error": 365.5263448600724
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182.86223341995225,
            "upper_bound": 1528.616170361431
          },
          "point_estimate": 883.25399231916,
          "standard_error": 323.5841131830977
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1241850.6558265584,
            "upper_bound": 1242742.38520457
          },
          "point_estimate": 1242235.3208658008,
          "standard_error": 224.5942466472153
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 452.0549825835555,
            "upper_bound": 1286.5515114507316
          },
          "point_estimate": 946.4186572442368,
          "standard_error": 239.38650420449144
        }
      }
    },
    "memchr2/fallback/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/never",
        "directory_name": "memchr2/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83964.36336192675,
            "upper_bound": 84008.21495381062
          },
          "point_estimate": 83988.15214945561,
          "standard_error": 11.259519741107251
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83975.81883041901,
            "upper_bound": 84014.63510392609
          },
          "point_estimate": 83988.27159353349,
          "standard_error": 9.431989806855116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3671127239701502,
            "upper_bound": 57.37798858873171
          },
          "point_estimate": 23.878162820883304,
          "standard_error": 14.667727343296615
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83937.49414329087,
            "upper_bound": 84016.94369416183
          },
          "point_estimate": 83979.37804504964,
          "standard_error": 20.987216424528135
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.280170504335864,
            "upper_bound": 52.309234260327585
          },
          "point_estimate": 37.34370072720859,
          "standard_error": 10.631446685910031
        }
      }
    },
    "memchr2/fallback/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/rare",
        "directory_name": "memchr2/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89979.974339934,
            "upper_bound": 90025.39663328028
          },
          "point_estimate": 90000.16537580543,
          "standard_error": 11.692227129866057
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89972.12933168317,
            "upper_bound": 90009.24040154016
          },
          "point_estimate": 89999.57036775106,
          "standard_error": 9.095046964613015
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9238324905991304,
            "upper_bound": 50.86522391132691
          },
          "point_estimate": 15.876029091081,
          "standard_error": 12.8154441375361
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89979.11791215044,
            "upper_bound": 90006.50155286756
          },
          "point_estimate": 89995.48319403369,
          "standard_error": 7.026291540134614
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.514891680023515,
            "upper_bound": 55.77216123489433
          },
          "point_estimate": 38.858670875206315,
          "standard_error": 12.435889267498515
        }
      }
    },
    "memchr2/fallback/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/uncommon",
        "directory_name": "memchr2/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326052.6503013392,
            "upper_bound": 326200.3696875
          },
          "point_estimate": 326117.5900744047,
          "standard_error": 38.318281569751875
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326023.69866071426,
            "upper_bound": 326179.796875
          },
          "point_estimate": 326085.73363095237,
          "standard_error": 36.39276681650142
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.954879144991358,
            "upper_bound": 170.75050946859713
          },
          "point_estimate": 84.10941881929631,
          "standard_error": 41.22016781208248
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326031.10648003744,
            "upper_bound": 326184.78762898233
          },
          "point_estimate": 326106.8316558442,
          "standard_error": 42.431737210386174
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.9256099378427,
            "upper_bound": 177.1904154151351
          },
          "point_estimate": 128.0774931059107,
          "standard_error": 38.38267853647548
        }
      }
    },
    "memchr2/fallback/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/common",
        "directory_name": "memchr2/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 466.193836494939,
            "upper_bound": 466.5057701480157
          },
          "point_estimate": 466.35491480278824,
          "standard_error": 0.07994832607157394
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 466.13030285528487,
            "upper_bound": 466.5977776779526
          },
          "point_estimate": 466.3563908069581,
          "standard_error": 0.10917201182099104
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.051023270478819514,
            "upper_bound": 0.5012436392794932
          },
          "point_estimate": 0.2602963798540039,
          "standard_error": 0.11305777405531352
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 466.0741035115207,
            "upper_bound": 466.4342277517857
          },
          "point_estimate": 466.2412332411793,
          "standard_error": 0.09149750863400036
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14494211054483036,
            "upper_bound": 0.3309515339145045
          },
          "point_estimate": 0.26713293252332476,
          "standard_error": 0.04729566073565981
        }
      }
    },
    "memchr2/fallback/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/never",
        "directory_name": "memchr2/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.50230783790757,
            "upper_bound": 103.65460947884762
          },
          "point_estimate": 103.5676850191156,
          "standard_error": 0.038555160855891654
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.49420444130725,
            "upper_bound": 103.66348335980356
          },
          "point_estimate": 103.50385711703748,
          "standard_error": 0.03734583001110214
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0029145690336610593,
            "upper_bound": 0.195589825552627
          },
          "point_estimate": 0.023571697711236662,
          "standard_error": 0.03692379005899116
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.49175132261894,
            "upper_bound": 103.53590323486702
          },
          "point_estimate": 103.50448419337636,
          "standard_error": 0.01167480685918268
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01812359180617644,
            "upper_bound": 0.16106623916765803
          },
          "point_estimate": 0.12820901604870433,
          "standard_error": 0.038683814011712184
        }
      }
    },
    "memchr2/fallback/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/rare",
        "directory_name": "memchr2/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.5482996216544,
            "upper_bound": 118.78270294580594
          },
          "point_estimate": 118.0964950639205,
          "standard_error": 0.31996542457457894
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.30653604776862,
            "upper_bound": 118.5991370184713
          },
          "point_estimate": 117.83999937299888,
          "standard_error": 0.331615817289922
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1239880998148362,
            "upper_bound": 1.4754329413939584
          },
          "point_estimate": 0.8973947062197054,
          "standard_error": 0.34586821527673034
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.43477894657184,
            "upper_bound": 117.98661979483074
          },
          "point_estimate": 117.68718336972668,
          "standard_error": 0.13964556145303192
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.37738779018404706,
            "upper_bound": 1.4990369750084276
          },
          "point_estimate": 1.0683363425361587,
          "standard_error": 0.31772726559693015
        }
      }
    },
    "memchr2/fallback/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/uncommon",
        "directory_name": "memchr2/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.309471085319,
            "upper_bound": 177.5831954794492
          },
          "point_estimate": 177.4463659865527,
          "standard_error": 0.06987671462960192
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.30594369202348,
            "upper_bound": 177.61763421607276
          },
          "point_estimate": 177.42294277991826,
          "standard_error": 0.08693032132754741
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016946924889925833,
            "upper_bound": 0.41808106952618
          },
          "point_estimate": 0.1738323469392293,
          "standard_error": 0.094496703424713
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.2975614782543,
            "upper_bound": 177.47152393576636
          },
          "point_estimate": 177.39329335919146,
          "standard_error": 0.04378678337322935
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.115376222072599,
            "upper_bound": 0.3095163746905653
          },
          "point_estimate": 0.2329784205525623,
          "standard_error": 0.050137261924603306
        }
      }
    },
    "memchr2/fallback/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/never",
        "directory_name": "memchr2/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.499092461691808,
            "upper_bound": 13.517050324402994
          },
          "point_estimate": 13.506979531193124,
          "standard_error": 0.004646706983728465
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.498053481960184,
            "upper_bound": 13.513974373423723
          },
          "point_estimate": 13.499644264066756,
          "standard_error": 0.0050127122203676095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000839636147373791,
            "upper_bound": 0.0213316351837128
          },
          "point_estimate": 0.008217303667651348,
          "standard_error": 0.005748940221459002
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.498677286662224,
            "upper_bound": 13.512217470271272
          },
          "point_estimate": 13.502573270836123,
          "standard_error": 0.0036811500500771865
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005616673081062227,
            "upper_bound": 0.02182481040725274
          },
          "point_estimate": 0.015470759257115474,
          "standard_error": 0.004744227645659799
        }
      }
    },
    "memchr2/fallback/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/rare",
        "directory_name": "memchr2/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.84562888543782,
            "upper_bound": 21.86904687466494
          },
          "point_estimate": 21.856822410848796,
          "standard_error": 0.0059939146409909725
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.842395655656706,
            "upper_bound": 21.876410114889456
          },
          "point_estimate": 21.84939535083199,
          "standard_error": 0.00848930308221533
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002190256200250183,
            "upper_bound": 0.03216948079398612
          },
          "point_estimate": 0.015422105740268343,
          "standard_error": 0.008021515420522103
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.844143478911164,
            "upper_bound": 21.86799822278531
          },
          "point_estimate": 21.856006639227218,
          "standard_error": 0.006393927336259916
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009768852725005078,
            "upper_bound": 0.02473934013141767
          },
          "point_estimate": 0.020010473531984856,
          "standard_error": 0.003664033198152711
        }
      }
    },
    "memchr2/fallback/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/uncommon",
        "directory_name": "memchr2/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.50688500773791,
            "upper_bound": 60.68851343699878
          },
          "point_estimate": 60.601542889778216,
          "standard_error": 0.04663364353110908
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.50104023991149,
            "upper_bound": 60.731526986888255
          },
          "point_estimate": 60.61203909211993,
          "standard_error": 0.06762720033023158
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04178995594304184,
            "upper_bound": 0.26694037420703803
          },
          "point_estimate": 0.16748556784524857,
          "standard_error": 0.057240999593336506
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.531824752462995,
            "upper_bound": 60.709726380872596
          },
          "point_estimate": 60.61576594526116,
          "standard_error": 0.04482418667756718
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08629181972992779,
            "upper_bound": 0.19879395577542716
          },
          "point_estimate": 0.15566557736692122,
          "standard_error": 0.029986947357281225
        }
      }
    },
    "memchr2/krate/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/krate/empty/never",
        "directory_name": "memchr2/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03067667649328519,
            "upper_bound": 0.030690669634335577
          },
          "point_estimate": 0.030682744346139296,
          "standard_error": 3.6328695968927535e-6
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03067511281861401,
            "upper_bound": 0.030688503267728783
          },
          "point_estimate": 0.030677706985555303,
          "standard_error": 3.95646896901473e-6
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.028640699757147e-6,
            "upper_bound": 0.00001685517995561284
          },
          "point_estimate": 7.744751563909622e-6,
          "standard_error": 3.9103508624196825e-6
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03067525524127937,
            "upper_bound": 0.03067909679061588
          },
          "point_estimate": 0.030676710488082553,
          "standard_error": 9.937632070950887e-7
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.5089310537194825e-6,
            "upper_bound": 0.000017236799921831975
          },
          "point_estimate": 0.000012059642029967306,
          "standard_error": 3.8376058543008565e-6
        }
      }
    },
    "memchr2/krate/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/common",
        "directory_name": "memchr2/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414420.1034523133,
            "upper_bound": 415148.8456093749
          },
          "point_estimate": 414771.3675229979,
          "standard_error": 186.38330448522572
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414285.8863636363,
            "upper_bound": 415090.90795454546
          },
          "point_estimate": 414773.7208806818,
          "standard_error": 212.82965569989824
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.75795022468084,
            "upper_bound": 1067.0306809144497
          },
          "point_estimate": 463.07016530158114,
          "standard_error": 241.08335731002657
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414306.45608108107,
            "upper_bound": 415050.9725983324
          },
          "point_estimate": 414698.0378984652,
          "standard_error": 190.0124575887176
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 283.26371756874505,
            "upper_bound": 839.5013349395392
          },
          "point_estimate": 621.0912505325321,
          "standard_error": 144.6947273167593
        }
      }
    },
    "memchr2/krate/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/never",
        "directory_name": "memchr2/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11587.46511611698,
            "upper_bound": 11601.644088780824
          },
          "point_estimate": 11594.922626943637,
          "standard_error": 3.6357162574018376
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11584.523824708454,
            "upper_bound": 11604.402710459184
          },
          "point_estimate": 11600.105771683671,
          "standard_error": 5.770013257400496
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6727944792300045,
            "upper_bound": 19.87572768284978
          },
          "point_estimate": 11.463414774163136,
          "standard_error": 4.848926362488524
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11582.828302256175,
            "upper_bound": 11600.60180052518
          },
          "point_estimate": 11592.957725119268,
          "standard_error": 4.614410162312786
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.75661519775769,
            "upper_bound": 15.042671920218666
          },
          "point_estimate": 12.11070728668507,
          "standard_error": 2.173325782321359
        }
      }
    },
    "memchr2/krate/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/rare",
        "directory_name": "memchr2/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15337.566672121124,
            "upper_bound": 15367.261650995855
          },
          "point_estimate": 15352.041520304654,
          "standard_error": 7.627568438497556
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15331.619851380045,
            "upper_bound": 15372.021125265392
          },
          "point_estimate": 15351.977062481044,
          "standard_error": 11.802251461459123
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.0836213104964925,
            "upper_bound": 45.930962923415315
          },
          "point_estimate": 28.995267093158525,
          "standard_error": 10.41493668867801
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15334.730807895168,
            "upper_bound": 15378.859483232613
          },
          "point_estimate": 15355.1183014862,
          "standard_error": 12.398455689998716
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.498957476152023,
            "upper_bound": 32.03657305962649
          },
          "point_estimate": 25.37985390061648,
          "standard_error": 4.570078236126122
        }
      }
    },
    "memchr2/krate/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/uncommon",
        "directory_name": "memchr2/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162336.30555345182,
            "upper_bound": 162831.96687044273
          },
          "point_estimate": 162568.6070838648,
          "standard_error": 127.56487040599328
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162207.47857142857,
            "upper_bound": 162899.76488095237
          },
          "point_estimate": 162460.10453869047,
          "standard_error": 180.8522285388838
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.81037495528307,
            "upper_bound": 727.0233433427186
          },
          "point_estimate": 390.99371399597646,
          "standard_error": 172.3853795331463
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162365.59525593408,
            "upper_bound": 162813.8678632639
          },
          "point_estimate": 162632.2303919295,
          "standard_error": 110.8557581334143
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195.18314831145457,
            "upper_bound": 552.3211584274668
          },
          "point_estimate": 426.1382504344851,
          "standard_error": 93.10569675955196
        }
      }
    },
    "memchr2/krate/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/common",
        "directory_name": "memchr2/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 411.9662547230246,
            "upper_bound": 413.2096391171905
          },
          "point_estimate": 412.57319311143453,
          "standard_error": 0.31751357867863267
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 411.65214355568696,
            "upper_bound": 413.69653619595624
          },
          "point_estimate": 412.0945232720781,
          "standard_error": 0.6141938319331222
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.16091594110373003,
            "upper_bound": 1.662202188103525
          },
          "point_estimate": 0.8378659755669937,
          "standard_error": 0.4404732579219493
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 411.6719538440027,
            "upper_bound": 412.7397539943479
          },
          "point_estimate": 412.03537004967785,
          "standard_error": 0.2771880388459755
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.653640047843398,
            "upper_bound": 1.208437780371823
          },
          "point_estimate": 1.056252221702374,
          "standard_error": 0.14467159870541738
        }
      }
    },
    "memchr2/krate/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/never",
        "directory_name": "memchr2/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.934299175786098,
            "upper_bound": 11.948863612283612
          },
          "point_estimate": 11.94087815611772,
          "standard_error": 0.003743839679763084
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.931974611930803,
            "upper_bound": 11.949837390546849
          },
          "point_estimate": 11.936612809113846,
          "standard_error": 0.004166909180992919
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007252808048865306,
            "upper_bound": 0.018099073409589893
          },
          "point_estimate": 0.007435510873375892,
          "standard_error": 0.004399750786271754
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.933035431511788,
            "upper_bound": 11.94053776855052
          },
          "point_estimate": 11.935781689979192,
          "standard_error": 0.0019210478698770824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0040908658884579625,
            "upper_bound": 0.016215028632036704
          },
          "point_estimate": 0.01251623714755399,
          "standard_error": 0.003222809763050759
        }
      }
    },
    "memchr2/krate/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/rare",
        "directory_name": "memchr2/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.299256736662382,
            "upper_bound": 23.316197326850904
          },
          "point_estimate": 23.30737181170244,
          "standard_error": 0.004328841940207132
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.29397827804209,
            "upper_bound": 23.318821007592117
          },
          "point_estimate": 23.304665322719835,
          "standard_error": 0.006476602884148443
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00228791592747503,
            "upper_bound": 0.02468651655990313
          },
          "point_estimate": 0.016242003725088796,
          "standard_error": 0.0059684854849500904
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.29906467732107,
            "upper_bound": 23.312353479647207
          },
          "point_estimate": 23.30543241121739,
          "standard_error": 0.003365455039398773
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007516178579176273,
            "upper_bound": 0.017701196584539943
          },
          "point_estimate": 0.014394363182065993,
          "standard_error": 0.0025849820816962127
        }
      }
    },
    "memchr2/krate/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/uncommon",
        "directory_name": "memchr2/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.10591358556272,
            "upper_bound": 108.40598999571486
          },
          "point_estimate": 108.2749161506194,
          "standard_error": 0.07814573280230656
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.1490339278774,
            "upper_bound": 108.44656619781202
          },
          "point_estimate": 108.3889572714998,
          "standard_error": 0.0869227583354193
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02983118696759985,
            "upper_bound": 0.3577708922863875
          },
          "point_estimate": 0.14920166695000706,
          "standard_error": 0.08715011914858332
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107.86650044228271,
            "upper_bound": 108.3389586224834
          },
          "point_estimate": 108.0860275513725,
          "standard_error": 0.12774371510842356
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09383282192132264,
            "upper_bound": 0.3697204494217239
          },
          "point_estimate": 0.26071605455120545,
          "standard_error": 0.08055189973300338
        }
      }
    },
    "memchr2/krate/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/never",
        "directory_name": "memchr2/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.414377519514809,
            "upper_bound": 4.425338656170608
          },
          "point_estimate": 4.420415404861452,
          "standard_error": 0.0028085659509231066
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.418191872416715,
            "upper_bound": 4.4267520920793055
          },
          "point_estimate": 4.420942104489432,
          "standard_error": 0.001773836224212876
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007461848208063846,
            "upper_bound": 0.013431447272828537
          },
          "point_estimate": 0.0026443489543986755,
          "standard_error": 0.0032859285009052986
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.419702546105126,
            "upper_bound": 4.423122010630224
          },
          "point_estimate": 4.421357682244972,
          "standard_error": 0.0008582417924383434
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025081907401612144,
            "upper_bound": 0.013395233484907742
          },
          "point_estimate": 0.00937366519232052,
          "standard_error": 0.0029642537855617285
        }
      }
    },
    "memchr2/krate/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/rare",
        "directory_name": "memchr2/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.669277363605476,
            "upper_bound": 10.69554723207848
          },
          "point_estimate": 10.6822324769544,
          "standard_error": 0.006746800518821835
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.662737726870358,
            "upper_bound": 10.70379437663415
          },
          "point_estimate": 10.678345958024613,
          "standard_error": 0.01071777941814049
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005587446618974764,
            "upper_bound": 0.0377732282420622
          },
          "point_estimate": 0.023952546041071505,
          "standard_error": 0.008416054234821597
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.65919661433058,
            "upper_bound": 10.690604877254376
          },
          "point_estimate": 10.670957738534176,
          "standard_error": 0.008094315383361991
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01351031499833423,
            "upper_bound": 0.02716401674050509
          },
          "point_estimate": 0.022511530636419197,
          "standard_error": 0.003440047844376129
        }
      }
    },
    "memchr2/krate/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/uncommon",
        "directory_name": "memchr2/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.555585572358694,
            "upper_bound": 55.642738224619656
          },
          "point_estimate": 55.596882335342286,
          "standard_error": 0.022410237245221783
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.532325563952355,
            "upper_bound": 55.66589161644658
          },
          "point_estimate": 55.5687252120418,
          "standard_error": 0.039102746657097565
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003988528012385123,
            "upper_bound": 0.13219621228021075
          },
          "point_estimate": 0.058796604852296014,
          "standard_error": 0.034899221357943024
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.53821817934769,
            "upper_bound": 55.603586932269245
          },
          "point_estimate": 55.56028070225939,
          "standard_error": 0.017001834045988734
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04155139605600877,
            "upper_bound": 0.0929186641496094
          },
          "point_estimate": 0.07462006297375737,
          "standard_error": 0.013782195843619742
        }
      }
    },
    "memchr2/naive/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/naive/empty/never",
        "directory_name": "memchr2/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6583716214109673,
            "upper_bound": 0.668508821369762
          },
          "point_estimate": 0.6631537759971097,
          "standard_error": 0.0026007042039418644
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6548807660163003,
            "upper_bound": 0.6708227162325413
          },
          "point_estimate": 0.6621819413161772,
          "standard_error": 0.0034861096737534942
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005024957497190192,
            "upper_bound": 0.015560331447981334
          },
          "point_estimate": 0.008860082545891622,
          "standard_error": 0.003836763493445877
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.659808815369731,
            "upper_bound": 0.6673664934808368
          },
          "point_estimate": 0.6635881382464023,
          "standard_error": 0.001884954013152285
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00408078754982173,
            "upper_bound": 0.010766869316700909
          },
          "point_estimate": 0.008687453293004799,
          "standard_error": 0.0017300607139893102
        }
      }
    },
    "memchr2/naive/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/common",
        "directory_name": "memchr2/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 857624.607784238,
            "upper_bound": 861367.1971807863
          },
          "point_estimate": 859405.3050230714,
          "standard_error": 955.3409687076768
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 857837.5206718347,
            "upper_bound": 860606.3709302326
          },
          "point_estimate": 859468.508513289,
          "standard_error": 649.0978154490035
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.21487027759292,
            "upper_bound": 4754.377391833304
          },
          "point_estimate": 1628.3364657617776,
          "standard_error": 1138.8699151250091
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 857942.7391370329,
            "upper_bound": 859792.4125128093
          },
          "point_estimate": 859000.8909090909,
          "standard_error": 465.74206342505875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 977.4294581076988,
            "upper_bound": 4501.993774465361
          },
          "point_estimate": 3181.186104418139,
          "standard_error": 912.2750899208426
        }
      }
    },
    "memchr2/naive/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/never",
        "directory_name": "memchr2/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219180.41440763057,
            "upper_bound": 219563.56654618477
          },
          "point_estimate": 219326.12949799196,
          "standard_error": 107.29420468146866
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219168.0843373494,
            "upper_bound": 219300.73383534135
          },
          "point_estimate": 219236.37424698795,
          "standard_error": 42.6976579441162
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.452000496433072,
            "upper_bound": 190.65253215738727
          },
          "point_estimate": 92.6174695906456,
          "standard_error": 51.09953514688022
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219190.03340519973,
            "upper_bound": 219275.90340755472
          },
          "point_estimate": 219229.71406665625,
          "standard_error": 21.766451795297716
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.76752554264039,
            "upper_bound": 549.4467010186
          },
          "point_estimate": 358.34092104324156,
          "standard_error": 170.26759942927094
        }
      }
    },
    "memchr2/naive/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/rare",
        "directory_name": "memchr2/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226603.21140625,
            "upper_bound": 228437.435125
          },
          "point_estimate": 227441.1895729167,
          "standard_error": 470.3306117576652
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226348.94875,
            "upper_bound": 228059.815625
          },
          "point_estimate": 227424.41328125,
          "standard_error": 411.79582320456785
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 197.99968210981305,
            "upper_bound": 2301.8374612592024
          },
          "point_estimate": 1180.63605716452,
          "standard_error": 542.5496089909747
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226666.35924997716,
            "upper_bound": 227793.8014224138
          },
          "point_estimate": 227240.48944805196,
          "standard_error": 298.9502992808258
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 606.8616238149066,
            "upper_bound": 2194.363497654502
          },
          "point_estimate": 1564.5775154444646,
          "standard_error": 444.0213386279946
        }
      }
    },
    "memchr2/naive/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/uncommon",
        "directory_name": "memchr2/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357207.29323897057,
            "upper_bound": 357967.427801208
          },
          "point_estimate": 357535.82055905694,
          "standard_error": 197.7967651387593
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357139.8778594771,
            "upper_bound": 357731.4931372549
          },
          "point_estimate": 357355.3090861344,
          "standard_error": 160.55362230621273
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.68283538037224,
            "upper_bound": 791.4001165870869
          },
          "point_estimate": 334.652031730266,
          "standard_error": 186.0231101335227
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357112.4071936056,
            "upper_bound": 357452.2870718368
          },
          "point_estimate": 357247.94733893557,
          "standard_error": 86.61025420672132
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205.5528067287855,
            "upper_bound": 947.0457425527908
          },
          "point_estimate": 658.2617995054028,
          "standard_error": 220.3160588987583
        }
      }
    },
    "memchr2/naive/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/common",
        "directory_name": "memchr2/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.9212718838101,
            "upper_bound": 336.08509156460195
          },
          "point_estimate": 336.00654765240563,
          "standard_error": 0.0419771281966702
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.91922446080525,
            "upper_bound": 336.10567651704804
          },
          "point_estimate": 336.0216566086588,
          "standard_error": 0.053468540579233365
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025487306288809564,
            "upper_bound": 0.2354770687258312
          },
          "point_estimate": 0.1277192497009357,
          "standard_error": 0.05170470829796906
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.94461323198254,
            "upper_bound": 336.0335239594591
          },
          "point_estimate": 335.9863709364823,
          "standard_error": 0.022723120168269516
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06923768962793268,
            "upper_bound": 0.18764888483252
          },
          "point_estimate": 0.1398274643795907,
          "standard_error": 0.03153894188930202
        }
      }
    },
    "memchr2/naive/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/never",
        "directory_name": "memchr2/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279.7305389729564,
            "upper_bound": 279.8955556487357
          },
          "point_estimate": 279.8027149222989,
          "standard_error": 0.042587006875854656
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279.7375293124593,
            "upper_bound": 279.85151432739985
          },
          "point_estimate": 279.7674832463581,
          "standard_error": 0.028759490066542347
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01228881330070873,
            "upper_bound": 0.1781183414342742
          },
          "point_estimate": 0.051626215120757206,
          "standard_error": 0.04238263523125357
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279.75285236325993,
            "upper_bound": 279.83082881956886
          },
          "point_estimate": 279.7896061513032,
          "standard_error": 0.020344436481443
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0395435969721457,
            "upper_bound": 0.20445042775934968
          },
          "point_estimate": 0.14108791728584635,
          "standard_error": 0.048592587248707526
        }
      }
    },
    "memchr2/naive/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/rare",
        "directory_name": "memchr2/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296.71228286961355,
            "upper_bound": 296.9388041982084
          },
          "point_estimate": 296.83217742165664,
          "standard_error": 0.05811273632681575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296.6741904372701,
            "upper_bound": 296.96504991534823
          },
          "point_estimate": 296.87870678381694,
          "standard_error": 0.06506828771744466
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02761471201486778,
            "upper_bound": 0.3225142679971674
          },
          "point_estimate": 0.13257704097826073,
          "standard_error": 0.08226920526304245
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296.7117795742889,
            "upper_bound": 296.9541334635997
          },
          "point_estimate": 296.8195713595763,
          "standard_error": 0.06281538083706724
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0883203290865143,
            "upper_bound": 0.2554457496577223
          },
          "point_estimate": 0.19419139712038705,
          "standard_error": 0.043616532129313546
        }
      }
    },
    "memchr2/naive/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/uncommon",
        "directory_name": "memchr2/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 375.020451426789,
            "upper_bound": 375.3165426988746
          },
          "point_estimate": 375.1562932485122,
          "standard_error": 0.07646241930822628
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 374.9689552061445,
            "upper_bound": 375.3140411134992
          },
          "point_estimate": 375.10255802474217,
          "standard_error": 0.08400537254162121
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022514272298646915,
            "upper_bound": 0.3833077601292757
          },
          "point_estimate": 0.18907940827176525,
          "standard_error": 0.09137432335235111
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 374.97605138563824,
            "upper_bound": 375.165908215578
          },
          "point_estimate": 375.0578992698837,
          "standard_error": 0.048464845684434495
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0947452629322726,
            "upper_bound": 0.33772217526604503
          },
          "point_estimate": 0.25520314428557583,
          "standard_error": 0.0646614203723962
        }
      }
    },
    "memchr2/naive/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/never",
        "directory_name": "memchr2/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.102408333733734,
            "upper_bound": 36.13044892343497
          },
          "point_estimate": 36.1164948219213,
          "standard_error": 0.007211709667286212
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.10069544014237,
            "upper_bound": 36.137800452365646
          },
          "point_estimate": 36.108250903030296,
          "standard_error": 0.011224244453554286
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012950414758761786,
            "upper_bound": 0.04454035285134558
          },
          "point_estimate": 0.03022528664668921,
          "standard_error": 0.012162189089124078
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.10175626016096,
            "upper_bound": 36.11854118466843
          },
          "point_estimate": 36.10911679503242,
          "standard_error": 0.004183717913299808
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014336153959085404,
            "upper_bound": 0.03049197321397307
          },
          "point_estimate": 0.02407173076945808,
          "standard_error": 0.004225495317110553
        }
      }
    },
    "memchr2/naive/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/rare",
        "directory_name": "memchr2/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.29455050565622,
            "upper_bound": 39.324816869372754
          },
          "point_estimate": 39.30870410938199,
          "standard_error": 0.0077831846567371025
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.28754785152979,
            "upper_bound": 39.32563359195099
          },
          "point_estimate": 39.30192203869757,
          "standard_error": 0.01064421270990062
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004703373764036203,
            "upper_bound": 0.04305672416506831
          },
          "point_estimate": 0.021787898521806848,
          "standard_error": 0.010123637286795445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.29127691894217,
            "upper_bound": 39.34222138650365
          },
          "point_estimate": 39.32185072487864,
          "standard_error": 0.013021254010279102
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01197920033145288,
            "upper_bound": 0.03316451153580383
          },
          "point_estimate": 0.025894364720056674,
          "standard_error": 0.005477167453816707
        }
      }
    },
    "memchr2/naive/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr2/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/uncommon",
        "directory_name": "memchr2/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.901984716602676,
            "upper_bound": 35.955351973317306
          },
          "point_estimate": 35.923564501241614,
          "standard_error": 0.01426228495607144
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.8949801412508,
            "upper_bound": 35.93077708917378
          },
          "point_estimate": 35.90850413828542,
          "standard_error": 0.010311023149777969
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004066903424582922,
            "upper_bound": 0.04254240743185683
          },
          "point_estimate": 0.021904041272883,
          "standard_error": 0.010027546688054789
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.89984373716194,
            "upper_bound": 35.923723227108
          },
          "point_estimate": 35.91201368743838,
          "standard_error": 0.006233201212770375
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011581202970254554,
            "upper_bound": 0.07125773752127436
          },
          "point_estimate": 0.0474933107360168,
          "standard_error": 0.01930970964466724
        }
      }
    },
    "memchr3/fallback/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/fallback/empty/never",
        "directory_name": "memchr3/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.713082291793607,
            "upper_bound": 2.715747954464272
          },
          "point_estimate": 2.7143644747579265,
          "standard_error": 0.0006843351386612535
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7124110049442978,
            "upper_bound": 2.717023119349825
          },
          "point_estimate": 2.713584478334771,
          "standard_error": 0.0012283730552128916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003832701598239561,
            "upper_bound": 0.003636976933947346
          },
          "point_estimate": 0.002222714784218579,
          "standard_error": 0.0009546852831545322
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.712293934525484,
            "upper_bound": 2.7142967997133747
          },
          "point_estimate": 2.7130033122877433,
          "standard_error": 0.0005258062679172838
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012925023307748806,
            "upper_bound": 0.0026520159594647704
          },
          "point_estimate": 0.002287743503833002,
          "standard_error": 0.00033707506516920905
        }
      }
    },
    "memchr3/fallback/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/common",
        "directory_name": "memchr3/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2016427.5321457812,
            "upper_bound": 2018176.0323099412
          },
          "point_estimate": 2017309.1522347536,
          "standard_error": 448.84864384697414
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2015977.712865497,
            "upper_bound": 2018744.0657894737
          },
          "point_estimate": 2017189.2005012531,
          "standard_error": 772.6578830220599
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 315.20205493043943,
            "upper_bound": 2537.8619636282106
          },
          "point_estimate": 2132.536595034633,
          "standard_error": 624.0966232961448
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2015877.3365650969,
            "upper_bound": 2018057.7774287208
          },
          "point_estimate": 2016823.7547505123,
          "standard_error": 564.0063011028111
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 952.098439266464,
            "upper_bound": 1765.240597031324
          },
          "point_estimate": 1496.8133091119885,
          "standard_error": 207.07341083832793
        }
      }
    },
    "memchr3/fallback/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/never",
        "directory_name": "memchr3/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109761.23462703192,
            "upper_bound": 109849.7929797404
          },
          "point_estimate": 109804.95329126026,
          "standard_error": 22.487658141437095
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109761.48655335628,
            "upper_bound": 109842.77409638556
          },
          "point_estimate": 109804.24743975904,
          "standard_error": 18.39512410189277
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.704919957500827,
            "upper_bound": 127.74318053331244
          },
          "point_estimate": 45.55893764499065,
          "standard_error": 27.42628942278637
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109770.77446578768,
            "upper_bound": 109828.220641697
          },
          "point_estimate": 109802.88278047254,
          "standard_error": 14.943272478576674
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.522073290805313,
            "upper_bound": 104.04635889537178
          },
          "point_estimate": 74.94061747964861,
          "standard_error": 19.55976077132668
        }
      }
    },
    "memchr3/fallback/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/rare",
        "directory_name": "memchr3/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117527.17761519496,
            "upper_bound": 117924.52832659116
          },
          "point_estimate": 117745.22970026196,
          "standard_error": 101.25411261182198
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117519.50899214056,
            "upper_bound": 117953.78721682848
          },
          "point_estimate": 117910.18365695792,
          "standard_error": 107.80044829503848
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.627995159446597,
            "upper_bound": 471.2085532848501
          },
          "point_estimate": 76.91839018781812,
          "standard_error": 108.74901470579164
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117344.71260242374,
            "upper_bound": 117888.0725505452
          },
          "point_estimate": 117621.742495692,
          "standard_error": 134.59839383936625
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.13592692815779,
            "upper_bound": 422.6611328658514
          },
          "point_estimate": 338.57461025357543,
          "standard_error": 91.87055471712152
        }
      }
    },
    "memchr3/fallback/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/uncommon",
        "directory_name": "memchr3/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499753.5334246576,
            "upper_bound": 499988.004783105
          },
          "point_estimate": 499870.01355512056,
          "standard_error": 60.23973900021594
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499705.0376712329,
            "upper_bound": 500016.4726027397
          },
          "point_estimate": 499870.7751956947,
          "standard_error": 99.04224680748138
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.405926679960146,
            "upper_bound": 323.69930178747654
          },
          "point_estimate": 230.73639076660223,
          "standard_error": 77.17792219874943
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499808.3273476297,
            "upper_bound": 500091.466046586
          },
          "point_estimate": 499973.21106564667,
          "standard_error": 73.60676019527546
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122.94638908309554,
            "upper_bound": 249.77888928582135
          },
          "point_estimate": 200.8666195768391,
          "standard_error": 32.61448896461001
        }
      }
    },
    "memchr3/fallback/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/common",
        "directory_name": "memchr3/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 700.8675515725139,
            "upper_bound": 702.1233809392022
          },
          "point_estimate": 701.5691572553667,
          "standard_error": 0.32345979885940984
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 701.2480739896894,
            "upper_bound": 702.2864043302247
          },
          "point_estimate": 701.7142788184378,
          "standard_error": 0.28818285924648923
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09218081613747092,
            "upper_bound": 1.370628240712451
          },
          "point_estimate": 0.6657544030401495,
          "standard_error": 0.30832920070622555
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 701.4635253825844,
            "upper_bound": 702.3197430359759
          },
          "point_estimate": 701.826517067725,
          "standard_error": 0.2204513522468384
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3794870262314648,
            "upper_bound": 1.5505034727040936
          },
          "point_estimate": 1.07861994329583,
          "standard_error": 0.35059388522101137
        }
      }
    },
    "memchr3/fallback/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/never",
        "directory_name": "memchr3/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.88388990726915,
            "upper_bound": 133.05086327320203
          },
          "point_estimate": 132.95833469363205,
          "standard_error": 0.04343384920182364
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.8615456517683,
            "upper_bound": 133.02368388854134
          },
          "point_estimate": 132.90483315711876,
          "standard_error": 0.0429902921176353
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007535961814815963,
            "upper_bound": 0.203463300075641
          },
          "point_estimate": 0.07531798899360831,
          "standard_error": 0.049635494039399285
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.87046865340545,
            "upper_bound": 132.98982519525302
          },
          "point_estimate": 132.93311121729545,
          "standard_error": 0.029992934877513738
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04635916438710095,
            "upper_bound": 0.20006171870056844
          },
          "point_estimate": 0.1448122429152358,
          "standard_error": 0.04208142338000083
        }
      }
    },
    "memchr3/fallback/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/rare",
        "directory_name": "memchr3/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.82303112249437,
            "upper_bound": 157.1483386767824
          },
          "point_estimate": 156.98069702664662,
          "standard_error": 0.08270768884088779
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.7626427019108,
            "upper_bound": 157.17337283472898
          },
          "point_estimate": 156.96490625256422,
          "standard_error": 0.10905871969636706
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07887243231901014,
            "upper_bound": 0.46709777882303954
          },
          "point_estimate": 0.2687962786705801,
          "standard_error": 0.0984256768724526
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.7305929347876,
            "upper_bound": 156.97947895616997
          },
          "point_estimate": 156.83919189127326,
          "standard_error": 0.06356095305554138
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14863240750939155,
            "upper_bound": 0.3555936998686478
          },
          "point_estimate": 0.2753967700732159,
          "standard_error": 0.05353369814983107
        }
      }
    },
    "memchr3/fallback/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/uncommon",
        "directory_name": "memchr3/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263.20327765065207,
            "upper_bound": 263.46249898788784
          },
          "point_estimate": 263.33547711458266,
          "standard_error": 0.06633161511295825
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263.2029915179962,
            "upper_bound": 263.4994692208452
          },
          "point_estimate": 263.3201078824585,
          "standard_error": 0.0894454671610375
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01399983327428107,
            "upper_bound": 0.3760844858154264
          },
          "point_estimate": 0.2534948503119326,
          "standard_error": 0.08814344012959612
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263.1601480544599,
            "upper_bound": 263.419079034445
          },
          "point_estimate": 263.286603265412,
          "standard_error": 0.06741220631902348
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11485403962776176,
            "upper_bound": 0.2890821182439096
          },
          "point_estimate": 0.22018461699865843,
          "standard_error": 0.04530133365935857
        }
      }
    },
    "memchr3/fallback/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/never",
        "directory_name": "memchr3/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.437196778499544,
            "upper_bound": 17.44704075276166
          },
          "point_estimate": 17.441766711451308,
          "standard_error": 0.002513290937722542
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.437655477190436,
            "upper_bound": 17.445039698449275
          },
          "point_estimate": 17.43994072280819,
          "standard_error": 0.0022509131783082868
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010915130158578978,
            "upper_bound": 0.012385233240180126
          },
          "point_estimate": 0.0052484776127724635,
          "standard_error": 0.0028067045043554655
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.43911056312763,
            "upper_bound": 17.443691350885153
          },
          "point_estimate": 17.441584319542798,
          "standard_error": 0.0011635249157571042
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002926139528289202,
            "upper_bound": 0.011910507216243052
          },
          "point_estimate": 0.008356636827231968,
          "standard_error": 0.002445311419866781
        }
      }
    },
    "memchr3/fallback/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/rare",
        "directory_name": "memchr3/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.994907567285026,
            "upper_bound": 31.072059155661098
          },
          "point_estimate": 31.032755778965058,
          "standard_error": 0.019647622280069457
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.985191480638395,
            "upper_bound": 31.107185120965234
          },
          "point_estimate": 31.001200944697608,
          "standard_error": 0.03876018367744315
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006031576773452081,
            "upper_bound": 0.09466483649220749
          },
          "point_estimate": 0.06043005790669313,
          "standard_error": 0.0295159464512332
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.002097232483973,
            "upper_bound": 31.09283428279849
          },
          "point_estimate": 31.055464893510752,
          "standard_error": 0.023329309411334345
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.042104823219718375,
            "upper_bound": 0.07605879990195782
          },
          "point_estimate": 0.06564750137811085,
          "standard_error": 0.008609670249694137
        }
      }
    },
    "memchr3/fallback/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/uncommon",
        "directory_name": "memchr3/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.26602917378351,
            "upper_bound": 88.4645369323512
          },
          "point_estimate": 88.3620964327653,
          "standard_error": 0.05082694555523764
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.23359836482535,
            "upper_bound": 88.45311630613406
          },
          "point_estimate": 88.35484984278256,
          "standard_error": 0.04822198598903094
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02938631937697329,
            "upper_bound": 0.2911827719990143
          },
          "point_estimate": 0.09995737021935291,
          "standard_error": 0.06826113646315118
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.20582275514579,
            "upper_bound": 88.4338361079151
          },
          "point_estimate": 88.3279496042088,
          "standard_error": 0.05825694859916596
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07685669337228032,
            "upper_bound": 0.22874355180271924
          },
          "point_estimate": 0.16969027093660588,
          "standard_error": 0.03933379325460897
        }
      }
    },
    "memchr3/krate/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/krate/empty/never",
        "directory_name": "memchr3/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03068763989592576,
            "upper_bound": 0.030718969963749385
          },
          "point_estimate": 0.030701918848123268,
          "standard_error": 8.070485010820347e-6
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03068261903955611,
            "upper_bound": 0.030716635023913114
          },
          "point_estimate": 0.030693126853673473,
          "standard_error": 8.659985426915242e-6
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.603171411904677e-6,
            "upper_bound": 0.00004180355594686595
          },
          "point_estimate": 0.000019725772635675452,
          "standard_error": 9.86854270374223e-6
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030685223002666837,
            "upper_bound": 0.030702923670211227
          },
          "point_estimate": 0.03069406958831394,
          "standard_error": 4.448929123400882e-6
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00001043472002309107,
            "upper_bound": 0.00003573592273752576
          },
          "point_estimate": 0.00002693423969432742,
          "standard_error": 6.671354755909137e-6
        }
      }
    },
    "memchr3/krate/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/common",
        "directory_name": "memchr3/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629441.0561901683,
            "upper_bound": 630523.8865258622
          },
          "point_estimate": 630006.0951765189,
          "standard_error": 276.5768012665277
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629130.9655172414,
            "upper_bound": 630796.158045977
          },
          "point_estimate": 630224.0813218391,
          "standard_error": 386.71379734016335
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 251.5861386368737,
            "upper_bound": 1488.268362371085
          },
          "point_estimate": 908.9505173110888,
          "standard_error": 351.5981951895985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629193.6198860316,
            "upper_bound": 630521.9171690268
          },
          "point_estimate": 629931.4465293328,
          "standard_error": 338.7056836195217
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 455.1431492224207,
            "upper_bound": 1146.4559489915011
          },
          "point_estimate": 926.6939625877042,
          "standard_error": 168.8245999082391
        }
      }
    },
    "memchr3/krate/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/never",
        "directory_name": "memchr3/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15099.311558108697,
            "upper_bound": 15121.746903113004
          },
          "point_estimate": 15110.089530984049,
          "standard_error": 5.729184267908892
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15095.577956151288,
            "upper_bound": 15123.35082432807
          },
          "point_estimate": 15108.870647785296,
          "standard_error": 6.427742070396944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.454362551841467,
            "upper_bound": 33.03310229696276
          },
          "point_estimate": 15.390878953688697,
          "standard_error": 7.83381706153795
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15096.798097970515,
            "upper_bound": 15114.09792803043
          },
          "point_estimate": 15105.641974069156,
          "standard_error": 4.396916809366424
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.716576063034164,
            "upper_bound": 24.7537008104458
          },
          "point_estimate": 19.009644548309375,
          "standard_error": 3.9327696867630633
        }
      }
    },
    "memchr3/krate/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/rare",
        "directory_name": "memchr3/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19960.19199536783,
            "upper_bound": 19972.054252747253
          },
          "point_estimate": 19966.259684676435,
          "standard_error": 3.0533187012337115
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19958.655082417583,
            "upper_bound": 19971.64411630037
          },
          "point_estimate": 19968.549102564102,
          "standard_error": 2.517982725427031
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.18326130444095667,
            "upper_bound": 17.55424391270434
          },
          "point_estimate": 5.223205138038846,
          "standard_error": 5.032477546728353
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19957.648236043995,
            "upper_bound": 19977.16383365798
          },
          "point_estimate": 19966.98099614671,
          "standard_error": 5.1546240595390165
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.964651814731204,
            "upper_bound": 13.437419302282292
          },
          "point_estimate": 10.162572255777034,
          "standard_error": 2.3107629663027125
        }
      }
    },
    "memchr3/krate/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/uncommon",
        "directory_name": "memchr3/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218716.807510283,
            "upper_bound": 219228.876497006
          },
          "point_estimate": 218954.56396088775,
          "standard_error": 131.26540506831054
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218628.72275449103,
            "upper_bound": 219223.70188195037
          },
          "point_estimate": 218822.73353293416,
          "standard_error": 165.89063106157943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.20997025091377,
            "upper_bound": 723.418035859347
          },
          "point_estimate": 413.8008817553486,
          "standard_error": 155.53095767577236
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218696.10408801335,
            "upper_bound": 219143.12368403075
          },
          "point_estimate": 218903.72755268685,
          "standard_error": 113.79145793083792
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 209.0447270836271,
            "upper_bound": 580.9929560853075
          },
          "point_estimate": 437.8022766453736,
          "standard_error": 102.62960009503308
        }
      }
    },
    "memchr3/krate/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/common",
        "directory_name": "memchr3/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 627.28053290059,
            "upper_bound": 629.3534329926858
          },
          "point_estimate": 628.374269479553,
          "standard_error": 0.5329430185740706
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 626.8940679064623,
            "upper_bound": 629.7117017231546
          },
          "point_estimate": 629.287643236143,
          "standard_error": 0.7772226759747283
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05185516767892075,
            "upper_bound": 2.8259433659369857
          },
          "point_estimate": 1.259869173609324,
          "standard_error": 0.7597918072277833
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 626.1393547026997,
            "upper_bound": 628.8184769569731
          },
          "point_estimate": 627.0454774178966,
          "standard_error": 0.6730339283472657
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7736121746612366,
            "upper_bound": 2.1301752275346404
          },
          "point_estimate": 1.77119397026056,
          "standard_error": 0.3303372604356907
        }
      }
    },
    "memchr3/krate/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/never",
        "directory_name": "memchr3/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.920521891789098,
            "upper_bound": 14.95779224972066
          },
          "point_estimate": 14.941732893386083,
          "standard_error": 0.009627454911270091
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.937256876242916,
            "upper_bound": 14.954996105068073
          },
          "point_estimate": 14.946482711143716,
          "standard_error": 0.004662988069938421
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002047720517632972,
            "upper_bound": 0.03498147531683861
          },
          "point_estimate": 0.00952736075724338,
          "standard_error": 0.008003355796044451
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.94034160995375,
            "upper_bound": 14.94964543957119
          },
          "point_estimate": 14.944854149208258,
          "standard_error": 0.002328322056976903
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006690953252692991,
            "upper_bound": 0.04716880696858629
          },
          "point_estimate": 0.032111176223747935,
          "standard_error": 0.011918217403104968
        }
      }
    },
    "memchr3/krate/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/rare",
        "directory_name": "memchr3/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.234295885552484,
            "upper_bound": 33.34521693625247
          },
          "point_estimate": 33.28615530502409,
          "standard_error": 0.02839378790969352
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.21926846749743,
            "upper_bound": 33.338190342898415
          },
          "point_estimate": 33.28042897065008,
          "standard_error": 0.02612501882058608
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009804400340301162,
            "upper_bound": 0.14911756205165658
          },
          "point_estimate": 0.05654244401711832,
          "standard_error": 0.037985341620697104
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.214512839416635,
            "upper_bound": 33.29619019514861
          },
          "point_estimate": 33.26054257368475,
          "standard_error": 0.021268562004116035
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04141404148433502,
            "upper_bound": 0.12832190888244355
          },
          "point_estimate": 0.09460717997934832,
          "standard_error": 0.023838004035490985
        }
      }
    },
    "memchr3/krate/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/uncommon",
        "directory_name": "memchr3/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.72436521025705,
            "upper_bound": 173.40378531097144
          },
          "point_estimate": 173.08153394748055,
          "standard_error": 0.1736538695720557
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.53709259100535,
            "upper_bound": 173.60402030841448
          },
          "point_estimate": 173.2453617991867,
          "standard_error": 0.2755516669769865
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1220411620540259,
            "upper_bound": 0.9568323751032488
          },
          "point_estimate": 0.5675794941439559,
          "standard_error": 0.22175141112556115
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.10508411933012,
            "upper_bound": 173.52278903406167
          },
          "point_estimate": 173.3370662136926,
          "standard_error": 0.10590410256307792
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.31983476275918454,
            "upper_bound": 0.7132232181268783
          },
          "point_estimate": 0.5806581485132766,
          "standard_error": 0.10072520813515085
        }
      }
    },
    "memchr3/krate/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/never",
        "directory_name": "memchr3/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.149258550670119,
            "upper_bound": 5.16765599030729
          },
          "point_estimate": 5.157815580288014,
          "standard_error": 0.0047143094910184835
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.146355450334681,
            "upper_bound": 5.168432677384809
          },
          "point_estimate": 5.156451020949817,
          "standard_error": 0.005243657173294353
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011727142202640115,
            "upper_bound": 0.02520286499679135
          },
          "point_estimate": 0.01428135510940707,
          "standard_error": 0.006364662373116601
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.150053109858807,
            "upper_bound": 5.173143425881271
          },
          "point_estimate": 5.159144600683967,
          "standard_error": 0.005877724968362093
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006421494149785211,
            "upper_bound": 0.020479177585696604
          },
          "point_estimate": 0.015678074518165167,
          "standard_error": 0.003677079436412776
        }
      }
    },
    "memchr3/krate/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/rare",
        "directory_name": "memchr3/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.784380866669288,
            "upper_bound": 19.821783675330128
          },
          "point_estimate": 19.801421656110676,
          "standard_error": 0.009653964177286155
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.780318752038177,
            "upper_bound": 19.83062374270775
          },
          "point_estimate": 19.784893938553022,
          "standard_error": 0.012570767781879369
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010967229163357404,
            "upper_bound": 0.05838841480509033
          },
          "point_estimate": 0.00876228770612804,
          "standard_error": 0.013181256374227191
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.78062532454711,
            "upper_bound": 19.84497484538865
          },
          "point_estimate": 19.815421674602863,
          "standard_error": 0.01750344938199645
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0056690056343345115,
            "upper_bound": 0.04274486701598653
          },
          "point_estimate": 0.03225042446226193,
          "standard_error": 0.008405740393826619
        }
      }
    },
    "memchr3/krate/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/uncommon",
        "directory_name": "memchr3/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.95911753062109,
            "upper_bound": 82.18572323691693
          },
          "point_estimate": 82.08617003864009,
          "standard_error": 0.0585170795960354
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.99487707178233,
            "upper_bound": 82.20063147719212
          },
          "point_estimate": 82.15020620303937,
          "standard_error": 0.048395022622524306
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014784213812057924,
            "upper_bound": 0.2695822430116259
          },
          "point_estimate": 0.07551023710169409,
          "standard_error": 0.06488132050947241
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.92065199359517,
            "upper_bound": 82.18184623033527
          },
          "point_estimate": 82.08417787240724,
          "standard_error": 0.06732054381054309
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05522486760763184,
            "upper_bound": 0.2708674864442894
          },
          "point_estimate": 0.1953678573131237,
          "standard_error": 0.05904027290457481
        }
      }
    },
    "memchr3/naive/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/naive/empty/never",
        "directory_name": "memchr3/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7119869903318532,
            "upper_bound": 0.7425493853076597
          },
          "point_estimate": 0.728583496945418,
          "standard_error": 0.007870962856293023
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7141008972644127,
            "upper_bound": 0.7450716290490129
          },
          "point_estimate": 0.733800286033129,
          "standard_error": 0.007724540673932876
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002812123840650102,
            "upper_bound": 0.040853960891284104
          },
          "point_estimate": 0.017018526623915554,
          "standard_error": 0.009256539890467038
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7161730948113099,
            "upper_bound": 0.7413732556347036
          },
          "point_estimate": 0.7313770529347694,
          "standard_error": 0.006611300197525922
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009898063088005985,
            "upper_bound": 0.03587267468496934
          },
          "point_estimate": 0.026219069406099055,
          "standard_error": 0.00701768821598067
        }
      }
    },
    "memchr3/naive/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/common",
        "directory_name": "memchr3/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1160498.126049107,
            "upper_bound": 1161960.419435826
          },
          "point_estimate": 1161217.1084449403,
          "standard_error": 374.3368293499227
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1159987.10625,
            "upper_bound": 1162224.1328125
          },
          "point_estimate": 1161396.5171875,
          "standard_error": 629.3054137197275
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263.844881253317,
            "upper_bound": 2243.9451754746133
          },
          "point_estimate": 1423.9677778445184,
          "standard_error": 509.4590047941618
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1160349.6273976122,
            "upper_bound": 1161789.7298029675
          },
          "point_estimate": 1161153.2957792208,
          "standard_error": 371.2037986808341
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 775.1851102602736,
            "upper_bound": 1552.9571707811665
          },
          "point_estimate": 1249.8087254311367,
          "standard_error": 203.6711159806329
        }
      }
    },
    "memchr3/naive/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/never",
        "directory_name": "memchr3/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292235.79246,
            "upper_bound": 292440.0356088889
          },
          "point_estimate": 292334.993275873,
          "standard_error": 52.38780414560831
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292144.52,
            "upper_bound": 292501.424
          },
          "point_estimate": 292325.2668,
          "standard_error": 82.31489446044947
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6803289524452234,
            "upper_bound": 296.26119867362706
          },
          "point_estimate": 264.57293050287706,
          "standard_error": 75.30713487720148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292175.2654814815,
            "upper_bound": 292477.3589123903
          },
          "point_estimate": 292315.3626181818,
          "standard_error": 80.17399262363851
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.86014974261796,
            "upper_bound": 214.3872732631144
          },
          "point_estimate": 174.8253792682075,
          "standard_error": 28.97886322807669
        }
      }
    },
    "memchr3/naive/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/rare",
        "directory_name": "memchr3/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296502.96181797655,
            "upper_bound": 296937.4298468835
          },
          "point_estimate": 296694.1782794554,
          "standard_error": 112.77445703473586
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296470.5753387534,
            "upper_bound": 296844.48799845146
          },
          "point_estimate": 296575.42530487804,
          "standard_error": 87.27887131169071
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.807184983370654,
            "upper_bound": 490.3981525945057
          },
          "point_estimate": 158.19832236074654,
          "standard_error": 122.3808279918404
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296515.3431530405,
            "upper_bound": 296647.38293802686
          },
          "point_estimate": 296563.84305775526,
          "standard_error": 33.667376448848685
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.66854876756204,
            "upper_bound": 525.7757813796596
          },
          "point_estimate": 374.6368144879156,
          "standard_error": 117.1931036551776
        }
      }
    },
    "memchr3/naive/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/uncommon",
        "directory_name": "memchr3/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464672.2471304501,
            "upper_bound": 465012.16686482826
          },
          "point_estimate": 464824.92310327507,
          "standard_error": 87.60791753391054
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464605.5297468354,
            "upper_bound": 464969.67088607594
          },
          "point_estimate": 464756.9193037974,
          "standard_error": 86.12313282397062
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.127022193556066,
            "upper_bound": 428.134121006723
          },
          "point_estimate": 194.1191649081071,
          "standard_error": 97.787233381606
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464581.4553874653,
            "upper_bound": 464809.90002483974
          },
          "point_estimate": 464664.42590826895,
          "standard_error": 58.7187243967358
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.76187938168805,
            "upper_bound": 397.0190180535821
          },
          "point_estimate": 291.16403496956605,
          "standard_error": 79.95405009038032
        }
      }
    },
    "memchr3/naive/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/common",
        "directory_name": "memchr3/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 424.39647742543104,
            "upper_bound": 434.95944339640096
          },
          "point_estimate": 429.9007318055189,
          "standard_error": 2.7112102802201044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 421.86891704089953,
            "upper_bound": 438.1510445978385
          },
          "point_estimate": 431.1665541734782,
          "standard_error": 3.948139147938796
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6971405070422867,
            "upper_bound": 15.812591885073733
          },
          "point_estimate": 9.371397245100663,
          "standard_error": 3.744278236361821
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 422.6481124656347,
            "upper_bound": 431.3213429950075
          },
          "point_estimate": 427.7602581323026,
          "standard_error": 2.2079199504911156
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.759300903096171,
            "upper_bound": 11.110645907450987
          },
          "point_estimate": 9.027838540030332,
          "standard_error": 1.6193586565813616
        }
      }
    },
    "memchr3/naive/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/never",
        "directory_name": "memchr3/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.45724834288603,
            "upper_bound": 335.7526017880986
          },
          "point_estimate": 335.6024916471305,
          "standard_error": 0.07585733609404863
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.4321380884696,
            "upper_bound": 335.8454003839551
          },
          "point_estimate": 335.5624419298667,
          "standard_error": 0.09446223348029648
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0405487019549258,
            "upper_bound": 0.4557491510379035
          },
          "point_estimate": 0.2539745781245101,
          "standard_error": 0.11453074890261448
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.42208027783977,
            "upper_bound": 335.6886608373132
          },
          "point_estimate": 335.5467374963471,
          "standard_error": 0.06802826734053811
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.136452714433574,
            "upper_bound": 0.3170731390401402
          },
          "point_estimate": 0.25276220395732074,
          "standard_error": 0.04515832576136987
        }
      }
    },
    "memchr3/naive/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/rare",
        "directory_name": "memchr3/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 362.42528741167314,
            "upper_bound": 362.686948483904
          },
          "point_estimate": 362.564348593373,
          "standard_error": 0.06691292288617254
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 362.4688190576772,
            "upper_bound": 362.6846461556855
          },
          "point_estimate": 362.5918093662955,
          "standard_error": 0.05430755829323713
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03801665277385779,
            "upper_bound": 0.33015130779124013
          },
          "point_estimate": 0.12584118531707114,
          "standard_error": 0.07186845537087666
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 362.4924283563384,
            "upper_bound": 362.61479003494054
          },
          "point_estimate": 362.5500991730126,
          "standard_error": 0.0310099363104203
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07810837535443649,
            "upper_bound": 0.3146620243853338
          },
          "point_estimate": 0.2221079999607536,
          "standard_error": 0.06291359559693688
        }
      }
    },
    "memchr3/naive/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/uncommon",
        "directory_name": "memchr3/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 411.2590522413492,
            "upper_bound": 411.68868003846586
          },
          "point_estimate": 411.4571701130928,
          "standard_error": 0.11060104798269715
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 411.1296626186946,
            "upper_bound": 411.7042922462283
          },
          "point_estimate": 411.3992554510553,
          "standard_error": 0.13304751809736806
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04420110693734629,
            "upper_bound": 0.5707215102339411
          },
          "point_estimate": 0.30630477086722196,
          "standard_error": 0.14266893649796314
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 411.1599304306792,
            "upper_bound": 411.6589525493081
          },
          "point_estimate": 411.3964158389908,
          "standard_error": 0.1384671160323145
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1659024126114793,
            "upper_bound": 0.4880012975533363
          },
          "point_estimate": 0.368280820227214,
          "standard_error": 0.08698920909408156
        }
      }
    },
    "memchr3/naive/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/never",
        "directory_name": "memchr3/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.25496297871645,
            "upper_bound": 44.3135634737209
          },
          "point_estimate": 44.28585173725903,
          "standard_error": 0.0150625544103212
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.255006342027976,
            "upper_bound": 44.31825634524469
          },
          "point_estimate": 44.292213135094045,
          "standard_error": 0.016543424343843788
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004481865472137374,
            "upper_bound": 0.08251283505197235
          },
          "point_estimate": 0.03221873927246423,
          "standard_error": 0.01887160691433915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.26544306003541,
            "upper_bound": 44.316975246011864
          },
          "point_estimate": 44.29692883457562,
          "standard_error": 0.013280481973896929
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02134104889577188,
            "upper_bound": 0.06780487322091973
          },
          "point_estimate": 0.050313402388492225,
          "standard_error": 0.012303268719835612
        }
      }
    },
    "memchr3/naive/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/rare",
        "directory_name": "memchr3/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.92515517785518,
            "upper_bound": 43.98731840069458
          },
          "point_estimate": 43.955892169229834,
          "standard_error": 0.01587931518279243
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.92016149590612,
            "upper_bound": 44.00402783298438
          },
          "point_estimate": 43.94610693823077,
          "standard_error": 0.019965893086849217
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012343774734069224,
            "upper_bound": 0.08841366148901966
          },
          "point_estimate": 0.04420683074450983,
          "standard_error": 0.02224157598005072
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.89815069824007,
            "upper_bound": 43.998868404227075
          },
          "point_estimate": 43.94431673613453,
          "standard_error": 0.026273508725758853
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02609870343747205,
            "upper_bound": 0.06810540686539829
          },
          "point_estimate": 0.05294547832850042,
          "standard_error": 0.01040949309814715
        }
      }
    },
    "memchr3/naive/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memchr3/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/uncommon",
        "directory_name": "memchr3/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.98440890894571,
            "upper_bound": 43.06162331057682
          },
          "point_estimate": 43.02174701183056,
          "standard_error": 0.019813977170253787
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.964563965568416,
            "upper_bound": 43.08240397744138
          },
          "point_estimate": 43.008885722766394,
          "standard_error": 0.030359565173754845
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018990519520371733,
            "upper_bound": 0.11054557024282452
          },
          "point_estimate": 0.07500234559630373,
          "standard_error": 0.023961836325636407
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.99359337420364,
            "upper_bound": 43.05115864277138
          },
          "point_estimate": 43.02128511678289,
          "standard_error": 0.014458914897670096
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03762162885968416,
            "upper_bound": 0.08099478641391646
          },
          "point_estimate": 0.06613025625201849,
          "standard_error": 0.010976908943641731
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51880.72147091837,
            "upper_bound": 51938.6588091454
          },
          "point_estimate": 51909.30657568027,
          "standard_error": 14.831153304520136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51861.92214285714,
            "upper_bound": 51957.17476190476
          },
          "point_estimate": 51904.423190476184,
          "standard_error": 23.750259163042767
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.881675771303708,
            "upper_bound": 82.34029316316656
          },
          "point_estimate": 70.6107652464091,
          "standard_error": 18.517210336305094
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51876.87899159664,
            "upper_bound": 51935.72991077144
          },
          "point_estimate": 51907.43541001855,
          "standard_error": 14.982616204227307
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.38979707193573,
            "upper_bound": 59.21171768904464
          },
          "point_estimate": 49.296055203687565,
          "standard_error": 7.328675810844408
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52712.164070921,
            "upper_bound": 52881.35602690107
          },
          "point_estimate": 52786.70507486617,
          "standard_error": 43.93856097722061
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52685.90764292635,
            "upper_bound": 52835.51283914728
          },
          "point_estimate": 52747.37986111111,
          "standard_error": 34.24093357151198
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.455352001151866,
            "upper_bound": 184.17210283494424
          },
          "point_estimate": 81.6564682111568,
          "standard_error": 48.586652762866215
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52693.386761891765,
            "upper_bound": 52771.314031819835
          },
          "point_estimate": 52732.044140742975,
          "standard_error": 19.538901838906515
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.97655684172439,
            "upper_bound": 208.75136456751332
          },
          "point_estimate": 146.5854962856013,
          "standard_error": 46.85298586645053
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53344.67311936228,
            "upper_bound": 53430.69997727433
          },
          "point_estimate": 53387.31717222572,
          "standard_error": 21.95072318353578
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53344.616837983354,
            "upper_bound": 53424.615088105726
          },
          "point_estimate": 53389.50946612125,
          "standard_error": 19.73833873371187
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.621639956403298,
            "upper_bound": 120.2856290686138
          },
          "point_estimate": 44.69316851637871,
          "standard_error": 26.190326256488323
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53358.13301767109,
            "upper_bound": 53416.211721782856
          },
          "point_estimate": 53387.56373553025,
          "standard_error": 14.941528224106287
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.572140639228586,
            "upper_bound": 99.48124798313
          },
          "point_estimate": 73.0682449361631,
          "standard_error": 17.88278198862031
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15302.14440936911,
            "upper_bound": 15339.234610126154
          },
          "point_estimate": 15319.56184818002,
          "standard_error": 9.539096851200007
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15290.433866891322,
            "upper_bound": 15342.881142937378
          },
          "point_estimate": 15308.641638584668,
          "standard_error": 12.456110769395202
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.185148409879233,
            "upper_bound": 54.40257086264921
          },
          "point_estimate": 28.17902744833316,
          "standard_error": 13.183000602823414
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15298.476535127433,
            "upper_bound": 15328.38634081904
          },
          "point_estimate": 15312.90510837099,
          "standard_error": 7.74458899807145
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.43468216533579,
            "upper_bound": 39.05172903743517
          },
          "point_estimate": 31.89979641302408,
          "standard_error": 6.277002625694863
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26720.12562261467,
            "upper_bound": 26773.049805088696
          },
          "point_estimate": 26744.93191888422,
          "standard_error": 13.569255227740276
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26706.77022058823,
            "upper_bound": 26779.53936887255
          },
          "point_estimate": 26732.694038865546,
          "standard_error": 19.61700897806436
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.507816507608374,
            "upper_bound": 74.52381430193782
          },
          "point_estimate": 41.95991528447428,
          "standard_error": 18.293256828370684
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26710.84576330725,
            "upper_bound": 26746.61721087119
          },
          "point_estimate": 26725.189270435447,
          "standard_error": 9.206724799067228
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.38277486754059,
            "upper_bound": 55.38277863784878
          },
          "point_estimate": 45.18383758467906,
          "standard_error": 8.684186253632294
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16397.161424762624,
            "upper_bound": 16427.842277974847
          },
          "point_estimate": 16410.687615195817,
          "standard_error": 7.907969476817708
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16391.121380199937,
            "upper_bound": 16424.18409831954
          },
          "point_estimate": 16399.487669300222,
          "standard_error": 10.491004875169926
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3949814473251294,
            "upper_bound": 42.615597804748475
          },
          "point_estimate": 16.449934772282905,
          "standard_error": 9.741612433841578
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16394.568096709998,
            "upper_bound": 16416.326079104918
          },
          "point_estimate": 16404.23679868664,
          "standard_error": 5.7304091616472075
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.883525909928444,
            "upper_bound": 36.92806831089778
          },
          "point_estimate": 26.342226365897012,
          "standard_error": 7.592438392860928
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16705.518388102362,
            "upper_bound": 16748.44790259873
          },
          "point_estimate": 16725.91895681692,
          "standard_error": 11.004909732737223
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16695.02232965009,
            "upper_bound": 16756.185255524862
          },
          "point_estimate": 16717.9049756643,
          "standard_error": 15.517081992143885
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.621229775683174,
            "upper_bound": 59.64282287068771
          },
          "point_estimate": 34.48297991266805,
          "standard_error": 13.899399690279816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16706.889736177138,
            "upper_bound": 16731.349995789533
          },
          "point_estimate": 16719.602388127525,
          "standard_error": 6.2186392109341835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.646319432612692,
            "upper_bound": 46.133550445696414
          },
          "point_estimate": 36.562980948384975,
          "standard_error": 7.033357158734185
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19298.2101853752,
            "upper_bound": 19332.61544525053
          },
          "point_estimate": 19313.091902654494,
          "standard_error": 8.914626010074782
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19292.61120699829,
            "upper_bound": 19322.814073287307
          },
          "point_estimate": 19304.78066914498,
          "standard_error": 8.053012337514522
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8422597256560596,
            "upper_bound": 36.662920538691445
          },
          "point_estimate": 22.952100835218356,
          "standard_error": 9.006178654887142
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19291.02772261293,
            "upper_bound": 19311.771779823062
          },
          "point_estimate": 19299.934644219295,
          "standard_error": 5.3704485236238
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.326942600359097,
            "upper_bound": 42.95690134337848
          },
          "point_estimate": 29.670394784965456,
          "standard_error": 9.959296230706972
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39240.73123367059,
            "upper_bound": 39313.49804250874
          },
          "point_estimate": 39277.16371468854,
          "standard_error": 18.649197347984774
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39207.73902087833,
            "upper_bound": 39325.02240820734
          },
          "point_estimate": 39291.31406737461,
          "standard_error": 34.6350363601029
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.979187273493992,
            "upper_bound": 102.68445789621144
          },
          "point_estimate": 70.78518269579511,
          "standard_error": 26.704515461769336
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39239.48197731087,
            "upper_bound": 39310.42952364062
          },
          "point_estimate": 39281.41196600376,
          "standard_error": 18.094044897475744
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.00550368977194,
            "upper_bound": 73.97507166998712
          },
          "point_estimate": 62.07404081086733,
          "standard_error": 8.71742474736386
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16457.276580516864,
            "upper_bound": 16494.162121557696
          },
          "point_estimate": 16473.854279217507,
          "standard_error": 9.486201849628673
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16450.51050642479,
            "upper_bound": 16490.42869047619
          },
          "point_estimate": 16464.462937767697,
          "standard_error": 10.251823821108442
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.234064049178813,
            "upper_bound": 44.75056470022617
          },
          "point_estimate": 25.42822660940873,
          "standard_error": 10.0867964099263
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16460.22887166491,
            "upper_bound": 16481.17915145495
          },
          "point_estimate": 16471.55875253998,
          "standard_error": 5.374355530745451
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.22941158259419,
            "upper_bound": 43.94509759360238
          },
          "point_estimate": 31.658931970403003,
          "standard_error": 9.078620508089015
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20174.584036205637,
            "upper_bound": 20192.856615636814
          },
          "point_estimate": 20183.24204657025,
          "standard_error": 4.683839392601931
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20175.091474753885,
            "upper_bound": 20190.974347584677
          },
          "point_estimate": 20180.44697390339,
          "standard_error": 4.139158257743408
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4816199625907156,
            "upper_bound": 24.903928075361417
          },
          "point_estimate": 9.040210278147397,
          "standard_error": 5.614942372604662
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20175.39595409332,
            "upper_bound": 20186.49261521377
          },
          "point_estimate": 20180.719983847357,
          "standard_error": 2.911233719276965
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.834365902299598,
            "upper_bound": 21.586094530645664
          },
          "point_estimate": 15.639929000558316,
          "standard_error": 4.133542891218266
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16804.583908615336,
            "upper_bound": 16847.525816840276
          },
          "point_estimate": 16824.996892434596,
          "standard_error": 10.978451130107333
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16794.075,
            "upper_bound": 16853.146527777775
          },
          "point_estimate": 16814.7939686214,
          "standard_error": 15.333967952798664
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.418368409965233,
            "upper_bound": 59.9173303077816
          },
          "point_estimate": 38.7096299898496,
          "standard_error": 13.663276248139098
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16800.495060457055,
            "upper_bound": 16825.540016039908
          },
          "point_estimate": 16812.582650312652,
          "standard_error": 6.253165746885241
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.71361433852435,
            "upper_bound": 46.169213204331506
          },
          "point_estimate": 36.529469596966784,
          "standard_error": 7.110864427523533
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19359.466107011776,
            "upper_bound": 19420.615736236905
          },
          "point_estimate": 19388.096893683774,
          "standard_error": 15.741399985657742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19351.876960871366,
            "upper_bound": 19427.154102290893
          },
          "point_estimate": 19357.68733351092,
          "standard_error": 24.904630042249856
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.1295295626249255,
            "upper_bound": 97.45519826449872
          },
          "point_estimate": 21.688180759513653,
          "standard_error": 26.230789181970096
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19351.07832130576,
            "upper_bound": 19381.885633961538
          },
          "point_estimate": 19360.475883732677,
          "standard_error": 8.1197293586514
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.180257754181063,
            "upper_bound": 67.98842673049869
          },
          "point_estimate": 52.43474883267178,
          "standard_error": 11.447658282064443
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16740.032857544615,
            "upper_bound": 16779.556632767588
          },
          "point_estimate": 16759.435595256356,
          "standard_error": 10.11409613159211
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16733.091290842152,
            "upper_bound": 16780.727910722504
          },
          "point_estimate": 16761.90081556476,
          "standard_error": 11.046584686744444
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.097066359004738,
            "upper_bound": 61.80732772460576
          },
          "point_estimate": 27.42018314028789,
          "standard_error": 15.270368256143932
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16734.843648521797,
            "upper_bound": 16765.068039041515
          },
          "point_estimate": 16749.730109191314,
          "standard_error": 7.7161608698904764
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.619894846118186,
            "upper_bound": 44.07401530416896
          },
          "point_estimate": 33.62979965613205,
          "standard_error": 6.940720352380345
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16624.614300111243,
            "upper_bound": 16653.124199586087
          },
          "point_estimate": 16636.16485363087,
          "standard_error": 7.659186356584166
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16622.572148419604,
            "upper_bound": 16641.133665445108
          },
          "point_estimate": 16628.743526819795,
          "standard_error": 3.7675561873037062
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.962284774488252,
            "upper_bound": 18.50313178556981
          },
          "point_estimate": 4.811456213247849,
          "standard_error": 5.63682690543083
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16624.711744795644,
            "upper_bound": 16640.231901308962
          },
          "point_estimate": 16631.358270222678,
          "standard_error": 4.0955572679931915
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.8960972874557425,
            "upper_bound": 38.007283453718
          },
          "point_estimate": 25.538263959119437,
          "standard_error": 10.408704473185455
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16848.177096018135,
            "upper_bound": 16880.538740792254
          },
          "point_estimate": 16861.479284242516,
          "standard_error": 8.652185033893332
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16845.526796476588,
            "upper_bound": 16869.42669860403
          },
          "point_estimate": 16850.27561427909,
          "standard_error": 5.35361719377295
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5393838063145191,
            "upper_bound": 27.772305873810645
          },
          "point_estimate": 7.250340024268005,
          "standard_error": 6.404535168210492
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16846.50176844396,
            "upper_bound": 16852.80894735645
          },
          "point_estimate": 16849.07383149998,
          "standard_error": 1.653344477751281
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.476682615544404,
            "upper_bound": 42.34183111162103
          },
          "point_estimate": 28.940175033795732,
          "standard_error": 11.410616658130763
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19534.258897517386,
            "upper_bound": 19569.853226395506
          },
          "point_estimate": 19551.247262694997,
          "standard_error": 9.12810833648207
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19523.01964477933,
            "upper_bound": 19569.473223896664
          },
          "point_estimate": 19549.044422766412,
          "standard_error": 14.158940675058869
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.83284159880805,
            "upper_bound": 58.46702418266993
          },
          "point_estimate": 32.190990401322296,
          "standard_error": 12.592478868714675
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19532.65317131738,
            "upper_bound": 19561.73722692279
          },
          "point_estimate": 19547.75727985685,
          "standard_error": 7.5257611484537
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.896490576298714,
            "upper_bound": 39.820121297822624
          },
          "point_estimate": 30.425962742373866,
          "standard_error": 6.403407455764691
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17029.670515333244,
            "upper_bound": 17067.878510540442
          },
          "point_estimate": 17044.33665615042,
          "standard_error": 10.658358981990968
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17026.9605358818,
            "upper_bound": 17044.324577861164
          },
          "point_estimate": 17032.257813663302,
          "standard_error": 5.213523841576561
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7864188062256083,
            "upper_bound": 21.37252535696025
          },
          "point_estimate": 9.10556267644884,
          "standard_error": 5.779377900457638
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17027.94069322026,
            "upper_bound": 17034.360816862307
          },
          "point_estimate": 17030.444221632028,
          "standard_error": 1.6274146052943048
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.156072740547099,
            "upper_bound": 53.94831463661747
          },
          "point_estimate": 35.38652952334553,
          "standard_error": 16.4033951144772
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19858.639599866274,
            "upper_bound": 19901.378777837577
          },
          "point_estimate": 19876.75136916721,
          "standard_error": 11.194411725500816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19855.38287671233,
            "upper_bound": 19888.853808219177
          },
          "point_estimate": 19863.2997564688,
          "standard_error": 7.981936725022478
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.070177954627574,
            "upper_bound": 40.95661440712165
          },
          "point_estimate": 15.792418619171864,
          "standard_error": 9.873706306636032
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19856.87043328715,
            "upper_bound": 19866.7986510638
          },
          "point_estimate": 19861.593823163137,
          "standard_error": 2.5178231441500643
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.140777816460801,
            "upper_bound": 53.83671100331848
          },
          "point_estimate": 37.28773079143141,
          "standard_error": 13.28409316804575
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71752.28857715744,
            "upper_bound": 71877.77832937294
          },
          "point_estimate": 71812.17891570988,
          "standard_error": 32.17398266601887
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71713.54808959157,
            "upper_bound": 71873.94219367589
          },
          "point_estimate": 71814.91970794906,
          "standard_error": 45.4246399131954
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.452943265381844,
            "upper_bound": 196.61983146977593
          },
          "point_estimate": 107.7265637989314,
          "standard_error": 42.025205094239034
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71767.55234057408,
            "upper_bound": 71877.2931998529
          },
          "point_estimate": 71821.51110312612,
          "standard_error": 27.500985001767653
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.4624399959584,
            "upper_bound": 141.55170742231388
          },
          "point_estimate": 107.0090715668658,
          "standard_error": 23.322403543217103
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231758.5398397818,
            "upper_bound": 1234143.301583664
          },
          "point_estimate": 1232912.3464616402,
          "standard_error": 609.4193469667741
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231067.6,
            "upper_bound": 1234684.572222222
          },
          "point_estimate": 1232703.624074074,
          "standard_error": 1034.4480934019275
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.49405415349256,
            "upper_bound": 3543.3296583431693
          },
          "point_estimate": 2448.7348737482016,
          "standard_error": 855.4559980932618
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231542.436952433,
            "upper_bound": 1233876.5353806124
          },
          "point_estimate": 1232576.5032900432,
          "standard_error": 601.8691789309551
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1182.9205038133011,
            "upper_bound": 2510.1717612298035
          },
          "point_estimate": 2024.444366836863,
          "standard_error": 348.97516971698906
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203004.5458923694,
            "upper_bound": 203327.90334120335
          },
          "point_estimate": 203146.77542653185,
          "standard_error": 83.8804740487737
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202961.85288640595,
            "upper_bound": 203304.78184357545
          },
          "point_estimate": 203028.3066640064,
          "standard_error": 77.22533804114842
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.933752144675086,
            "upper_bound": 373.5302958824876
          },
          "point_estimate": 109.990489080795,
          "standard_error": 84.10294945888927
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202997.95106450288,
            "upper_bound": 203092.4768272812
          },
          "point_estimate": 203043.11350214033,
          "standard_error": 24.531576087384806
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.24394616511231,
            "upper_bound": 379.03273320506656
          },
          "point_estimate": 279.64283485042654,
          "standard_error": 85.75373882112885
        }
      }
    },
    "memmem/bstr/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/bstr_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6077.6107090598825,
            "upper_bound": 6091.596752912181
          },
          "point_estimate": 6084.2002326818665,
          "standard_error": 3.592496073315599
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6073.659683816547,
            "upper_bound": 6090.285642462113
          },
          "point_estimate": 6083.238036050127,
          "standard_error": 3.9054932029805047
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3304507043944467,
            "upper_bound": 19.443361521745324
          },
          "point_estimate": 12.32482292514891,
          "standard_error": 4.75631941817343
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6076.358160213501,
            "upper_bound": 6091.881008954337
          },
          "point_estimate": 6084.730275824393,
          "standard_error": 3.996673378927971
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.860799462093996,
            "upper_bound": 16.084341026132257
          },
          "point_estimate": 12.01516263641798,
          "standard_error": 2.810114094991399
        }
      }
    },
    "memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/bstr_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6145.936711361631,
            "upper_bound": 6164.687714787293
          },
          "point_estimate": 6154.869974026829,
          "standard_error": 4.817807074367919
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6140.422060318536,
            "upper_bound": 6169.454708149779
          },
          "point_estimate": 6152.875703151474,
          "standard_error": 6.6922439277025765
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.600320644561378,
            "upper_bound": 29.357450026637817
          },
          "point_estimate": 14.678725013318909,
          "standard_error": 6.8326779757396965
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6145.2256623987,
            "upper_bound": 6158.790313645836
          },
          "point_estimate": 6152.194580309559,
          "standard_error": 3.43694335633418
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.27013872119976,
            "upper_bound": 19.93772199610122
          },
          "point_estimate": 16.081556991058807,
          "standard_error": 2.9816977182011426
        }
      }
    },
    "memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/bstr_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15605.167152190474,
            "upper_bound": 15650.691306613306
          },
          "point_estimate": 15626.534932150704,
          "standard_error": 11.6939849063726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15595.979377362868,
            "upper_bound": 15659.793744625968
          },
          "point_estimate": 15619.06754621668,
          "standard_error": 13.748962623198912
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.229650821776365,
            "upper_bound": 68.17082458508271
          },
          "point_estimate": 27.83782232607148,
          "standard_error": 15.041291556599564
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15600.840313060442,
            "upper_bound": 15627.09408691051
          },
          "point_estimate": 15611.531717122089,
          "standard_error": 6.642499118660593
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.116398843525918,
            "upper_bound": 48.895688784984266
          },
          "point_estimate": 39.008210820992666,
          "standard_error": 8.021229530611855
        }
      }
    },
    "memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/bstr_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.607779289400646,
            "upper_bound": 56.73452464068955
          },
          "point_estimate": 56.163164548300486,
          "standard_error": 0.2887864030103647
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.38318164966269,
            "upper_bound": 57.00867288293082
          },
          "point_estimate": 55.93723358837149,
          "standard_error": 0.4556808563145485
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.25599941268162907,
            "upper_bound": 1.584966702800192
          },
          "point_estimate": 1.0940583602785294,
          "standard_error": 0.35086524440396216
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.61345479013352,
            "upper_bound": 56.72897205472389
          },
          "point_estimate": 56.14453101226451,
          "standard_error": 0.2835214422706578
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5855205527648218,
            "upper_bound": 1.176161711818993
          },
          "point_estimate": 0.9647717609225672,
          "standard_error": 0.15077653318596426
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.919958140573755,
            "upper_bound": 32.014462361252434
          },
          "point_estimate": 31.964457413162176,
          "standard_error": 0.024272445711699453
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.89720651714813,
            "upper_bound": 32.01908852929819
          },
          "point_estimate": 31.93955458057103,
          "standard_error": 0.031928949602847236
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011181665656123153,
            "upper_bound": 0.13404059192274984
          },
          "point_estimate": 0.08655127872390983,
          "standard_error": 0.030375776536539213
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.89736550350986,
            "upper_bound": 31.987575003169255
          },
          "point_estimate": 31.94278721942286,
          "standard_error": 0.023936541380191426
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04015053705597396,
            "upper_bound": 0.10578375472790769
          },
          "point_estimate": 0.08130709530420309,
          "standard_error": 0.017544621938692915
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.67150705217253,
            "upper_bound": 35.69604299919083
          },
          "point_estimate": 35.68173354947223,
          "standard_error": 0.006505408697782414
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.66862879184444,
            "upper_bound": 35.686719358200094
          },
          "point_estimate": 35.67561428362173,
          "standard_error": 0.004074590317072503
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019411048913503333,
            "upper_bound": 0.021698866025027484
          },
          "point_estimate": 0.007986592526873077,
          "standard_error": 0.005270973939645338
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.66889436167239,
            "upper_bound": 35.679719199191204
          },
          "point_estimate": 35.67321971290909,
          "standard_error": 0.002771568701184152
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004768962710476841,
            "upper_bound": 0.03198371256374812
          },
          "point_estimate": 0.021750871127599863,
          "standard_error": 0.008213695923221366
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.278538597160143,
            "upper_bound": 27.308810840108485
          },
          "point_estimate": 27.292763448909245,
          "standard_error": 0.0077164819036969964
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.27996935499193,
            "upper_bound": 27.30219025050016
          },
          "point_estimate": 27.291595663042862,
          "standard_error": 0.005344945318891492
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030768644513701404,
            "upper_bound": 0.03855727142893149
          },
          "point_estimate": 0.013065170979920774,
          "standard_error": 0.008996329167755251
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.28856789418319,
            "upper_bound": 27.308561675133948
          },
          "point_estimate": 27.29666647504404,
          "standard_error": 0.005075763462246962
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008455635104958217,
            "upper_bound": 0.03659766029250839
          },
          "point_estimate": 0.02578634204872972,
          "standard_error": 0.007373899550044685
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.38838695038136,
            "upper_bound": 20.4051627679891
          },
          "point_estimate": 20.395994821333836,
          "standard_error": 0.004305817030865804
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.385154404867254,
            "upper_bound": 20.403193721774755
          },
          "point_estimate": 20.392730750659233,
          "standard_error": 0.004156752847122678
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001916119515305492,
            "upper_bound": 0.02098578811910509
          },
          "point_estimate": 0.012081199627814667,
          "standard_error": 0.005093356676248419
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.389261906404485,
            "upper_bound": 20.39818914698342
          },
          "point_estimate": 20.39375932833905,
          "standard_error": 0.0022693201578853736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0054589012017077715,
            "upper_bound": 0.01937883630756037
          },
          "point_estimate": 0.014345318550859772,
          "standard_error": 0.0037529932540644856
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.94798715866968,
            "upper_bound": 30.96965527212275
          },
          "point_estimate": 30.958649071644583,
          "standard_error": 0.005542121766803203
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.94718200737199,
            "upper_bound": 30.975872414974283
          },
          "point_estimate": 30.95401179867502,
          "standard_error": 0.007518357665142717
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002690885102021956,
            "upper_bound": 0.03399513863710382
          },
          "point_estimate": 0.011430224253437206,
          "standard_error": 0.00827625241195776
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.947914117288565,
            "upper_bound": 30.966621136241347
          },
          "point_estimate": 30.958147014315028,
          "standard_error": 0.00478540113432174
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009822375965653636,
            "upper_bound": 0.023344095382461995
          },
          "point_estimate": 0.01844884069367688,
          "standard_error": 0.0033530588111227696
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.06255011241311,
            "upper_bound": 44.08994464960508
          },
          "point_estimate": 44.075221310356575,
          "standard_error": 0.007030300816290667
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.057885910947455,
            "upper_bound": 44.09245688192168
          },
          "point_estimate": 44.06820632673248,
          "standard_error": 0.007708980843548573
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002268981248143866,
            "upper_bound": 0.0375900051592986
          },
          "point_estimate": 0.017810823850843763,
          "standard_error": 0.009079739816262844
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.057205394033225,
            "upper_bound": 44.07283558305499
          },
          "point_estimate": 44.06382077914521,
          "standard_error": 0.003995215279923501
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008290444474160743,
            "upper_bound": 0.02923387011210292
          },
          "point_estimate": 0.02341426221414259,
          "standard_error": 0.005259312637732775
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.18279256406419,
            "upper_bound": 51.422996540868965
          },
          "point_estimate": 51.30914521929159,
          "standard_error": 0.0594415251941356
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.0477777149681,
            "upper_bound": 51.40323553277135
          },
          "point_estimate": 51.3837964291126,
          "standard_error": 0.08611373655429219
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004077014693390382,
            "upper_bound": 0.35375291039395285
          },
          "point_estimate": 0.02919840525297344,
          "standard_error": 0.11098520744161457
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.24867314315814,
            "upper_bound": 51.39030158807688
          },
          "point_estimate": 51.346709523261985,
          "standard_error": 0.037077903276048256
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10761642604358422,
            "upper_bound": 0.2477448861427101
          },
          "point_estimate": 0.19865848530562483,
          "standard_error": 0.036442653972219054
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.087026650145425,
            "upper_bound": 39.11698404263272
          },
          "point_estimate": 39.102750248074614,
          "standard_error": 0.007655161189161687
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.091197034388955,
            "upper_bound": 39.1245134778799
          },
          "point_estimate": 39.098929349497574,
          "standard_error": 0.008489654496413904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002536243055112522,
            "upper_bound": 0.04364548193945041
          },
          "point_estimate": 0.02124896512497075,
          "standard_error": 0.01082970989978536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.08858096436427,
            "upper_bound": 39.10777719929632
          },
          "point_estimate": 39.09709820447853,
          "standard_error": 0.004842739568555652
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01237051151836856,
            "upper_bound": 0.03474580879785501
          },
          "point_estimate": 0.025610687655697847,
          "standard_error": 0.006148960400849969
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.1973344235708,
            "upper_bound": 60.285215987883475
          },
          "point_estimate": 60.23882636554373,
          "standard_error": 0.02248440962478482
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.17053172297611,
            "upper_bound": 60.30130656763481
          },
          "point_estimate": 60.223193322456126,
          "standard_error": 0.03267790880710372
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010393100482705735,
            "upper_bound": 0.12453783073766345
          },
          "point_estimate": 0.07578662646750518,
          "standard_error": 0.03067161391671881
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.1949331106508,
            "upper_bound": 60.256667336205005
          },
          "point_estimate": 60.22653594896223,
          "standard_error": 0.01568544899731527
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03750466779849244,
            "upper_bound": 0.0944964462029614
          },
          "point_estimate": 0.07511910274406651,
          "standard_error": 0.014742977654536396
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.02839979026127,
            "upper_bound": 42.202530480357545
          },
          "point_estimate": 42.108008017481936,
          "standard_error": 0.0447733734512513
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.01040456357579,
            "upper_bound": 42.20358800645113
          },
          "point_estimate": 42.068394096870975,
          "standard_error": 0.045981266008137366
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022258420831898256,
            "upper_bound": 0.223901794999473
          },
          "point_estimate": 0.09562436874505392,
          "standard_error": 0.05287319158591709
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.01281828911214,
            "upper_bound": 42.14899549813449
          },
          "point_estimate": 42.074944742528054,
          "standard_error": 0.034981557952990676
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05536832605204295,
            "upper_bound": 0.19908764937098308
          },
          "point_estimate": 0.1486239570605443,
          "standard_error": 0.03832750657911849
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.9168394087761,
            "upper_bound": 33.93311759311188
          },
          "point_estimate": 33.92507329993464,
          "standard_error": 0.004176516755946953
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.91371081677292,
            "upper_bound": 33.936423977634114
          },
          "point_estimate": 33.92667744150977,
          "standard_error": 0.006015220342304504
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010560234855960331,
            "upper_bound": 0.0244871104609198
          },
          "point_estimate": 0.013411553606866024,
          "standard_error": 0.005509890501898874
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.922992428538585,
            "upper_bound": 33.93983565362461
          },
          "point_estimate": 33.9308791067839,
          "standard_error": 0.004419561025253106
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008142151065456354,
            "upper_bound": 0.017211446500739074
          },
          "point_estimate": 0.01393597240085948,
          "standard_error": 0.002310187677969773
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.64111077590098,
            "upper_bound": 67.14847493889724
          },
          "point_estimate": 66.90241538965397,
          "standard_error": 0.12992856503282468
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.59021617448104,
            "upper_bound": 67.35118913585129
          },
          "point_estimate": 66.89452706019142,
          "standard_error": 0.17558488687851845
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04821380578840417,
            "upper_bound": 0.7598791417799636
          },
          "point_estimate": 0.49834518759052626,
          "standard_error": 0.18840831164210284
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.60192902072613,
            "upper_bound": 67.05657258156083
          },
          "point_estimate": 66.78810297580502,
          "standard_error": 0.11766918265230912
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24144320991019336,
            "upper_bound": 0.5531080970301728
          },
          "point_estimate": 0.4341703053865786,
          "standard_error": 0.08315802301278143
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103022.7302549575,
            "upper_bound": 103168.98453788953
          },
          "point_estimate": 103098.0908316471,
          "standard_error": 37.32502256454895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103025.33508700933,
            "upper_bound": 103170.97698300282
          },
          "point_estimate": 103112.0207743154,
          "standard_error": 36.00602824773543
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.812999843553113,
            "upper_bound": 213.6021962078035
          },
          "point_estimate": 92.36867336011558,
          "standard_error": 52.56634170762319
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102954.0955972896,
            "upper_bound": 103121.34150187482
          },
          "point_estimate": 103044.71937750636,
          "standard_error": 45.117414298588194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.53860403700264,
            "upper_bound": 166.2905387998516
          },
          "point_estimate": 124.76093586294668,
          "standard_error": 28.05842268509911
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57194.91494234801,
            "upper_bound": 57565.53369221698
          },
          "point_estimate": 57365.26857704403,
          "standard_error": 95.15429338841476
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57124.30070754717,
            "upper_bound": 57576.5
          },
          "point_estimate": 57231.058097484274,
          "standard_error": 129.23994906476784
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.429886420894057,
            "upper_bound": 490.24367164234
          },
          "point_estimate": 158.9334544818223,
          "standard_error": 139.07758108603105
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57137.52046173145,
            "upper_bound": 57274.18860800831
          },
          "point_estimate": 57195.478399901986,
          "standard_error": 36.27181606411356
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.3130583746552,
            "upper_bound": 396.19573164958473
          },
          "point_estimate": 317.271523839475,
          "standard_error": 70.98317504401825
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147688.05349464447,
            "upper_bound": 147913.55064750288
          },
          "point_estimate": 147787.35904632855,
          "standard_error": 58.40197819015331
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147660.7443541102,
            "upper_bound": 147881.4837398374
          },
          "point_estimate": 147748.89796747966,
          "standard_error": 52.74117118510474
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.3227627590139,
            "upper_bound": 256.3803750147481
          },
          "point_estimate": 153.54097293803076,
          "standard_error": 61.85233679186908
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147676.1434055604,
            "upper_bound": 147776.73833257964
          },
          "point_estimate": 147724.44747122796,
          "standard_error": 25.25032599523116
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.1128195425951,
            "upper_bound": 276.19263920449174
          },
          "point_estimate": 194.50102524783375,
          "standard_error": 60.68078223173374
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388629.81545997976,
            "upper_bound": 389329.079066616
          },
          "point_estimate": 388963.8493110435,
          "standard_error": 178.71542938739648
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388447.41489361704,
            "upper_bound": 389439.2659574468
          },
          "point_estimate": 388760.0,
          "standard_error": 251.3396993141066
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.747833903672,
            "upper_bound": 963.211555240006
          },
          "point_estimate": 533.8787302026197,
          "standard_error": 226.95476409968035
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388550.0471262155,
            "upper_bound": 389355.5884252949
          },
          "point_estimate": 388912.4458690246,
          "standard_error": 208.5388899893761
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299.9282368780233,
            "upper_bound": 736.2443430179167
          },
          "point_estimate": 598.0769758628766,
          "standard_error": 109.06702015516652
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 577480.6081944443,
            "upper_bound": 578638.6314281306
          },
          "point_estimate": 578046.2552336861,
          "standard_error": 297.0610719096545
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 577100.6547619047,
            "upper_bound": 578940.2063492064
          },
          "point_estimate": 577985.938271605,
          "standard_error": 527.2922923932232
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.08964859591914,
            "upper_bound": 1733.3764642263347
          },
          "point_estimate": 1339.893516026965,
          "standard_error": 445.73342492196247
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 577467.1121922069,
            "upper_bound": 578598.9342007615
          },
          "point_estimate": 577994.514533086,
          "standard_error": 294.21413453418444
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 614.3305981581003,
            "upper_bound": 1213.5615721602053
          },
          "point_estimate": 994.9284221676631,
          "standard_error": 155.9029000994792
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47414.108073790885,
            "upper_bound": 47574.7017667537
          },
          "point_estimate": 47490.3622643914,
          "standard_error": 41.22578722001014
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47376.086161879895,
            "upper_bound": 47599.237597911226
          },
          "point_estimate": 47453.64898825066,
          "standard_error": 62.13442052274988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.909014524352184,
            "upper_bound": 221.45304835301735
          },
          "point_estimate": 119.06557298408369,
          "standard_error": 55.421963624722125
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47382.717677207234,
            "upper_bound": 47531.16289105151
          },
          "point_estimate": 47443.88985792275,
          "standard_error": 39.99594076774188
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.30866688221337,
            "upper_bound": 168.38936036303537
          },
          "point_estimate": 137.56452228501442,
          "standard_error": 25.19056168280844
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99848.32587890283,
            "upper_bound": 100009.68292582416
          },
          "point_estimate": 99925.997293847,
          "standard_error": 41.398803644103815
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99832.17223748474,
            "upper_bound": 100022.44793956044
          },
          "point_estimate": 99900.10662774726,
          "standard_error": 46.606759128160505
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.228923207193166,
            "upper_bound": 241.6508636906137
          },
          "point_estimate": 135.57643605457676,
          "standard_error": 59.65344855773376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99830.37072927304,
            "upper_bound": 99970.867875679
          },
          "point_estimate": 99899.16459968605,
          "standard_error": 35.30619292232239
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.86804985395469,
            "upper_bound": 174.9096705992939
          },
          "point_estimate": 138.35346817839192,
          "standard_error": 26.444211154113063
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70792.22036510454,
            "upper_bound": 71102.71373094541
          },
          "point_estimate": 70947.47652584015,
          "standard_error": 79.53362453979997
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70730.1411985367,
            "upper_bound": 71140.830078125
          },
          "point_estimate": 70938.725390625,
          "standard_error": 112.1886155495214
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.65972557257111,
            "upper_bound": 437.6218141056597
          },
          "point_estimate": 249.87981023659813,
          "standard_error": 96.8759365080295
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70733.60576201307,
            "upper_bound": 71028.14216694079
          },
          "point_estimate": 70870.38492288961,
          "standard_error": 74.82857681549115
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.13762291592963,
            "upper_bound": 337.05407401852835
          },
          "point_estimate": 265.27582147459447,
          "standard_error": 47.271777041794294
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308348.01500907994,
            "upper_bound": 309076.95759681024
          },
          "point_estimate": 308709.6574821765,
          "standard_error": 186.3798435771949
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308239.9521186441,
            "upper_bound": 309184.3523002421
          },
          "point_estimate": 308614.91031073447,
          "standard_error": 232.31908615656644
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.2950553404012,
            "upper_bound": 1119.224666951834
          },
          "point_estimate": 643.5682988851003,
          "standard_error": 260.60730628919055
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308357.8550436511,
            "upper_bound": 308945.87146895245
          },
          "point_estimate": 308656.87495047326,
          "standard_error": 149.8872507644005
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 350.46471878765794,
            "upper_bound": 781.485625937976
          },
          "point_estimate": 621.7336331865761,
          "standard_error": 110.28172615945496
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36162.76137671537,
            "upper_bound": 36268.61883023461
          },
          "point_estimate": 36206.12145663377,
          "standard_error": 27.976216567584963
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36155.56554733447,
            "upper_bound": 36213.240371845946
          },
          "point_estimate": 36194.36560701638,
          "standard_error": 15.017032563912572
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.767421960512528,
            "upper_bound": 85.78916832254944
          },
          "point_estimate": 37.86380246523341,
          "standard_error": 21.166085810468193
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36179.966286751725,
            "upper_bound": 36204.9559012273
          },
          "point_estimate": 36194.54573653438,
          "standard_error": 6.324484000664374
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.224888027778608,
            "upper_bound": 139.61995286606282
          },
          "point_estimate": 93.00500959695133,
          "standard_error": 37.41707248077557
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65834.8664733922,
            "upper_bound": 65972.34818689614
          },
          "point_estimate": 65887.9121240942,
          "standard_error": 37.98955966285573
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65838.03532608696,
            "upper_bound": 65870.78532608696
          },
          "point_estimate": 65864.37511322464,
          "standard_error": 12.90391377801567
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.031188822648287,
            "upper_bound": 68.53076650072727
          },
          "point_estimate": 14.232691160362013,
          "standard_error": 19.39328421467636
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65807.5221656099,
            "upper_bound": 65867.4345738543
          },
          "point_estimate": 65839.3740589121,
          "standard_error": 17.078135723871576
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.135282959032343,
            "upper_bound": 194.5295387254752
          },
          "point_estimate": 126.81047391865638,
          "standard_error": 60.22071625828778
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166302.5556324201,
            "upper_bound": 166612.389419439
          },
          "point_estimate": 166446.3683387693,
          "standard_error": 79.51170097887072
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166238.05251141553,
            "upper_bound": 166656.8984018265
          },
          "point_estimate": 166356.76217656012,
          "standard_error": 109.44645818127518
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.251367859630207,
            "upper_bound": 439.31671274850765
          },
          "point_estimate": 181.66419335016303,
          "standard_error": 106.77257345061822
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166253.3984220138,
            "upper_bound": 166516.0051996761
          },
          "point_estimate": 166376.72803178558,
          "standard_error": 67.06170889014093
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.56090869553452,
            "upper_bound": 328.30712705413146
          },
          "point_estimate": 264.7006551555046,
          "standard_error": 54.55277859143706
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35659.90217600121,
            "upper_bound": 35720.98153593626
          },
          "point_estimate": 35686.987296758416,
          "standard_error": 15.689874871551826
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35655.60897121361,
            "upper_bound": 35706.57521379504
          },
          "point_estimate": 35673.30998527969,
          "standard_error": 13.585365958951105
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.798808010981943,
            "upper_bound": 68.48242603313068
          },
          "point_estimate": 39.29869600495735,
          "standard_error": 16.813358582850846
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35665.49327921945,
            "upper_bound": 35702.35603018629
          },
          "point_estimate": 35687.840933943386,
          "standard_error": 9.41040490786734
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.5983270860116,
            "upper_bound": 74.59498882664019
          },
          "point_estimate": 52.29640648125166,
          "standard_error": 16.28159363932106
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/bstr_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11542.170155558577,
            "upper_bound": 11559.949726790812
          },
          "point_estimate": 11550.402614869186,
          "standard_error": 4.555496012964903
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11538.783380907073,
            "upper_bound": 11558.016904535363
          },
          "point_estimate": 11548.564111277245,
          "standard_error": 6.130744590209503
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.835146542110465,
            "upper_bound": 24.81432096763929
          },
          "point_estimate": 14.08968406955367,
          "standard_error": 5.718784505868821
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11538.38960529209,
            "upper_bound": 11556.945189870805
          },
          "point_estimate": 11547.39944559088,
          "standard_error": 4.848854155497344
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.467830497227903,
            "upper_bound": 20.390874911536734
          },
          "point_estimate": 15.133387001603374,
          "standard_error": 3.6479350463027895
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/bstr_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 505282.47086450073,
            "upper_bound": 505906.4387279541
          },
          "point_estimate": 505528.6488321208,
          "standard_error": 169.75813760688146
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 505226.5721450617,
            "upper_bound": 505609.8287037037
          },
          "point_estimate": 505307.3598958333,
          "standard_error": 118.5254103219101
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.99248017895385,
            "upper_bound": 475.0990757667454
          },
          "point_estimate": 182.13663457889672,
          "standard_error": 115.01196545550437
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 505260.9097957277,
            "upper_bound": 505409.0373159304
          },
          "point_estimate": 505316.5130952381,
          "standard_error": 37.91293596023147
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122.9591348953714,
            "upper_bound": 855.3308750205066
          },
          "point_estimate": 563.552573356776,
          "standard_error": 244.23104589999957
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/bstr_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1082.417693967585,
            "upper_bound": 1083.3479457671367
          },
          "point_estimate": 1082.8179429645322,
          "standard_error": 0.24147026880107716
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1082.352311931562,
            "upper_bound": 1083.095657643976
          },
          "point_estimate": 1082.4950191570883,
          "standard_error": 0.1992489703072668
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05243177799844148,
            "upper_bound": 0.979007479740554
          },
          "point_estimate": 0.3143328714071847,
          "standard_error": 0.25784263919260375
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1082.343202092614,
            "upper_bound": 1082.6501553768692
          },
          "point_estimate": 1082.4599807256052,
          "standard_error": 0.07839291236237346
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2123470549966772,
            "upper_bound": 1.1444270521190152
          },
          "point_estimate": 0.8055588109782503,
          "standard_error": 0.26299998751698717
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51855.47628436484,
            "upper_bound": 51975.53641428943
          },
          "point_estimate": 51912.8306213691,
          "standard_error": 30.84754506903701
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51821.91757925072,
            "upper_bound": 51995.625
          },
          "point_estimate": 51873.25105067243,
          "standard_error": 48.10199294795581
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.57898839776173,
            "upper_bound": 169.57664461188884
          },
          "point_estimate": 97.47968301362413,
          "standard_error": 40.471660517824354
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51823.24356309349,
            "upper_bound": 51945.113758578686
          },
          "point_estimate": 51874.38362214155,
          "standard_error": 32.587702999659996
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.63604434947441,
            "upper_bound": 127.0329754426138
          },
          "point_estimate": 102.95901233007264,
          "standard_error": 18.44416093206485
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52622.53803495624,
            "upper_bound": 52705.10042498794
          },
          "point_estimate": 52663.04653056302,
          "standard_error": 21.160957904198163
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52589.19702294811,
            "upper_bound": 52723.508683068016
          },
          "point_estimate": 52662.40229136517,
          "standard_error": 33.667833600079426
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.048175472836007,
            "upper_bound": 120.02584714552304
          },
          "point_estimate": 82.05453181386501,
          "standard_error": 29.181901599307903
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52608.65201028446,
            "upper_bound": 52696.398233446715
          },
          "point_estimate": 52655.46764523465,
          "standard_error": 22.364371791456986
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.11330095414831,
            "upper_bound": 86.67016577811003
          },
          "point_estimate": 70.63148286480765,
          "standard_error": 11.522635893302995
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53309.68798984336,
            "upper_bound": 53378.86599950178
          },
          "point_estimate": 53342.63722292147,
          "standard_error": 17.741638269946883
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53284.639500734214,
            "upper_bound": 53381.686674008815
          },
          "point_estimate": 53341.41651982379,
          "standard_error": 21.352572955918507
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.107676649410212,
            "upper_bound": 103.00437701124474
          },
          "point_estimate": 63.469943146312595,
          "standard_error": 27.022439565720724
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53308.20833155126,
            "upper_bound": 53378.94823240625
          },
          "point_estimate": 53338.53842515781,
          "standard_error": 17.762501688252563
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.532825978097662,
            "upper_bound": 75.65877047593793
          },
          "point_estimate": 59.296971992421106,
          "standard_error": 11.774129342133092
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15220.845898500913,
            "upper_bound": 15249.547445931288
          },
          "point_estimate": 15234.485306748364,
          "standard_error": 7.39669554532941
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15211.431591099916,
            "upper_bound": 15255.335217504296
          },
          "point_estimate": 15233.241484163636,
          "standard_error": 10.93149146149714
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6590555121072637,
            "upper_bound": 43.38591770581628
          },
          "point_estimate": 30.79417736296382,
          "standard_error": 12.478590925618365
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15217.652610005904,
            "upper_bound": 15246.99211618655
          },
          "point_estimate": 15230.619118496952,
          "standard_error": 7.503706453906059
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.501862936693426,
            "upper_bound": 30.644277512438112
          },
          "point_estimate": 24.75648274697836,
          "standard_error": 4.716756779290621
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26525.844176512604,
            "upper_bound": 26563.867441733673
          },
          "point_estimate": 26544.5667044637,
          "standard_error": 9.641407471053045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26526.1328320802,
            "upper_bound": 26564.61969115497
          },
          "point_estimate": 26542.20878817414,
          "standard_error": 8.058181153272768
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.0006754145658,
            "upper_bound": 53.38073818485146
          },
          "point_estimate": 19.714643235520075,
          "standard_error": 12.802591718219318
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26534.51431685274,
            "upper_bound": 26560.722938242565
          },
          "point_estimate": 26547.811323763955,
          "standard_error": 6.615636861151226
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.116509308754022,
            "upper_bound": 43.93847963534741
          },
          "point_estimate": 32.074412518683445,
          "standard_error": 7.811086029559975
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16508.777118610713,
            "upper_bound": 16542.596698947094
          },
          "point_estimate": 16525.898046131406,
          "standard_error": 8.696505687194206
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16500.350454959054,
            "upper_bound": 16549.818926296633
          },
          "point_estimate": 16528.744118029375,
          "standard_error": 11.026415855423066
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.297572782992104,
            "upper_bound": 51.23076221604496
          },
          "point_estimate": 36.6709771515074,
          "standard_error": 14.058412713820694
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16507.500800878697,
            "upper_bound": 16537.31764844719
          },
          "point_estimate": 16522.616883116883,
          "standard_error": 7.812703702781339
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.397092316485207,
            "upper_bound": 35.63733155432272
          },
          "point_estimate": 28.9730135846956,
          "standard_error": 4.860651140827985
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16493.14640326637,
            "upper_bound": 16502.662421626534
          },
          "point_estimate": 16497.74089912233,
          "standard_error": 2.4438469560406
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16490.255338482508,
            "upper_bound": 16505.402407996364
          },
          "point_estimate": 16495.35909813721,
          "standard_error": 4.362332544251114
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.984831490920478,
            "upper_bound": 13.190482282858971
          },
          "point_estimate": 8.551665420645028,
          "standard_error": 3.473813293049271
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16491.028787759285,
            "upper_bound": 16498.59334603304
          },
          "point_estimate": 16493.816699611158,
          "standard_error": 1.933705800274908
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.641206507666334,
            "upper_bound": 9.735935404082923
          },
          "point_estimate": 8.14363341588401,
          "standard_error": 1.266061392558067
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19271.32608959623,
            "upper_bound": 19301.428532044964
          },
          "point_estimate": 19284.249868300285,
          "standard_error": 7.8623828632539245
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19266.84403183024,
            "upper_bound": 19295.659770114944
          },
          "point_estimate": 19273.479528440905,
          "standard_error": 6.942806546813105
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9530993393307652,
            "upper_bound": 32.88442572211268
          },
          "point_estimate": 9.84407930179825,
          "standard_error": 7.787909530525343
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19270.627221980285,
            "upper_bound": 19277.95492832553
          },
          "point_estimate": 19273.783958110856,
          "standard_error": 1.850539130637227
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.163297997239972,
            "upper_bound": 37.93448850359557
          },
          "point_estimate": 26.329009563098943,
          "standard_error": 9.038008283104237
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37210.12427545787,
            "upper_bound": 37332.12727293447
          },
          "point_estimate": 37270.64783374033,
          "standard_error": 31.356367196309257
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37175.18853479854,
            "upper_bound": 37371.86461538461
          },
          "point_estimate": 37277.09647435897,
          "standard_error": 63.66431447393567
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.350997696573176,
            "upper_bound": 155.79006048203507
          },
          "point_estimate": 143.25004495679937,
          "standard_error": 41.198185400508535
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37190.95387748391,
            "upper_bound": 37369.2625525134
          },
          "point_estimate": 37306.99891042291,
          "standard_error": 43.87421257266298
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.6951558651419,
            "upper_bound": 121.0098730747664
          },
          "point_estimate": 104.76850981297612,
          "standard_error": 12.822317556529956
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15961.405807976244,
            "upper_bound": 15990.280900307422
          },
          "point_estimate": 15975.199450927495,
          "standard_error": 7.383893160209261
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15959.279801116758,
            "upper_bound": 16007.216805738544
          },
          "point_estimate": 15964.534197042893,
          "standard_error": 11.800334040938102
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1515326431499764,
            "upper_bound": 37.30759575838674
          },
          "point_estimate": 13.43927716588506,
          "standard_error": 10.976916719973204
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15952.603624739508,
            "upper_bound": 15977.151050857332
          },
          "point_estimate": 15962.60389097069,
          "standard_error": 6.255214210379661
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.860849516010616,
            "upper_bound": 29.473801106249137
          },
          "point_estimate": 24.5513374952916,
          "standard_error": 4.136905794828519
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19917.90894983849,
            "upper_bound": 19981.04087445254
          },
          "point_estimate": 19945.694869698473,
          "standard_error": 16.361842049712532
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19906.216575242273,
            "upper_bound": 19968.675321509112
          },
          "point_estimate": 19933.15002742732,
          "standard_error": 16.57087755617742
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.610536170574504,
            "upper_bound": 72.14032186790736
          },
          "point_estimate": 35.66149357057165,
          "standard_error": 15.501492377326032
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19913.51909152267,
            "upper_bound": 19948.043692752624
          },
          "point_estimate": 19930.189710125313,
          "standard_error": 8.91069449431538
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.480737844582148,
            "upper_bound": 77.88908292465868
          },
          "point_estimate": 54.63034963178449,
          "standard_error": 17.16502428080337
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16783.803976531082,
            "upper_bound": 16819.397554683896
          },
          "point_estimate": 16800.817404465248,
          "standard_error": 9.161862127909956
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16774.077988001845,
            "upper_bound": 16828.491174434705
          },
          "point_estimate": 16794.812598061835,
          "standard_error": 11.308041405095793
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.9100799965973545,
            "upper_bound": 56.03562543571765
          },
          "point_estimate": 28.762462295086536,
          "standard_error": 12.426699181534117
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16781.1647354986,
            "upper_bound": 16810.101659490774
          },
          "point_estimate": 16797.328930414304,
          "standard_error": 7.305993773941769
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.77111706158383,
            "upper_bound": 38.056209737143774
          },
          "point_estimate": 30.502845631806196,
          "standard_error": 5.918717174909878
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19281.15469839583,
            "upper_bound": 19318.64723160518
          },
          "point_estimate": 19298.949673789284,
          "standard_error": 9.625606047885247
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19272.99920297556,
            "upper_bound": 19329.069758615453
          },
          "point_estimate": 19288.049238398868,
          "standard_error": 14.87193191016832
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.585510272780921,
            "upper_bound": 52.30110601222325
          },
          "point_estimate": 23.83064131443437,
          "standard_error": 12.640992276336798
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19281.78607511873,
            "upper_bound": 19318.679223855353
          },
          "point_estimate": 19297.3846391653,
          "standard_error": 9.41088594056614
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.759137706603102,
            "upper_bound": 38.235979759514926
          },
          "point_estimate": 32.10574453704599,
          "standard_error": 5.553413960665735
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16357.070899500704,
            "upper_bound": 16400.296138543745
          },
          "point_estimate": 16376.121832854067,
          "standard_error": 11.15864838115519
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16353.094634720346,
            "upper_bound": 16385.24752140604
          },
          "point_estimate": 16371.98369197837,
          "standard_error": 8.683956000804788
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.461574318041056,
            "upper_bound": 46.74240044513006
          },
          "point_estimate": 18.52326503842886,
          "standard_error": 10.647652290465205
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16354.124361793069,
            "upper_bound": 16380.302658394572
          },
          "point_estimate": 16366.883320555064,
          "standard_error": 6.717813927689905
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.967127374099377,
            "upper_bound": 53.70981007801653
          },
          "point_estimate": 37.191070927561675,
          "standard_error": 12.319744913517065
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16531.914290191427,
            "upper_bound": 16557.60955493027
          },
          "point_estimate": 16544.585395147144,
          "standard_error": 6.597133495036343
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16522.024101864485,
            "upper_bound": 16560.975557071397
          },
          "point_estimate": 16546.49212710323,
          "standard_error": 10.74999971135761
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.677104136631241,
            "upper_bound": 37.44334662947252
          },
          "point_estimate": 25.149262222907428,
          "standard_error": 8.670468385059841
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16529.76975079962,
            "upper_bound": 16557.598562726977
          },
          "point_estimate": 16545.904374479545,
          "standard_error": 7.146166316948514
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.563498777474658,
            "upper_bound": 27.02042368869588
          },
          "point_estimate": 21.987253041519395,
          "standard_error": 3.507223860485365
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16359.967332858536,
            "upper_bound": 16379.43788643533
          },
          "point_estimate": 16369.760674585292,
          "standard_error": 4.955593127740015
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16354.628774222623,
            "upper_bound": 16378.703019378096
          },
          "point_estimate": 16374.988497607244,
          "standard_error": 6.458747852203859
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7071614207963686,
            "upper_bound": 29.595847976146867
          },
          "point_estimate": 12.624482425712436,
          "standard_error": 8.181729314134405
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16355.504180236803,
            "upper_bound": 16378.000555438755
          },
          "point_estimate": 16367.794938635046,
          "standard_error": 6.506192000989608
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.570430309909597,
            "upper_bound": 21.585434375889797
          },
          "point_estimate": 16.568064827574673,
          "standard_error": 3.2653607762574657
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19484.140788923592,
            "upper_bound": 19531.221802228098
          },
          "point_estimate": 19506.673993119424,
          "standard_error": 12.10867413150176
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19472.711485157368,
            "upper_bound": 19545.67789699571
          },
          "point_estimate": 19489.331179959467,
          "standard_error": 20.860295734597365
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.3431078403233516,
            "upper_bound": 67.65111368629213
          },
          "point_estimate": 30.321034200607492,
          "standard_error": 17.7548430904047
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19481.411063656786,
            "upper_bound": 19536.98581891913
          },
          "point_estimate": 19511.175154673652,
          "standard_error": 14.335892474604254
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.026568293302837,
            "upper_bound": 47.1424661474143
          },
          "point_estimate": 40.38090603837962,
          "standard_error": 6.504668380002255
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17003.873705378508,
            "upper_bound": 17035.778462896084
          },
          "point_estimate": 17019.135901644524,
          "standard_error": 8.178607966531187
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16997.31897627965,
            "upper_bound": 17034.90856741573
          },
          "point_estimate": 17017.71817649813,
          "standard_error": 10.603793502069514
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.438134857609693,
            "upper_bound": 48.974107329271966
          },
          "point_estimate": 27.865163414469468,
          "standard_error": 10.046799714880436
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16994.388158954534,
            "upper_bound": 17045.63364636897
          },
          "point_estimate": 17020.46759083613,
          "standard_error": 13.082292010226675
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.437767903137562,
            "upper_bound": 36.047824880002416
          },
          "point_estimate": 27.268696440588094,
          "standard_error": 5.894940194576934
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19777.75158069793,
            "upper_bound": 19824.42329334787
          },
          "point_estimate": 19798.40424027626,
          "standard_error": 12.072485393911627
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19774.75808796801,
            "upper_bound": 19818.32311886587
          },
          "point_estimate": 19785.00049981825,
          "standard_error": 11.1374526849427
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.486468240961944,
            "upper_bound": 53.616118132095366
          },
          "point_estimate": 16.596995316034175,
          "standard_error": 14.16012736637768
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19779.44307037366,
            "upper_bound": 19815.018270407545
          },
          "point_estimate": 19799.214428755546,
          "standard_error": 9.251672424568897
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.19852423299288,
            "upper_bound": 56.62710747823227
          },
          "point_estimate": 40.45148696435144,
          "standard_error": 12.21452862587571
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71557.24713207725,
            "upper_bound": 71715.66957133483
          },
          "point_estimate": 71636.29772067555,
          "standard_error": 40.59564138417396
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71505.43499718785,
            "upper_bound": 71774.18307086614
          },
          "point_estimate": 71633.40997375328,
          "standard_error": 87.91824832975131
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.63349819011021,
            "upper_bound": 210.3620632731978
          },
          "point_estimate": 197.77460465808656,
          "standard_error": 55.81901929977733
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71509.04646088518,
            "upper_bound": 71697.03017939723
          },
          "point_estimate": 71585.89543409347,
          "standard_error": 49.41250347391942
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.67638106360052,
            "upper_bound": 150.99292146421698
          },
          "point_estimate": 135.60078801098174,
          "standard_error": 14.747499941995835
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231551.944457672,
            "upper_bound": 1233031.8734074074
          },
          "point_estimate": 1232290.2309232806,
          "standard_error": 381.0791273551375
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1230910.419047619,
            "upper_bound": 1233653.54
          },
          "point_estimate": 1232163.5694444445,
          "standard_error": 717.4270372362593
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176.22112687143647,
            "upper_bound": 2100.128808215198
          },
          "point_estimate": 1875.2071550418068,
          "standard_error": 508.6529832329759
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231550.5830985915,
            "upper_bound": 1233168.9441214136
          },
          "point_estimate": 1232341.5707359307,
          "standard_error": 429.3031691568086
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 836.4189201903984,
            "upper_bound": 1467.251802593605
          },
          "point_estimate": 1274.3809262975258,
          "standard_error": 161.57061687087003
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203037.08571710117,
            "upper_bound": 203494.34230788337
          },
          "point_estimate": 203236.6660211049,
          "standard_error": 118.2636266216744
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202979.38547486035,
            "upper_bound": 203376.72067039105
          },
          "point_estimate": 203182.74036312848,
          "standard_error": 97.16126972119127
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.107827304460756,
            "upper_bound": 470.2078240543751
          },
          "point_estimate": 294.54457521769694,
          "standard_error": 108.02874649999364
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203017.89128090948,
            "upper_bound": 203296.31905263895
          },
          "point_estimate": 203174.4089240369,
          "standard_error": 70.4032322717255
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.5654562819851,
            "upper_bound": 570.6112646039691
          },
          "point_estimate": 395.0396210459301,
          "standard_error": 131.69815585499197
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/bstr_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6012.239932206919,
            "upper_bound": 6031.4832293426425
          },
          "point_estimate": 6021.445320051589,
          "standard_error": 4.914722280845083
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6011.129872706232,
            "upper_bound": 6029.716358075715
          },
          "point_estimate": 6019.98061203872,
          "standard_error": 5.123360914162728
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8594001852058233,
            "upper_bound": 26.908714031447374
          },
          "point_estimate": 13.1351871509436,
          "standard_error": 5.908564794210009
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6005.798478391166,
            "upper_bound": 6022.702704545228
          },
          "point_estimate": 6013.501664544746,
          "standard_error": 4.254407335407779
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.095198350146437,
            "upper_bound": 22.43314060474067
          },
          "point_estimate": 16.41402554403725,
          "standard_error": 4.02208623829407
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/bstr_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6069.128820693728,
            "upper_bound": 6089.604007140469
          },
          "point_estimate": 6078.595099909517,
          "standard_error": 5.261853809565038
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6065.347570489246,
            "upper_bound": 6091.0552741558
          },
          "point_estimate": 6073.522045899604,
          "standard_error": 5.900531648186118
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8763609395289237,
            "upper_bound": 28.482749102738495
          },
          "point_estimate": 12.995837967761815,
          "standard_error": 6.820507185636274
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6067.739938778275,
            "upper_bound": 6079.716597487498
          },
          "point_estimate": 6073.367795160373,
          "standard_error": 3.0339428020067207
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.936053505184956,
            "upper_bound": 21.915771227766783
          },
          "point_estimate": 17.584292763640782,
          "standard_error": 3.9410011509568976
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/bstr_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15562.670821201958,
            "upper_bound": 15582.24911082657
          },
          "point_estimate": 15571.788530519265,
          "standard_error": 4.98342336364349
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15561.488907971949,
            "upper_bound": 15579.419370667976
          },
          "point_estimate": 15571.702724106675,
          "standard_error": 4.558564421105976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4780732418347546,
            "upper_bound": 25.32589285222202
          },
          "point_estimate": 12.673701128941326,
          "standard_error": 6.007822355232683
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15563.089498532629,
            "upper_bound": 15575.923416993066
          },
          "point_estimate": 15569.907395738654,
          "standard_error": 3.235668004431941
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.249275411599596,
            "upper_bound": 23.229037341694816
          },
          "point_estimate": 16.63997985041736,
          "standard_error": 4.528979819554522
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/bstr_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.29063813284575,
            "upper_bound": 36.49371459007472
          },
          "point_estimate": 36.382079027137465,
          "standard_error": 0.0523193111748787
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.26855452358867,
            "upper_bound": 36.478546951146875
          },
          "point_estimate": 36.31695803848565,
          "standard_error": 0.05435044982270429
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011605729338327025,
            "upper_bound": 0.2558996408437291
          },
          "point_estimate": 0.0763363133158538,
          "standard_error": 0.06264200107697274
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.27612969761837,
            "upper_bound": 36.392377974715295
          },
          "point_estimate": 36.32581063425992,
          "standard_error": 0.03149313355664535
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05127220318307083,
            "upper_bound": 0.23015213154914735
          },
          "point_estimate": 0.17430480971677084,
          "standard_error": 0.046038493189898784
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.83467981520767,
            "upper_bound": 8.838950090045572
          },
          "point_estimate": 8.836442128386718,
          "standard_error": 0.0011305983810744903
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.83411953081029,
            "upper_bound": 8.837600281606528
          },
          "point_estimate": 8.834880398663827,
          "standard_error": 0.0010656913872229188
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000036157073843017434,
            "upper_bound": 0.0043269630303105645
          },
          "point_estimate": 0.001196777118569905,
          "standard_error": 0.0011467928307962883
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834387431793617,
            "upper_bound": 8.840402672810756
          },
          "point_estimate": 8.836977293525702,
          "standard_error": 0.001528227576762857
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010551701809199709,
            "upper_bound": 0.005554714879681124
          },
          "point_estimate": 0.0037472491456110742,
          "standard_error": 0.0014271499661080798
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834205997547759,
            "upper_bound": 8.85228805669876
          },
          "point_estimate": 8.840869262173603,
          "standard_error": 0.00522170181212544
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.833681260384349,
            "upper_bound": 8.838907019181441
          },
          "point_estimate": 8.834388964858176,
          "standard_error": 0.001935248300411813
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00012170948339886068,
            "upper_bound": 0.0067435540740280525
          },
          "point_estimate": 0.0011612041498342694,
          "standard_error": 0.0022533463134244507
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.83396515199221,
            "upper_bound": 8.838749128696952
          },
          "point_estimate": 8.836356208300721,
          "standard_error": 0.001241752479821474
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006578575255852328,
            "upper_bound": 0.026779727204679228
          },
          "point_estimate": 0.01746408751902479,
          "standard_error": 0.008818483332784136
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834461057962145,
            "upper_bound": 8.839950544443731
          },
          "point_estimate": 8.83689246774525,
          "standard_error": 0.00142610238823741
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.833568378062557,
            "upper_bound": 8.84035283686909
          },
          "point_estimate": 8.83480526115217,
          "standard_error": 0.00182441034641026
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000121852352513104,
            "upper_bound": 0.007474846092967164
          },
          "point_estimate": 0.0018887715193240517,
          "standard_error": 0.002078766518754257
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.83391788327918,
            "upper_bound": 8.838946559173854
          },
          "point_estimate": 8.836014412027309,
          "standard_error": 0.0013627773248549634
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012100696841903758,
            "upper_bound": 0.006458696830592866
          },
          "point_estimate": 0.0047661764078266965,
          "standard_error": 0.001284466059162008
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.835614777728972,
            "upper_bound": 8.842540493441183
          },
          "point_estimate": 8.8384421602541,
          "standard_error": 0.0018452498251091088
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834725092179616,
            "upper_bound": 8.839614154569093
          },
          "point_estimate": 8.83633430068554,
          "standard_error": 0.001315097409926224
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005078669611360518,
            "upper_bound": 0.005832178286284338
          },
          "point_estimate": 0.0026468184467336275,
          "standard_error": 0.0013913881041782445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.835477944486161,
            "upper_bound": 8.838408372356552
          },
          "point_estimate": 8.83678406290787,
          "standard_error": 0.0007393450161841153
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013183100721159183,
            "upper_bound": 0.009154931337625152
          },
          "point_estimate": 0.006143376131313485,
          "standard_error": 0.0024257132984475535
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.57118689384919,
            "upper_bound": 9.58023904649475
          },
          "point_estimate": 9.574536269266227,
          "standard_error": 0.0025924634411512754
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.57051579132227,
            "upper_bound": 9.57344503890306
          },
          "point_estimate": 9.572401873549538,
          "standard_error": 0.0010046374634017409
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00035217122980088207,
            "upper_bound": 0.0036757915676947097
          },
          "point_estimate": 0.0019559507291765474,
          "standard_error": 0.001043223238349944
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.570523014080011,
            "upper_bound": 9.573080585373855
          },
          "point_estimate": 9.57183932787902,
          "standard_error": 0.0006660862843866996
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010771068014293992,
            "upper_bound": 0.013301868248186823
          },
          "point_estimate": 0.008652856449117388,
          "standard_error": 0.004318716383909562
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.314040308791707,
            "upper_bound": 10.320246248814874
          },
          "point_estimate": 10.31695529187374,
          "standard_error": 0.0016032821820704817
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.313003655779912,
            "upper_bound": 10.321746863322677
          },
          "point_estimate": 10.314676535490191,
          "standard_error": 0.00234660616908628
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000623658787500946,
            "upper_bound": 0.0084903065749804
          },
          "point_estimate": 0.003621529557679653,
          "standard_error": 0.0021506994642800353
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.31370191957038,
            "upper_bound": 10.317092749551056
          },
          "point_estimate": 10.31523712136318,
          "standard_error": 0.0008675960803961691
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023741812072769785,
            "upper_bound": 0.006563101125210424
          },
          "point_estimate": 0.005363275783778512,
          "standard_error": 0.001017601591292575
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.802549541534287,
            "upper_bound": 7.811825676920288
          },
          "point_estimate": 7.806503470894628,
          "standard_error": 0.0024201994894871154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.801691883426861,
            "upper_bound": 7.80963015902947
          },
          "point_estimate": 7.804397772402318,
          "standard_error": 0.0017838370766848743
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007006327979002096,
            "upper_bound": 0.009323573239062846
          },
          "point_estimate": 0.003415827959330428,
          "standard_error": 0.0022166400527783552
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.803115037086725,
            "upper_bound": 7.808483891159831
          },
          "point_estimate": 7.805366726944305,
          "standard_error": 0.0013763919376739051
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018752748617612777,
            "upper_bound": 0.01153995957023604
          },
          "point_estimate": 0.008066438733133296,
          "standard_error": 0.0027692531671307087
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.75247420802659,
            "upper_bound": 9.772826914914464
          },
          "point_estimate": 9.76026667391552,
          "standard_error": 0.005656336883061682
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.750429264018855,
            "upper_bound": 9.76006410533834
          },
          "point_estimate": 9.754587508034,
          "standard_error": 0.0031673506360626767
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008677098369746039,
            "upper_bound": 0.012299151774308362
          },
          "point_estimate": 0.007099099931571729,
          "standard_error": 0.003005125056186764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.752655965861514,
            "upper_bound": 9.757886778493862
          },
          "point_estimate": 9.755495837893454,
          "standard_error": 0.00132155767799191
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00329969807197731,
            "upper_bound": 0.0289912094889849
          },
          "point_estimate": 0.018955034372671188,
          "standard_error": 0.008853390004522508
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.54619074960669,
            "upper_bound": 12.5586793619125
          },
          "point_estimate": 12.551334576698393,
          "standard_error": 0.003317635207042692
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.545737432949268,
            "upper_bound": 12.552386079910898
          },
          "point_estimate": 12.548289971365609,
          "standard_error": 0.0015672589895684352
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00004112206353325211,
            "upper_bound": 0.009703379751976618
          },
          "point_estimate": 0.004104027125976423,
          "standard_error": 0.002711003678070885
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.547451751228488,
            "upper_bound": 12.550404243018464
          },
          "point_estimate": 12.549110020270438,
          "standard_error": 0.0007527252965084064
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023299459676565893,
            "upper_bound": 0.01662707266034427
          },
          "point_estimate": 0.01107936406828938,
          "standard_error": 0.00450729381161454
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.836386356684264,
            "upper_bound": 8.841092153558149
          },
          "point_estimate": 8.838430614212525,
          "standard_error": 0.001222159124674906
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.836051705623593,
            "upper_bound": 8.840286728450335
          },
          "point_estimate": 8.837149087674184,
          "standard_error": 0.0008596733191823228
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002126993408990453,
            "upper_bound": 0.0048933915033217415
          },
          "point_estimate": 0.0015053623419890429,
          "standard_error": 0.0012184259433663629
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.836983778426093,
            "upper_bound": 8.838766065143208
          },
          "point_estimate": 8.837807605702663,
          "standard_error": 0.0004465697949484487
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010521878511175846,
            "upper_bound": 0.005768614317080293
          },
          "point_estimate": 0.00408841918540901,
          "standard_error": 0.0013286262093531885
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.816164336084908,
            "upper_bound": 9.82216755066702
          },
          "point_estimate": 9.818935731650416,
          "standard_error": 0.001536835515585387
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.816293148759469,
            "upper_bound": 9.821203052832908
          },
          "point_estimate": 9.81809176438346,
          "standard_error": 0.0011239363901437635
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006744922748486204,
            "upper_bound": 0.007707446270407165
          },
          "point_estimate": 0.002389925532189055,
          "standard_error": 0.001756112253144944
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.816864914190573,
            "upper_bound": 9.821024333510476
          },
          "point_estimate": 9.819105177187115,
          "standard_error": 0.0010632388357595473
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001477703771015314,
            "upper_bound": 0.007245073857376076
          },
          "point_estimate": 0.005138936666229717,
          "standard_error": 0.0015045591219049036
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.200450362403313,
            "upper_bound": 19.21248417046723
          },
          "point_estimate": 19.20652953069558,
          "standard_error": 0.0030674460122757773
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.200419748700575,
            "upper_bound": 19.212020143708425
          },
          "point_estimate": 19.20760257939277,
          "standard_error": 0.0034052220886309865
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005655935088058238,
            "upper_bound": 0.017520590540751408
          },
          "point_estimate": 0.007267708402548885,
          "standard_error": 0.0039657893823505696
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.202765758863723,
            "upper_bound": 19.216323980728955
          },
          "point_estimate": 19.20891764346222,
          "standard_error": 0.0034863385122323276
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004354601017386011,
            "upper_bound": 0.01382311551215189
          },
          "point_estimate": 0.01019884290807736,
          "standard_error": 0.002522805936284538
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102779.91963983054,
            "upper_bound": 103024.06410860014
          },
          "point_estimate": 102889.59370773922,
          "standard_error": 62.77772706798782
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102743.63672316384,
            "upper_bound": 103033.65607344631
          },
          "point_estimate": 102831.63393417632,
          "standard_error": 62.08890950208398
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.28147790626923,
            "upper_bound": 273.4701421788357
          },
          "point_estimate": 116.9954094954198,
          "standard_error": 65.5908309521843
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102767.03220442268,
            "upper_bound": 102863.64179748508
          },
          "point_estimate": 102814.00412356004,
          "standard_error": 25.46994028562727
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.742304117139874,
            "upper_bound": 271.9908216477709
          },
          "point_estimate": 208.53483804553855,
          "standard_error": 56.4076039401836
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57099.3886795121,
            "upper_bound": 57510.821322830234
          },
          "point_estimate": 57293.73255039744,
          "standard_error": 104.87499547726088
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57019.93924646782,
            "upper_bound": 57597.82025117739
          },
          "point_estimate": 57077.28623009643,
          "standard_error": 179.65454855006794
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.382078850037704,
            "upper_bound": 629.3496685756226
          },
          "point_estimate": 104.40471526837132,
          "standard_error": 178.77612382710075
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57027.494557493024,
            "upper_bound": 57177.629536411034
          },
          "point_estimate": 57069.810051173314,
          "standard_error": 39.56508039565333
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170.63916659345844,
            "upper_bound": 430.13238288437776
          },
          "point_estimate": 351.21966697787764,
          "standard_error": 65.53824533894402
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148023.64069047617,
            "upper_bound": 148322.88203903328
          },
          "point_estimate": 148170.7570626855,
          "standard_error": 77.26111469167613
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147972.51219512196,
            "upper_bound": 148493.4783197832
          },
          "point_estimate": 148106.50076219512,
          "standard_error": 106.79051945277196
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.318316343897424,
            "upper_bound": 421.1133227676596
          },
          "point_estimate": 216.93307892306768,
          "standard_error": 139.63598477003623
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147967.69315848386,
            "upper_bound": 148329.84716283806
          },
          "point_estimate": 148143.65237039383,
          "standard_error": 93.39623604180017
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131.1462378064424,
            "upper_bound": 311.891482630375
          },
          "point_estimate": 256.8961217816473,
          "standard_error": 42.86994918305325
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 373583.76208170346,
            "upper_bound": 374490.17024943314
          },
          "point_estimate": 373972.45597222226,
          "standard_error": 236.22376599836383
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 373451.0306122449,
            "upper_bound": 374283.2274234694
          },
          "point_estimate": 373735.887755102,
          "standard_error": 196.62212222408724
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.57991425874324,
            "upper_bound": 936.3556805192148
          },
          "point_estimate": 440.3321921825409,
          "standard_error": 226.18922953616635
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 373465.79320152174,
            "upper_bound": 374066.0879404139
          },
          "point_estimate": 373676.0741849987,
          "standard_error": 155.24292625337972
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203.72656162065104,
            "upper_bound": 1119.1618480149823
          },
          "point_estimate": 784.355701036755,
          "standard_error": 263.58532378970045
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483648.2747483292,
            "upper_bound": 485852.968891604
          },
          "point_estimate": 484566.8671167502,
          "standard_error": 581.4756415828745
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483615.12266081874,
            "upper_bound": 485074.8736842105
          },
          "point_estimate": 484034.2006578947,
          "standard_error": 370.76815815182505
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 186.2345522857833,
            "upper_bound": 1900.6712198746843
          },
          "point_estimate": 731.8098839156613,
          "standard_error": 471.5663450952454
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483619.623963465,
            "upper_bound": 484555.2492327758
          },
          "point_estimate": 483991.0743335612,
          "standard_error": 239.72454060722916
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 494.963600236173,
            "upper_bound": 2876.8783150682757
          },
          "point_estimate": 1934.980673284804,
          "standard_error": 749.5314404644597
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46993.536859791784,
            "upper_bound": 47067.68296910613
          },
          "point_estimate": 47025.565368847405,
          "standard_error": 19.39112068473355
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46985.90885079776,
            "upper_bound": 47059.89708926261
          },
          "point_estimate": 47001.721652652006,
          "standard_error": 15.296494488232472
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.76789089048467,
            "upper_bound": 81.72532670238331
          },
          "point_estimate": 17.127714961126294,
          "standard_error": 16.26292604949961
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46992.23031324994,
            "upper_bound": 47010.44055147678
          },
          "point_estimate": 47001.46208565044,
          "standard_error": 4.677940043285686
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.542436674183442,
            "upper_bound": 88.36898321300215
          },
          "point_estimate": 64.58449797277375,
          "standard_error": 21.32244010472981
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99510.93598938356,
            "upper_bound": 99773.19634094369
          },
          "point_estimate": 99633.9848888889,
          "standard_error": 67.40521566215033
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99440.58328767124,
            "upper_bound": 99814.84383561644
          },
          "point_estimate": 99598.1049086758,
          "standard_error": 95.4881766874026
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.39897727313163,
            "upper_bound": 378.6852790852284
          },
          "point_estimate": 242.97697986437288,
          "standard_error": 84.15579537532403
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99495.89964349344,
            "upper_bound": 99657.07370871004
          },
          "point_estimate": 99567.67102650774,
          "standard_error": 41.42298557112959
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.04202125033156,
            "upper_bound": 288.1411052457304
          },
          "point_estimate": 223.73732362309337,
          "standard_error": 46.69655383972658
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70187.31198919285,
            "upper_bound": 70404.65589195144
          },
          "point_estimate": 70290.1603143901,
          "standard_error": 55.83681407572246
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70135.6837524178,
            "upper_bound": 70461.70212765958
          },
          "point_estimate": 70232.7248549323,
          "standard_error": 81.87516092197795
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.497740327462996,
            "upper_bound": 321.83770734233997
          },
          "point_estimate": 155.39984898190056,
          "standard_error": 72.6201625622915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70149.67625449329,
            "upper_bound": 70286.21243702115
          },
          "point_estimate": 70200.00434575096,
          "standard_error": 35.034252519545475
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.50700322301688,
            "upper_bound": 222.6115898727788
          },
          "point_estimate": 186.4034417080354,
          "standard_error": 33.96570824328521
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263776.10195652174,
            "upper_bound": 264523.7160609903
          },
          "point_estimate": 264116.7881602255,
          "standard_error": 192.5276606746267
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263671.21235909825,
            "upper_bound": 264572.3423913043
          },
          "point_estimate": 263970.38206521736,
          "standard_error": 178.8117397464724
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.72040761408388,
            "upper_bound": 915.819730480065
          },
          "point_estimate": 296.53255863641493,
          "standard_error": 220.27540352629535
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263831.96471056377,
            "upper_bound": 264335.15329353313
          },
          "point_estimate": 264001.1714850367,
          "standard_error": 131.0337317497052
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208.1309978251219,
            "upper_bound": 843.8210277491763
          },
          "point_estimate": 646.3550596640487,
          "standard_error": 167.34778905466925
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35914.35686900057,
            "upper_bound": 35986.58196028609
          },
          "point_estimate": 35949.72181947582,
          "standard_error": 18.560566100883843
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35893.57608695652,
            "upper_bound": 36006.64410408432
          },
          "point_estimate": 35937.17824851778,
          "standard_error": 29.614081534946276
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.155086570896138,
            "upper_bound": 100.07556798614624
          },
          "point_estimate": 69.0408340509577,
          "standard_error": 22.711802958134346
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35915.19918721817,
            "upper_bound": 35971.77600068402
          },
          "point_estimate": 35938.41174477696,
          "standard_error": 14.23583283728776
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.2323976907321,
            "upper_bound": 74.93890718623872
          },
          "point_estimate": 61.87878183776755,
          "standard_error": 9.587915908328382
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65602.6522826087,
            "upper_bound": 65695.1149715321
          },
          "point_estimate": 65646.99227024385,
          "standard_error": 23.63396027695359
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65599.39573268921,
            "upper_bound": 65692.55832686335
          },
          "point_estimate": 65637.44071557972,
          "standard_error": 23.983667035060893
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.83637307065234,
            "upper_bound": 135.4278400775531
          },
          "point_estimate": 66.41774818678716,
          "standard_error": 29.899285621667648
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65619.59684073263,
            "upper_bound": 65679.04624692914
          },
          "point_estimate": 65645.35581121777,
          "standard_error": 15.069733792336168
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.70350676493705,
            "upper_bound": 105.243027399873
          },
          "point_estimate": 78.91454859153329,
          "standard_error": 17.809284767352885
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155320.6519929078,
            "upper_bound": 155571.14863660926
          },
          "point_estimate": 155431.21221816953,
          "standard_error": 64.82362121624945
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155276.2560638298,
            "upper_bound": 155490.5829787234
          },
          "point_estimate": 155420.83988517395,
          "standard_error": 66.9102688860524
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.322404479497735,
            "upper_bound": 294.5859903870599
          },
          "point_estimate": 152.6596916514423,
          "standard_error": 64.07407991711624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155274.35463691026,
            "upper_bound": 155453.18907862296
          },
          "point_estimate": 155362.29557336282,
          "standard_error": 46.73432588783765
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.7002362345621,
            "upper_bound": 309.8172970814716
          },
          "point_estimate": 215.9108893659987,
          "standard_error": 69.04282275870682
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35683.899753622296,
            "upper_bound": 35768.08919258895
          },
          "point_estimate": 35720.664628083396,
          "standard_error": 21.72188828425756
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35673.4571054355,
            "upper_bound": 35744.363403187075
          },
          "point_estimate": 35714.03411918795,
          "standard_error": 14.824868424589308
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.73597860702961,
            "upper_bound": 92.13747154695346
          },
          "point_estimate": 37.58495307536202,
          "standard_error": 27.9044600723927
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35682.56579710013,
            "upper_bound": 35715.70303373255
          },
          "point_estimate": 35702.57418926849,
          "standard_error": 8.487778683414508
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.547832963385112,
            "upper_bound": 103.3429854434896
          },
          "point_estimate": 72.22990109932802,
          "standard_error": 23.217723747059978
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11381.280641107434,
            "upper_bound": 11428.492953839355
          },
          "point_estimate": 11403.15374521159,
          "standard_error": 12.098791505080603
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11377.997125078766,
            "upper_bound": 11427.843975355318
          },
          "point_estimate": 11391.739922585291,
          "standard_error": 13.042106358499606
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.130266225018401,
            "upper_bound": 64.60708611932509
          },
          "point_estimate": 28.049527975555975,
          "standard_error": 15.004239095218056
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11378.63298073804,
            "upper_bound": 11424.134081839184
          },
          "point_estimate": 11401.667088110376,
          "standard_error": 11.684936013310391
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.625706469063765,
            "upper_bound": 54.575722322699555
          },
          "point_estimate": 40.29856672548266,
          "standard_error": 10.088964075812632
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499721.8343281148,
            "upper_bound": 500338.3650636008
          },
          "point_estimate": 500014.611676995,
          "standard_error": 158.22251022240997
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499654.33659491193,
            "upper_bound": 500363.03500761034
          },
          "point_estimate": 499887.82648401824,
          "standard_error": 208.54159798798057
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.46858467748446,
            "upper_bound": 888.8799031233467
          },
          "point_estimate": 419.206270868114,
          "standard_error": 202.22538622378605
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499688.332524424,
            "upper_bound": 500215.7810603433
          },
          "point_estimate": 499996.5743106209,
          "standard_error": 133.14118026070184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 260.6898202483587,
            "upper_bound": 691.3174276518757
          },
          "point_estimate": 527.5267737709253,
          "standard_error": 114.06075282163908
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1028.6272517098928,
            "upper_bound": 1032.0572211639535
          },
          "point_estimate": 1030.4392016841207,
          "standard_error": 0.8827528578761041
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1027.8069689479817,
            "upper_bound": 1032.9044701396188
          },
          "point_estimate": 1031.2465032150535,
          "standard_error": 1.3560024057957711
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3372652739921316,
            "upper_bound": 5.1248111332935755
          },
          "point_estimate": 2.6070569385728604,
          "standard_error": 1.3644759778455438
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1027.3259344077217,
            "upper_bound": 1032.4567956044248
          },
          "point_estimate": 1030.2501733557217,
          "standard_error": 1.3199897715931457
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4057675991172824,
            "upper_bound": 3.5780464018761786
          },
          "point_estimate": 2.9427717350195572,
          "standard_error": 0.5562374851517478
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51973.634910034,
            "upper_bound": 52086.036160544216
          },
          "point_estimate": 52029.666324319725,
          "standard_error": 28.803673036626066
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51961.87630952381,
            "upper_bound": 52129.08190476191
          },
          "point_estimate": 52001.29428571429,
          "standard_error": 49.59630084753885
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.877189593127053,
            "upper_bound": 164.0694903871847
          },
          "point_estimate": 106.71083910550496,
          "standard_error": 42.94650145631552
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51923.289178596075,
            "upper_bound": 52080.502815775406
          },
          "point_estimate": 51990.41262337662,
          "standard_error": 40.64387628327346
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.826944185633465,
            "upper_bound": 117.27567412191874
          },
          "point_estimate": 96.21493627560308,
          "standard_error": 14.68572941271953
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52820.22424228267,
            "upper_bound": 53163.22248932235
          },
          "point_estimate": 52977.528151704966,
          "standard_error": 88.20075175667448
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52755.22093023256,
            "upper_bound": 53177.84215116279
          },
          "point_estimate": 52813.7006621447,
          "standard_error": 131.91481562769144
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7277204328216839,
            "upper_bound": 540.4969049972871
          },
          "point_estimate": 114.7452497131046,
          "standard_error": 138.0698732226404
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52773.64410883986,
            "upper_bound": 52923.278260339786
          },
          "point_estimate": 52820.892339927515,
          "standard_error": 39.22980411287941
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131.26788946259842,
            "upper_bound": 398.8672546428171
          },
          "point_estimate": 294.0963456184221,
          "standard_error": 74.82605581714165
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53292.84355010357,
            "upper_bound": 53468.542488712
          },
          "point_estimate": 53361.355848927065,
          "standard_error": 48.312801571038506
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53275.55522971652,
            "upper_bound": 53354.39034352744
          },
          "point_estimate": 53336.2193914956,
          "standard_error": 25.799107998179508
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.169977109931,
            "upper_bound": 108.93344818334836
          },
          "point_estimate": 50.57255963001928,
          "standard_error": 28.635486442386355
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53267.43137173288,
            "upper_bound": 53348.12231671554
          },
          "point_estimate": 53301.48347107438,
          "standard_error": 20.712398207995207
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.980001403213592,
            "upper_bound": 245.29101708582832
          },
          "point_estimate": 160.56996365778963,
          "standard_error": 72.6311981324119
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15313.05153111392,
            "upper_bound": 15351.905767465
          },
          "point_estimate": 15331.808222128571,
          "standard_error": 9.95006599703563
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15307.379098187948,
            "upper_bound": 15358.899599662873
          },
          "point_estimate": 15329.272603830126,
          "standard_error": 12.111323652867943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.870879470773993,
            "upper_bound": 60.8632037375551
          },
          "point_estimate": 25.849465839014872,
          "standard_error": 13.269253418325006
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15310.626764325783,
            "upper_bound": 15330.562727920971
          },
          "point_estimate": 15320.23530300294,
          "standard_error": 5.055327117582254
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.15815532331954,
            "upper_bound": 41.90352965890873
          },
          "point_estimate": 33.15485938396423,
          "standard_error": 6.329908237850233
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26515.28752385004,
            "upper_bound": 26566.650472890833
          },
          "point_estimate": 26539.412034981993,
          "standard_error": 13.09521255938844
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26509.186725018233,
            "upper_bound": 26562.02005835157
          },
          "point_estimate": 26533.432064997163,
          "standard_error": 13.957907857907148
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.25324497602793,
            "upper_bound": 69.95999422355712
          },
          "point_estimate": 37.088046688384,
          "standard_error": 15.680002458731217
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26513.97532829853,
            "upper_bound": 26556.388013090167
          },
          "point_estimate": 26534.19572404255,
          "standard_error": 10.634660522321637
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.131014145782252,
            "upper_bound": 59.54590456908297
          },
          "point_estimate": 43.67819565126376,
          "standard_error": 10.74130031167078
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16509.158618810878,
            "upper_bound": 16537.897984307358
          },
          "point_estimate": 16523.269123322512,
          "standard_error": 7.3673208506276495
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16503.687462121212,
            "upper_bound": 16543.563465909094
          },
          "point_estimate": 16523.56515827922,
          "standard_error": 8.438784061726412
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.573072267522929,
            "upper_bound": 47.99429392293199
          },
          "point_estimate": 16.854970292808094,
          "standard_error": 11.839082806649664
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16505.84238006572,
            "upper_bound": 16544.37113341204
          },
          "point_estimate": 16523.768229043682,
          "standard_error": 10.187515975272104
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.042390457866937,
            "upper_bound": 31.235965191859098
          },
          "point_estimate": 24.622655685736877,
          "standard_error": 4.669972666600982
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16514.07524984502,
            "upper_bound": 16536.358356212968
          },
          "point_estimate": 16524.3951305451,
          "standard_error": 5.72779122059892
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16508.614782016346,
            "upper_bound": 16534.12791401756
          },
          "point_estimate": 16521.56167625391,
          "standard_error": 6.380186583474703
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0834545553941852,
            "upper_bound": 29.293244393659787
          },
          "point_estimate": 16.14828582669284,
          "standard_error": 6.736237070143654
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16520.202983243453,
            "upper_bound": 16533.7641155032
          },
          "point_estimate": 16525.51379147646,
          "standard_error": 3.428522987217781
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.333846684498399,
            "upper_bound": 26.53584338182898
          },
          "point_estimate": 19.172009483718103,
          "standard_error": 5.175445403140618
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/krate_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19103.74617706821,
            "upper_bound": 19165.10868375186
          },
          "point_estimate": 19131.47335951713,
          "standard_error": 15.91367748136618
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19098.125446898,
            "upper_bound": 19162.09608307045
          },
          "point_estimate": 19114.51535977167,
          "standard_error": 13.412535188594957
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7309248904051722,
            "upper_bound": 70.40593226739229
          },
          "point_estimate": 28.281196441477437,
          "standard_error": 17.803258792712306
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19101.06511879083,
            "upper_bound": 19126.472372643915
          },
          "point_estimate": 19112.79606565884,
          "standard_error": 6.671859336989887
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.807404171975929,
            "upper_bound": 69.76144538978042
          },
          "point_estimate": 53.14966981032307,
          "standard_error": 14.813439792132076
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38839.646002502974,
            "upper_bound": 38898.76780966598
          },
          "point_estimate": 38869.91882484509,
          "standard_error": 15.120224608430474
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38836.1607486631,
            "upper_bound": 38908.47189839573
          },
          "point_estimate": 38874.97185828877,
          "standard_error": 21.03834471044118
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.055283671606075,
            "upper_bound": 85.8578061976548
          },
          "point_estimate": 48.25662188837165,
          "standard_error": 18.523127690919715
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38823.706371279426,
            "upper_bound": 38888.83534234368
          },
          "point_estimate": 38853.52323355789,
          "standard_error": 16.783434299786055
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.001438834846734,
            "upper_bound": 64.4233874547567
          },
          "point_estimate": 50.37422404564376,
          "standard_error": 9.46463740782552
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16616.229737542224,
            "upper_bound": 16638.84853264411
          },
          "point_estimate": 16625.366074388872,
          "standard_error": 6.072290299057193
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16614.643336602378,
            "upper_bound": 16626.506636155606
          },
          "point_estimate": 16621.27980549199,
          "standard_error": 3.3562877007142013
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.282570659705633,
            "upper_bound": 15.841691923178317
          },
          "point_estimate": 7.920845961589157,
          "standard_error": 3.63456556107997
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16616.8202728222,
            "upper_bound": 16624.06761086029
          },
          "point_estimate": 16620.59340366727,
          "standard_error": 1.858658636288881
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.427345490656218,
            "upper_bound": 30.737626277213323
          },
          "point_estimate": 20.327984568214433,
          "standard_error": 8.580707399717273
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20018.15038079551,
            "upper_bound": 20030.344153687303
          },
          "point_estimate": 20024.282937579253,
          "standard_error": 3.1087125114037906
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20018.922865013777,
            "upper_bound": 20031.554053522235
          },
          "point_estimate": 20023.509308999084,
          "standard_error": 3.1135596549116484
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6587631425715329,
            "upper_bound": 18.9588186386223
          },
          "point_estimate": 8.785892209037721,
          "standard_error": 4.873432880416124
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20018.26286465058,
            "upper_bound": 20032.886315467083
          },
          "point_estimate": 20024.924117205108,
          "standard_error": 3.876273888799368
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.2279987573969935,
            "upper_bound": 13.611443158079393
          },
          "point_estimate": 10.317162505809783,
          "standard_error": 2.1828063145637184
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16688.45709980223,
            "upper_bound": 16800.00699614162
          },
          "point_estimate": 16740.61715947056,
          "standard_error": 28.74745101887985
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16665.87126608456,
            "upper_bound": 16816.155962775734
          },
          "point_estimate": 16715.23825827206,
          "standard_error": 36.18179329163089
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.783371625427584,
            "upper_bound": 157.80112134445608
          },
          "point_estimate": 74.91529159240069,
          "standard_error": 36.95334182836749
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16669.948853044738,
            "upper_bound": 16735.34756572701
          },
          "point_estimate": 16692.504028600077,
          "standard_error": 16.890111659617027
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.69175415337305,
            "upper_bound": 122.17114048615991
          },
          "point_estimate": 96.13636194757396,
          "standard_error": 20.94436968382164
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19349.071324484,
            "upper_bound": 19405.5916001065
          },
          "point_estimate": 19376.817383614787,
          "standard_error": 14.473330740878312
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19332.180954916577,
            "upper_bound": 19413.56275292865
          },
          "point_estimate": 19378.51479854455,
          "standard_error": 25.22698435669006
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.671690141574343,
            "upper_bound": 87.91873105893875
          },
          "point_estimate": 62.09717718562714,
          "standard_error": 19.950201237954847
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19350.247081111767,
            "upper_bound": 19403.368830887528
          },
          "point_estimate": 19378.395442789373,
          "standard_error": 13.461460500532205
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.124833651434383,
            "upper_bound": 59.59557505927101
          },
          "point_estimate": 48.40269219535936,
          "standard_error": 7.772226397436747
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16509.524294703933,
            "upper_bound": 16545.284753326327
          },
          "point_estimate": 16528.909558497817,
          "standard_error": 9.163468969106235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16515.69898540812,
            "upper_bound": 16550.828264445314
          },
          "point_estimate": 16533.28259613923,
          "standard_error": 8.777588474176381
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.132986912975504,
            "upper_bound": 44.95173751590185
          },
          "point_estimate": 18.938530147128716,
          "standard_error": 9.92123976577655
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16524.93218930198,
            "upper_bound": 16552.379776303456
          },
          "point_estimate": 16538.712201159535,
          "standard_error": 7.11099279655707
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.469824220772445,
            "upper_bound": 42.771087877755306
          },
          "point_estimate": 30.62096014419776,
          "standard_error": 8.626154112365981
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16699.025599197223,
            "upper_bound": 16742.02459261768
          },
          "point_estimate": 16716.966342109103,
          "standard_error": 11.38561358704264
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16694.69140229885,
            "upper_bound": 16725.1820945083
          },
          "point_estimate": 16703.418074712645,
          "standard_error": 8.910899571230466
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9542366601955226,
            "upper_bound": 39.729064597351154
          },
          "point_estimate": 13.141911018410864,
          "standard_error": 9.82225724714133
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16697.017221350765,
            "upper_bound": 16713.91495476143
          },
          "point_estimate": 16705.069594864905,
          "standard_error": 4.317944690173663
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.41350399709076,
            "upper_bound": 55.68665437657214
          },
          "point_estimate": 37.89313254318718,
          "standard_error": 14.052159681672782
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16685.258275264125,
            "upper_bound": 16697.414705762265
          },
          "point_estimate": 16690.97736436483,
          "standard_error": 3.1415372718592174
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16683.020946256314,
            "upper_bound": 16698.81938064615
          },
          "point_estimate": 16688.60274587863,
          "standard_error": 4.490484845922297
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5891128978343678,
            "upper_bound": 17.306931364765184
          },
          "point_estimate": 8.456381586204055,
          "standard_error": 4.194334160937023
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16683.154918791675,
            "upper_bound": 16690.325113727915
          },
          "point_estimate": 16686.4883212332,
          "standard_error": 1.8152602156062856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.311471530612049,
            "upper_bound": 13.289190366600186
          },
          "point_estimate": 10.475873949761937,
          "standard_error": 2.043643963473633
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19719.742990408973,
            "upper_bound": 19782.318217126187
          },
          "point_estimate": 19747.913989969493,
          "standard_error": 16.126446706440387
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19707.82736156352,
            "upper_bound": 19783.785016286645
          },
          "point_estimate": 19729.35497647485,
          "standard_error": 18.65499771424862
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.752602188420983,
            "upper_bound": 86.5190853858052
          },
          "point_estimate": 34.52960180011409,
          "standard_error": 19.452780794736483
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19714.752824940188,
            "upper_bound": 19752.006286924458
          },
          "point_estimate": 19730.332941325774,
          "standard_error": 9.357368366713558
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.58588138406428,
            "upper_bound": 71.98028791011264
          },
          "point_estimate": 53.72702769311548,
          "standard_error": 13.841305294444544
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17030.535603829157,
            "upper_bound": 17047.902606875083
          },
          "point_estimate": 17038.637893552608,
          "standard_error": 4.46799019235664
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17027.14211704676,
            "upper_bound": 17049.894564198687
          },
          "point_estimate": 17033.861561119294,
          "standard_error": 5.848684133959391
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.423893461136076,
            "upper_bound": 23.661657952307426
          },
          "point_estimate": 10.674194335647558,
          "standard_error": 5.651094303839417
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17026.855025773195,
            "upper_bound": 17039.651835312226
          },
          "point_estimate": 17031.146570673936,
          "standard_error": 3.319865343546914
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.88189923814628,
            "upper_bound": 18.003367186232566
          },
          "point_estimate": 14.836962283927846,
          "standard_error": 3.012538605486356
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19678.528495330105,
            "upper_bound": 19704.58291466912
          },
          "point_estimate": 19691.16093578214,
          "standard_error": 6.661202613433037
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19675.51078493266,
            "upper_bound": 19709.206887755103
          },
          "point_estimate": 19687.265205627707,
          "standard_error": 8.57659105728676
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.878656024499474,
            "upper_bound": 37.70365140043317
          },
          "point_estimate": 18.789056456203532,
          "standard_error": 8.77968381597577
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19679.27388842837,
            "upper_bound": 19703.219565157066
          },
          "point_estimate": 19688.7475909372,
          "standard_error": 6.153816699550367
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.2440377487525,
            "upper_bound": 28.014653905968743
          },
          "point_estimate": 22.274384743546776,
          "standard_error": 4.138813369467512
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71813.40001235178,
            "upper_bound": 71887.83792325428
          },
          "point_estimate": 71852.17492588933,
          "standard_error": 19.121381950943544
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71792.60770750989,
            "upper_bound": 71896.02529644269
          },
          "point_estimate": 71866.83300395258,
          "standard_error": 24.941919993938345
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9560718802651268,
            "upper_bound": 108.50346376537328
          },
          "point_estimate": 44.873261535345065,
          "standard_error": 28.44256574991915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71829.77558395623,
            "upper_bound": 71881.87327075099
          },
          "point_estimate": 71861.6826446281,
          "standard_error": 13.313301217876834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.58581681957566,
            "upper_bound": 78.69252765254599
          },
          "point_estimate": 63.90712733305493,
          "standard_error": 11.829789057432984
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231581.6615805554,
            "upper_bound": 1233920.0213809523
          },
          "point_estimate": 1232670.495579365,
          "standard_error": 603.2055934250589
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231032.8777777776,
            "upper_bound": 1234106.066666667
          },
          "point_estimate": 1231860.0783333334,
          "standard_error": 930.0664718717436
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.5244646865774,
            "upper_bound": 3280.9072567522294
          },
          "point_estimate": 1344.0180594723772,
          "standard_error": 962.3015154458504
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231298.446682654,
            "upper_bound": 1234768.339526518
          },
          "point_estimate": 1232782.0376623378,
          "standard_error": 888.8208080101028
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 952.1435055055694,
            "upper_bound": 2572.157517767584
          },
          "point_estimate": 2013.7699927685148,
          "standard_error": 427.9036574920043
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201727.87458641973,
            "upper_bound": 202346.43601446212
          },
          "point_estimate": 202039.7203000441,
          "standard_error": 158.02803620362613
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201603.39263888888,
            "upper_bound": 202392.18518518517
          },
          "point_estimate": 202091.55410052912,
          "standard_error": 165.30186505340924
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.28244560790922,
            "upper_bound": 922.9915840302914
          },
          "point_estimate": 378.86504359327193,
          "standard_error": 230.13395470795055
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201454.04650654705,
            "upper_bound": 202170.24102956167
          },
          "point_estimate": 201820.50937950937,
          "standard_error": 194.29275801582253
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269.64429390846226,
            "upper_bound": 683.9733192675209
          },
          "point_estimate": 527.6613362437442,
          "standard_error": 105.905983195552
        }
      }
    },
    "memmem/krate/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6231.598516545697,
            "upper_bound": 6252.013234977422
          },
          "point_estimate": 6241.576383680598,
          "standard_error": 5.247503835061781
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6226.386187940217,
            "upper_bound": 6259.430902888556
          },
          "point_estimate": 6237.423357241024,
          "standard_error": 9.750437783449788
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.653538564667736,
            "upper_bound": 27.966532143934653
          },
          "point_estimate": 19.414552416876138,
          "standard_error": 6.9388685304442985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6228.087559873133,
            "upper_bound": 6245.771710216214
          },
          "point_estimate": 6234.352146393377,
          "standard_error": 4.59252787062603
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.120406193322124,
            "upper_bound": 20.23425220817617
          },
          "point_estimate": 17.481591100191693,
          "standard_error": 2.335175048488128
        }
      }
    },
    "memmem/krate/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6253.95133820608,
            "upper_bound": 6274.875555260798
          },
          "point_estimate": 6264.912604069841,
          "standard_error": 5.369777415614953
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6251.81948523061,
            "upper_bound": 6280.286109123214
          },
          "point_estimate": 6268.73722193432,
          "standard_error": 7.293113286679122
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.666718834971664,
            "upper_bound": 29.211942103888113
          },
          "point_estimate": 17.68574533149298,
          "standard_error": 6.742680883571544
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6250.4676994430065,
            "upper_bound": 6278.0196916269715
          },
          "point_estimate": 6264.231120822518,
          "standard_error": 7.213934748208415
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.56598620290701,
            "upper_bound": 22.874502976935176
          },
          "point_estimate": 18.008942623559555,
          "standard_error": 3.643588724821025
        }
      }
    },
    "memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15759.533241182424,
            "upper_bound": 15791.292869308329
          },
          "point_estimate": 15774.903462688771,
          "standard_error": 8.121601634717225
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15755.262434350005,
            "upper_bound": 15796.095576756288
          },
          "point_estimate": 15768.688483459298,
          "standard_error": 10.825697593080683
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.4207269892173775,
            "upper_bound": 44.167391872709494
          },
          "point_estimate": 22.856399553743493,
          "standard_error": 10.36620914289468
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15757.381400388367,
            "upper_bound": 15792.80594237016
          },
          "point_estimate": 15774.650992892624,
          "standard_error": 9.14589589650308
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.039179003391398,
            "upper_bound": 33.80110938793792
          },
          "point_estimate": 26.96485337444066,
          "standard_error": 4.938790454099847
        }
      }
    },
    "memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.499317210057576,
            "upper_bound": 57.98146353319258
          },
          "point_estimate": 57.7334313595523,
          "standard_error": 0.12400200293322183
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.27938680922936,
            "upper_bound": 58.07630534498461
          },
          "point_estimate": 57.659110053515946,
          "standard_error": 0.18146913113123583
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018206805987457175,
            "upper_bound": 0.7089721029588719
          },
          "point_estimate": 0.5680192884286127,
          "standard_error": 0.1803508744138569
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.60051935449504,
            "upper_bound": 58.059812586499824
          },
          "point_estimate": 57.80802887007235,
          "standard_error": 0.11766305741736256
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.23345399052785248,
            "upper_bound": 0.5085927462938971
          },
          "point_estimate": 0.4144004169752029,
          "standard_error": 0.06905812587105306
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.6491266819086,
            "upper_bound": 28.585983490764704
          },
          "point_estimate": 28.125115512628128,
          "standard_error": 0.23974766242928383
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.490140379144925,
            "upper_bound": 28.859785676482804
          },
          "point_estimate": 28.12813036027095,
          "standard_error": 0.38954534297013615
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.17940675385998808,
            "upper_bound": 1.3186965478639725
          },
          "point_estimate": 0.9327137222269272,
          "standard_error": 0.29392596580752656
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.081069442390927,
            "upper_bound": 28.868096999012216
          },
          "point_estimate": 28.588870407137488,
          "standard_error": 0.2009606962051898
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4931073123781476,
            "upper_bound": 0.9779300117711448
          },
          "point_estimate": 0.7997051997273561,
          "standard_error": 0.1250331656893541
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.846508146930137,
            "upper_bound": 24.886716933140885
          },
          "point_estimate": 24.86210435853526,
          "standard_error": 0.011070617045297848
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.842378002246203,
            "upper_bound": 24.86323748812542
          },
          "point_estimate": 24.850272161321115,
          "standard_error": 0.007250987508752277
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015330407273562254,
            "upper_bound": 0.026517416174815605
          },
          "point_estimate": 0.01384540432531146,
          "standard_error": 0.006998454207511809
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.844979013075264,
            "upper_bound": 24.857604447240487
          },
          "point_estimate": 24.851962961348903,
          "standard_error": 0.0032554028111748947
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007360092417215089,
            "upper_bound": 0.056449242635361975
          },
          "point_estimate": 0.03696629318811873,
          "standard_error": 0.016770430242153914
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.232923067226743,
            "upper_bound": 25.59056024571793
          },
          "point_estimate": 25.413010213717904,
          "standard_error": 0.09134169792181308
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.173081036930437,
            "upper_bound": 25.6766931222076
          },
          "point_estimate": 25.43784248869355,
          "standard_error": 0.11924170061752244
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06900074096887888,
            "upper_bound": 0.5221791378948042
          },
          "point_estimate": 0.35417525879514206,
          "standard_error": 0.12343349179450118
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.36554756986217,
            "upper_bound": 25.755098829686244
          },
          "point_estimate": 25.564841153802863,
          "standard_error": 0.10487684214901666
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.17098111521875833,
            "upper_bound": 0.3823155935496117
          },
          "point_estimate": 0.30474492313986495,
          "standard_error": 0.05372476722844459
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.54827844528089,
            "upper_bound": 29.766188792443387
          },
          "point_estimate": 29.653205035447805,
          "standard_error": 0.055900568672688306
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.539101459926137,
            "upper_bound": 29.8079225455524
          },
          "point_estimate": 29.59418239264332,
          "standard_error": 0.06922907325417428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030230597762093667,
            "upper_bound": 0.2982791099969023
          },
          "point_estimate": 0.1470617259761926,
          "standard_error": 0.08021447783211351
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.531021528037456,
            "upper_bound": 29.77440471372917
          },
          "point_estimate": 29.62402101867021,
          "standard_error": 0.06307624778023208
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08866126642165346,
            "upper_bound": 0.24054446822292713
          },
          "point_estimate": 0.1864491864442154,
          "standard_error": 0.03809549166342501
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.487901443895908,
            "upper_bound": 20.508118487988625
          },
          "point_estimate": 20.496388873006488,
          "standard_error": 0.005304362850693576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.48523161011775,
            "upper_bound": 20.501026028903844
          },
          "point_estimate": 20.492407053028515,
          "standard_error": 0.00403823594285581
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0021464027204843683,
            "upper_bound": 0.018348085410561443
          },
          "point_estimate": 0.008806322231744622,
          "standard_error": 0.004291399500149091
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.48547964379234,
            "upper_bound": 20.493776039322977
          },
          "point_estimate": 20.4891811965084,
          "standard_error": 0.002086641630791936
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004927705447532782,
            "upper_bound": 0.025928278756391827
          },
          "point_estimate": 0.017627028897008006,
          "standard_error": 0.0065130327308058015
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.00850815873326,
            "upper_bound": 27.02919514542299
          },
          "point_estimate": 27.017816335566287,
          "standard_error": 0.005297684048568595
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.007781990453196,
            "upper_bound": 27.031542374742955
          },
          "point_estimate": 27.01199823495592,
          "standard_error": 0.004855957597232038
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00127478862069185,
            "upper_bound": 0.02411821773651942
          },
          "point_estimate": 0.006595984979743278,
          "standard_error": 0.005639048503343961
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.008165671676423,
            "upper_bound": 27.0286965893865
          },
          "point_estimate": 27.015301882437686,
          "standard_error": 0.005471291264227889
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0043058103405579145,
            "upper_bound": 0.022481076061651516
          },
          "point_estimate": 0.01772668212913673,
          "standard_error": 0.004665004982720343
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.71310481198918,
            "upper_bound": 38.36848699035328
          },
          "point_estimate": 38.042932410221354,
          "standard_error": 0.1677235633234943
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.68822174909389,
            "upper_bound": 38.4626690207686
          },
          "point_estimate": 38.01565448036662,
          "standard_error": 0.1968245128095694
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.124472373090931,
            "upper_bound": 0.974070308472962
          },
          "point_estimate": 0.43743750256558606,
          "standard_error": 0.2086046277678676
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.70366035299515,
            "upper_bound": 38.1231802987172
          },
          "point_estimate": 37.8638878807249,
          "standard_error": 0.10758391558305395
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.28336925119598205,
            "upper_bound": 0.7343934222377305
          },
          "point_estimate": 0.558308491277637,
          "standard_error": 0.11510863093793068
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.049734213919187,
            "upper_bound": 28.096153134527547
          },
          "point_estimate": 28.06906236848577,
          "standard_error": 0.012285327523631016
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.041476643072563,
            "upper_bound": 28.0802195007887
          },
          "point_estimate": 28.05893458167027,
          "standard_error": 0.008830718093548476
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002203984807348403,
            "upper_bound": 0.040388463047543885
          },
          "point_estimate": 0.0204355909148868,
          "standard_error": 0.010680101337589612
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.04413233413005,
            "upper_bound": 28.064519068800653
          },
          "point_estimate": 28.055297610545317,
          "standard_error": 0.005307820937487944
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010080251660788605,
            "upper_bound": 0.06061278139466517
          },
          "point_estimate": 0.041057135865621465,
          "standard_error": 0.01556667148095798
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.32584224554624,
            "upper_bound": 37.34827629890969
          },
          "point_estimate": 37.33714914758902,
          "standard_error": 0.0057457510737458205
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.32027443953069,
            "upper_bound": 37.35221737696892
          },
          "point_estimate": 37.340495952807615,
          "standard_error": 0.008837839580830944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005284815037612839,
            "upper_bound": 0.03218998742726558
          },
          "point_estimate": 0.022443924380012394,
          "standard_error": 0.006995054172395189
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.332385514977105,
            "upper_bound": 37.35027352736482
          },
          "point_estimate": 37.34291454431442,
          "standard_error": 0.004555070756756531
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011573462476821896,
            "upper_bound": 0.023278839369585743
          },
          "point_estimate": 0.0190768766986966,
          "standard_error": 0.0029916219715397075
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.22852905857101,
            "upper_bound": 30.711173115887803
          },
          "point_estimate": 30.48090546616591,
          "standard_error": 0.12345036943797752
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.214046510389736,
            "upper_bound": 30.81500924122843
          },
          "point_estimate": 30.497062116930955,
          "standard_error": 0.16510413434739937
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14061416320054115,
            "upper_bound": 0.7416699764627332
          },
          "point_estimate": 0.4136541328630296,
          "standard_error": 0.1464243537402654
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.021221906804765,
            "upper_bound": 30.692365118108
          },
          "point_estimate": 30.359706275082576,
          "standard_error": 0.1735013266890328
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2214953706302709,
            "upper_bound": 0.5356551898156536
          },
          "point_estimate": 0.410503292229085,
          "standard_error": 0.08443327324394397
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.14381409416256,
            "upper_bound": 19.179845025793902
          },
          "point_estimate": 19.156658770071957,
          "standard_error": 0.010683943014641845
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.142309703444138,
            "upper_bound": 19.149741506251484
          },
          "point_estimate": 19.146552782539317,
          "standard_error": 0.0031916264848548923
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006202379970690942,
            "upper_bound": 0.008841193424707112
          },
          "point_estimate": 0.00571297679610174,
          "standard_error": 0.0036447426028577857
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.143673315075713,
            "upper_bound": 19.149134410757725
          },
          "point_estimate": 19.146017393565145,
          "standard_error": 0.00141727351919103
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002587298981183713,
            "upper_bound": 0.054815870927135905
          },
          "point_estimate": 0.03566023721687257,
          "standard_error": 0.018902664562283
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.186575167488883,
            "upper_bound": 31.246108338207826
          },
          "point_estimate": 31.21325264046772,
          "standard_error": 0.015376781941385946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.17719865980224,
            "upper_bound": 31.24436802965983
          },
          "point_estimate": 31.19564209924173,
          "standard_error": 0.016297906348430886
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004256920028683218,
            "upper_bound": 0.07113747036107748
          },
          "point_estimate": 0.03183292371970017,
          "standard_error": 0.017755903674747466
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.183230039080893,
            "upper_bound": 31.218784462289157
          },
          "point_estimate": 31.195416643416024,
          "standard_error": 0.009169525993249016
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017497334043414504,
            "upper_bound": 0.06985729034674774
          },
          "point_estimate": 0.051321347513980875,
          "standard_error": 0.014275735740479889
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103473.91762795928,
            "upper_bound": 103642.5453125
          },
          "point_estimate": 103555.81652777776,
          "standard_error": 43.339593525921416
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103432.9719854798,
            "upper_bound": 103723.37997159093
          },
          "point_estimate": 103512.19573863636,
          "standard_error": 74.77280723132459
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.08052567332439,
            "upper_bound": 234.44799614548631
          },
          "point_estimate": 151.51979854105255,
          "standard_error": 56.074448658882275
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103442.11245265152,
            "upper_bound": 103570.04962758604
          },
          "point_estimate": 103500.661533353,
          "standard_error": 32.5964176434986
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.84759190044886,
            "upper_bound": 170.35644308691346
          },
          "point_estimate": 144.36024586810228,
          "standard_error": 21.316842606200932
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56852.58555537574,
            "upper_bound": 57208.09030500992
          },
          "point_estimate": 57017.84464298115,
          "standard_error": 92.33423515873945
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56790.16036458333,
            "upper_bound": 57366.1265625
          },
          "point_estimate": 56827.73272879464,
          "standard_error": 156.3247040731734
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.695408124521824,
            "upper_bound": 489.3658852104557
          },
          "point_estimate": 77.9897309513368,
          "standard_error": 139.20326222692265
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56797.900783862875,
            "upper_bound": 56926.17113725568
          },
          "point_estimate": 56834.769107142856,
          "standard_error": 34.447956509336144
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.53159279009326,
            "upper_bound": 358.7758905173728
          },
          "point_estimate": 308.1333102834789,
          "standard_error": 55.81470020637072
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148692.69319029324,
            "upper_bound": 149051.6119504619
          },
          "point_estimate": 148859.3191658535,
          "standard_error": 92.61424628800944
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148688.14523565574,
            "upper_bound": 149032.20382513662
          },
          "point_estimate": 148750.1382058288,
          "standard_error": 93.07408125197124
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.870489931366436,
            "upper_bound": 480.6319228810114
          },
          "point_estimate": 125.25475991768248,
          "standard_error": 127.15460907279194
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148582.31599264103,
            "upper_bound": 148948.68766278535
          },
          "point_estimate": 148738.52700659996,
          "standard_error": 95.01855276782992
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.50361186458515,
            "upper_bound": 407.6680487769733
          },
          "point_estimate": 308.5211946951974,
          "standard_error": 76.012187628147
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396366.90308402345,
            "upper_bound": 397466.0962269021
          },
          "point_estimate": 396907.14120514144,
          "standard_error": 281.3221506564424
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396392.39130434784,
            "upper_bound": 397437.7445652174
          },
          "point_estimate": 396810.7820652174,
          "standard_error": 262.44712559208585
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123.3136412890624,
            "upper_bound": 1592.9352696364365
          },
          "point_estimate": 774.9203585250241,
          "standard_error": 363.4284810572929
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396139.0237400246,
            "upper_bound": 397484.6553760385
          },
          "point_estimate": 396756.7917278374,
          "standard_error": 338.2581562277154
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415.0331743206538,
            "upper_bound": 1259.2465892061798
          },
          "point_estimate": 934.4418209183056,
          "standard_error": 215.4278313267118
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579435.1184479718,
            "upper_bound": 580128.8703804484
          },
          "point_estimate": 579775.9071478962,
          "standard_error": 177.8139300206077
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579301.3968253969,
            "upper_bound": 580326.0634920635
          },
          "point_estimate": 579723.8492063492,
          "standard_error": 283.7371500743322
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.83351918627265,
            "upper_bound": 1020.0338247478192
          },
          "point_estimate": 644.076728565354,
          "standard_error": 226.79747861304932
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579327.9421480944,
            "upper_bound": 580313.8134027459
          },
          "point_estimate": 579781.5150690579,
          "standard_error": 253.5629049284402
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355.90458071850315,
            "upper_bound": 727.9371193208839
          },
          "point_estimate": 593.1295997773187,
          "standard_error": 94.9309350468314
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52213.06407028256,
            "upper_bound": 52400.159422485056
          },
          "point_estimate": 52300.02326673964,
          "standard_error": 48.02428172182047
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52171.31315852491,
            "upper_bound": 52405.860426929394
          },
          "point_estimate": 52285.85162037037,
          "standard_error": 54.58329365077449
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.649647089712936,
            "upper_bound": 260.9198439309648
          },
          "point_estimate": 128.91761207667713,
          "standard_error": 62.28547022747874
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52206.421369788855,
            "upper_bound": 52303.18818112007
          },
          "point_estimate": 52259.00190326914,
          "standard_error": 24.320197559957062
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.65982290460965,
            "upper_bound": 211.1467679268424
          },
          "point_estimate": 159.61834600902415,
          "standard_error": 37.40040509216431
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100597.34804690104,
            "upper_bound": 100734.1395961589
          },
          "point_estimate": 100664.36228645971,
          "standard_error": 34.92887932605977
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100585.85980662984,
            "upper_bound": 100757.04834254144
          },
          "point_estimate": 100659.44830307814,
          "standard_error": 34.1240360934039
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.60126828257924,
            "upper_bound": 218.47234621340627
          },
          "point_estimate": 75.14641925980841,
          "standard_error": 52.971276297251414
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100543.98973954224,
            "upper_bound": 100702.59248658676
          },
          "point_estimate": 100629.19321231254,
          "standard_error": 41.95866207718049
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.28736749491027,
            "upper_bound": 153.66370038360395
          },
          "point_estimate": 116.11697450623078,
          "standard_error": 25.36353452207041
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70735.91151275179,
            "upper_bound": 70839.71211339692
          },
          "point_estimate": 70784.71635678703,
          "standard_error": 26.75922624581587
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70710.76754385965,
            "upper_bound": 70866.23440545809
          },
          "point_estimate": 70752.99317738792,
          "standard_error": 41.39822884977029
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.068842077224434,
            "upper_bound": 145.45746642345935
          },
          "point_estimate": 64.4552872572589,
          "standard_error": 37.880188970810025
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70737.52530265723,
            "upper_bound": 70887.08040338942
          },
          "point_estimate": 70816.41813118655,
          "standard_error": 39.521535646422144
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.10253619360858,
            "upper_bound": 107.47500154696584
          },
          "point_estimate": 89.25115837438058,
          "standard_error": 16.382047299910244
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308529.734677655,
            "upper_bound": 309286.38154728274
          },
          "point_estimate": 308888.2608551924,
          "standard_error": 193.79870947150712
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308232.2917675545,
            "upper_bound": 309421.0037076271
          },
          "point_estimate": 308807.9543314501,
          "standard_error": 247.4337745360094
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.65581184492518,
            "upper_bound": 1158.831370952062
          },
          "point_estimate": 658.0329722158848,
          "standard_error": 284.9621215216488
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308460.12864789093,
            "upper_bound": 309057.21302524064
          },
          "point_estimate": 308729.8429892142,
          "standard_error": 149.30731383618794
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322.663709931352,
            "upper_bound": 813.5149875911202
          },
          "point_estimate": 644.9881088783212,
          "standard_error": 126.42433651799809
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36996.44559742023,
            "upper_bound": 37059.75540200595
          },
          "point_estimate": 37025.16795687454,
          "standard_error": 16.32561340947431
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36979.80334917402,
            "upper_bound": 37059.933299389
          },
          "point_estimate": 37015.78854378819,
          "standard_error": 21.479272576676905
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.28113743248496,
            "upper_bound": 89.59803063945714
          },
          "point_estimate": 56.31726791735179,
          "standard_error": 21.022509524107384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36992.55313500007,
            "upper_bound": 37039.65335577976
          },
          "point_estimate": 37013.64568466157,
          "standard_error": 11.918420364657992
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.481298973629567,
            "upper_bound": 74.02806403835926
          },
          "point_estimate": 54.307843213222554,
          "standard_error": 13.991164801342984
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66605.58248920403,
            "upper_bound": 66725.21040515945
          },
          "point_estimate": 66665.84914361578,
          "standard_error": 30.70533230808807
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66563.85324754902,
            "upper_bound": 66752.82978451798
          },
          "point_estimate": 66680.04453343837,
          "standard_error": 51.95835062107291
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.431506222564927,
            "upper_bound": 169.39479482351484
          },
          "point_estimate": 134.65163131043664,
          "standard_error": 37.42223539227322
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66575.01336255656,
            "upper_bound": 66737.9720125989
          },
          "point_estimate": 66649.02537242169,
          "standard_error": 42.96610326845859
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.68117330381902,
            "upper_bound": 122.0264962767146
          },
          "point_estimate": 102.21086604741043,
          "standard_error": 14.685413970803982
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166654.35088175331,
            "upper_bound": 166937.19721984217
          },
          "point_estimate": 166797.9967815276,
          "standard_error": 72.63961261278334
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166542.5152905199,
            "upper_bound": 167003.7505733945
          },
          "point_estimate": 166860.5784731324,
          "standard_error": 139.5241083929548
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.41229141743361,
            "upper_bound": 382.7306895354332
          },
          "point_estimate": 318.1658032199272,
          "standard_error": 94.26037230484792
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166704.20816087065,
            "upper_bound": 166961.84866084243
          },
          "point_estimate": 166827.3463004885,
          "standard_error": 65.29474147327488
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159.82201207876824,
            "upper_bound": 277.9872084072543
          },
          "point_estimate": 242.07901141168932,
          "standard_error": 30.488249207526348
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35977.47461810467,
            "upper_bound": 36021.99712595375
          },
          "point_estimate": 35998.44325691498,
          "standard_error": 11.435474558040662
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35967.22059405941,
            "upper_bound": 36027.08886138614
          },
          "point_estimate": 35984.96464108911,
          "standard_error": 16.35639015795183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.739709174903601,
            "upper_bound": 64.41317056930924
          },
          "point_estimate": 36.6890485235567,
          "standard_error": 14.444657333062612
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35967.64432778927,
            "upper_bound": 35990.61057608378
          },
          "point_estimate": 35975.5218747589,
          "standard_error": 5.909585900983741
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.476827154193852,
            "upper_bound": 48.624254871014784
          },
          "point_estimate": 38.15737127931877,
          "standard_error": 7.714731847644802
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11838.012474168669,
            "upper_bound": 11879.573685260224
          },
          "point_estimate": 11858.013218638578,
          "standard_error": 10.671086068810562
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11829.947827985156,
            "upper_bound": 11888.814603798295
          },
          "point_estimate": 11852.99173215455,
          "standard_error": 13.2477669048792
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.742213190715299,
            "upper_bound": 56.40595453232109
          },
          "point_estimate": 41.485591832791286,
          "standard_error": 14.820596295279222
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11844.678271255569,
            "upper_bound": 11896.629100648754
          },
          "point_estimate": 11868.610267990032,
          "standard_error": 13.20384662003146
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.51506998795408,
            "upper_bound": 45.55348367986696
          },
          "point_estimate": 35.44995165297334,
          "standard_error": 7.033145359056007
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506326.3757188878,
            "upper_bound": 507023.1082553048
          },
          "point_estimate": 506599.40524526016,
          "standard_error": 190.94611417846235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506270.2453703704,
            "upper_bound": 506647.9552469136
          },
          "point_estimate": 506338.23164682544,
          "standard_error": 118.52538879874864
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.555253052478477,
            "upper_bound": 475.6232194726877
          },
          "point_estimate": 139.2404062780437,
          "standard_error": 127.03103365441125
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506303.5401814387,
            "upper_bound": 506578.7743130227
          },
          "point_estimate": 506435.01132756134,
          "standard_error": 71.55908101691783
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.95855787066138,
            "upper_bound": 966.258441393148
          },
          "point_estimate": 636.0762677457461,
          "standard_error": 282.26177122052263
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1078.0473059061092,
            "upper_bound": 1079.048268490038
          },
          "point_estimate": 1078.4656117426,
          "standard_error": 0.26283918089853503
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1077.8623761986337,
            "upper_bound": 1078.555652388547
          },
          "point_estimate": 1078.3519099375826,
          "standard_error": 0.1678437323301892
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07765572400379679,
            "upper_bound": 0.8931606717206116
          },
          "point_estimate": 0.34921372526660105,
          "standard_error": 0.22090515141205816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1078.1545755218772,
            "upper_bound": 1078.4973964242188
          },
          "point_estimate": 1078.367924785947,
          "standard_error": 0.08724861985100373
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2452651770701758,
            "upper_bound": 1.3082971528650025
          },
          "point_estimate": 0.8785767065820296,
          "standard_error": 0.3399176238564795
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51841.1132562708,
            "upper_bound": 51970.50525575709
          },
          "point_estimate": 51904.39729915314,
          "standard_error": 33.008133394285274
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51802.30470756063,
            "upper_bound": 51988.1035128388
          },
          "point_estimate": 51912.233475986686,
          "standard_error": 44.41442044106599
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.905176204552866,
            "upper_bound": 198.28041540516537
          },
          "point_estimate": 121.00477972762776,
          "standard_error": 44.61159674023825
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51843.01379839183,
            "upper_bound": 51988.963168215625
          },
          "point_estimate": 51918.22112010671,
          "standard_error": 37.09458970372738
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.405964106013506,
            "upper_bound": 140.08422505942963
          },
          "point_estimate": 109.88595693059663,
          "standard_error": 20.30277232230301
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52671.11156165336,
            "upper_bound": 52868.64885861152
          },
          "point_estimate": 52751.36799508143,
          "standard_error": 52.76204417531781
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52642.38606676342,
            "upper_bound": 52781.10856313498
          },
          "point_estimate": 52698.99542815675,
          "standard_error": 37.089482764395385
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.41863696699686,
            "upper_bound": 159.15638990300363
          },
          "point_estimate": 87.51557400506871,
          "standard_error": 36.54494704957838
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52660.63153542377,
            "upper_bound": 52763.34821376984
          },
          "point_estimate": 52707.666284658735,
          "standard_error": 26.120476319356953
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.42375112644335,
            "upper_bound": 263.01223324666114
          },
          "point_estimate": 175.53842325074567,
          "standard_error": 70.93264388746216
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52994.42472556756,
            "upper_bound": 53119.90854217713
          },
          "point_estimate": 53058.31068763843,
          "standard_error": 32.021896807560495
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52994.261458333334,
            "upper_bound": 53131.06455910853
          },
          "point_estimate": 53064.51844545958,
          "standard_error": 29.978188855713373
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.3125160266317,
            "upper_bound": 183.87349972977628
          },
          "point_estimate": 67.05914970335574,
          "standard_error": 44.53505908918828
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53038.817830568594,
            "upper_bound": 53103.012630529
          },
          "point_estimate": 53065.30677665358,
          "standard_error": 16.390098712843507
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.19955591475744,
            "upper_bound": 142.39526276584547
          },
          "point_estimate": 106.5300014477968,
          "standard_error": 23.803268096949644
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15222.02732898438,
            "upper_bound": 15251.415338680588
          },
          "point_estimate": 15236.561335441313,
          "standard_error": 7.530337133204867
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15216.97784876414,
            "upper_bound": 15258.634082763116
          },
          "point_estimate": 15233.169250104733,
          "standard_error": 9.053641981172277
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.9462500765668262,
            "upper_bound": 43.86028545299589
          },
          "point_estimate": 25.005304932413033,
          "standard_error": 11.812274977510478
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15218.345212663577,
            "upper_bound": 15249.306276317971
          },
          "point_estimate": 15233.580030359251,
          "standard_error": 7.89398821835211
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.954970330025338,
            "upper_bound": 31.742148777473737
          },
          "point_estimate": 25.020700119882,
          "standard_error": 4.589299570699312
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26451.56736404896,
            "upper_bound": 26486.987213239387
          },
          "point_estimate": 26469.391743629378,
          "standard_error": 9.069221374801126
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26435.56382202772,
            "upper_bound": 26497.825883088462
          },
          "point_estimate": 26470.12320386579,
          "standard_error": 16.33327824030008
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.760214503863619,
            "upper_bound": 49.47616864812078
          },
          "point_estimate": 42.75611815230657,
          "standard_error": 11.743912108937932
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26446.08908693127,
            "upper_bound": 26484.085436421105
          },
          "point_estimate": 26462.67319143293,
          "standard_error": 9.84326387525161
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.398124677262803,
            "upper_bound": 35.39148873743398
          },
          "point_estimate": 30.21724406753822,
          "standard_error": 4.05707705268423
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16634.158270728356,
            "upper_bound": 16661.30915725047
          },
          "point_estimate": 16647.187989642185,
          "standard_error": 6.945304471852562
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16625.66264887769,
            "upper_bound": 16665.751951951952
          },
          "point_estimate": 16644.263742556115,
          "standard_error": 10.933926498007931
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.578893642365029,
            "upper_bound": 41.536247162264715
          },
          "point_estimate": 24.500592785366603,
          "standard_error": 8.957330595321462
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16633.028076300176,
            "upper_bound": 16667.31561363366
          },
          "point_estimate": 16652.188032672777,
          "standard_error": 8.68331928609512
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.1654989539772,
            "upper_bound": 29.007657634836296
          },
          "point_estimate": 23.20405048144272,
          "standard_error": 4.1216555507043555
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16464.420149668655,
            "upper_bound": 16496.6905700166
          },
          "point_estimate": 16479.907638323024,
          "standard_error": 8.300804384177257
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16456.473726799457,
            "upper_bound": 16505.437943262412
          },
          "point_estimate": 16471.292270258033,
          "standard_error": 11.414590722340392
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.457643651057369,
            "upper_bound": 47.61783680928499
          },
          "point_estimate": 29.9267430121539,
          "standard_error": 11.448568693492003
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16454.419039305194,
            "upper_bound": 16476.74696835552
          },
          "point_estimate": 16462.989779708747,
          "standard_error": 5.692134527304992
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.820127951147091,
            "upper_bound": 33.96001151108272
          },
          "point_estimate": 27.605372246406283,
          "standard_error": 5.006756005311307
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19094.60087848501,
            "upper_bound": 19127.07993864967
          },
          "point_estimate": 19110.04333949133,
          "standard_error": 8.340109055832936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19085.84665307733,
            "upper_bound": 19129.428590215677
          },
          "point_estimate": 19104.028194225262,
          "standard_error": 11.02907408090765
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.803805026061487,
            "upper_bound": 46.92603838310482
          },
          "point_estimate": 28.63013629251706,
          "standard_error": 10.29724160193438
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19085.502366983375,
            "upper_bound": 19113.088016367354
          },
          "point_estimate": 19096.823115653417,
          "standard_error": 7.069537492640055
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.376675852695186,
            "upper_bound": 35.84542298670686
          },
          "point_estimate": 27.788409192447627,
          "standard_error": 5.661182403165279
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37135.0583763092,
            "upper_bound": 37195.80667079755
          },
          "point_estimate": 37164.591257155866,
          "standard_error": 15.567279934054842
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37115.70773565574,
            "upper_bound": 37203.35114168618
          },
          "point_estimate": 37161.8899931694,
          "standard_error": 21.31572967620646
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.76030050639305,
            "upper_bound": 92.26146721448922
          },
          "point_estimate": 53.59423610452322,
          "standard_error": 19.42271212068148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37138.91824311552,
            "upper_bound": 37189.886829359166
          },
          "point_estimate": 37161.08439429423,
          "standard_error": 12.900237240932706
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.378043999013,
            "upper_bound": 65.07567427750443
          },
          "point_estimate": 51.92614042292641,
          "standard_error": 9.164245741791712
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15773.652202484474,
            "upper_bound": 15795.654747343
          },
          "point_estimate": 15783.352009247756,
          "standard_error": 5.686203990622138
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15770.349033816425,
            "upper_bound": 15789.707739130436
          },
          "point_estimate": 15780.712282608696,
          "standard_error": 4.965326841919914
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.258623387438539,
            "upper_bound": 24.562604413928035
          },
          "point_estimate": 13.011407873349969,
          "standard_error": 5.388109344205181
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15769.271030540092,
            "upper_bound": 15782.606845351764
          },
          "point_estimate": 15776.052739695087,
          "standard_error": 3.416281246076531
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.080575760870341,
            "upper_bound": 27.100180346748164
          },
          "point_estimate": 18.943895827614895,
          "standard_error": 5.996463451542073
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20106.061952671316,
            "upper_bound": 20139.51322752019
          },
          "point_estimate": 20122.248096755888,
          "standard_error": 8.561880596765397
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20096.458940449797,
            "upper_bound": 20137.72448956023
          },
          "point_estimate": 20128.32603473762,
          "standard_error": 12.680699777010634
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.577787668173075,
            "upper_bound": 55.56288975081072
          },
          "point_estimate": 30.96678992279069,
          "standard_error": 12.90771734523207
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20113.071726728267,
            "upper_bound": 20142.314440989
          },
          "point_estimate": 20130.15343104789,
          "standard_error": 7.415310194283406
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.506574004991474,
            "upper_bound": 37.20871530699138
          },
          "point_estimate": 28.623217425629292,
          "standard_error": 5.672823680449247
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16585.680223404717,
            "upper_bound": 16625.061872080034
          },
          "point_estimate": 16601.927630981398,
          "standard_error": 10.44653221230295
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16580.904079067644,
            "upper_bound": 16609.868220597196
          },
          "point_estimate": 16588.53296879081,
          "standard_error": 8.062105858650622
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8120774535920974,
            "upper_bound": 34.616063229896454
          },
          "point_estimate": 18.49339620960999,
          "standard_error": 8.44950664878551
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16584.922288706777,
            "upper_bound": 16600.628252751954
          },
          "point_estimate": 16592.680559842353,
          "standard_error": 4.052591184225474
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.078609612983758,
            "upper_bound": 51.58186917608059
          },
          "point_estimate": 34.84415805908406,
          "standard_error": 13.35781154388997
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19276.718028968466,
            "upper_bound": 19311.046671809803
          },
          "point_estimate": 19292.33108055366,
          "standard_error": 8.848754670502569
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19271.258789166222,
            "upper_bound": 19318.025402726147
          },
          "point_estimate": 19284.096396117307,
          "standard_error": 9.257439724504698
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.7670829566633457,
            "upper_bound": 41.48137045314956
          },
          "point_estimate": 14.71506625495389,
          "standard_error": 9.31105952269128
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19273.64137954227,
            "upper_bound": 19285.20367256086
          },
          "point_estimate": 19279.458709850955,
          "standard_error": 2.9317178397314296
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.351355610805417,
            "upper_bound": 37.21805060219301
          },
          "point_estimate": 29.55747642860309,
          "standard_error": 7.299749233185338
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16462.4917558579,
            "upper_bound": 16483.347054421767
          },
          "point_estimate": 16472.919405139834,
          "standard_error": 5.3476858350495045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16454.120068027212,
            "upper_bound": 16487.797256235826
          },
          "point_estimate": 16473.301451247164,
          "standard_error": 9.055177544457424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.316820113835213,
            "upper_bound": 28.68912755416084
          },
          "point_estimate": 23.228665905070994,
          "standard_error": 6.551762555091064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16460.350923387596,
            "upper_bound": 16484.319255848448
          },
          "point_estimate": 16472.147318078747,
          "standard_error": 6.05918725604496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.190421545403307,
            "upper_bound": 21.46529138336058
          },
          "point_estimate": 17.81789116859403,
          "standard_error": 2.6252970379948386
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16494.510824916808,
            "upper_bound": 16522.99936204768
          },
          "point_estimate": 16507.874508578934,
          "standard_error": 7.347210468413502
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16487.448249204186,
            "upper_bound": 16521.071415037135
          },
          "point_estimate": 16506.608664111394,
          "standard_error": 11.272260783040746
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.364785024391997,
            "upper_bound": 45.44937337758845
          },
          "point_estimate": 24.177245678088063,
          "standard_error": 10.363629006752054
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16488.88805840179,
            "upper_bound": 16515.039663383457
          },
          "point_estimate": 16499.74813108674,
          "standard_error": 6.701154711738627
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.131737853620988,
            "upper_bound": 32.82176896927701
          },
          "point_estimate": 24.54826312284266,
          "standard_error": 5.686542103010753
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16481.840989794888,
            "upper_bound": 16533.089959479337
          },
          "point_estimate": 16504.32070146631,
          "standard_error": 13.31100285825022
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16476.658318209316,
            "upper_bound": 16526.444782214156
          },
          "point_estimate": 16490.597228523897,
          "standard_error": 11.41364513108472
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.778086964119279,
            "upper_bound": 59.01804012281885
          },
          "point_estimate": 22.130420875256448,
          "standard_error": 13.229800705807325
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16471.90895270125,
            "upper_bound": 16500.38649041338
          },
          "point_estimate": 16482.790766493035,
          "standard_error": 7.275237538067318
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.973742351809344,
            "upper_bound": 61.50450961086083
          },
          "point_estimate": 44.41513520307189,
          "standard_error": 13.673481386893954
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19658.074364412976,
            "upper_bound": 19674.51749747201
          },
          "point_estimate": 19665.78010301123,
          "standard_error": 4.20777226191712
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19654.84174276428,
            "upper_bound": 19672.908559046587
          },
          "point_estimate": 19664.288581918863,
          "standard_error": 4.380098538756571
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5533576975350005,
            "upper_bound": 22.393684737861136
          },
          "point_estimate": 12.995696681523794,
          "standard_error": 5.2349385545373375
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19659.926861035245,
            "upper_bound": 19667.99763251876
          },
          "point_estimate": 19664.070566053662,
          "standard_error": 2.034072666537974
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.548543552512613,
            "upper_bound": 18.858079600218836
          },
          "point_estimate": 14.027229815203569,
          "standard_error": 3.3661226937606097
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16998.508386912195,
            "upper_bound": 17031.562382958804
          },
          "point_estimate": 17015.098480935292,
          "standard_error": 8.472969392679506
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16982.27489076155,
            "upper_bound": 17040.043102372034
          },
          "point_estimate": 17013.981764981276,
          "standard_error": 14.953355874988445
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.736011616534711,
            "upper_bound": 45.40144868132254
          },
          "point_estimate": 40.10055800529839,
          "standard_error": 12.761287445140004
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16987.301384562612,
            "upper_bound": 17031.767661147867
          },
          "point_estimate": 17007.078592100785,
          "standard_error": 11.36182211887319
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.396858017942368,
            "upper_bound": 33.23967307919064
          },
          "point_estimate": 28.20109524363794,
          "standard_error": 3.9363234425335127
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19605.30943193116,
            "upper_bound": 19639.749997685896
          },
          "point_estimate": 19621.874501782713,
          "standard_error": 8.853507772895986
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19590.731521478283,
            "upper_bound": 19644.335313174943
          },
          "point_estimate": 19621.794284171552,
          "standard_error": 11.263315834618853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.424679825563542,
            "upper_bound": 52.71362058142639
          },
          "point_estimate": 31.112284714025655,
          "standard_error": 14.322087743668694
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19599.7306713768,
            "upper_bound": 19626.63873730387
          },
          "point_estimate": 19613.70382457715,
          "standard_error": 6.937661791744706
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.880401535564433,
            "upper_bound": 37.647859201816715
          },
          "point_estimate": 29.61529148122072,
          "standard_error": 5.717725277523137
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71504.67903664974,
            "upper_bound": 71618.38753535105
          },
          "point_estimate": 71556.39661034559,
          "standard_error": 29.32635400067142
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71485.80472440945,
            "upper_bound": 71606.30696358267
          },
          "point_estimate": 71534.2124015748,
          "standard_error": 30.05345768157735
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.90934346630399,
            "upper_bound": 142.02681485549456
          },
          "point_estimate": 77.66861168212989,
          "standard_error": 32.71508604375158
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71493.07590426526,
            "upper_bound": 71547.57867039951
          },
          "point_estimate": 71522.53246753247,
          "standard_error": 14.287904024566556
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.2308133101872,
            "upper_bound": 132.0230993552329
          },
          "point_estimate": 97.73717319878575,
          "standard_error": 26.00859626514339
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1232020.4949091931,
            "upper_bound": 1235638.1082777777
          },
          "point_estimate": 1233738.690156085,
          "standard_error": 925.3490993163076
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1230722.433333333,
            "upper_bound": 1235439.1416666666
          },
          "point_estimate": 1234140.802962963,
          "standard_error": 1401.1419764644493
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 423.2328724862986,
            "upper_bound": 6117.553431391808
          },
          "point_estimate": 2966.045382342175,
          "standard_error": 1405.1574330210217
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231834.8540112297,
            "upper_bound": 1234478.8009920637
          },
          "point_estimate": 1233390.563982684,
          "standard_error": 670.6251314495267
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1722.663809192116,
            "upper_bound": 4054.9331141091766
          },
          "point_estimate": 3076.883331857383,
          "standard_error": 660.4215902610098
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202191.13774176585,
            "upper_bound": 202447.15562221123
          },
          "point_estimate": 202320.57187257495,
          "standard_error": 65.33482368726568
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202149.95,
            "upper_bound": 202436.5777777778
          },
          "point_estimate": 202364.53442460316,
          "standard_error": 71.6405041494089
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.363824780504396,
            "upper_bound": 405.8304434617333
          },
          "point_estimate": 145.28471404108876,
          "standard_error": 105.45054649179578
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202196.82874963255,
            "upper_bound": 202557.2095050803
          },
          "point_estimate": 202398.515007215,
          "standard_error": 94.33693190378348
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.9951601254566,
            "upper_bound": 281.2965872201564
          },
          "point_estimate": 217.348493362871,
          "standard_error": 43.11630246866321
        }
      }
    },
    "memmem/krate/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6122.9793067826995,
            "upper_bound": 6131.8727766248085
          },
          "point_estimate": 6127.324950740071,
          "standard_error": 2.275916248483905
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6121.398185654009,
            "upper_bound": 6132.316751054852
          },
          "point_estimate": 6127.3305907173,
          "standard_error": 2.758772450450791
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2822175164763263,
            "upper_bound": 12.639221076874096
          },
          "point_estimate": 7.914689317994948,
          "standard_error": 2.9163000364286744
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6120.35457049598,
            "upper_bound": 6130.258521075736
          },
          "point_estimate": 6125.874975724697,
          "standard_error": 2.538054388165378
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.099810787384922,
            "upper_bound": 9.66114227712496
          },
          "point_estimate": 7.550181779838831,
          "standard_error": 1.4326424666081417
        }
      }
    },
    "memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6185.13086989447,
            "upper_bound": 6216.76582014201
          },
          "point_estimate": 6199.933281789461,
          "standard_error": 8.124080863150262
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6179.188753989968,
            "upper_bound": 6216.683310533515
          },
          "point_estimate": 6196.631643296854,
          "standard_error": 9.550156506526696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.853533357494003,
            "upper_bound": 44.003278203052886
          },
          "point_estimate": 23.72747285166698,
          "standard_error": 9.781751953147996
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6180.472431441479,
            "upper_bound": 6201.029157166743
          },
          "point_estimate": 6190.3316294170945,
          "standard_error": 5.136141273690745
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.668640781179391,
            "upper_bound": 35.233721512216405
          },
          "point_estimate": 27.05227485268307,
          "standard_error": 5.948776806139613
        }
      }
    },
    "memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15587.336635774132,
            "upper_bound": 15623.768037754096
          },
          "point_estimate": 15604.351342796615,
          "standard_error": 9.305986130927062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15584.954129772446,
            "upper_bound": 15624.862953441874
          },
          "point_estimate": 15596.974855398455,
          "standard_error": 9.113210188465173
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.403811040068305,
            "upper_bound": 51.16484905308301
          },
          "point_estimate": 22.131343303534873,
          "standard_error": 11.863603083482738
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15578.175129696325,
            "upper_bound": 15604.064966749313
          },
          "point_estimate": 15589.08415962786,
          "standard_error": 6.505702213859389
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.476277491280545,
            "upper_bound": 41.909638006398005
          },
          "point_estimate": 31.10862669810826,
          "standard_error": 7.63504020359626
        }
      }
    },
    "memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.99171870880372,
            "upper_bound": 35.27289948168329
          },
          "point_estimate": 35.1076463517248,
          "standard_error": 0.07485528216776678
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.97595297761537,
            "upper_bound": 35.182636193422255
          },
          "point_estimate": 35.01349536442599,
          "standard_error": 0.04397130200082461
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012989842119640672,
            "upper_bound": 0.235213811820201
          },
          "point_estimate": 0.055939145726680566,
          "standard_error": 0.055002285263370435
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.98801130373075,
            "upper_bound": 35.03780855147313
          },
          "point_estimate": 35.00721904587614,
          "standard_error": 0.012835698867828472
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03145990061848015,
            "upper_bound": 0.36434592815647227
          },
          "point_estimate": 0.2498193804995406,
          "standard_error": 0.0969749590939978
        }
      }
    },
    "memmem/krate/prebuilt/sliceslice-haystack/words": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/sliceslice-haystack/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/sliceslice-haystack/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/sliceslice-haystack/words",
        "directory_name": "memmem/krate_prebuilt_sliceslice-haystack_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174636.7751146978,
            "upper_bound": 175022.73013965203
          },
          "point_estimate": 174808.4049536401,
          "standard_error": 98.86049892292635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174592.1750801282,
            "upper_bound": 174882.60673076924
          },
          "point_estimate": 174804.91641483517,
          "standard_error": 67.4675567361109
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.528671505381187,
            "upper_bound": 450.2141554686601
          },
          "point_estimate": 140.44961793920982,
          "standard_error": 106.0906119301526
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174557.1209488481,
            "upper_bound": 174865.4158166109
          },
          "point_estimate": 174738.03827422578,
          "standard_error": 80.61322526156408
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.67385103772897,
            "upper_bound": 475.2919571529873
          },
          "point_estimate": 330.02667443545346,
          "standard_error": 106.6025841276895
        }
      }
    },
    "memmem/krate/prebuilt/sliceslice-i386/words": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/krate_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25053592.537083335,
            "upper_bound": 25099895.500416663
          },
          "point_estimate": 25075388.75208333,
          "standard_error": 11883.345347814931
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25051810.5625,
            "upper_bound": 25107453.391666666
          },
          "point_estimate": 25066780.270833336,
          "standard_error": 11845.383366466807
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5475.365252796517,
            "upper_bound": 66946.71841520761
          },
          "point_estimate": 23822.40093332152,
          "standard_error": 15601.943452184478
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25050730.869688384,
            "upper_bound": 25113925.56153846
          },
          "point_estimate": 25080859.588311687,
          "standard_error": 17617.61963505382
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13787.004425900172,
            "upper_bound": 51351.95152107376
          },
          "point_estimate": 39657.01076761045,
          "standard_error": 9029.05992359148
        }
      }
    },
    "memmem/krate/prebuilt/sliceslice-words/words": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/krate_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80356636.10000001,
            "upper_bound": 80406705.14083333
          },
          "point_estimate": 80379114.96666667,
          "standard_error": 12889.67444933877
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80352866.0,
            "upper_bound": 80410565.16666667
          },
          "point_estimate": 80363186.0,
          "standard_error": 13171.371876844432
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2642.4873530791074,
            "upper_bound": 60519.978025562974
          },
          "point_estimate": 16809.47140157591,
          "standard_error": 14122.979078982373
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11502.78455846758,
            "upper_bound": 55070.16862291202
          },
          "point_estimate": 42957.29707668502,
          "standard_error": 11352.088652643535
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834627112684986,
            "upper_bound": 8.840192086493778
          },
          "point_estimate": 8.837201893787398,
          "standard_error": 0.0014256901375490115
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834764251348876,
            "upper_bound": 8.839305505016487
          },
          "point_estimate": 8.83647990181768,
          "standard_error": 0.0010912405518039665
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005296474894045919,
            "upper_bound": 0.0070713109050964005
          },
          "point_estimate": 0.002496521539770701,
          "standard_error": 0.0015642525683071153
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.835252398390217,
            "upper_bound": 8.83756351739096
          },
          "point_estimate": 8.836549551738521,
          "standard_error": 0.0005849423729898768
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00164802830734954,
            "upper_bound": 0.006706545283323954
          },
          "point_estimate": 0.004748906558030345,
          "standard_error": 0.0013601718000343633
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834728925588493,
            "upper_bound": 8.83978064455078
          },
          "point_estimate": 8.836849982773098,
          "standard_error": 0.001327514876336393
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834757768889943,
            "upper_bound": 8.838159397997277
          },
          "point_estimate": 8.835380145345894,
          "standard_error": 0.0007716759801500138
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00025429753756995763,
            "upper_bound": 0.004699685535886168
          },
          "point_estimate": 0.001177032854779407,
          "standard_error": 0.0011838262909721986
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834703385684548,
            "upper_bound": 8.837602733480038
          },
          "point_estimate": 8.835802261516992,
          "standard_error": 0.0007637185770753049
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007896927277399947,
            "upper_bound": 0.006412547895453606
          },
          "point_estimate": 0.004416411631217548,
          "standard_error": 0.0016162266094813106
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.836121516339349,
            "upper_bound": 8.848270566678078
          },
          "point_estimate": 8.840855422512606,
          "standard_error": 0.0033468170914720406
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.835016963010963,
            "upper_bound": 8.841545009821044
          },
          "point_estimate": 8.836229905660588,
          "standard_error": 0.0021142559256513154
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003774996513966882,
            "upper_bound": 0.00811574565478218
          },
          "point_estimate": 0.002315103636058648,
          "standard_error": 0.0021603420747949675
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.835712175892397,
            "upper_bound": 8.838332015326307
          },
          "point_estimate": 8.836694395869516,
          "standard_error": 0.0006776616607776973
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00202084705972696,
            "upper_bound": 0.017020087814079404
          },
          "point_estimate": 0.011165515934826476,
          "standard_error": 0.005029306826309118
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834904315242882,
            "upper_bound": 8.840299517642523
          },
          "point_estimate": 8.837439950406372,
          "standard_error": 0.001382263087983596
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.835173648587725,
            "upper_bound": 8.839112374065412
          },
          "point_estimate": 8.837033941468148,
          "standard_error": 0.0011439411580280315
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006932529272849703,
            "upper_bound": 0.006671301231896823
          },
          "point_estimate": 0.0026921270176838105,
          "standard_error": 0.0014408010950869574
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.835421603698364,
            "upper_bound": 8.83830494757801
          },
          "point_estimate": 8.836737486320935,
          "standard_error": 0.0007416300743548692
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001506370173473697,
            "upper_bound": 0.006634087469053301
          },
          "point_estimate": 0.004604690961385202,
          "standard_error": 0.001365269434519799
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.719043767736746,
            "upper_bound": 9.72430741447457
          },
          "point_estimate": 9.721126712640682,
          "standard_error": 0.001429359028588591
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.718286226772546,
            "upper_bound": 9.72115142103002
          },
          "point_estimate": 9.720466658941682,
          "standard_error": 0.0008074923191601776
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00005028881814266496,
            "upper_bound": 0.0036328179799789977
          },
          "point_estimate": 0.0016840957238409512,
          "standard_error": 0.0009246791091420884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.718431827969212,
            "upper_bound": 9.720519776256804
          },
          "point_estimate": 9.719406460170708,
          "standard_error": 0.0005331622119394375
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010206507532405584,
            "upper_bound": 0.007229661311344437
          },
          "point_estimate": 0.004755239264711331,
          "standard_error": 0.002086463490286982
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.064146919431852,
            "upper_bound": 10.070679959575664
          },
          "point_estimate": 10.0673030228525,
          "standard_error": 0.0016700242455978576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.06430866956516,
            "upper_bound": 10.07110148638203
          },
          "point_estimate": 10.06629811887734,
          "standard_error": 0.001692730843870363
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006448147500472237,
            "upper_bound": 0.009732804080373305
          },
          "point_estimate": 0.0035789130715132415,
          "standard_error": 0.0022404894188859836
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.064795636892624,
            "upper_bound": 10.069371562199985
          },
          "point_estimate": 10.066842392511663,
          "standard_error": 0.0011754065817772882
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0021685913157600024,
            "upper_bound": 0.007548123902827997
          },
          "point_estimate": 0.005582765352877294,
          "standard_error": 0.0013368574369864474
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.839175050065566,
            "upper_bound": 7.855251223509109
          },
          "point_estimate": 7.848551231421387,
          "standard_error": 0.004220125327150571
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.845314846353139,
            "upper_bound": 7.855213736140687
          },
          "point_estimate": 7.851787277524259,
          "standard_error": 0.0021435125340380306
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007743222322386947,
            "upper_bound": 0.016260579984539072
          },
          "point_estimate": 0.0033899865773873326,
          "standard_error": 0.00392626803298481
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.8279817128586515,
            "upper_bound": 7.854423927840331
          },
          "point_estimate": 7.842897639930137,
          "standard_error": 0.007339039853676587
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024319264864868587,
            "upper_bound": 0.020667303044860263
          },
          "point_estimate": 0.014040346164941203,
          "standard_error": 0.005302942971092709
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.754572455251443,
            "upper_bound": 9.77789917325398
          },
          "point_estimate": 9.764585621498547,
          "standard_error": 0.006090699482790693
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.751253250724064,
            "upper_bound": 9.77347602696001
          },
          "point_estimate": 9.757942921653347,
          "standard_error": 0.0044186155755992365
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014934793939670855,
            "upper_bound": 0.023086787397643773
          },
          "point_estimate": 0.008203437289260358,
          "standard_error": 0.005468186468834835
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.751889611365552,
            "upper_bound": 9.763600161170825
          },
          "point_estimate": 9.755765712062056,
          "standard_error": 0.0030577236478919437
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003940788563166864,
            "upper_bound": 0.028273424663879936
          },
          "point_estimate": 0.020255411588352756,
          "standard_error": 0.006787638040128092
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.379540822487504,
            "upper_bound": 12.451946390284371
          },
          "point_estimate": 12.412180790494425,
          "standard_error": 0.018708773100391807
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.369214690404874,
            "upper_bound": 12.44146053379576
          },
          "point_estimate": 12.401334356699303,
          "standard_error": 0.01748701798964431
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013142546646745155,
            "upper_bound": 0.08530038079879407
          },
          "point_estimate": 0.05355584275485663,
          "standard_error": 0.019235909492737624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.380816069825856,
            "upper_bound": 12.43099477970712
          },
          "point_estimate": 12.40798646211113,
          "standard_error": 0.013089955043641828
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02490698439704383,
            "upper_bound": 0.08776059084596721
          },
          "point_estimate": 0.062380255690802223,
          "standard_error": 0.018462093356832777
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.861811137405024,
            "upper_bound": 8.867145970856138
          },
          "point_estimate": 8.86418459716921,
          "standard_error": 0.0013779195827480514
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.861066056391866,
            "upper_bound": 8.86629315053129
          },
          "point_estimate": 8.862802202044534,
          "standard_error": 0.0012661223546662124
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00027876425014778327,
            "upper_bound": 0.006292931044923414
          },
          "point_estimate": 0.002601743938091576,
          "standard_error": 0.0015068797189129002
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.86140850213592,
            "upper_bound": 8.864189317048027
          },
          "point_estimate": 8.862531448226708,
          "standard_error": 0.0007084846414565503
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00124514833308883,
            "upper_bound": 0.006252170695776249
          },
          "point_estimate": 0.0045980642263344414,
          "standard_error": 0.0013256897128149873
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.817200068670608,
            "upper_bound": 9.820891442185658
          },
          "point_estimate": 9.818970873515468,
          "standard_error": 0.0009500783816666024
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.816462188870773,
            "upper_bound": 9.82210549919906
          },
          "point_estimate": 9.81824324861658,
          "standard_error": 0.001320123984826166
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005665582012241026,
            "upper_bound": 0.005189852982970249
          },
          "point_estimate": 0.0028635184365201163,
          "standard_error": 0.0012276267383465383
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.817844372674008,
            "upper_bound": 9.821545668806936
          },
          "point_estimate": 9.819400690755044,
          "standard_error": 0.0009432296470170744
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015897331045332623,
            "upper_bound": 0.003957616256125018
          },
          "point_estimate": 0.003173184214975167,
          "standard_error": 0.0005865964625445763
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.189052209243467,
            "upper_bound": 19.212042267914377
          },
          "point_estimate": 19.20033396109175,
          "standard_error": 0.005884650297506742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.18530923230805,
            "upper_bound": 19.216609623548862
          },
          "point_estimate": 19.19533844556536,
          "standard_error": 0.007811572976354947
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010767354495420229,
            "upper_bound": 0.03370124810211133
          },
          "point_estimate": 0.02399724260140713,
          "standard_error": 0.008471426711724251
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.190940030338776,
            "upper_bound": 19.208946698866463
          },
          "point_estimate": 19.199180714200445,
          "standard_error": 0.004565986573716119
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011149501454146688,
            "upper_bound": 0.024445313619527103
          },
          "point_estimate": 0.019652382815668844,
          "standard_error": 0.0033914646629890745
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103351.68193947004,
            "upper_bound": 103574.823828125
          },
          "point_estimate": 103461.37196800594,
          "standard_error": 57.193847988000606
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103253.32528409093,
            "upper_bound": 103672.93465909093
          },
          "point_estimate": 103431.95951704546,
          "standard_error": 97.95442184034292
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.22741679288447,
            "upper_bound": 320.94946209319727
          },
          "point_estimate": 265.9947214140176,
          "standard_error": 79.41693167657478
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103354.19639010268,
            "upper_bound": 103497.35740188956
          },
          "point_estimate": 103427.30068624557,
          "standard_error": 35.70903616680209
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.35270465581428,
            "upper_bound": 223.5667592214392
          },
          "point_estimate": 190.81328632938943,
          "standard_error": 26.799195972492335
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57256.466817144734,
            "upper_bound": 57307.16986739157
          },
          "point_estimate": 57279.87848681415,
          "standard_error": 13.011406595587497
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57247.54920134983,
            "upper_bound": 57305.73874015748
          },
          "point_estimate": 57265.91929133858,
          "standard_error": 18.090899817610733
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.714547347362623,
            "upper_bound": 72.92024639569672
          },
          "point_estimate": 35.721086672912776,
          "standard_error": 16.996878424913476
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57256.28881889763,
            "upper_bound": 57303.30854336028
          },
          "point_estimate": 57286.8998179773,
          "standard_error": 11.387322612768962
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.665143987345072,
            "upper_bound": 58.190167326253565
          },
          "point_estimate": 43.63310622584614,
          "standard_error": 10.403038668840496
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148181.40176052798,
            "upper_bound": 148561.74504834792
          },
          "point_estimate": 148369.99754972465,
          "standard_error": 97.52810547097614
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148100.1573469388,
            "upper_bound": 148654.9510204082
          },
          "point_estimate": 148357.43527696794,
          "standard_error": 174.59808898333844
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.80709426176664,
            "upper_bound": 524.2927364062139
          },
          "point_estimate": 376.5916556951177,
          "standard_error": 127.86438217025596
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148075.59043208306,
            "upper_bound": 148466.01306404916
          },
          "point_estimate": 148215.00709249935,
          "standard_error": 99.4263490953982
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.81544881242525,
            "upper_bound": 391.8449385638984
          },
          "point_estimate": 324.8599536993616,
          "standard_error": 47.85042749687742
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398097.50530816504,
            "upper_bound": 399124.2064389234
          },
          "point_estimate": 398560.5924443581,
          "standard_error": 263.77628168112545
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398041.8558812112,
            "upper_bound": 399027.7652173913
          },
          "point_estimate": 398174.0298913043,
          "standard_error": 256.2067458999327
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.13557893492973,
            "upper_bound": 1285.4809402215783
          },
          "point_estimate": 204.3493558285317,
          "standard_error": 312.8821325080034
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397952.7016444471,
            "upper_bound": 398291.6096839924
          },
          "point_estimate": 398088.0716261999,
          "standard_error": 86.60398080947984
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.07497564588655,
            "upper_bound": 1134.4946674997873
          },
          "point_estimate": 879.8085513651137,
          "standard_error": 240.29023219865948
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578820.7059542707,
            "upper_bound": 580372.8757747543
          },
          "point_estimate": 579516.9458056184,
          "standard_error": 401.1325399973914
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578592.424404762,
            "upper_bound": 580267.2505668934
          },
          "point_estimate": 579133.1841269841,
          "standard_error": 346.17883837594013
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.34216701130117,
            "upper_bound": 1873.21568281421
          },
          "point_estimate": 661.1016567816505,
          "standard_error": 442.62134726394305
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578865.3058921223,
            "upper_bound": 579440.0960518544
          },
          "point_estimate": 579152.2573490002,
          "standard_error": 147.99378613661486
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401.11591464382246,
            "upper_bound": 1799.5174546417254
          },
          "point_estimate": 1338.728163609724,
          "standard_error": 376.0440010317148
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51884.31989035715,
            "upper_bound": 51982.815769047615
          },
          "point_estimate": 51931.50568605442,
          "standard_error": 25.08787762481944
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51879.09392857143,
            "upper_bound": 51969.87742857143
          },
          "point_estimate": 51930.943035714285,
          "standard_error": 22.535084244227537
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.784541666510524,
            "upper_bound": 137.8727960522679
          },
          "point_estimate": 59.17471017801117,
          "standard_error": 31.139944508049687
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51908.19768355151,
            "upper_bound": 51956.5800427716
          },
          "point_estimate": 51931.51519109462,
          "standard_error": 12.193221124849318
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.51750295712191,
            "upper_bound": 114.79872149745812
          },
          "point_estimate": 83.46496696120194,
          "standard_error": 20.685047272988935
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100160.05706749312,
            "upper_bound": 100295.21437480868
          },
          "point_estimate": 100222.51827900214,
          "standard_error": 34.77175692828614
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100130.064738292,
            "upper_bound": 100320.0988292011
          },
          "point_estimate": 100204.01346801348,
          "standard_error": 39.50703425816586
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.97776692158063,
            "upper_bound": 168.70088502149798
          },
          "point_estimate": 86.87316555594681,
          "standard_error": 41.65833415624865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100167.08254739911,
            "upper_bound": 100272.18835169084
          },
          "point_estimate": 100214.22228185038,
          "standard_error": 26.124996114502125
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.34360173726927,
            "upper_bound": 145.4984315232925
          },
          "point_estimate": 115.6664111630716,
          "standard_error": 26.585859843556666
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70329.94681956343,
            "upper_bound": 70466.59605684505
          },
          "point_estimate": 70399.50130000614,
          "standard_error": 35.036487453841374
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70287.90425531915,
            "upper_bound": 70510.95696324951
          },
          "point_estimate": 70401.88287126584,
          "standard_error": 61.86126605708696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.09807260428917,
            "upper_bound": 185.90468141905689
          },
          "point_estimate": 126.77321093437432,
          "standard_error": 45.93057166819859
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70312.82600172987,
            "upper_bound": 70471.31261649376
          },
          "point_estimate": 70389.53667763571,
          "standard_error": 40.47307665694312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.8716863570329,
            "upper_bound": 138.99975179335777
          },
          "point_estimate": 116.7427871395224,
          "standard_error": 16.665283675002527
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308529.5756887275,
            "upper_bound": 309032.5547341606
          },
          "point_estimate": 308777.61854183476,
          "standard_error": 128.78460581509225
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308395.7615012106,
            "upper_bound": 309193.5572033898
          },
          "point_estimate": 308659.8995762712,
          "standard_error": 193.3252838578616
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.918856836794758,
            "upper_bound": 734.0773377302605
          },
          "point_estimate": 491.03376077396183,
          "standard_error": 176.65376585757485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308477.12994149135,
            "upper_bound": 308855.2447394658
          },
          "point_estimate": 308666.1565485362,
          "standard_error": 94.79141077978367
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257.32874449250426,
            "upper_bound": 521.3183617310567
          },
          "point_estimate": 430.28954266551625,
          "standard_error": 67.29130236759244
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36395.46621029756,
            "upper_bound": 36445.80300317619
          },
          "point_estimate": 36422.037284353064,
          "standard_error": 12.86151166506298
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36385.12671347376,
            "upper_bound": 36458.217318622534
          },
          "point_estimate": 36436.9470661986,
          "standard_error": 20.561511897389483
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.905801424742076,
            "upper_bound": 73.16948109817349
          },
          "point_estimate": 39.63761514383239,
          "standard_error": 17.59761331079106
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36391.03559723477,
            "upper_bound": 36452.21804188075
          },
          "point_estimate": 36420.70981515977,
          "standard_error": 16.31577598231525
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.06746688314087,
            "upper_bound": 54.28446792486346
          },
          "point_estimate": 42.9235882452552,
          "standard_error": 8.349100528933157
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66199.28096715307,
            "upper_bound": 66314.84991250436
          },
          "point_estimate": 66256.83554460116,
          "standard_error": 29.659920102930403
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66171.27970749543,
            "upper_bound": 66353.46985178898
          },
          "point_estimate": 66235.86566118221,
          "standard_error": 56.66942460022187
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.374303078738084,
            "upper_bound": 161.53195873034088
          },
          "point_estimate": 127.3345691424632,
          "standard_error": 39.68056230314002
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66201.13537859189,
            "upper_bound": 66332.85633173956
          },
          "point_estimate": 66272.5819558869,
          "standard_error": 35.21804043589875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.59021614442979,
            "upper_bound": 116.28606976193988
          },
          "point_estimate": 98.65013764432358,
          "standard_error": 13.240902956265774
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167808.23486943165,
            "upper_bound": 168158.56549923195
          },
          "point_estimate": 167984.81250384025,
          "standard_error": 90.00481930616492
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167643.35867895544,
            "upper_bound": 168244.66889400923
          },
          "point_estimate": 168026.8283410138,
          "standard_error": 151.8114135570754
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.41950387256714,
            "upper_bound": 503.08648139105463
          },
          "point_estimate": 361.28809197295374,
          "standard_error": 117.3831397194174
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167857.2620010136,
            "upper_bound": 168179.4093866366
          },
          "point_estimate": 168004.615512598,
          "standard_error": 81.11542659658728
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.4548034525163,
            "upper_bound": 356.628392932108
          },
          "point_estimate": 300.0300628406201,
          "standard_error": 42.95386333511445
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35862.19052361423,
            "upper_bound": 35979.83147993513
          },
          "point_estimate": 35916.769685547115,
          "standard_error": 30.18631280177912
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35840.74311023622,
            "upper_bound": 35990.75606955381
          },
          "point_estimate": 35887.31304680665,
          "standard_error": 37.55781176177426
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.31780628164141,
            "upper_bound": 160.2646469710084
          },
          "point_estimate": 82.8410943585112,
          "standard_error": 37.115719935285625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35843.3633898298,
            "upper_bound": 35893.62945595542
          },
          "point_estimate": 35861.66419112384,
          "standard_error": 12.94390574654604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.12195858857226,
            "upper_bound": 131.11802124713557
          },
          "point_estimate": 100.78768877015096,
          "standard_error": 23.102545687986705
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11436.388645899056,
            "upper_bound": 11492.32502862776
          },
          "point_estimate": 11467.299282597263,
          "standard_error": 14.356789975484077
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11456.836487907463,
            "upper_bound": 11493.882113564669
          },
          "point_estimate": 11470.57858044164,
          "standard_error": 9.84673517571084
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.491716683662277,
            "upper_bound": 62.47026190039361
          },
          "point_estimate": 26.5636470709884,
          "standard_error": 15.163728127775856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11418.876320808438,
            "upper_bound": 11483.581589875888
          },
          "point_estimate": 11457.318500553074,
          "standard_error": 16.793771139022677
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.460596577219157,
            "upper_bound": 68.63868917657474
          },
          "point_estimate": 47.70417244178763,
          "standard_error": 15.311388204537854
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506280.94953373016,
            "upper_bound": 507478.44251322746
          },
          "point_estimate": 506830.81823743385,
          "standard_error": 306.57153533105173
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 505953.962962963,
            "upper_bound": 507390.2642195767
          },
          "point_estimate": 506555.1857638889,
          "standard_error": 376.6655928108572
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175.37922188638203,
            "upper_bound": 1643.8550039407492
          },
          "point_estimate": 955.9260421955772,
          "standard_error": 355.8836500973404
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506012.5930519292,
            "upper_bound": 506979.12367724866
          },
          "point_estimate": 506428.89538239536,
          "standard_error": 247.9763081542471
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477.73385797240945,
            "upper_bound": 1379.8959330756766
          },
          "point_estimate": 1019.8848529311092,
          "standard_error": 254.81191581813343
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1065.379667998109,
            "upper_bound": 1066.1269366574195
          },
          "point_estimate": 1065.742864442034,
          "standard_error": 0.19103180348038976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1065.2874361220615,
            "upper_bound": 1066.3442663162505
          },
          "point_estimate": 1065.533440729385,
          "standard_error": 0.29769659274052684
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.053578963374260984,
            "upper_bound": 1.021549167827833
          },
          "point_estimate": 0.5059461703868023,
          "standard_error": 0.2707398694261967
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1065.4557622905854,
            "upper_bound": 1066.0267784156997
          },
          "point_estimate": 1065.7042611586107,
          "standard_error": 0.14351162737860348
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.34459599552013515,
            "upper_bound": 0.7731884134625676
          },
          "point_estimate": 0.6356788683475121,
          "standard_error": 0.10514961470914654
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51420.655487391654,
            "upper_bound": 51494.634316334574
          },
          "point_estimate": 51452.04973933355,
          "standard_error": 19.316279831267337
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51409.40854215918,
            "upper_bound": 51475.372619047615
          },
          "point_estimate": 51431.78033096927,
          "standard_error": 17.803749070902054
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.322468050826366,
            "upper_bound": 73.90322748227982
          },
          "point_estimate": 44.246467098628,
          "standard_error": 16.90686596532637
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51413.3080553867,
            "upper_bound": 51460.13029674578
          },
          "point_estimate": 51430.18360136318,
          "standard_error": 11.980297589365788
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.266505600701517,
            "upper_bound": 93.72551069367886
          },
          "point_estimate": 64.27939842206978,
          "standard_error": 22.44947936005049
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53240.86994452602,
            "upper_bound": 53350.08617270354
          },
          "point_estimate": 53292.49331905694,
          "standard_error": 28.11784837711167
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53209.284875183555,
            "upper_bound": 53360.77141458639
          },
          "point_estimate": 53269.42867107195,
          "standard_error": 45.39261956859134
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.812096085013864,
            "upper_bound": 163.2355633574943
          },
          "point_estimate": 94.2661688599666,
          "standard_error": 41.51304135517752
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53223.23287432098,
            "upper_bound": 53357.8062562327
          },
          "point_estimate": 53272.46761256365,
          "standard_error": 35.10849401711642
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.42290117284436,
            "upper_bound": 119.45601882739108
          },
          "point_estimate": 93.758208275004,
          "standard_error": 18.40703277222968
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52782.31777679869,
            "upper_bound": 52913.163458614516
          },
          "point_estimate": 52845.23182510843,
          "standard_error": 33.49001673410887
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52761.388517441854,
            "upper_bound": 52926.23968715393
          },
          "point_estimate": 52816.299691133725,
          "standard_error": 43.09792008676903
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.66675329909357,
            "upper_bound": 184.3656166105695
          },
          "point_estimate": 117.34158476270127,
          "standard_error": 43.77589727354117
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52776.44226642696,
            "upper_bound": 52884.44160631615
          },
          "point_estimate": 52830.357656297194,
          "standard_error": 27.76649279528708
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.284914459608345,
            "upper_bound": 143.458478224546
          },
          "point_estimate": 111.7677551399074,
          "standard_error": 21.809022612777305
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15262.096167137644,
            "upper_bound": 15282.210043540565
          },
          "point_estimate": 15272.000632064692,
          "standard_error": 5.162041457580299
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15255.181993546576,
            "upper_bound": 15283.72502805836
          },
          "point_estimate": 15275.590446127946,
          "standard_error": 7.7408021475959155
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8102008591996746,
            "upper_bound": 31.250505568929473
          },
          "point_estimate": 19.659157984313296,
          "standard_error": 7.215358492249856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15261.586836913466,
            "upper_bound": 15280.94028817742
          },
          "point_estimate": 15273.039832305742,
          "standard_error": 4.936321587981437
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.296581324543247,
            "upper_bound": 21.71273482904634
          },
          "point_estimate": 17.190027666377908,
          "standard_error": 3.0107619787480595
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26509.789953486703,
            "upper_bound": 26559.844367036698
          },
          "point_estimate": 26533.997986299382,
          "standard_error": 12.82426116165076
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26498.35244525548,
            "upper_bound": 26583.706569343067
          },
          "point_estimate": 26520.841864355232,
          "standard_error": 22.111867798620228
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.108036242917597,
            "upper_bound": 69.11100547924057
          },
          "point_estimate": 43.29714079701367,
          "standard_error": 16.49480944617636
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26498.864948693536,
            "upper_bound": 26563.70702980903
          },
          "point_estimate": 26526.578464309412,
          "standard_error": 17.66565333350516
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.2834784527808,
            "upper_bound": 50.235757736202
          },
          "point_estimate": 42.70676688191266,
          "standard_error": 6.298291261282594
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16530.440903161427,
            "upper_bound": 16562.056597664545
          },
          "point_estimate": 16545.98862156434,
          "standard_error": 8.110943825359094
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16525.79075674856,
            "upper_bound": 16571.139621246588
          },
          "point_estimate": 16537.659551612574,
          "standard_error": 13.670793726088704
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.42846559731042,
            "upper_bound": 43.16230134358875
          },
          "point_estimate": 29.866455401291823,
          "standard_error": 11.182324191995876
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16532.150723059924,
            "upper_bound": 16567.66637750234
          },
          "point_estimate": 16553.036589343323,
          "standard_error": 9.131013741441745
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.37169809116178,
            "upper_bound": 32.97655936951506
          },
          "point_estimate": 27.053232257863097,
          "standard_error": 4.18948019475133
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16496.4761162702,
            "upper_bound": 16533.411361508657
          },
          "point_estimate": 16515.00211812091,
          "standard_error": 9.44265293731677
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16485.555202180825,
            "upper_bound": 16541.159119592103
          },
          "point_estimate": 16515.215250643647,
          "standard_error": 12.234144012450251
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.974032522292437,
            "upper_bound": 58.7864249354309
          },
          "point_estimate": 30.462178029534368,
          "standard_error": 14.149602303906022
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16491.63272109064,
            "upper_bound": 16532.461982445893
          },
          "point_estimate": 16513.341077550347,
          "standard_error": 10.340040196425871
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.35318025093961,
            "upper_bound": 38.98907942272601
          },
          "point_estimate": 31.444243277164507,
          "standard_error": 5.345423875145819
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19318.40485505219,
            "upper_bound": 19367.173596786575
          },
          "point_estimate": 19338.54604811692,
          "standard_error": 12.89616219739995
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19311.57712462976,
            "upper_bound": 19347.1004784689
          },
          "point_estimate": 19326.884024455074,
          "standard_error": 9.119321432852072
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.513719152509595,
            "upper_bound": 41.4296444000432
          },
          "point_estimate": 22.479991068204686,
          "standard_error": 9.449403733357224
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19311.620464620464,
            "upper_bound": 19342.337077931817
          },
          "point_estimate": 19326.167620152308,
          "standard_error": 8.104283745226379
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.68217113789284,
            "upper_bound": 63.97895129895461
          },
          "point_estimate": 42.96442682484078,
          "standard_error": 16.659434686314512
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95769.28139348372,
            "upper_bound": 95829.72913157896
          },
          "point_estimate": 95798.5439555138,
          "standard_error": 15.519079763191362
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95764.13605263158,
            "upper_bound": 95838.40131578948
          },
          "point_estimate": 95783.82664473684,
          "standard_error": 20.380114873912593
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.020547905636468,
            "upper_bound": 85.80447021350935
          },
          "point_estimate": 34.51791859770198,
          "standard_error": 21.42881925984492
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95756.12744534384,
            "upper_bound": 95801.425959281
          },
          "point_estimate": 95777.58556390976,
          "standard_error": 11.402551727419617
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.347361078373098,
            "upper_bound": 65.03098679337836
          },
          "point_estimate": 51.80642554023526,
          "standard_error": 9.719447235454757
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106223.02663159445,
            "upper_bound": 106294.470675413
          },
          "point_estimate": 106256.90486776344,
          "standard_error": 18.14725962713767
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106226.12293488823,
            "upper_bound": 106278.21671525751
          },
          "point_estimate": 106255.39791753436,
          "standard_error": 13.817620290862672
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.744423638013552,
            "upper_bound": 93.06844834770423
          },
          "point_estimate": 38.61711870215603,
          "standard_error": 19.40189394840804
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106228.21381195336,
            "upper_bound": 106276.12557016956
          },
          "point_estimate": 106251.8719548673,
          "standard_error": 12.83183511214971
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.295042087100132,
            "upper_bound": 84.66578214543104
          },
          "point_estimate": 60.51813187081592,
          "standard_error": 17.087354854574023
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20170.933032894533,
            "upper_bound": 20205.027674748595
          },
          "point_estimate": 20185.115730921098,
          "standard_error": 8.962064673367705
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20168.625714109447,
            "upper_bound": 20195.20988339811
          },
          "point_estimate": 20173.343223672033,
          "standard_error": 5.875524767523279
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.776908736730664,
            "upper_bound": 29.679073913220332
          },
          "point_estimate": 9.734614774428556,
          "standard_error": 7.37219994083095
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20168.673740923263,
            "upper_bound": 20179.41714521941
          },
          "point_estimate": 20174.14131687302,
          "standard_error": 2.8039266901450337
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.0720862848142465,
            "upper_bound": 43.24747870554783
          },
          "point_estimate": 29.890884839954666,
          "standard_error": 11.020456829365989
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16778.367679369483,
            "upper_bound": 16792.123092106427
          },
          "point_estimate": 16784.361163727626,
          "standard_error": 3.5690309874835653
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16777.690476190477,
            "upper_bound": 16790.501981375077
          },
          "point_estimate": 16779.616119586994,
          "standard_error": 3.03590675635407
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3842444915579335,
            "upper_bound": 14.81429019006309
          },
          "point_estimate": 4.604732326942041,
          "standard_error": 3.824415995454342
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16779.760023503004,
            "upper_bound": 16792.39343323278
          },
          "point_estimate": 16784.29545904858,
          "standard_error": 3.2328284404539867
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.871579611696604,
            "upper_bound": 16.60839024832032
          },
          "point_estimate": 11.91358955188574,
          "standard_error": 3.800622826185477
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19150.096249238966,
            "upper_bound": 19182.94560498102
          },
          "point_estimate": 19166.90325030943,
          "standard_error": 8.386198145663224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19145.00462328767,
            "upper_bound": 19192.3177028451
          },
          "point_estimate": 19168.719737881984,
          "standard_error": 10.59609914592191
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.999237683271302,
            "upper_bound": 48.67652682812483
          },
          "point_estimate": 31.639214612156938,
          "standard_error": 11.371554085248338
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19162.06460405538,
            "upper_bound": 19195.9559895964
          },
          "point_estimate": 19181.826286042724,
          "standard_error": 8.750058634541205
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.051169701843495,
            "upper_bound": 34.999009867579815
          },
          "point_estimate": 27.98428957824709,
          "standard_error": 5.03832044289909
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16459.080359800522,
            "upper_bound": 16498.75011227167
          },
          "point_estimate": 16480.021511811054,
          "standard_error": 10.188563728980636
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16448.994802704612,
            "upper_bound": 16509.762265635138
          },
          "point_estimate": 16488.697850438995,
          "standard_error": 16.28202740325072
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.0287363392806177,
            "upper_bound": 57.48087434470725
          },
          "point_estimate": 31.91681625329859,
          "standard_error": 14.833454930207326
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16459.968003408838,
            "upper_bound": 16499.354673977894
          },
          "point_estimate": 16480.78210127747,
          "standard_error": 10.064395882464442
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.565479128610484,
            "upper_bound": 43.05406106151846
          },
          "point_estimate": 33.966352632775425,
          "standard_error": 6.682760225132099
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16721.57452902048,
            "upper_bound": 16745.423064268187
          },
          "point_estimate": 16733.699482647793,
          "standard_error": 6.107601536908144
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16716.944448096772,
            "upper_bound": 16748.413023469857
          },
          "point_estimate": 16737.17714884696,
          "standard_error": 8.984044626908346
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.0123070857649874,
            "upper_bound": 34.59672580854727
          },
          "point_estimate": 17.060683551998274,
          "standard_error": 8.23363540665418
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16713.067494530667,
            "upper_bound": 16743.138576179444
          },
          "point_estimate": 16727.27596894592,
          "standard_error": 8.006110384404264
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.385242441634304,
            "upper_bound": 25.39601628370885
          },
          "point_estimate": 20.38941630712748,
          "standard_error": 3.5092272503606825
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16532.04299818016,
            "upper_bound": 16568.16653321201
          },
          "point_estimate": 16548.799968152867,
          "standard_error": 9.288679074659768
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16525.37443130118,
            "upper_bound": 16572.392402183803
          },
          "point_estimate": 16537.083439490445,
          "standard_error": 12.075265346765905
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.797876984883688,
            "upper_bound": 49.844835739279745
          },
          "point_estimate": 25.685909639523647,
          "standard_error": 11.87980666083039
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16525.920823671968,
            "upper_bound": 16565.858861129876
          },
          "point_estimate": 16543.52227290453,
          "standard_error": 10.65060347151605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.191504188581195,
            "upper_bound": 40.107082117204534
          },
          "point_estimate": 30.938162749244064,
          "standard_error": 7.020265282559534
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19725.822186915324,
            "upper_bound": 19759.617860561462
          },
          "point_estimate": 19739.11126695292,
          "standard_error": 9.24699261104502
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19722.00162954916,
            "upper_bound": 19741.59851982618
          },
          "point_estimate": 19728.7477147513,
          "standard_error": 5.690670242546033
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7997362418156289,
            "upper_bound": 22.44086280818486
          },
          "point_estimate": 11.946586841137425,
          "standard_error": 5.482192001009786
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19725.7526625428,
            "upper_bound": 19734.446866680253
          },
          "point_estimate": 19729.453344808368,
          "standard_error": 2.227281401399354
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.026475747872774,
            "upper_bound": 46.77031478975084
          },
          "point_estimate": 30.797424847962656,
          "standard_error": 13.54689332167522
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16878.516624311596,
            "upper_bound": 16929.191845949543
          },
          "point_estimate": 16901.602181046746,
          "standard_error": 12.999066634059368
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16865.250116117048,
            "upper_bound": 16926.16620219848
          },
          "point_estimate": 16898.48842313052,
          "standard_error": 15.293740034863443
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.15953013299723,
            "upper_bound": 65.56706567524458
          },
          "point_estimate": 44.01330578851467,
          "standard_error": 16.85069767055899
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16873.478384253154,
            "upper_bound": 16909.44856042709
          },
          "point_estimate": 16890.61343821065,
          "standard_error": 9.094512597511406
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.573306792112728,
            "upper_bound": 58.893917042227294
          },
          "point_estimate": 43.4683817239404,
          "standard_error": 11.09768257183923
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19663.905311298968,
            "upper_bound": 19689.233047153713
          },
          "point_estimate": 19676.579849844024,
          "standard_error": 6.488315786667602
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19652.502947107183,
            "upper_bound": 19696.81335504886
          },
          "point_estimate": 19678.479370249726,
          "standard_error": 11.566491510817595
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.946922647644895,
            "upper_bound": 36.38445674818362
          },
          "point_estimate": 27.6931685881568,
          "standard_error": 8.347297463519812
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19656.001693074955,
            "upper_bound": 19692.7706582773
          },
          "point_estimate": 19675.53813472087,
          "standard_error": 9.711172001129574
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.774382068027103,
            "upper_bound": 25.320247850721508
          },
          "point_estimate": 21.617722135378827,
          "standard_error": 2.92396105979577
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61263.25405518081,
            "upper_bound": 61372.59535502154
          },
          "point_estimate": 61318.13322278434,
          "standard_error": 28.039884839311764
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61235.68971332209,
            "upper_bound": 61405.9915682968
          },
          "point_estimate": 61317.14624789207,
          "standard_error": 51.75518818813743
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.908120411641187,
            "upper_bound": 151.0833474723154
          },
          "point_estimate": 123.23362230625946,
          "standard_error": 35.40138511183764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61292.713770546485,
            "upper_bound": 61377.52603408944
          },
          "point_estimate": 61335.012496441166,
          "standard_error": 21.67955909820425
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.42057019099416,
            "upper_bound": 111.16746556455536
          },
          "point_estimate": 93.14148308019404,
          "standard_error": 13.076224458363736
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 885006.359393424,
            "upper_bound": 885629.5696990505
          },
          "point_estimate": 885293.6603316326,
          "standard_error": 159.935050121836
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 884938.1507936509,
            "upper_bound": 885615.788265306
          },
          "point_estimate": 885036.4598214286,
          "standard_error": 195.90234601624664
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.214328736749,
            "upper_bound": 806.7439794273656
          },
          "point_estimate": 185.8141538439847,
          "standard_error": 205.34406529686123
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 884953.0676117776,
            "upper_bound": 885413.7176220807
          },
          "point_estimate": 885162.9403834261,
          "standard_error": 129.9512914781827
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123.30215427222866,
            "upper_bound": 674.7803902800642
          },
          "point_estimate": 534.4474848703741,
          "standard_error": 128.62056573124914
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201980.8534799383,
            "upper_bound": 202425.8976510417
          },
          "point_estimate": 202201.30801697535,
          "standard_error": 113.0691125413288
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202056.3135802469,
            "upper_bound": 202393.45
          },
          "point_estimate": 202175.72708333333,
          "standard_error": 77.62719163387082
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.228564764055854,
            "upper_bound": 596.3257330241936
          },
          "point_estimate": 160.21706319722458,
          "standard_error": 138.39204063234737
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201919.370838276,
            "upper_bound": 202214.45240079364
          },
          "point_estimate": 202101.65424242424,
          "standard_error": 76.37911565233392
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.6763076317428,
            "upper_bound": 525.1798421364841
          },
          "point_estimate": 377.41519940098976,
          "standard_error": 104.03369808419356
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6219.873492756104,
            "upper_bound": 6244.962721445753
          },
          "point_estimate": 6232.9839002830195,
          "standard_error": 6.442198245536912
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6205.441478439425,
            "upper_bound": 6251.715434633812
          },
          "point_estimate": 6238.721052931782,
          "standard_error": 10.085490578796144
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.951912614431055,
            "upper_bound": 30.02098634815243
          },
          "point_estimate": 20.713592643896316,
          "standard_error": 8.029462229528086
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6216.276030036296,
            "upper_bound": 6250.456194353736
          },
          "point_estimate": 6234.647570601883,
          "standard_error": 8.62747193460393
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.602483374926615,
            "upper_bound": 25.092011618644086
          },
          "point_estimate": 21.45619891529959,
          "standard_error": 3.550799476707175
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6233.4801618457295,
            "upper_bound": 6243.91138946281
          },
          "point_estimate": 6238.459997403692,
          "standard_error": 2.660017273370815
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6232.1640505433115,
            "upper_bound": 6243.689492325856
          },
          "point_estimate": 6236.68014520202,
          "standard_error": 2.6946008995236483
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0932600649563111,
            "upper_bound": 14.828424788395676
          },
          "point_estimate": 7.483633295169543,
          "standard_error": 3.8701817452445657
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6232.576655505263,
            "upper_bound": 6244.206533556081
          },
          "point_estimate": 6237.425257593646,
          "standard_error": 2.931909032945837
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.268693945358684,
            "upper_bound": 11.68301654390888
          },
          "point_estimate": 8.838246782416297,
          "standard_error": 1.9474191637010232
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15775.413829690888,
            "upper_bound": 15804.904079330649
          },
          "point_estimate": 15789.857682005992,
          "standard_error": 7.53115762849884
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15772.95498373102,
            "upper_bound": 15806.211062906725
          },
          "point_estimate": 15787.176934201012,
          "standard_error": 7.991776239584003
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.174221097261284,
            "upper_bound": 42.60004307233387
          },
          "point_estimate": 21.081596089935218,
          "standard_error": 10.26655370037316
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15773.716547054712,
            "upper_bound": 15789.887632210231
          },
          "point_estimate": 15781.821551116996,
          "standard_error": 4.085923193402899
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.732106342095644,
            "upper_bound": 32.87649461538117
          },
          "point_estimate": 25.095547354120903,
          "standard_error": 5.1992073723546035
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.33075706332447,
            "upper_bound": 58.44797229249321
          },
          "point_estimate": 58.40058851021762,
          "standard_error": 0.03121830944237272
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.39583825498071,
            "upper_bound": 58.43617977311406
          },
          "point_estimate": 58.42609363022318,
          "standard_error": 0.011704062397994026
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00431913022604405,
            "upper_bound": 0.08727003855504027
          },
          "point_estimate": 0.017551371164700014,
          "standard_error": 0.019426029652581816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.41434834283853,
            "upper_bound": 58.43754244396299
          },
          "point_estimate": 58.42740914651992,
          "standard_error": 0.005888850815595909
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013430002071210348,
            "upper_bound": 0.15762691887361466
          },
          "point_estimate": 0.10440299714379073,
          "standard_error": 0.045572990016664275
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.161445792251655,
            "upper_bound": 33.18661794493546
          },
          "point_estimate": 33.173068639732,
          "standard_error": 0.0064899027337885174
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.15996146256233,
            "upper_bound": 33.191731402347706
          },
          "point_estimate": 33.164089790892305,
          "standard_error": 0.007917374436463603
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012490808380464654,
            "upper_bound": 0.031287429151264506
          },
          "point_estimate": 0.006332585785211818,
          "standard_error": 0.009122346839511889
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.158745068396456,
            "upper_bound": 33.19440238832835
          },
          "point_estimate": 33.175937743685026,
          "standard_error": 0.009310672773061014
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00694871548315272,
            "upper_bound": 0.027408328034433035
          },
          "point_estimate": 0.021636133405388915,
          "standard_error": 0.004851085524941766
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.30557051969841,
            "upper_bound": 37.326511034089975
          },
          "point_estimate": 37.31515576121673,
          "standard_error": 0.005391748642822236
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.30128830614423,
            "upper_bound": 37.32622321648469
          },
          "point_estimate": 37.30908900131352,
          "standard_error": 0.006220701857095618
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0033985885099781,
            "upper_bound": 0.027851264087681266
          },
          "point_estimate": 0.01293288838791229,
          "standard_error": 0.006235133318412266
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.304581894423514,
            "upper_bound": 37.32098115074509
          },
          "point_estimate": 37.31249777170535,
          "standard_error": 0.004143565447026981
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007217836004741989,
            "upper_bound": 0.02396626310901625
          },
          "point_estimate": 0.01795480709730768,
          "standard_error": 0.004515041992294404
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.73223394562272,
            "upper_bound": 28.750518532729103
          },
          "point_estimate": 28.740405468907092,
          "standard_error": 0.00473433302754045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.731233795653853,
            "upper_bound": 28.74882698534817
          },
          "point_estimate": 28.73357275637757,
          "standard_error": 0.004495054419442273
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004762417218453983,
            "upper_bound": 0.022234402285090975
          },
          "point_estimate": 0.0036785594429466846,
          "standard_error": 0.005374204151958384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.73283494844223,
            "upper_bound": 28.74728416776957
          },
          "point_estimate": 28.738998816819528,
          "standard_error": 0.0037314472149885896
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0036023445279544336,
            "upper_bound": 0.020990711558650735
          },
          "point_estimate": 0.01580480700126928,
          "standard_error": 0.00453832337265282
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.877231254161178,
            "upper_bound": 21.889658628152123
          },
          "point_estimate": 21.88259089557031,
          "standard_error": 0.003245358456145886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.876868750925063,
            "upper_bound": 21.88768313056833
          },
          "point_estimate": 21.878853608744883,
          "standard_error": 0.0022678228748669008
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007457396353622676,
            "upper_bound": 0.013017772908137897
          },
          "point_estimate": 0.00322416427996811,
          "standard_error": 0.0029927333993864415
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.87720333471564,
            "upper_bound": 21.88724281981816
          },
          "point_estimate": 21.881005390773954,
          "standard_error": 0.002764181358277977
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001862117423796909,
            "upper_bound": 0.014952840904411766
          },
          "point_estimate": 0.010818054369238113,
          "standard_error": 0.0035241906734348578
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.693893067376344,
            "upper_bound": 32.81104753523879
          },
          "point_estimate": 32.74678552432119,
          "standard_error": 0.03030893524125714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.682591423620536,
            "upper_bound": 32.83507170736503
          },
          "point_estimate": 32.69724948877101,
          "standard_error": 0.03772669298682379
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006731901183302945,
            "upper_bound": 0.18476937985323655
          },
          "point_estimate": 0.02974053517323599,
          "standard_error": 0.040208045733905566
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.68345997587654,
            "upper_bound": 32.7502592204496
          },
          "point_estimate": 32.70611582043628,
          "standard_error": 0.017455606027984287
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021637213991746763,
            "upper_bound": 0.13453469752426336
          },
          "point_estimate": 0.10092420653655404,
          "standard_error": 0.026385492690150295
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.71526544079642,
            "upper_bound": 45.74071659377205
          },
          "point_estimate": 45.72746447641536,
          "standard_error": 0.006515707391065018
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.70505134270748,
            "upper_bound": 45.74245774233801
          },
          "point_estimate": 45.72681036899377,
          "standard_error": 0.008425812510624497
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0042899834642592255,
            "upper_bound": 0.03889186536688369
          },
          "point_estimate": 0.024330396332934764,
          "standard_error": 0.008727564603298875
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.71604039062849,
            "upper_bound": 45.73680429589022
          },
          "point_estimate": 45.72703554122707,
          "standard_error": 0.005267818600924287
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011784862522065617,
            "upper_bound": 0.028389059531355933
          },
          "point_estimate": 0.02170771012392059,
          "standard_error": 0.0044935326849996765
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.43175679814134,
            "upper_bound": 53.63413937272664
          },
          "point_estimate": 53.5329807141663,
          "standard_error": 0.05188552391744755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.392980840928374,
            "upper_bound": 53.69013127157975
          },
          "point_estimate": 53.52470883417483,
          "standard_error": 0.06742722153387731
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.043023864906474994,
            "upper_bound": 0.308335686735102
          },
          "point_estimate": 0.18752774555701296,
          "standard_error": 0.07227157335941717
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.44729878079399,
            "upper_bound": 53.69528483746683
          },
          "point_estimate": 53.58152890835146,
          "standard_error": 0.06566086139359675
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09992419038504792,
            "upper_bound": 0.21541212932761472
          },
          "point_estimate": 0.17287914020265233,
          "standard_error": 0.02971384554755476
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.834119509027936,
            "upper_bound": 40.86386368188539
          },
          "point_estimate": 40.84667479477191,
          "standard_error": 0.00778792377532914
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.83374705276479,
            "upper_bound": 40.8499255079972
          },
          "point_estimate": 40.842228244846886,
          "standard_error": 0.005186483932228752
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015877265929089368,
            "upper_bound": 0.025655762097322583
          },
          "point_estimate": 0.009888048246501432,
          "standard_error": 0.0060629219846856305
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.83786314106392,
            "upper_bound": 40.84652651962855
          },
          "point_estimate": 40.84207595156322,
          "standard_error": 0.002221061798021004
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006412117876378861,
            "upper_bound": 0.03834576855640278
          },
          "point_estimate": 0.025902736938709384,
          "standard_error": 0.009837058527059291
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.74502849208842,
            "upper_bound": 68.85029351846997
          },
          "point_estimate": 68.79383909617096,
          "standard_error": 0.026297749976680752
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.76661696870164,
            "upper_bound": 68.8103391949182
          },
          "point_estimate": 68.79130669203462,
          "standard_error": 0.011261685266042395
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004008072663208579,
            "upper_bound": 0.13286934698247377
          },
          "point_estimate": 0.028055339825763217,
          "standard_error": 0.027178334979225113
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.74849985948308,
            "upper_bound": 68.8050037148263
          },
          "point_estimate": 68.78432957210917,
          "standard_error": 0.014575661932491057
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015554886871721805,
            "upper_bound": 0.1294178669076084
          },
          "point_estimate": 0.08738469857401372,
          "standard_error": 0.029300490025105
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.549709990559855,
            "upper_bound": 43.60038165169207
          },
          "point_estimate": 43.57685013804354,
          "standard_error": 0.013000535324213231
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.54684485588413,
            "upper_bound": 43.60963579909202
          },
          "point_estimate": 43.59617187724956,
          "standard_error": 0.020741048921380192
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002758393728565762,
            "upper_bound": 0.08181565328667793
          },
          "point_estimate": 0.032388116614526154,
          "standard_error": 0.01897860246181196
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.52960891428025,
            "upper_bound": 43.60929511594319
          },
          "point_estimate": 43.57747923968878,
          "standard_error": 0.020946039890880543
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02127256358747468,
            "upper_bound": 0.05667241020741962
          },
          "point_estimate": 0.04329863314261441,
          "standard_error": 0.00953407739168576
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.081803259849515,
            "upper_bound": 36.11172869501133
          },
          "point_estimate": 36.094505362423824,
          "standard_error": 0.007812385325674526
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.07663170926666,
            "upper_bound": 36.10170435403534
          },
          "point_estimate": 36.08828055866553,
          "standard_error": 0.007520161603253139
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003865219454323149,
            "upper_bound": 0.03122636168663327
          },
          "point_estimate": 0.017102253288215883,
          "standard_error": 0.006629720521370575
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.07982974756398,
            "upper_bound": 36.09688917612518
          },
          "point_estimate": 36.08823180269813,
          "standard_error": 0.00438023043461946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008978687977439833,
            "upper_bound": 0.03832888578338207
          },
          "point_estimate": 0.026134501775681954,
          "standard_error": 0.009249889521366754
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.4216992751277,
            "upper_bound": 73.57571214310335
          },
          "point_estimate": 73.4971668245042,
          "standard_error": 0.03914527394109394
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.39032162435069,
            "upper_bound": 73.54199726549827
          },
          "point_estimate": 73.52437102750218,
          "standard_error": 0.03869463507265879
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0031076819993036183,
            "upper_bound": 0.23840475128116417
          },
          "point_estimate": 0.02696049515147398,
          "standard_error": 0.0663077962354049
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.35341205484413,
            "upper_bound": 73.53523236103446
          },
          "point_estimate": 73.44739508789904,
          "standard_error": 0.04810548029309166
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05524817164844675,
            "upper_bound": 0.17904743198879475
          },
          "point_estimate": 0.13103638954875027,
          "standard_error": 0.03168881225893517
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154019.08459417877,
            "upper_bound": 154233.66878235555
          },
          "point_estimate": 154120.15579718188,
          "standard_error": 55.13986555375252
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153991.34480932204,
            "upper_bound": 154284.75423728814
          },
          "point_estimate": 154075.2244249395,
          "standard_error": 70.09233407989002
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.337668724495455,
            "upper_bound": 310.8423711763497
          },
          "point_estimate": 130.95426616029854,
          "standard_error": 73.05264329676686
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154002.62927883017,
            "upper_bound": 154131.18362267784
          },
          "point_estimate": 154064.41812678846,
          "standard_error": 33.05821477822263
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.98693718525249,
            "upper_bound": 231.4979647005776
          },
          "point_estimate": 184.4862111973836,
          "standard_error": 38.788681938081936
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57916.63253056885,
            "upper_bound": 58003.627574238635
          },
          "point_estimate": 57956.18132300449,
          "standard_error": 22.245962230302464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57899.07728601807,
            "upper_bound": 57992.91818181818
          },
          "point_estimate": 57943.05889154704,
          "standard_error": 21.024180750085524
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.226279771356635,
            "upper_bound": 106.33924665277702
          },
          "point_estimate": 53.02880194531452,
          "standard_error": 26.662119961934508
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57900.5858291847,
            "upper_bound": 57982.36061089362
          },
          "point_estimate": 57943.65593736407,
          "standard_error": 21.447583215179307
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.536503657741093,
            "upper_bound": 102.45672506476603
          },
          "point_estimate": 73.8052916433034,
          "standard_error": 20.536061742661147
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238874.86358189385,
            "upper_bound": 239624.55127139745
          },
          "point_estimate": 239225.67932876857,
          "standard_error": 192.54840459466809
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238744.4995850192,
            "upper_bound": 239614.72031590412
          },
          "point_estimate": 239100.26873638344,
          "standard_error": 237.39391659439127
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.5627510848382,
            "upper_bound": 1038.9908802470452
          },
          "point_estimate": 564.5353322650736,
          "standard_error": 225.09795819447515
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238735.42775433057,
            "upper_bound": 239463.64356312228
          },
          "point_estimate": 239012.72801969273,
          "standard_error": 183.0385258398861
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296.81926198494403,
            "upper_bound": 845.4822574673868
          },
          "point_estimate": 642.0009880640118,
          "standard_error": 146.3646798439814
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 794733.9547673826,
            "upper_bound": 795734.784437543
          },
          "point_estimate": 795189.6280883367,
          "standard_error": 257.55439352527225
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 794622.8304347827,
            "upper_bound": 795602.7771739131
          },
          "point_estimate": 795084.8161231885,
          "standard_error": 249.54636311340352
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 196.79289215833236,
            "upper_bound": 1203.633747761712
          },
          "point_estimate": 651.0650668833676,
          "standard_error": 263.63775146410785
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 794838.4851725064,
            "upper_bound": 795523.4455431147
          },
          "point_estimate": 795230.1554488989,
          "standard_error": 178.24477865270617
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 341.2777041613468,
            "upper_bound": 1202.0601920620277
          },
          "point_estimate": 858.2978443568257,
          "standard_error": 245.57074945494617
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1727294.602792208,
            "upper_bound": 1729703.1283858223
          },
          "point_estimate": 1728483.7261057,
          "standard_error": 616.997560975446
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1727228.1515151516,
            "upper_bound": 1730506.9668560603
          },
          "point_estimate": 1727816.388888889,
          "standard_error": 888.4148988531696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220.6462563098303,
            "upper_bound": 3517.8357180005837
          },
          "point_estimate": 1800.134699859507,
          "standard_error": 983.4995586447426
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1727432.978899853,
            "upper_bound": 1730014.8630853994
          },
          "point_estimate": 1728397.2913813458,
          "standard_error": 680.16104062812
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1120.5888483604358,
            "upper_bound": 2630.730363749588
          },
          "point_estimate": 2060.0759646609204,
          "standard_error": 379.69720247320487
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72286.62869261051,
            "upper_bound": 72384.85260695314
          },
          "point_estimate": 72330.92637126098,
          "standard_error": 25.388992218672577
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72283.24654714475,
            "upper_bound": 72386.77602091632
          },
          "point_estimate": 72306.71425409472,
          "standard_error": 22.168485511233776
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.109768895513393,
            "upper_bound": 123.88754526071448
          },
          "point_estimate": 30.184757565034325,
          "standard_error": 27.713073711598685
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72277.81413416103,
            "upper_bound": 72336.1481766274
          },
          "point_estimate": 72302.70214208102,
          "standard_error": 14.859918655059316
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.5719511924927,
            "upper_bound": 111.30571932888134
          },
          "point_estimate": 84.61658845123205,
          "standard_error": 23.278403510021736
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205902.26872464913,
            "upper_bound": 206382.5687679468
          },
          "point_estimate": 206142.83596471167,
          "standard_error": 123.15934018400188
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205884.8602636535,
            "upper_bound": 206362.58474576272
          },
          "point_estimate": 206157.67781476997,
          "standard_error": 126.19670572813752
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.706401871871506,
            "upper_bound": 724.1149898562509
          },
          "point_estimate": 316.1553617835681,
          "standard_error": 170.22093829276884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206092.95973006543,
            "upper_bound": 206372.69162800888
          },
          "point_estimate": 206258.50587717368,
          "standard_error": 70.91444781098156
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 186.15021289903737,
            "upper_bound": 551.5962814378618
          },
          "point_estimate": 409.6275636391212,
          "standard_error": 92.16667690361145
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138181.82848391574,
            "upper_bound": 138425.9658375581
          },
          "point_estimate": 138302.99175463215,
          "standard_error": 62.63604304328976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138101.91397338404,
            "upper_bound": 138527.80544993663
          },
          "point_estimate": 138256.66751161808,
          "standard_error": 124.59026242466824
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.06656309361383,
            "upper_bound": 328.8237623751262
          },
          "point_estimate": 298.9919789757261,
          "standard_error": 85.41643061879884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138175.56109700806,
            "upper_bound": 138461.11206850852
          },
          "point_estimate": 138326.13557848995,
          "standard_error": 72.47578862611053
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.13052090750568,
            "upper_bound": 237.5385872492776
          },
          "point_estimate": 209.247210745304,
          "standard_error": 25.12083450180703
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 863859.3989823735,
            "upper_bound": 864839.375389535
          },
          "point_estimate": 864291.6353857511,
          "standard_error": 253.01180139312015
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 863713.6519933555,
            "upper_bound": 864750.4625322997
          },
          "point_estimate": 863954.6496124031,
          "standard_error": 267.89493938254964
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.76993130183698,
            "upper_bound": 1179.8722140917232
          },
          "point_estimate": 599.6662919120175,
          "standard_error": 282.7168664298556
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 863829.3704916327,
            "upper_bound": 864641.149905026
          },
          "point_estimate": 864182.7571126547,
          "standard_error": 218.67232297591923
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299.1007617676538,
            "upper_bound": 1172.4242258673346
          },
          "point_estimate": 843.459066472127,
          "standard_error": 245.473696757274
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62564.112835628235,
            "upper_bound": 62706.4819518892
          },
          "point_estimate": 62628.00678694369,
          "standard_error": 36.50999191233741
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62542.32849766412,
            "upper_bound": 62729.60269650028
          },
          "point_estimate": 62587.86213425129,
          "standard_error": 39.781344847632944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.18194443305274,
            "upper_bound": 162.01593929231572
          },
          "point_estimate": 60.06164265142204,
          "standard_error": 36.96227078980445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62542.08412355083,
            "upper_bound": 62609.18032995556
          },
          "point_estimate": 62569.71543018083,
          "standard_error": 17.543453769352183
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.71365944173107,
            "upper_bound": 153.2074360747591
          },
          "point_estimate": 122.03053436064422,
          "standard_error": 31.913736296475584
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122718.3940884009,
            "upper_bound": 123095.8676498452
          },
          "point_estimate": 122905.7207602692,
          "standard_error": 97.032451960291
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122637.0571609556,
            "upper_bound": 123179.85506756758
          },
          "point_estimate": 122871.4914602102,
          "standard_error": 123.97540667231104
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.91456905845544,
            "upper_bound": 593.7175318158352
          },
          "point_estimate": 305.03974101464564,
          "standard_error": 132.61928859442682
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122637.33223197908,
            "upper_bound": 122896.72283564052
          },
          "point_estimate": 122759.97805370306,
          "standard_error": 65.42485174579701
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184.77025568812033,
            "upper_bound": 401.76317272390406
          },
          "point_estimate": 322.6660310162377,
          "standard_error": 55.81118544275371
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389533.9972340426,
            "upper_bound": 389907.6920567375
          },
          "point_estimate": 389730.596749409,
          "standard_error": 96.23708947046788
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389466.9556737589,
            "upper_bound": 389981.878250591
          },
          "point_estimate": 389853.72739361704,
          "standard_error": 147.42032338647013
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.98813971563786,
            "upper_bound": 523.2559276784247
          },
          "point_estimate": 253.79666836656355,
          "standard_error": 131.07977843334297
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389477.5102161007,
            "upper_bound": 389917.6790679481
          },
          "point_estimate": 389704.1196186792,
          "standard_error": 111.79506995623052
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151.75962788539073,
            "upper_bound": 384.7633280364877
          },
          "point_estimate": 319.5912203843415,
          "standard_error": 57.62979164999699
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58154.58462966667,
            "upper_bound": 58246.425896507935
          },
          "point_estimate": 58196.28298539682,
          "standard_error": 23.79557848738782
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58141.8752,
            "upper_bound": 58252.418844444444
          },
          "point_estimate": 58161.7126,
          "standard_error": 27.48780820671876
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.218348174100651,
            "upper_bound": 127.7604499451289
          },
          "point_estimate": 35.36593977212811,
          "standard_error": 29.654020305839897
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58151.31136,
            "upper_bound": 58253.622098759566
          },
          "point_estimate": 58197.10836363636,
          "standard_error": 25.99128515695357
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.066827601158128,
            "upper_bound": 106.21555844967877
          },
          "point_estimate": 79.3282917561606,
          "standard_error": 20.87286929959451
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22409.693515015475,
            "upper_bound": 22466.079945201232
          },
          "point_estimate": 22437.418644994836,
          "standard_error": 14.432040067469918
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22399.067894736843,
            "upper_bound": 22482.511248710012
          },
          "point_estimate": 22427.511996904024,
          "standard_error": 23.401962295045657
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.061625881645222,
            "upper_bound": 79.84745652359639
          },
          "point_estimate": 55.96357421387813,
          "standard_error": 18.39503665931801
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22417.24809175993,
            "upper_bound": 22483.348195078357
          },
          "point_estimate": 22450.752331631214,
          "standard_error": 17.253417283216343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.3015545925217,
            "upper_bound": 58.76234364223669
          },
          "point_estimate": 48.134819829963426,
          "standard_error": 7.7035168109874
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-repeated-rare-huge_common-m"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1979314.8933399124,
            "upper_bound": 1984524.103464912
          },
          "point_estimate": 1981565.5743713449,
          "standard_error": 1354.2813492359255
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1978794.0690789472,
            "upper_bound": 1983081.005263158
          },
          "point_estimate": 1980226.3545321636,
          "standard_error": 1168.9879509553818
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 485.7563242708109,
            "upper_bound": 5269.7699281532905
          },
          "point_estimate": 3396.6979895648483,
          "standard_error": 1264.5923453035484
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1978514.3907894737,
            "upper_bound": 1980530.3463118763
          },
          "point_estimate": 1979485.27518797,
          "standard_error": 509.27755418519456
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1555.4848961857576,
            "upper_bound": 6532.132461185092
          },
          "point_estimate": 4521.660516795486,
          "standard_error": 1516.9860084679342
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-repeated-rare-small_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4026.64764338179,
            "upper_bound": 4029.4701097146863
          },
          "point_estimate": 4027.7881021323383,
          "standard_error": 0.7617216388309657
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4026.471888495183,
            "upper_bound": 4028.125188240505
          },
          "point_estimate": 4026.9705089875615,
          "standard_error": 0.41036857340003
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09147433344406504,
            "upper_bound": 2.123522647495696
          },
          "point_estimate": 0.9538187060762024,
          "standard_error": 0.5621953802924365
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4026.601069789506,
            "upper_bound": 4027.425760544598
          },
          "point_estimate": 4026.9357505964304,
          "standard_error": 0.20787657366302828
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.45923717270226344,
            "upper_bound": 3.783629189945766
          },
          "point_estimate": 2.528006615011986,
          "standard_error": 1.0386727047882292
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51857.09655492153,
            "upper_bound": 51957.32005720004
          },
          "point_estimate": 51906.311915517515,
          "standard_error": 25.70326866060663
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51825.6959700428,
            "upper_bound": 51987.95805991441
          },
          "point_estimate": 51889.76604850214,
          "standard_error": 50.95119846274907
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.111363219593546,
            "upper_bound": 135.6211792137397
          },
          "point_estimate": 111.6322992187947,
          "standard_error": 34.87861977025135
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51835.408127423114,
            "upper_bound": 51939.18151994014
          },
          "point_estimate": 51878.59180762176,
          "standard_error": 27.156508293253868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.908650680426334,
            "upper_bound": 98.89627719470175
          },
          "point_estimate": 85.53016090258237,
          "standard_error": 11.028625405782146
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53161.711399400694,
            "upper_bound": 53244.42662483442
          },
          "point_estimate": 53199.3787108113,
          "standard_error": 21.2205713661387
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53142.623426061495,
            "upper_bound": 53241.144460712545
          },
          "point_estimate": 53187.63113103953,
          "standard_error": 25.56711219870788
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.737494889900514,
            "upper_bound": 109.27349105659968
          },
          "point_estimate": 60.80418173163833,
          "standard_error": 26.450634157020133
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53153.99811830781,
            "upper_bound": 53291.859325506
          },
          "point_estimate": 53218.61971820273,
          "standard_error": 38.74657184098719
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.26485482845464,
            "upper_bound": 94.35112408333774
          },
          "point_estimate": 70.37864403690385,
          "standard_error": 17.547301818289082
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52782.60588159744,
            "upper_bound": 52886.10697145622
          },
          "point_estimate": 52833.01238895801,
          "standard_error": 26.54213568025071
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52750.55261248186,
            "upper_bound": 52895.55101596517
          },
          "point_estimate": 52833.82497782615,
          "standard_error": 41.955358940472216
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.95038198576847,
            "upper_bound": 156.61258690026813
          },
          "point_estimate": 95.77206411369455,
          "standard_error": 34.345946719141054
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52774.81216120676,
            "upper_bound": 52877.98198879213
          },
          "point_estimate": 52833.65427025804,
          "standard_error": 25.91091153831133
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.79910296532554,
            "upper_bound": 111.98199087251449
          },
          "point_estimate": 88.61666173289183,
          "standard_error": 15.66377257196348
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15227.106506613478,
            "upper_bound": 15265.655254522911
          },
          "point_estimate": 15244.92288824359,
          "standard_error": 9.914081725834132
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15217.535414920369,
            "upper_bound": 15266.556370494553
          },
          "point_estimate": 15234.509817686505,
          "standard_error": 10.93202022029062
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3436388603195,
            "upper_bound": 53.077790186486254
          },
          "point_estimate": 26.29690689063931,
          "standard_error": 12.582017798569384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15224.329500047386,
            "upper_bound": 15250.629176826946
          },
          "point_estimate": 15236.65883018909,
          "standard_error": 6.737507894645326
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.18235799589114,
            "upper_bound": 42.397253406166634
          },
          "point_estimate": 33.01377165423493,
          "standard_error": 7.655867039688834
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26454.539472303204,
            "upper_bound": 26485.44808984755
          },
          "point_estimate": 26470.04610043847,
          "standard_error": 7.920999978912226
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26443.482090795504,
            "upper_bound": 26492.283325234857
          },
          "point_estimate": 26468.507252186588,
          "standard_error": 12.639517370307502
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.17907422778153,
            "upper_bound": 43.941934500487704
          },
          "point_estimate": 36.09005555460771,
          "standard_error": 10.064272487110852
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26444.18110615675,
            "upper_bound": 26491.877238025347
          },
          "point_estimate": 26467.516108818298,
          "standard_error": 12.347157152660396
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.132277431797547,
            "upper_bound": 32.17425631972571
          },
          "point_estimate": 26.36848262968526,
          "standard_error": 4.080318456907473
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16464.466415241845,
            "upper_bound": 16493.094096362103
          },
          "point_estimate": 16478.441584671695,
          "standard_error": 7.355559189379717
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16457.119332223734,
            "upper_bound": 16501.430503858377
          },
          "point_estimate": 16472.86353835679,
          "standard_error": 12.100090300426125
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.725006669430479,
            "upper_bound": 39.84010326455429
          },
          "point_estimate": 33.41846293825189,
          "standard_error": 9.207221688307568
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16460.98048785396,
            "upper_bound": 16495.39338731032
          },
          "point_estimate": 16478.855987407962,
          "standard_error": 8.797662441602721
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.791047949565096,
            "upper_bound": 29.589739759147285
          },
          "point_estimate": 24.48822032396541,
          "standard_error": 3.7589788617403097
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16461.542637569884,
            "upper_bound": 16512.192726276822
          },
          "point_estimate": 16483.427090888486,
          "standard_error": 13.228715515783472
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16455.341455122394,
            "upper_bound": 16496.50956482321
          },
          "point_estimate": 16473.618155031734,
          "standard_error": 10.230134352647251
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.902296504084224,
            "upper_bound": 51.350773072930075
          },
          "point_estimate": 25.493906375361608,
          "standard_error": 12.626581330227156
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16464.14272374749,
            "upper_bound": 16486.33580731676
          },
          "point_estimate": 16475.909040279756,
          "standard_error": 5.574838502556777
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.90956168732093,
            "upper_bound": 63.596331596414274
          },
          "point_estimate": 43.96639354822192,
          "standard_error": 14.935181062234935
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19097.320030637253,
            "upper_bound": 19119.20323729491
          },
          "point_estimate": 19108.55680697279,
          "standard_error": 5.587559129636102
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19092.912768232294,
            "upper_bound": 19120.624212184874
          },
          "point_estimate": 19111.125166316524,
          "standard_error": 6.822319155369395
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.879740206226811,
            "upper_bound": 29.943613806632104
          },
          "point_estimate": 13.64636979449279,
          "standard_error": 7.873252221454968
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19101.041209776595,
            "upper_bound": 19115.999382258997
          },
          "point_estimate": 19109.63798564881,
          "standard_error": 3.8333588158243
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.623063802024857,
            "upper_bound": 23.99743485402255
          },
          "point_estimate": 18.645237482479423,
          "standard_error": 3.7477499214487975
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93944.80820624418,
            "upper_bound": 94008.93207363375
          },
          "point_estimate": 93974.5740317018,
          "standard_error": 16.46382316061198
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93933.84185236768,
            "upper_bound": 94020.93457355088
          },
          "point_estimate": 93946.5817084494,
          "standard_error": 22.97161978314806
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.713038792015838,
            "upper_bound": 88.01565658503941
          },
          "point_estimate": 30.89524283591313,
          "standard_error": 23.79554972277356
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93936.6396760266,
            "upper_bound": 94000.36494031164
          },
          "point_estimate": 93960.2434685092,
          "standard_error": 16.402577145729506
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.028587046253666,
            "upper_bound": 69.91622306098618
          },
          "point_estimate": 54.90437395459344,
          "standard_error": 12.240449045498629
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105384.39871531792,
            "upper_bound": 105661.26560705683
          },
          "point_estimate": 105531.12368015414,
          "standard_error": 71.39141632064603
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105500.4346098266,
            "upper_bound": 105661.07514450866
          },
          "point_estimate": 105528.02418111754,
          "standard_error": 33.59845795888378
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4297622223473745,
            "upper_bound": 370.0855556117439
          },
          "point_estimate": 18.597309857689904,
          "standard_error": 94.3061016282299
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105389.75181426945,
            "upper_bound": 105550.5983145968
          },
          "point_estimate": 105499.21683807524,
          "standard_error": 43.080013102605655
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.215607135001072,
            "upper_bound": 344.1173480742275
          },
          "point_estimate": 238.48486447816785,
          "standard_error": 75.4547698563588
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20119.398267312827,
            "upper_bound": 20179.946050884955
          },
          "point_estimate": 20146.40051923111,
          "standard_error": 15.671187621501494
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20109.12907909292,
            "upper_bound": 20179.421902654867
          },
          "point_estimate": 20127.698506637167,
          "standard_error": 17.13553688131671
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.5175643194476205,
            "upper_bound": 77.10566758021726
          },
          "point_estimate": 35.89906457284274,
          "standard_error": 17.433902781762804
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20111.759829823797,
            "upper_bound": 20160.918823924443
          },
          "point_estimate": 20135.45548212849,
          "standard_error": 13.320585358357857
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.749512626676673,
            "upper_bound": 73.33738211769676
          },
          "point_estimate": 52.43832802044801,
          "standard_error": 15.345120582373378
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16607.56378323594,
            "upper_bound": 16661.908235174367
          },
          "point_estimate": 16632.194364136267,
          "standard_error": 14.033858596209226
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16600.843159658743,
            "upper_bound": 16662.74989843591
          },
          "point_estimate": 16616.93279087229,
          "standard_error": 14.657329858579445
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.134831062331146,
            "upper_bound": 71.83447962809846
          },
          "point_estimate": 32.45414262538083,
          "standard_error": 17.66372052618182
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16599.386208104814,
            "upper_bound": 16642.226608280012
          },
          "point_estimate": 16619.419715567798,
          "standard_error": 11.165735106804044
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.231906417273905,
            "upper_bound": 63.473250005496155
          },
          "point_estimate": 46.933746101060215,
          "standard_error": 12.346331125347168
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19093.158731371943,
            "upper_bound": 19123.51286096246
          },
          "point_estimate": 19106.55941980455,
          "standard_error": 7.850419653385675
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19091.068546267088,
            "upper_bound": 19118.265425283327
          },
          "point_estimate": 19100.73004293726,
          "standard_error": 5.616561053018812
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.988833422893869,
            "upper_bound": 35.75745559764959
          },
          "point_estimate": 11.386887554150006,
          "standard_error": 8.565289414490806
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19093.866205898168,
            "upper_bound": 19127.121533873786
          },
          "point_estimate": 19109.33600584484,
          "standard_error": 8.488026017294647
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.925332312362196,
            "upper_bound": 36.436605237799554
          },
          "point_estimate": 26.17349856751954,
          "standard_error": 8.060742201683324
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16465.338119864824,
            "upper_bound": 16494.92950270064
          },
          "point_estimate": 16479.36227386167,
          "standard_error": 7.594233326734461
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16462.1688755475,
            "upper_bound": 16499.717716357045
          },
          "point_estimate": 16470.627718622563,
          "standard_error": 9.306421251790995
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.476887288002664,
            "upper_bound": 39.099606854554
          },
          "point_estimate": 13.430907347263764,
          "standard_error": 11.195146053031126
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16464.72934548275,
            "upper_bound": 16508.64135336519
          },
          "point_estimate": 16487.436245947076,
          "standard_error": 11.753101501755564
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.24812880111183,
            "upper_bound": 31.13798422352595
          },
          "point_estimate": 25.264139589211112,
          "standard_error": 4.971676211636702
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16495.414520999504,
            "upper_bound": 16518.732054164506
          },
          "point_estimate": 16506.567528064184,
          "standard_error": 5.96463735535833
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16488.013844757148,
            "upper_bound": 16518.50964593736
          },
          "point_estimate": 16508.43857402244,
          "standard_error": 7.587020135837399
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.815358420283709,
            "upper_bound": 36.54214487401939
          },
          "point_estimate": 16.00285973404791,
          "standard_error": 8.039794494574691
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16493.572878181076,
            "upper_bound": 16529.45781626187
          },
          "point_estimate": 16512.029336618896,
          "standard_error": 9.233744886443834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.618612934021115,
            "upper_bound": 26.386001210141902
          },
          "point_estimate": 19.923071478668643,
          "standard_error": 4.342366631467743
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16464.913635374563,
            "upper_bound": 16499.053744998702
          },
          "point_estimate": 16480.876765954075,
          "standard_error": 8.764963503920669
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16455.88756241489,
            "upper_bound": 16502.234888031475
          },
          "point_estimate": 16471.875472840067,
          "standard_error": 12.552002703158294
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.340383021749134,
            "upper_bound": 47.73422305631414
          },
          "point_estimate": 23.790207843640925,
          "standard_error": 11.5763508883925
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16459.451310031774,
            "upper_bound": 16501.021264391533
          },
          "point_estimate": 16475.657320890637,
          "standard_error": 10.803510513688831
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.741494872818125,
            "upper_bound": 36.60834144795787
          },
          "point_estimate": 29.122350409030595,
          "standard_error": 5.83187531534485
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19669.145766355352,
            "upper_bound": 19700.128633346867
          },
          "point_estimate": 19684.269707182822,
          "standard_error": 7.945286042435156
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19661.60956957228,
            "upper_bound": 19710.86118029237
          },
          "point_estimate": 19676.952874029957,
          "standard_error": 15.970557586796025
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.28090888614165,
            "upper_bound": 40.170887083251145
          },
          "point_estimate": 29.89361430414925,
          "standard_error": 11.735657544493789
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19659.26938374971,
            "upper_bound": 19681.876796171488
          },
          "point_estimate": 19666.689933131296,
          "standard_error": 5.84090138806741
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.339510067003296,
            "upper_bound": 30.19653978169559
          },
          "point_estimate": 26.499676418591523,
          "standard_error": 3.329893709507223
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16996.85653759164,
            "upper_bound": 17028.32254718453
          },
          "point_estimate": 17010.924848307597,
          "standard_error": 8.095390456677334
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16990.787747621274,
            "upper_bound": 17021.863593823116
          },
          "point_estimate": 17007.631463500235,
          "standard_error": 9.135780769373053
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.923346502233243,
            "upper_bound": 39.55215272316857
          },
          "point_estimate": 21.48510757831922,
          "standard_error": 8.207674545489619
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16995.237274345523,
            "upper_bound": 17017.91616469244
          },
          "point_estimate": 17006.024305222152,
          "standard_error": 5.883645395009519
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.63944835341759,
            "upper_bound": 37.75355162741101
          },
          "point_estimate": 26.85555349202433,
          "standard_error": 7.731913900546782
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19782.697557189545,
            "upper_bound": 19827.613565155232
          },
          "point_estimate": 19798.854215686275,
          "standard_error": 13.244833702442444
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19781.07198620189,
            "upper_bound": 19791.841435185182
          },
          "point_estimate": 19785.112064270153,
          "standard_error": 4.115197441039242
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.324528652954552,
            "upper_bound": 11.998313090362917
          },
          "point_estimate": 5.324829943590895,
          "standard_error": 4.691984343307333
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19782.729172601452,
            "upper_bound": 19791.35857043114
          },
          "point_estimate": 19786.545403615994,
          "standard_error": 2.224410044871332
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.733736652474005,
            "upper_bound": 67.81628665204931
          },
          "point_estimate": 44.10716393289129,
          "standard_error": 23.05084878009193
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-freq_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59564.88206764866,
            "upper_bound": 59633.00491680304
          },
          "point_estimate": 59591.56580332787,
          "standard_error": 18.595524695962297
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59558.29241680306,
            "upper_bound": 59598.48952536825
          },
          "point_estimate": 59568.625477359514,
          "standard_error": 12.863878799604796
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.633374175831672,
            "upper_bound": 49.85667051421163
          },
          "point_estimate": 21.08782098403516,
          "standard_error": 12.316507692676431
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59563.70234977788,
            "upper_bound": 59591.13986705117
          },
          "point_estimate": 59579.17564563097,
          "standard_error": 7.0564916467574434
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.294522691180166,
            "upper_bound": 94.28883417840238
          },
          "point_estimate": 62.10018788379535,
          "standard_error": 27.26167542438128
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-repeated_"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 884155.0898875662,
            "upper_bound": 885452.5106631946
          },
          "point_estimate": 884660.3876521165,
          "standard_error": 355.0650678409585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 884155.222718254,
            "upper_bound": 884672.9761904762
          },
          "point_estimate": 884316.4357142857,
          "standard_error": 162.69526326466055
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.31010694295513,
            "upper_bound": 761.6330316171768
          },
          "point_estimate": 278.93471171456645,
          "standard_error": 195.71700270933883
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 883954.1440391314,
            "upper_bound": 884505.7317275541
          },
          "point_estimate": 884220.3233147805,
          "standard_error": 142.437414103987
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194.6965401368188,
            "upper_bound": 1802.0698495431957
          },
          "point_estimate": 1180.6180096132116,
          "standard_error": 535.2474820062661
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector_rare-alph"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202315.43573544975,
            "upper_bound": 202649.4914007937
          },
          "point_estimate": 202503.9216926808,
          "standard_error": 86.17196966851051
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202402.3350308642,
            "upper_bound": 202656.90555555557
          },
          "point_estimate": 202565.47162698413,
          "standard_error": 57.3752878607878
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.394731162548048,
            "upper_bound": 400.9935296865149
          },
          "point_estimate": 128.34682647139698,
          "standard_error": 94.3955369953481
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202066.9212888326,
            "upper_bound": 202601.7988850957
          },
          "point_estimate": 202343.7970995671,
          "standard_error": 151.07681009264954
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.72207077597582,
            "upper_bound": 406.25448669550497
          },
          "point_estimate": 285.95294093231985,
          "standard_error": 91.76610176743704
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6126.467975172081,
            "upper_bound": 6155.800934267198
          },
          "point_estimate": 6141.219664291752,
          "standard_error": 7.500730099299307
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6118.144547759933,
            "upper_bound": 6166.6421676126065
          },
          "point_estimate": 6139.471975204283,
          "standard_error": 14.320129887340574
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.076727998628102,
            "upper_bound": 41.79336950654016
          },
          "point_estimate": 34.14518510808732,
          "standard_error": 10.571515650125148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6131.787568468683,
            "upper_bound": 6164.967773356517
          },
          "point_estimate": 6149.320456686171,
          "standard_error": 8.586322113874747
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.398022048071365,
            "upper_bound": 29.56874173856103
          },
          "point_estimate": 25.036912467473584,
          "standard_error": 3.379261213148962
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6179.022508373975,
            "upper_bound": 6201.100334185988
          },
          "point_estimate": 6189.2230245683395,
          "standard_error": 5.672998790991388
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6175.0031322067225,
            "upper_bound": 6197.074458432573
          },
          "point_estimate": 6185.792652991785,
          "standard_error": 4.802929341364247
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7494831026600344,
            "upper_bound": 27.916784104581794
          },
          "point_estimate": 10.43365228329247,
          "standard_error": 7.466006274348492
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6173.445300591802,
            "upper_bound": 6186.362882919054
          },
          "point_estimate": 6179.779418302011,
          "standard_error": 3.322405301217145
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.645802748410617,
            "upper_bound": 26.078875260014595
          },
          "point_estimate": 18.917748864052776,
          "standard_error": 5.17070070431339
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-repeated-rare-huge_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15715.077765153856,
            "upper_bound": 15731.31779108396
          },
          "point_estimate": 15721.715803846044,
          "standard_error": 4.320082685778668
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15715.4139994233,
            "upper_bound": 15722.523788927336
          },
          "point_estimate": 15718.253574346409,
          "standard_error": 2.112448511002916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.958314061706722,
            "upper_bound": 12.48259584240158
          },
          "point_estimate": 4.523424418686264,
          "standard_error": 2.9675858291285397
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15713.255618766238,
            "upper_bound": 15721.286871880457
          },
          "point_estimate": 15718.029172471128,
          "standard_error": 2.048222390776404
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.851928559828246,
            "upper_bound": 21.59669089758966
          },
          "point_estimate": 14.375219537960536,
          "standard_error": 5.9245173629314625
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-repeated-rare-small_never-tric"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.017468067824595,
            "upper_bound": 35.114330831164445
          },
          "point_estimate": 35.06166016194952,
          "standard_error": 0.02491633949661646
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.00178529114942,
            "upper_bound": 35.11503015895339
          },
          "point_estimate": 35.027249616150016,
          "standard_error": 0.03189515191976382
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00813166383085466,
            "upper_bound": 0.13660002776271513
          },
          "point_estimate": 0.06439091244868907,
          "standard_error": 0.03252784549764534
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.00798402013822,
            "upper_bound": 35.17074913430262
          },
          "point_estimate": 35.079041489523824,
          "standard_error": 0.04756111997181864
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0376346188081889,
            "upper_bound": 0.11218990407864766
          },
          "point_estimate": 0.08290586308194102,
          "standard_error": 0.021133259397280908
        }
      }
    },
    "memmem/krate_nopre/prebuilt/sliceslice-haystack/words": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/sliceslice-haystack/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/sliceslice-haystack/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/sliceslice-haystack/words",
        "directory_name": "memmem/krate_nopre_prebuilt_sliceslice-haystack_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174439.35325529732,
            "upper_bound": 174594.81860313663
          },
          "point_estimate": 174505.39666191995,
          "standard_error": 40.6941378257537
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174431.63564213563,
            "upper_bound": 174534.15598086122
          },
          "point_estimate": 174469.06339712918,
          "standard_error": 22.77406546160281
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.557805306218003,
            "upper_bound": 156.08487982462665
          },
          "point_estimate": 48.13377861914495,
          "standard_error": 41.498851960734775
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174446.0945765803,
            "upper_bound": 174537.17911379237
          },
          "point_estimate": 174478.53633256696,
          "standard_error": 23.513871790758763
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.170643325517055,
            "upper_bound": 198.79663176473105
          },
          "point_estimate": 135.5253200173388,
          "standard_error": 49.13128557212574
        }
      }
    },
    "memmem/krate_nopre/prebuilt/sliceslice-i386/words": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/krate_nopre_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25044917.571243055,
            "upper_bound": 25065268.81205555
          },
          "point_estimate": 25055908.946388885,
          "standard_error": 5231.690896614486
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25044440.6,
            "upper_bound": 25070527.9875
          },
          "point_estimate": 25060208.458333336,
          "standard_error": 7056.197436976893
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1588.4390792984643,
            "upper_bound": 30077.07324102613
          },
          "point_estimate": 15509.076787155163,
          "standard_error": 7010.92110286797
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25052736.609379247,
            "upper_bound": 25067981.878438864
          },
          "point_estimate": 25062333.75194805,
          "standard_error": 3905.0727015462694
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7799.390880865358,
            "upper_bound": 23051.1202339624
          },
          "point_estimate": 17423.437639792584,
          "standard_error": 4085.420747631513
        }
      }
    },
    "memmem/krate_nopre/prebuilt/sliceslice-words/words": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/krate_nopre_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79939330.53333333,
            "upper_bound": 79957965.32666667
          },
          "point_estimate": 79948338.73333332,
          "standard_error": 4762.378992876473
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79933366.33333333,
            "upper_bound": 79959644.66666666
          },
          "point_estimate": 79948322.16666667,
          "standard_error": 8667.191098155312
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1654.334470622328,
            "upper_bound": 30977.690950043412
          },
          "point_estimate": 20815.20943045248,
          "standard_error": 7677.839512522207
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9739.777715731872,
            "upper_bound": 19741.096111080937
          },
          "point_estimate": 15850.38420362322,
          "standard_error": 2700.79788261156
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.83474997194591,
            "upper_bound": 8.83896703631704
          },
          "point_estimate": 8.836506186093327,
          "standard_error": 0.0011079060287947128
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834471804038257,
            "upper_bound": 8.837137802777264
          },
          "point_estimate": 8.835901233235575,
          "standard_error": 0.0007572738839781667
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004921077405363434,
            "upper_bound": 0.003756301548648977
          },
          "point_estimate": 0.00160673525061255,
          "standard_error": 0.0008535145973359717
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.83475323406411,
            "upper_bound": 8.836261894072408
          },
          "point_estimate": 8.835521679324609,
          "standard_error": 0.0003845834345813319
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010442113615351048,
            "upper_bound": 0.0055004186892658674
          },
          "point_estimate": 0.003701215039755485,
          "standard_error": 0.0014146359810750537
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834817336159709,
            "upper_bound": 8.84022606375337
          },
          "point_estimate": 8.837235143170002,
          "standard_error": 0.001397261880941859
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.833958852958201,
            "upper_bound": 8.839714201847803
          },
          "point_estimate": 8.835342826751397,
          "standard_error": 0.0015271363353205566
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00040767641896542327,
            "upper_bound": 0.006888677021836617
          },
          "point_estimate": 0.002725578997351883,
          "standard_error": 0.0016413346487522452
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834720114198506,
            "upper_bound": 8.837268993095883
          },
          "point_estimate": 8.83588522832065,
          "standard_error": 0.0006416547843712634
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014860155590989286,
            "upper_bound": 0.006394165804599124
          },
          "point_estimate": 0.004647602651734301,
          "standard_error": 0.0013138798145402688
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.835765911671206,
            "upper_bound": 8.842497486532652
          },
          "point_estimate": 8.838369023174844,
          "standard_error": 0.001854531009311254
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.836078680603041,
            "upper_bound": 8.837796654171445
          },
          "point_estimate": 8.837094533232827,
          "standard_error": 0.0005655722660231937
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000051836568278976,
            "upper_bound": 0.0035554610797417703
          },
          "point_estimate": 0.0009712107927823374,
          "standard_error": 0.0010075853872367317
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.835748483929278,
            "upper_bound": 8.837125738877965
          },
          "point_estimate": 8.8364945950271,
          "standard_error": 0.0003577619987440213
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006445459975031233,
            "upper_bound": 0.0094634684286651
          },
          "point_estimate": 0.0061732651977733895,
          "standard_error": 0.0028908774769655127
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.835138987024667,
            "upper_bound": 8.839409614551238
          },
          "point_estimate": 8.836837078423855,
          "standard_error": 0.0011616314182353558
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.8347154971716,
            "upper_bound": 8.837314346789091
          },
          "point_estimate": 8.83550035513301,
          "standard_error": 0.0007563061327868455
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00033519360974168847,
            "upper_bound": 0.003054358745057276
          },
          "point_estimate": 0.0016632688458237787,
          "standard_error": 0.0007146121705586179
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.834909548975654,
            "upper_bound": 8.836980131200622
          },
          "point_estimate": 8.835735961399022,
          "standard_error": 0.0005307976707029926
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008470061954255944,
            "upper_bound": 0.005871437821427536
          },
          "point_estimate": 0.003872465849484779,
          "standard_error": 0.0016740687054180852
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.70304044358486,
            "upper_bound": 9.713150032406917
          },
          "point_estimate": 9.708406525518322,
          "standard_error": 0.0025933152846646258
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.701748477996546,
            "upper_bound": 9.716428237039704
          },
          "point_estimate": 9.709667075347216,
          "standard_error": 0.003453867809737157
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002780143937746608,
            "upper_bound": 0.01495303754313494
          },
          "point_estimate": 0.009185399451462704,
          "standard_error": 0.003641810485521747
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.70565634065612,
            "upper_bound": 9.713477142292817
          },
          "point_estimate": 9.709442903360072,
          "standard_error": 0.0019840052037791074
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0037135885640162255,
            "upper_bound": 0.010694467677313168
          },
          "point_estimate": 0.008648702611432964,
          "standard_error": 0.0017498825003160515
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.065607177952655,
            "upper_bound": 10.071935203223338
          },
          "point_estimate": 10.068478981629642,
          "standard_error": 0.0016208159338452265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.065703645748828,
            "upper_bound": 10.070798822014348
          },
          "point_estimate": 10.06719094935471,
          "standard_error": 0.0014940602622634826
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00039384985634057255,
            "upper_bound": 0.007627843669383216
          },
          "point_estimate": 0.003383203875715611,
          "standard_error": 0.0018467255510945851
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.066388399741252,
            "upper_bound": 10.069652626602526
          },
          "point_estimate": 10.068139361124798,
          "standard_error": 0.000836451437974644
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019287014176081472,
            "upper_bound": 0.00766032591182753
          },
          "point_estimate": 0.005397985145734516,
          "standard_error": 0.0016173438914047769
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.848596308370864,
            "upper_bound": 7.857321480685701
          },
          "point_estimate": 7.853482464468561,
          "standard_error": 0.0022484190591189114
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.851553989952986,
            "upper_bound": 7.856755049795726
          },
          "point_estimate": 7.855485564151341,
          "standard_error": 0.0016461902828719484
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005224697494244191,
            "upper_bound": 0.008934514620422223
          },
          "point_estimate": 0.004056349697697278,
          "standard_error": 0.002174012364597867
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.847580006432142,
            "upper_bound": 7.856530453516325
          },
          "point_estimate": 7.853562558154672,
          "standard_error": 0.002379354207379687
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00209942564439915,
            "upper_bound": 0.010847222782858554
          },
          "point_estimate": 0.007486693945512137,
          "standard_error": 0.0025764159668561467
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.759732840495133,
            "upper_bound": 9.784017674965456
          },
          "point_estimate": 9.771059020295498,
          "standard_error": 0.006224857799102344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.751421741309542,
            "upper_bound": 9.787000410606677
          },
          "point_estimate": 9.766595061634051,
          "standard_error": 0.008585156775961628
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013911789277016948,
            "upper_bound": 0.03284805909848372
          },
          "point_estimate": 0.022661885476999342,
          "standard_error": 0.008348277862593642
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.756077501522974,
            "upper_bound": 9.79644629607183
          },
          "point_estimate": 9.781888421271953,
          "standard_error": 0.009674618754480304
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00971378733580927,
            "upper_bound": 0.026267672579018356
          },
          "point_estimate": 0.020709155745286503,
          "standard_error": 0.0041807189159926254
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.398924848208866,
            "upper_bound": 12.450210803783175
          },
          "point_estimate": 12.42425485569253,
          "standard_error": 0.013072448738705366
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.398738393669122,
            "upper_bound": 12.452683814786823
          },
          "point_estimate": 12.418531613605332,
          "standard_error": 0.011506379565833738
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003808026691940224,
            "upper_bound": 0.07726753436884472
          },
          "point_estimate": 0.030006439132968653,
          "standard_error": 0.01833831098014864
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.403351281314464,
            "upper_bound": 12.438708729860616
          },
          "point_estimate": 12.417405382895842,
          "standard_error": 0.009024258530110916
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017129144957075612,
            "upper_bound": 0.058941049453692954
          },
          "point_estimate": 0.0436982055043761,
          "standard_error": 0.010216099519412108
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.862160026462863,
            "upper_bound": 8.865956080822547
          },
          "point_estimate": 8.863921720677862,
          "standard_error": 0.0009723986915021644
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.861350855887327,
            "upper_bound": 8.86607993971783
          },
          "point_estimate": 8.86312789355027,
          "standard_error": 0.0011579365763280344
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002443874914995644,
            "upper_bound": 0.005415201271183984
          },
          "point_estimate": 0.0030078102918704697,
          "standard_error": 0.0011699552036552648
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.862070792609153,
            "upper_bound": 8.865816456756573
          },
          "point_estimate": 8.863767184978242,
          "standard_error": 0.0009559048484357312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014950746583879133,
            "upper_bound": 0.004383331427979224
          },
          "point_estimate": 0.0032375765131159515,
          "standard_error": 0.0008127284553931734
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.816364766151809,
            "upper_bound": 9.819465039531265
          },
          "point_estimate": 9.817764015486317,
          "standard_error": 0.000801263438734051
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.816120275541431,
            "upper_bound": 9.819405332168657
          },
          "point_estimate": 9.81703127623649,
          "standard_error": 0.0006754349983645905
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00034172600045082697,
            "upper_bound": 0.003863846039548512
          },
          "point_estimate": 0.0012790602004002208,
          "standard_error": 0.0009069886390784928
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.816554369826152,
            "upper_bound": 9.818818656232512
          },
          "point_estimate": 9.817647293062826,
          "standard_error": 0.0005690597768047855
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000751475111437472,
            "upper_bound": 0.003602870275864794
          },
          "point_estimate": 0.0026711101953525466,
          "standard_error": 0.0007460877622414274
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.19233884082329,
            "upper_bound": 19.20405537841107
          },
          "point_estimate": 19.197769150266904,
          "standard_error": 0.003001880430505658
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.189396020097664,
            "upper_bound": 19.204326592650983
          },
          "point_estimate": 19.195086138665616,
          "standard_error": 0.004051996421925146
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009148197150830324,
            "upper_bound": 0.01610357691866675
          },
          "point_estimate": 0.008460945878218109,
          "standard_error": 0.004082513692953384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.190672962693736,
            "upper_bound": 19.196590104092643
          },
          "point_estimate": 19.193358102964883,
          "standard_error": 0.001535679003663719
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004009046989546518,
            "upper_bound": 0.012646291953582182
          },
          "point_estimate": 0.00999120723982097,
          "standard_error": 0.002136368933585838
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103545.53645044193,
            "upper_bound": 103863.94234834958
          },
          "point_estimate": 103690.1567449044,
          "standard_error": 82.21485795269672
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103482.66429924245,
            "upper_bound": 103848.64204545454
          },
          "point_estimate": 103595.66616161616,
          "standard_error": 113.14433849395463
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.90495969634503,
            "upper_bound": 468.1069236496689
          },
          "point_estimate": 228.31708490015333,
          "standard_error": 104.0298591479933
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103468.3262980338,
            "upper_bound": 103686.58808290157
          },
          "point_estimate": 103552.47203364816,
          "standard_error": 56.36980953066155
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.55358106463498,
            "upper_bound": 376.0380995520377
          },
          "point_estimate": 274.5139406805663,
          "standard_error": 72.30175298598148
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57395.0301366982,
            "upper_bound": 57583.6171564143
          },
          "point_estimate": 57476.92062565719,
          "standard_error": 49.031413434932325
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57347.10804416404,
            "upper_bound": 57534.97791798107
          },
          "point_estimate": 57449.195583596214,
          "standard_error": 46.75117580904557
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.793070472753733,
            "upper_bound": 210.64331436758872
          },
          "point_estimate": 129.0345264158038,
          "standard_error": 45.22690695653394
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57396.53895899054,
            "upper_bound": 57499.36174294833
          },
          "point_estimate": 57460.64400016388,
          "standard_error": 26.008817959535808
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.473609481925585,
            "upper_bound": 236.78106993679935
          },
          "point_estimate": 163.71102073301273,
          "standard_error": 54.60042235407373
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148409.7802542922,
            "upper_bound": 148653.97923364106
          },
          "point_estimate": 148518.90401036607,
          "standard_error": 62.95723504717561
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148395.57301587303,
            "upper_bound": 148655.01751700678
          },
          "point_estimate": 148445.2320699709,
          "standard_error": 54.633817522591904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.4528115328609,
            "upper_bound": 285.6695313569515
          },
          "point_estimate": 95.20182902413352,
          "standard_error": 66.75046736945893
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148413.0351669619,
            "upper_bound": 148507.79898681826
          },
          "point_estimate": 148447.0179379804,
          "standard_error": 24.417488178380953
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.121222780543214,
            "upper_bound": 270.4343260757983
          },
          "point_estimate": 209.95115383568356,
          "standard_error": 58.0011499887274
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399280.6286380494,
            "upper_bound": 399863.49375026167
          },
          "point_estimate": 399557.988622449,
          "standard_error": 149.37848072983053
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399118.081043956,
            "upper_bound": 399875.6875981162
          },
          "point_estimate": 399491.8956043956,
          "standard_error": 223.7642083379023
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.35459662664458,
            "upper_bound": 902.9631224308024
          },
          "point_estimate": 561.6137286282657,
          "standard_error": 189.65306974240343
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399298.6708791209,
            "upper_bound": 399959.5665416936
          },
          "point_estimate": 399589.2941058941,
          "standard_error": 168.23304386333973
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 273.14890623648074,
            "upper_bound": 648.7042792841031
          },
          "point_estimate": 497.9888222457662,
          "standard_error": 102.97222113959486
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578924.0161253778,
            "upper_bound": 580016.2193934241
          },
          "point_estimate": 579431.8698885109,
          "standard_error": 280.26703494915375
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578645.7301587302,
            "upper_bound": 580000.0162037037
          },
          "point_estimate": 579353.6037414966,
          "standard_error": 326.3648757881751
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121.60618653166436,
            "upper_bound": 1486.8663708249487
          },
          "point_estimate": 865.2901140030854,
          "standard_error": 361.4009240215708
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578679.4612970103,
            "upper_bound": 579677.5741890959
          },
          "point_estimate": 579090.9254174397,
          "standard_error": 255.07134403058123
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 428.1180468815926,
            "upper_bound": 1224.7691449634133
          },
          "point_estimate": 934.18455257714,
          "standard_error": 212.91367053097463
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52699.2364327156,
            "upper_bound": 52805.328007682714
          },
          "point_estimate": 52752.46811911683,
          "standard_error": 27.355553029664875
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52668.60125968992,
            "upper_bound": 52834.45188953488
          },
          "point_estimate": 52755.0988372093,
          "standard_error": 41.062654915254406
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.75931256593981,
            "upper_bound": 154.9489367840015
          },
          "point_estimate": 122.94506972135564,
          "standard_error": 32.39914849746163
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52673.53354310571,
            "upper_bound": 52799.43490139935
          },
          "point_estimate": 52734.14517139837,
          "standard_error": 33.512714326743854
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.88748110738066,
            "upper_bound": 111.34154835681804
          },
          "point_estimate": 90.82535731936596,
          "standard_error": 14.387977696327912
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100473.52821616542,
            "upper_bound": 100699.12254924592
          },
          "point_estimate": 100573.82864474782,
          "standard_error": 58.440503105480936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100428.3562277404,
            "upper_bound": 100667.9091566636
          },
          "point_estimate": 100511.67659279778,
          "standard_error": 61.589905613611414
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.714149639182327,
            "upper_bound": 262.80748571512044
          },
          "point_estimate": 125.09093322932736,
          "standard_error": 66.76071656801457
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100453.60319436164,
            "upper_bound": 100658.757156048
          },
          "point_estimate": 100557.74430334209,
          "standard_error": 52.85641414821555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.70423818976433,
            "upper_bound": 272.5526652976864
          },
          "point_estimate": 195.71509125746087,
          "standard_error": 58.12457171780081
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71160.20075322762,
            "upper_bound": 71362.16441279197
          },
          "point_estimate": 71251.91689104063,
          "standard_error": 52.122726130655465
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71145.43511242088,
            "upper_bound": 71361.46463654224
          },
          "point_estimate": 71185.27909065394,
          "standard_error": 55.14419739048139
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3567266949691466,
            "upper_bound": 278.22174188441284
          },
          "point_estimate": 60.087700544229826,
          "standard_error": 71.42353238969379
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71149.52232480499,
            "upper_bound": 71205.19500125112
          },
          "point_estimate": 71173.5201183885,
          "standard_error": 14.1795178068936
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.14783618075987,
            "upper_bound": 227.8952822173201
          },
          "point_estimate": 173.7806793842082,
          "standard_error": 43.72254065916626
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308730.61040383705,
            "upper_bound": 309075.35002354055
          },
          "point_estimate": 308908.41209779395,
          "standard_error": 88.52687884174375
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308613.91027710517,
            "upper_bound": 309156.66949152545
          },
          "point_estimate": 308956.76327683614,
          "standard_error": 146.75159231228602
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.0242806041617,
            "upper_bound": 506.94315638128353
          },
          "point_estimate": 302.92323513056436,
          "standard_error": 125.82205609206856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308626.2311220105,
            "upper_bound": 309036.1368138383
          },
          "point_estimate": 308833.02029495925,
          "standard_error": 107.80756912581492
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.00786557761325,
            "upper_bound": 355.62226663940294
          },
          "point_estimate": 294.2980812195532,
          "standard_error": 47.464292973542015
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36571.80038742079,
            "upper_bound": 36620.1004953517
          },
          "point_estimate": 36591.40848210286,
          "standard_error": 12.886979559529644
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36564.64616935484,
            "upper_bound": 36598.9566532258
          },
          "point_estimate": 36575.841949884794,
          "standard_error": 9.49509066249474
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.208020298510812,
            "upper_bound": 39.67626341858784
          },
          "point_estimate": 19.812736704708147,
          "standard_error": 9.346025430712936
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36572.04855143636,
            "upper_bound": 36616.29975829988
          },
          "point_estimate": 36591.324489421866,
          "standard_error": 11.033650595881616
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.55027390437479,
            "upper_bound": 64.3718502609561
          },
          "point_estimate": 43.02368231028497,
          "standard_error": 17.33413484361398
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66590.08236868356,
            "upper_bound": 66700.83589631572
          },
          "point_estimate": 66645.33246869085,
          "standard_error": 28.39432131124266
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66578.34450815494,
            "upper_bound": 66716.056146789
          },
          "point_estimate": 66641.03295107035,
          "standard_error": 37.742350220878386
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.257092875073145,
            "upper_bound": 159.61127243238298
          },
          "point_estimate": 101.80782788062167,
          "standard_error": 34.664776083029686
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66590.57520370919,
            "upper_bound": 66699.96039809381
          },
          "point_estimate": 66646.52916954605,
          "standard_error": 28.249480430343056
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.0613816671431,
            "upper_bound": 118.43703736240298
          },
          "point_estimate": 94.35013974113312,
          "standard_error": 16.513638142948796
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168903.34912845385,
            "upper_bound": 169577.80247434415
          },
          "point_estimate": 169226.57378123162,
          "standard_error": 172.52112507589527
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168790.33153292182,
            "upper_bound": 169707.34336419753
          },
          "point_estimate": 169098.09907407407,
          "standard_error": 205.36592059654689
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.87944418302502,
            "upper_bound": 908.108840089277
          },
          "point_estimate": 566.9047197502191,
          "standard_error": 257.8037400622799
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168724.8893585314,
            "upper_bound": 169275.02158710224
          },
          "point_estimate": 168961.1156084656,
          "standard_error": 142.49035715685145
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 275.5558119659269,
            "upper_bound": 728.0625599345134
          },
          "point_estimate": 575.2680463535308,
          "standard_error": 112.93603825013564
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35870.21140490586,
            "upper_bound": 35917.30622980616
          },
          "point_estimate": 35894.426092425456,
          "standard_error": 12.07321986375566
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35857.8146946834,
            "upper_bound": 35925.79028189097
          },
          "point_estimate": 35905.11762092794,
          "standard_error": 21.560200571601452
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.837457240198332,
            "upper_bound": 67.80134998087941
          },
          "point_estimate": 40.0590682769614,
          "standard_error": 17.355761914411765
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35865.541169919255,
            "upper_bound": 35915.04517005734
          },
          "point_estimate": 35888.18577710542,
          "standard_error": 12.639119292278574
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.59194385205726,
            "upper_bound": 48.557615072145175
          },
          "point_estimate": 40.305635021689234,
          "standard_error": 6.158207012246284
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11584.32829746404,
            "upper_bound": 11628.848111582784
          },
          "point_estimate": 11607.85320374504,
          "standard_error": 11.392507661844622
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11583.398915816326,
            "upper_bound": 11628.264732142858
          },
          "point_estimate": 11618.607694249577,
          "standard_error": 11.695515312110976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.925084149065595,
            "upper_bound": 61.84798900614391
          },
          "point_estimate": 14.601303027606289,
          "standard_error": 17.510305519782957
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11571.371239902212,
            "upper_bound": 11628.504303035135
          },
          "point_estimate": 11601.12536525974,
          "standard_error": 14.772214041649406
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.94021791482799,
            "upper_bound": 50.2081950153773
          },
          "point_estimate": 37.923633753931675,
          "standard_error": 8.781467606404954
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-repeated-rare-huge_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 505732.49495701055,
            "upper_bound": 506784.581600529
          },
          "point_estimate": 506237.3212014991,
          "standard_error": 270.09509686191944
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 505502.2037037037,
            "upper_bound": 507297.9444444445
          },
          "point_estimate": 506016.2006448413,
          "standard_error": 418.4165394131162
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220.55459219551375,
            "upper_bound": 1442.5981451295345
          },
          "point_estimate": 872.3291474992278,
          "standard_error": 358.44643710116895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 505760.8534633309,
            "upper_bound": 506865.41243104805
          },
          "point_estimate": 506287.65876623377,
          "standard_error": 287.5215847036295
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 468.5218096189213,
            "upper_bound": 1079.1908398257003
          },
          "point_estimate": 899.1226280081871,
          "standard_error": 146.13564854355232
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-repeated-rare-small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1056.1429840748788,
            "upper_bound": 1060.3208177230536
          },
          "point_estimate": 1057.9426479962522,
          "standard_error": 1.0629987964963894
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1055.8037512962337,
            "upper_bound": 1060.5499978018113
          },
          "point_estimate": 1056.6273479097667,
          "standard_error": 0.9278234562602802
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.21822676053859685,
            "upper_bound": 5.15757961255846
          },
          "point_estimate": 0.7118715088503249,
          "standard_error": 0.980870671814337
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1055.9420284127823,
            "upper_bound": 1056.917430792652
          },
          "point_estimate": 1056.3889274475928,
          "standard_error": 0.24571859763497889
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.575128940541935,
            "upper_bound": 4.464863169041598
          },
          "point_estimate": 3.527542604884155,
          "standard_error": 1.058570703232471
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248178.62101000216,
            "upper_bound": 248712.10537587656
          },
          "point_estimate": 248373.9762706567,
          "standard_error": 154.69397652627956
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248170.5824092194,
            "upper_bound": 248299.9821917808
          },
          "point_estimate": 248202.62063356163,
          "standard_error": 50.43553091995673
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.790590136928582,
            "upper_bound": 184.3359197410639
          },
          "point_estimate": 60.365457033314385,
          "standard_error": 63.73353184187622
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248179.57105664728,
            "upper_bound": 248246.8360003783
          },
          "point_estimate": 248206.19334637964,
          "standard_error": 17.14117834373615
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.19993682803772,
            "upper_bound": 793.1155966984634
          },
          "point_estimate": 516.1125087083196,
          "standard_error": 262.70280538132477
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195043.4755340875,
            "upper_bound": 195325.7564601468
          },
          "point_estimate": 195178.90677824712,
          "standard_error": 72.204591302413
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195003.36917562725,
            "upper_bound": 195361.2962365591
          },
          "point_estimate": 195130.21087216248,
          "standard_error": 98.49928771492716
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.486545403637265,
            "upper_bound": 389.4710421177896
          },
          "point_estimate": 222.9909184425404,
          "standard_error": 87.40441023166593
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195103.2683563748,
            "upper_bound": 195404.5784528521
          },
          "point_estimate": 195258.0498813015,
          "standard_error": 76.24082713253303
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126.66593301233382,
            "upper_bound": 303.48652604402434
          },
          "point_estimate": 239.9902524236495,
          "standard_error": 45.25939398272714
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176185.50791810444,
            "upper_bound": 176578.7864469749
          },
          "point_estimate": 176367.23408519285,
          "standard_error": 100.72292705604708
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176116.67632850242,
            "upper_bound": 176482.1082930757
          },
          "point_estimate": 176343.02667740203,
          "standard_error": 80.61279494585172
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.64000182812984,
            "upper_bound": 501.86635811907826
          },
          "point_estimate": 254.3643773681737,
          "standard_error": 123.82678409835432
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176290.78416093896,
            "upper_bound": 176554.2442976824
          },
          "point_estimate": 176426.75668486103,
          "standard_error": 67.36397040404559
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.608698988686,
            "upper_bound": 471.21793901990856
          },
          "point_estimate": 335.8322565549949,
          "standard_error": 95.39094674365658
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/libc_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37365.490866027496,
            "upper_bound": 37463.85438933536
          },
          "point_estimate": 37412.41929548167,
          "standard_error": 25.359282215353502
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37338.430484037075,
            "upper_bound": 37498.12481977343
          },
          "point_estimate": 37392.770608765306,
          "standard_error": 41.43247566606358
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.4787193814146,
            "upper_bound": 137.71555854887654
          },
          "point_estimate": 87.09996613990101,
          "standard_error": 36.43551350783903
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37371.38515540433,
            "upper_bound": 37440.369684712074
          },
          "point_estimate": 37400.10993352682,
          "standard_error": 17.6072623643749
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.04522041715992,
            "upper_bound": 102.00815616924082
          },
          "point_estimate": 84.53326631226074,
          "standard_error": 14.263171022258945
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/libc_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72469.66900667567,
            "upper_bound": 72612.22336039771
          },
          "point_estimate": 72532.11854542149,
          "standard_error": 36.92043183456816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72454.1372545532,
            "upper_bound": 72571.40861553786
          },
          "point_estimate": 72493.09023904383,
          "standard_error": 28.97931003546297
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.806444490076135,
            "upper_bound": 160.8718011280317
          },
          "point_estimate": 64.12401462518578,
          "standard_error": 45.168914910363426
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72468.27134938508,
            "upper_bound": 72543.21309291836
          },
          "point_estimate": 72497.16282402856,
          "standard_error": 18.95707659544651
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.33994692604976,
            "upper_bound": 173.07177419950867
          },
          "point_estimate": 122.8019313579808,
          "standard_error": 38.25108188662596
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69359.43057766874,
            "upper_bound": 69504.57556300361
          },
          "point_estimate": 69421.68186856294,
          "standard_error": 37.96718187065974
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69340.99141221374,
            "upper_bound": 69472.73473282444
          },
          "point_estimate": 69379.67814885496,
          "standard_error": 31.56479170869613
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.777853919688596,
            "upper_bound": 149.70163856364232
          },
          "point_estimate": 67.86646964118894,
          "standard_error": 35.622862120315034
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69343.70116005247,
            "upper_bound": 69434.93968618939
          },
          "point_estimate": 69383.92429860216,
          "standard_error": 23.200868058912764
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.839729577415014,
            "upper_bound": 181.77904662398143
          },
          "point_estimate": 126.45841612422709,
          "standard_error": 43.05987089652338
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/libc_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82669.55989857901,
            "upper_bound": 82759.47115118911
          },
          "point_estimate": 82712.40172659724,
          "standard_error": 23.090878770792234
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82645.88211845103,
            "upper_bound": 82779.6780561883
          },
          "point_estimate": 82698.49316628702,
          "standard_error": 32.819283850951905
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.606212409526208,
            "upper_bound": 134.90836562880705
          },
          "point_estimate": 79.60332791477327,
          "standard_error": 29.150907363814856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82671.51252509913,
            "upper_bound": 82775.5062726265
          },
          "point_estimate": 82717.69921604592,
          "standard_error": 26.965468121963724
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.633400276118806,
            "upper_bound": 95.47984279218046
          },
          "point_estimate": 77.02926949048566,
          "standard_error": 13.91113074992992
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/libc_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322784.3680956331,
            "upper_bound": 323034.8413054941
          },
          "point_estimate": 322903.4531468606,
          "standard_error": 64.24062862780889
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322730.3125,
            "upper_bound": 323084.26843657816
          },
          "point_estimate": 322858.76754108723,
          "standard_error": 80.42610526207272
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.30658384549395,
            "upper_bound": 366.2943483642462
          },
          "point_estimate": 194.43905044182816,
          "standard_error": 83.3988557641594
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322791.4365353525,
            "upper_bound": 322961.95372428565
          },
          "point_estimate": 322890.2002528445,
          "standard_error": 43.82616942336033
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.42391521685552,
            "upper_bound": 266.5671331744395
          },
          "point_estimate": 214.06205949060833,
          "standard_error": 41.55559041245774
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47890.268868064726,
            "upper_bound": 47976.18217588223
          },
          "point_estimate": 47927.78884003271,
          "standard_error": 22.23228541760259
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47872.778566710695,
            "upper_bound": 47968.17474995282
          },
          "point_estimate": 47911.715763980625,
          "standard_error": 24.015170616465873
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.5029407035720155,
            "upper_bound": 103.56638604112096
          },
          "point_estimate": 58.777075343551445,
          "standard_error": 24.68328768299336
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47884.33238254001,
            "upper_bound": 47933.96758383955
          },
          "point_estimate": 47910.65932165589,
          "standard_error": 12.46270576197788
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.734826576858207,
            "upper_bound": 105.69382220190693
          },
          "point_estimate": 74.19495393583038,
          "standard_error": 23.282240671375455
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34114.561802832235,
            "upper_bound": 34213.240260705206
          },
          "point_estimate": 34161.14959098455,
          "standard_error": 25.283190968593757
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34095.14845938375,
            "upper_bound": 34216.94146176989
          },
          "point_estimate": 34153.13641845627,
          "standard_error": 24.93524966421256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6946242985701807,
            "upper_bound": 139.131909730351
          },
          "point_estimate": 67.68539546501277,
          "standard_error": 36.36039500935825
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34148.64885843341,
            "upper_bound": 34241.16750845344
          },
          "point_estimate": 34194.58478664193,
          "standard_error": 23.51603435318444
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.66475241131452,
            "upper_bound": 108.45638505200375
          },
          "point_estimate": 83.89960755142481,
          "standard_error": 18.316482805284092
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37231.37107342386,
            "upper_bound": 37299.4648192048
          },
          "point_estimate": 37263.541521055704,
          "standard_error": 17.483210319242765
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37217.86011600136,
            "upper_bound": 37318.626791197545
          },
          "point_estimate": 37243.02541794609,
          "standard_error": 25.741472226793796
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.211021620153868,
            "upper_bound": 102.36863423683982
          },
          "point_estimate": 49.2894306755992,
          "standard_error": 24.21670643069016
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37217.62753716843,
            "upper_bound": 37279.8284215718
          },
          "point_estimate": 37241.248125058155,
          "standard_error": 16.035816680991953
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.46196682195408,
            "upper_bound": 71.07274402175682
          },
          "point_estimate": 58.23507064140578,
          "standard_error": 10.975643450665707
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83673.98456605223,
            "upper_bound": 83779.5084479921
          },
          "point_estimate": 83723.040785696,
          "standard_error": 27.147502032523608
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83652.30440348182,
            "upper_bound": 83803.33986175116
          },
          "point_estimate": 83686.28602150538,
          "standard_error": 39.11356314071926
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.704271087352664,
            "upper_bound": 151.36464858001483
          },
          "point_estimate": 69.13497760325659,
          "standard_error": 35.84225556757222
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83659.4329788467,
            "upper_bound": 83737.58126051415
          },
          "point_estimate": 83685.65461128733,
          "standard_error": 20.776162864649763
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.951678647740856,
            "upper_bound": 116.7584545527822
          },
          "point_estimate": 90.47387795347352,
          "standard_error": 19.682881073285245
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65691.87402911008,
            "upper_bound": 65863.06482168644
          },
          "point_estimate": 65761.93097616182,
          "standard_error": 45.466599592614656
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65678.46750902527,
            "upper_bound": 65797.43276173285
          },
          "point_estimate": 65721.23094378546,
          "standard_error": 26.590190807200003
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.614591851577476,
            "upper_bound": 133.45315138523583
          },
          "point_estimate": 50.96756638756017,
          "standard_error": 34.360211004560476
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65689.96389500992,
            "upper_bound": 65795.61743021305
          },
          "point_estimate": 65725.60198321534,
          "standard_error": 27.691428192854634
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.251800483689284,
            "upper_bound": 224.30960898360345
          },
          "point_estimate": 151.5294028677243,
          "standard_error": 59.380472613958034
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151030.46429282753,
            "upper_bound": 151175.73890535466
          },
          "point_estimate": 151097.3749772772,
          "standard_error": 37.290890800114035
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151006.81604426005,
            "upper_bound": 151184.48962655602
          },
          "point_estimate": 151060.0880260818,
          "standard_error": 43.616118051796875
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.908039278905372,
            "upper_bound": 196.43878406437696
          },
          "point_estimate": 87.80743018382708,
          "standard_error": 48.9583812862038
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151018.9217641125,
            "upper_bound": 151156.34026773812
          },
          "point_estimate": 151079.06232688474,
          "standard_error": 34.86709803020104
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.985703477107464,
            "upper_bound": 157.2550656868826
          },
          "point_estimate": 123.76180874635132,
          "standard_error": 27.052926977678496
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198768.5034946656,
            "upper_bound": 199056.2721572014
          },
          "point_estimate": 198913.3607318501,
          "standard_error": 73.4077595963087
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198765.37148711944,
            "upper_bound": 199014.4637978142
          },
          "point_estimate": 198953.00564663025,
          "standard_error": 65.24822851740618
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.78819271156669,
            "upper_bound": 428.07603240013697
          },
          "point_estimate": 103.10270946736584,
          "standard_error": 110.72148570378192
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198814.82507257513,
            "upper_bound": 199221.32536499848
          },
          "point_estimate": 199030.3495564545,
          "standard_error": 106.90353424025854
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.92472322009476,
            "upper_bound": 333.0458563986251
          },
          "point_estimate": 244.8529529297702,
          "standard_error": 58.097172041242665
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121959.5192207308,
            "upper_bound": 122309.82660431314
          },
          "point_estimate": 122111.6566056781,
          "standard_error": 90.69297645921152
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121894.46756152125,
            "upper_bound": 122247.37325023967
          },
          "point_estimate": 122028.25835197614,
          "standard_error": 81.52974612163342
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.843613086521195,
            "upper_bound": 375.937826715024
          },
          "point_estimate": 188.6886246702414,
          "standard_error": 92.08295276195176
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121937.82217478784,
            "upper_bound": 122113.31804746878
          },
          "point_estimate": 122018.80034864464,
          "standard_error": 43.88711378195383
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.16410066973305,
            "upper_bound": 425.6511961957952
          },
          "point_estimate": 302.12821638902693,
          "standard_error": 95.7023560946816
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52632.236370025304,
            "upper_bound": 52709.1173695652
          },
          "point_estimate": 52671.92845537151,
          "standard_error": 19.716377500160252
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52632.15640096618,
            "upper_bound": 52726.93252242926
          },
          "point_estimate": 52671.71503623188,
          "standard_error": 21.27388080954422
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.394109252783576,
            "upper_bound": 114.0975276845084
          },
          "point_estimate": 52.76532719849584,
          "standard_error": 27.246047207600235
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52648.02286218615,
            "upper_bound": 52702.13941533507
          },
          "point_estimate": 52673.06605684171,
          "standard_error": 13.543124195560154
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.577201238098496,
            "upper_bound": 86.15655649834255
          },
          "point_estimate": 66.00386654246084,
          "standard_error": 13.786833592808028
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77368.72436879434,
            "upper_bound": 77505.86051569149
          },
          "point_estimate": 77431.33977541371,
          "standard_error": 35.34622867600975
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77338.85418439716,
            "upper_bound": 77507.82638297872
          },
          "point_estimate": 77388.87579787234,
          "standard_error": 47.58119556548031
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.140722089312703,
            "upper_bound": 198.0607407815751
          },
          "point_estimate": 97.06245551084422,
          "standard_error": 45.66102023032073
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77347.78475851468,
            "upper_bound": 77494.72179513106
          },
          "point_estimate": 77435.19703232938,
          "standard_error": 37.666639727973816
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.96719077621102,
            "upper_bound": 158.44418219186144
          },
          "point_estimate": 118.02500995565951,
          "standard_error": 29.149670796180295
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40963.08195129046,
            "upper_bound": 41049.45421331267
          },
          "point_estimate": 41006.21763636119,
          "standard_error": 22.103931954088647
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40939.43628872068,
            "upper_bound": 41075.005411499435
          },
          "point_estimate": 41003.85787767757,
          "standard_error": 30.670015452415768
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.613861465475864,
            "upper_bound": 135.93426497288604
          },
          "point_estimate": 65.41185396835868,
          "standard_error": 29.256945284261818
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40956.27781111771,
            "upper_bound": 41072.91993092583
          },
          "point_estimate": 41014.91060484048,
          "standard_error": 31.1909779025336
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.20145990942059,
            "upper_bound": 90.87681037967324
          },
          "point_estimate": 73.48877899280004,
          "standard_error": 12.23850880793858
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93448.14008252282,
            "upper_bound": 93636.76874171576
          },
          "point_estimate": 93539.4306620234,
          "standard_error": 48.24893406046842
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93400.30956471935,
            "upper_bound": 93676.14304123713
          },
          "point_estimate": 93502.91169458762,
          "standard_error": 76.8544712438904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.96473155393066,
            "upper_bound": 268.4375260332539
          },
          "point_estimate": 167.2464636738024,
          "standard_error": 61.44236919024677
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93432.15520958343,
            "upper_bound": 93596.54071270367
          },
          "point_estimate": 93498.9424086223,
          "standard_error": 42.26051704301512
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.23482874729444,
            "upper_bound": 195.0998037656529
          },
          "point_estimate": 161.1868260884145,
          "standard_error": 26.379806221053236
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1319123.7682397957,
            "upper_bound": 1324227.909922194
          },
          "point_estimate": 1321551.6154421768,
          "standard_error": 1308.7796017643418
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1317577.7285714285,
            "upper_bound": 1324941.4166666667
          },
          "point_estimate": 1321397.1879251702,
          "standard_error": 1888.830237900018
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 728.755249562041,
            "upper_bound": 7527.261553864213
          },
          "point_estimate": 5052.923541542683,
          "standard_error": 1693.394342793476
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1318155.5359883085,
            "upper_bound": 1321223.2662573503
          },
          "point_estimate": 1319514.545083488,
          "standard_error": 793.9889798380133
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2334.49671095742,
            "upper_bound": 5645.080085211984
          },
          "point_estimate": 4368.065426847607,
          "standard_error": 885.8836701112378
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156683.58886655892,
            "upper_bound": 157282.98271426006
          },
          "point_estimate": 156945.7194360632,
          "standard_error": 155.8971179893542
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156577.60964439655,
            "upper_bound": 157171.74425287358
          },
          "point_estimate": 156725.8880387931,
          "standard_error": 194.96760304524557
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.680026807015953,
            "upper_bound": 764.1554050757616
          },
          "point_estimate": 254.7325629991538,
          "standard_error": 204.6692830303516
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156634.3225824847,
            "upper_bound": 157079.42982281902
          },
          "point_estimate": 156848.40821764444,
          "standard_error": 114.90210894211978
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198.9502015406554,
            "upper_bound": 740.3864931269904
          },
          "point_estimate": 520.2782538015517,
          "standard_error": 160.4897670816649
        }
      }
    },
    "memmem/libc/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/libc_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6717.9838006384525,
            "upper_bound": 6747.470389764184
          },
          "point_estimate": 6731.427607764391,
          "standard_error": 7.617660348194487
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6709.486839666358,
            "upper_bound": 6747.0975826382455
          },
          "point_estimate": 6725.67421686747,
          "standard_error": 7.540980520453344
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9260089872867434,
            "upper_bound": 38.55724056663053
          },
          "point_estimate": 19.149860345841866,
          "standard_error": 9.573004493451888
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6712.245857412877,
            "upper_bound": 6730.675370795713
          },
          "point_estimate": 6720.957508515581,
          "standard_error": 4.753011636777161
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.481244212303205,
            "upper_bound": 34.113048158767945
          },
          "point_estimate": 25.40925299025416,
          "standard_error": 6.684004874711032
        }
      }
    },
    "memmem/libc/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/libc_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7378.061482783418,
            "upper_bound": 7393.400294077835
          },
          "point_estimate": 7385.349294077833,
          "standard_error": 3.9431616523835897
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7374.9439086294415,
            "upper_bound": 7395.028928934011
          },
          "point_estimate": 7381.265549915397,
          "standard_error": 6.800761733023426
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2313207167057135,
            "upper_bound": 22.883797636372385
          },
          "point_estimate": 11.352614189313677,
          "standard_error": 6.604553468592414
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7380.130194652064,
            "upper_bound": 7393.845332230651
          },
          "point_estimate": 7387.864163491331,
          "standard_error": 3.4869988867813904
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.271735018722672,
            "upper_bound": 16.280938460618245
          },
          "point_estimate": 13.123889668238322,
          "standard_error": 2.350024181155588
        }
      }
    },
    "memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/libc_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47526.7636535163,
            "upper_bound": 47620.39459565936
          },
          "point_estimate": 47570.76449114814,
          "standard_error": 23.967767123462377
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47512.160419397114,
            "upper_bound": 47607.51939399613
          },
          "point_estimate": 47571.83099606815,
          "standard_error": 23.24998328050985
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.72418318170501,
            "upper_bound": 125.78481162008224
          },
          "point_estimate": 63.82944267414189,
          "standard_error": 29.69680719076524
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47549.78246286051,
            "upper_bound": 47592.73747209882
          },
          "point_estimate": 47573.73871763885,
          "standard_error": 10.81450319311041
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.1500523016417,
            "upper_bound": 109.20646692726434
          },
          "point_estimate": 79.82689192938614,
          "standard_error": 20.2442554338443
        }
      }
    },
    "memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/libc_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.81256833345896,
            "upper_bound": 116.26792025874518
          },
          "point_estimate": 116.05344656889208,
          "standard_error": 0.11570807598398627
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.52317359811964,
            "upper_bound": 116.3146410469018
          },
          "point_estimate": 116.2577020585607,
          "standard_error": 0.1908828396442064
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011068396335372448,
            "upper_bound": 0.5761172088021542
          },
          "point_estimate": 0.12348148732663454,
          "standard_error": 0.15074778189303237
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.06135895644536,
            "upper_bound": 116.30029396330173
          },
          "point_estimate": 116.22896872430906,
          "standard_error": 0.0629240408896443
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07815063939171372,
            "upper_bound": 0.43565320259380313
          },
          "point_estimate": 0.38492450494231495,
          "standard_error": 0.07210130660318717
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.924425489555535,
            "upper_bound": 21.035591326642088
          },
          "point_estimate": 20.983945218751497,
          "standard_error": 0.028706798485273657
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.87256422739805,
            "upper_bound": 21.052132273940217
          },
          "point_estimate": 21.03721400219205,
          "standard_error": 0.04615662884704863
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004737863238118523,
            "upper_bound": 0.1493975904927169
          },
          "point_estimate": 0.04720878274111747,
          "standard_error": 0.039176871770901546
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.000276375997345,
            "upper_bound": 21.051378055695597
          },
          "point_estimate": 21.034861461076908,
          "standard_error": 0.01329625633009961
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.031949229610796624,
            "upper_bound": 0.1108586135059254
          },
          "point_estimate": 0.0955618669172636,
          "standard_error": 0.017219065506727405
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.632443525189498,
            "upper_bound": 20.73327047041986
          },
          "point_estimate": 20.682980913677373,
          "standard_error": 0.02578539222645927
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.614044071758897,
            "upper_bound": 20.758865263489337
          },
          "point_estimate": 20.66597821650823,
          "standard_error": 0.038863994198927033
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008333353251814824,
            "upper_bound": 0.14813735009984505
          },
          "point_estimate": 0.10097223733725195,
          "standard_error": 0.034123832733197676
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.5832094442419,
            "upper_bound": 20.695457865666643
          },
          "point_estimate": 20.62501726561774,
          "standard_error": 0.02876499140483056
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05140732057203938,
            "upper_bound": 0.10739609465866112
          },
          "point_estimate": 0.08584474552191321,
          "standard_error": 0.014485890846074356
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.94195048633425,
            "upper_bound": 19.961376112762352
          },
          "point_estimate": 19.949889017325496,
          "standard_error": 0.005155928303827768
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.941253804729143,
            "upper_bound": 19.952298662385367
          },
          "point_estimate": 19.9453502997422,
          "standard_error": 0.0027596771496013247
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008761715471589549,
            "upper_bound": 0.01554207857550924
          },
          "point_estimate": 0.007365898177735662,
          "standard_error": 0.004157035137717659
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.94203376720852,
            "upper_bound": 19.95102843006052
          },
          "point_estimate": 19.9469029308256,
          "standard_error": 0.0023228727499521244
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004074247769434979,
            "upper_bound": 0.025756345625042466
          },
          "point_estimate": 0.017195009483884652,
          "standard_error": 0.0069048613754221074
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.412545995367182,
            "upper_bound": 17.82129007796574
          },
          "point_estimate": 17.608282175849435,
          "standard_error": 0.10481276675012428
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.362078429088104,
            "upper_bound": 17.810523667429653
          },
          "point_estimate": 17.555928608518446,
          "standard_error": 0.09474640578330804
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01806304985955168,
            "upper_bound": 0.5742133786839092
          },
          "point_estimate": 0.252915110253882,
          "standard_error": 0.17079214163336964
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.300998537906338,
            "upper_bound": 17.8381320643082
          },
          "point_estimate": 17.514854712234417,
          "standard_error": 0.13755154304645115
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.16225392440738814,
            "upper_bound": 0.46672179142155695
          },
          "point_estimate": 0.34987465471119755,
          "standard_error": 0.08045314235470716
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.390006320584984,
            "upper_bound": 25.408367499187968
          },
          "point_estimate": 25.39785219616017,
          "standard_error": 0.004771281721333486
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.39041768596765,
            "upper_bound": 25.399140818457735
          },
          "point_estimate": 25.396191823339237,
          "standard_error": 0.00231897447343734
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012789747816209643,
            "upper_bound": 0.016290472767063482
          },
          "point_estimate": 0.005094919027732961,
          "standard_error": 0.0039977418548249695
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.38824802817657,
            "upper_bound": 25.39702372502792
          },
          "point_estimate": 25.393443048696525,
          "standard_error": 0.002237358193306258
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0031515003240217138,
            "upper_bound": 0.02358524345248456
          },
          "point_estimate": 0.015925099766395473,
          "standard_error": 0.006036199933680101
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.923744857020004,
            "upper_bound": 27.975357658233825
          },
          "point_estimate": 27.952902707424364,
          "standard_error": 0.0131375225781642
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.92097029141567,
            "upper_bound": 27.97759951181608
          },
          "point_estimate": 27.96753436298527,
          "standard_error": 0.011241183430885338
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0026594254541003545,
            "upper_bound": 0.06518286992030656
          },
          "point_estimate": 0.009353234145812055,
          "standard_error": 0.012938825676826485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.96281209220095,
            "upper_bound": 27.978004837122253
          },
          "point_estimate": 27.97002929953629,
          "standard_error": 0.0038848162492505743
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008036798540874315,
            "upper_bound": 0.05540621603609872
          },
          "point_estimate": 0.0436956296881064,
          "standard_error": 0.012658603474152406
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.915380487164168,
            "upper_bound": 27.01163631114381
          },
          "point_estimate": 26.960167828777287,
          "standard_error": 0.024942720959676264
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.8977803918715,
            "upper_bound": 27.065920324341292
          },
          "point_estimate": 26.91489675046668,
          "standard_error": 0.042217285805804416
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006893932471363251,
            "upper_bound": 0.12473059037404098
          },
          "point_estimate": 0.03780165958888778,
          "standard_error": 0.032745384308224684
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.89669738304976,
            "upper_bound": 26.955493864480665
          },
          "point_estimate": 26.914036955732374,
          "standard_error": 0.015474938534562784
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028616480908662294,
            "upper_bound": 0.09526656085346658
          },
          "point_estimate": 0.08313627127358823,
          "standard_error": 0.014525169134013955
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.111349191314513,
            "upper_bound": 30.237440999637915
          },
          "point_estimate": 30.174585520223047,
          "standard_error": 0.03235929316571319
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.0662352861523,
            "upper_bound": 30.264318698491785
          },
          "point_estimate": 30.183242380026613,
          "standard_error": 0.05237111421078125
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02983317534013117,
            "upper_bound": 0.1806631012192807
          },
          "point_estimate": 0.14264805748838175,
          "standard_error": 0.03877757975467611
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.07431195142486,
            "upper_bound": 30.20868174846809
          },
          "point_estimate": 30.13495355428642,
          "standard_error": 0.03460009868148136
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0663064614451672,
            "upper_bound": 0.12801856157852243
          },
          "point_estimate": 0.1072662902785507,
          "standard_error": 0.015715539935671367
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.68532006396674,
            "upper_bound": 39.72108117835447
          },
          "point_estimate": 39.703741081852186,
          "standard_error": 0.009156464719322317
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.686602972978875,
            "upper_bound": 39.727723956399224
          },
          "point_estimate": 39.7032504532646,
          "standard_error": 0.010781719089910564
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006595090732433215,
            "upper_bound": 0.05296616797717685
          },
          "point_estimate": 0.02552431924584754,
          "standard_error": 0.01138801381180106
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.69184381663936,
            "upper_bound": 39.72122335757099
          },
          "point_estimate": 39.70590981289839,
          "standard_error": 0.007723074610585293
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01515351309584466,
            "upper_bound": 0.04098183450462591
          },
          "point_estimate": 0.030538036603935968,
          "standard_error": 0.006861192699628336
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.787425373649327,
            "upper_bound": 20.851407182851982
          },
          "point_estimate": 20.81813616091017,
          "standard_error": 0.016383567243220806
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.768107414638624,
            "upper_bound": 20.85421868690437
          },
          "point_estimate": 20.81731048741395,
          "standard_error": 0.021868868548921
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01038901856380946,
            "upper_bound": 0.09197036017099984
          },
          "point_estimate": 0.0638342849973129,
          "standard_error": 0.021413690562805315
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.76908834303333,
            "upper_bound": 20.81654549094862
          },
          "point_estimate": 20.790106373224503,
          "standard_error": 0.012241609396327612
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02948682079338426,
            "upper_bound": 0.06960362778658845
          },
          "point_estimate": 0.05459862736495999,
          "standard_error": 0.01048979303134778
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.17997563634589,
            "upper_bound": 25.20166979198805
          },
          "point_estimate": 25.190195740540368,
          "standard_error": 0.005549354465422032
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.17618802275704,
            "upper_bound": 25.198041059069283
          },
          "point_estimate": 25.190078039642337,
          "standard_error": 0.005013912958872872
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0029357754126905795,
            "upper_bound": 0.030917171539084343
          },
          "point_estimate": 0.012990542218225586,
          "standard_error": 0.0071878037831829104
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.180889982315247,
            "upper_bound": 25.19545875244225
          },
          "point_estimate": 25.18996445618869,
          "standard_error": 0.003725119086021968
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007488465853353061,
            "upper_bound": 0.02547424675586443
          },
          "point_estimate": 0.01852051159711843,
          "standard_error": 0.004856700612414567
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.35017912016605,
            "upper_bound": 32.37375935222092
          },
          "point_estimate": 32.36147789832616,
          "standard_error": 0.006049984743322166
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.34228879247394,
            "upper_bound": 32.38158481785054
          },
          "point_estimate": 32.35323492878117,
          "standard_error": 0.009991497273364984
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004547123678356576,
            "upper_bound": 0.032526804212975644
          },
          "point_estimate": 0.016680417071823595,
          "standard_error": 0.008604035576918573
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.34517216996469,
            "upper_bound": 32.357680730124436
          },
          "point_estimate": 32.34996034008986,
          "standard_error": 0.003234933454292103
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010298400610424838,
            "upper_bound": 0.0241223842955014
          },
          "point_estimate": 0.020162469379074725,
          "standard_error": 0.003391955038576268
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 894741.0929085851,
            "upper_bound": 895646.1906213704
          },
          "point_estimate": 895073.4095973674,
          "standard_error": 262.2112448065713
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 894701.2195121951,
            "upper_bound": 894995.3623693379
          },
          "point_estimate": 894778.2103658536,
          "standard_error": 97.61459823475036
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.961036276293627,
            "upper_bound": 332.3448017826556
          },
          "point_estimate": 162.4637269937081,
          "standard_error": 101.03419178495326
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 894729.4163444007,
            "upper_bound": 894906.5473476736
          },
          "point_estimate": 894796.1002850807,
          "standard_error": 45.286697626406536
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.34411990066396,
            "upper_bound": 1343.9259772069538
          },
          "point_estimate": 874.2629279422354,
          "standard_error": 443.51374529633665
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166918.2005504587,
            "upper_bound": 167163.26142447576
          },
          "point_estimate": 167022.65394641037,
          "standard_error": 63.973202229600474
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166867.74235474007,
            "upper_bound": 167077.9374180865
          },
          "point_estimate": 166972.59999999998,
          "standard_error": 56.153551739215395
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.678785505499356,
            "upper_bound": 250.01698461325296
          },
          "point_estimate": 150.5946389319002,
          "standard_error": 52.7753631433996
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166890.1074465948,
            "upper_bound": 167051.86752517268
          },
          "point_estimate": 166975.1324198737,
          "standard_error": 41.97199351141153
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.70378282919535,
            "upper_bound": 312.25046329636325
          },
          "point_estimate": 213.11734009346057,
          "standard_error": 75.2022222927497
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 702994.1804515797,
            "upper_bound": 704431.0065064102
          },
          "point_estimate": 703661.9173740842,
          "standard_error": 368.3743580928452
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 702659.8221153846,
            "upper_bound": 704384.848076923
          },
          "point_estimate": 703440.4266826923,
          "standard_error": 403.33597303438495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.22984313740903,
            "upper_bound": 1942.0132580705445
          },
          "point_estimate": 1257.5510221451157,
          "standard_error": 439.2297398773758
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 702869.6640934305,
            "upper_bound": 704071.450657186
          },
          "point_estimate": 703523.667982018,
          "standard_error": 307.7440915192121
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 547.4181850962298,
            "upper_bound": 1651.0847626256907
          },
          "point_estimate": 1227.874080547159,
          "standard_error": 301.243107925123
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355474.7218508244,
            "upper_bound": 356213.9893099861
          },
          "point_estimate": 355789.9020068578,
          "standard_error": 193.8961199811808
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355401.52773925103,
            "upper_bound": 356000.83155339805
          },
          "point_estimate": 355541.431553398,
          "standard_error": 136.5927681889721
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.874679886214013,
            "upper_bound": 729.0613399692145
          },
          "point_estimate": 345.44435444967354,
          "standard_error": 196.75787474655624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355394.1856124835,
            "upper_bound": 355759.85158985096
          },
          "point_estimate": 355607.760484176,
          "standard_error": 94.90151939245584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.33160648377876,
            "upper_bound": 940.9480426169548
          },
          "point_estimate": 647.980373713883,
          "standard_error": 229.32893525926676
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 538233.1266501225,
            "upper_bound": 539562.1766666665
          },
          "point_estimate": 538796.6295343137,
          "standard_error": 346.430082523949
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 538060.7105392157,
            "upper_bound": 539002.4338235294
          },
          "point_estimate": 538651.455882353,
          "standard_error": 294.82372321186887
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.21022497554097,
            "upper_bound": 1307.6691656077571
          },
          "point_estimate": 589.5882205621339,
          "standard_error": 303.1871838879313
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 538292.8681007747,
            "upper_bound": 538841.4681067768
          },
          "point_estimate": 538580.049236058,
          "standard_error": 141.48765186805295
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 367.52679594027944,
            "upper_bound": 1704.9867148225878
          },
          "point_estimate": 1156.6235736933183,
          "standard_error": 421.7343131008101
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233162.58761044336,
            "upper_bound": 233772.0774505431
          },
          "point_estimate": 233462.6632643976,
          "standard_error": 156.92885234819553
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232997.36132478632,
            "upper_bound": 234068.0360576923
          },
          "point_estimate": 233360.88190883189,
          "standard_error": 302.2523909851611
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.96420204269782,
            "upper_bound": 834.828721156388
          },
          "point_estimate": 625.9407202975804,
          "standard_error": 212.083258902924
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233032.01367312425,
            "upper_bound": 233674.6828898983
          },
          "point_estimate": 233345.52683982684,
          "standard_error": 161.83893175344917
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 342.14243281902964,
            "upper_bound": 603.1005643920162
          },
          "point_estimate": 523.1254048579274,
          "standard_error": 67.25620569194
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400700.765281975,
            "upper_bound": 401399.4672401012
          },
          "point_estimate": 401034.44467774295,
          "standard_error": 179.38936035611732
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400619.032967033,
            "upper_bound": 401567.8021978022
          },
          "point_estimate": 400823.93131868134,
          "standard_error": 267.74995544941277
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.88632967056672,
            "upper_bound": 991.228055479068
          },
          "point_estimate": 450.642167961008,
          "standard_error": 252.52770665575065
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400802.78673006967,
            "upper_bound": 401755.2293913854
          },
          "point_estimate": 401273.9035535893,
          "standard_error": 249.92733357116933
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 315.7341401179166,
            "upper_bound": 748.6839586925184
          },
          "point_estimate": 598.9672460052283,
          "standard_error": 110.055599593784
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389406.9938635597,
            "upper_bound": 391368.3563356974
          },
          "point_estimate": 390342.0995550489,
          "standard_error": 503.17627694080625
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388881.0460992908,
            "upper_bound": 391410.345212766
          },
          "point_estimate": 390179.60477879096,
          "standard_error": 830.3180344515376
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 212.04684588084683,
            "upper_bound": 3165.550069598119
          },
          "point_estimate": 1769.7249111343165,
          "standard_error": 742.0686035681259
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389542.3926671732,
            "upper_bound": 391364.9421903336
          },
          "point_estimate": 390573.4928433269,
          "standard_error": 465.67963369818233
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 974.943304108759,
            "upper_bound": 2152.5282187832663
          },
          "point_estimate": 1674.4854483803017,
          "standard_error": 325.7212160210407
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292496.60329953174,
            "upper_bound": 292976.47068698413
          },
          "point_estimate": 292737.9644269842,
          "standard_error": 122.29840647388572
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292391.8187,
            "upper_bound": 293040.112
          },
          "point_estimate": 292729.2625777778,
          "standard_error": 148.5107236962731
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.57641872888269,
            "upper_bound": 769.4848053789559
          },
          "point_estimate": 348.15598661900844,
          "standard_error": 183.13887621317164
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292350.17252183903,
            "upper_bound": 292783.74555181095
          },
          "point_estimate": 292585.9409454545,
          "standard_error": 109.87063276110388
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228.57045598299712,
            "upper_bound": 515.4626458896416
          },
          "point_estimate": 409.6164598522989,
          "standard_error": 74.24915268171243
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297248.5539789973,
            "upper_bound": 298037.6584974594
          },
          "point_estimate": 297619.3864082462,
          "standard_error": 202.71754269860855
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297095.10518292687,
            "upper_bound": 298160.8988095238
          },
          "point_estimate": 297323.3249322493,
          "standard_error": 300.1522941126103
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.549595492619154,
            "upper_bound": 1170.6879595006153
          },
          "point_estimate": 605.0317723479862,
          "standard_error": 280.1623597862797
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297079.7962641961,
            "upper_bound": 297757.00335419487
          },
          "point_estimate": 297340.9056277056,
          "standard_error": 173.62352916293753
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 338.11096944719463,
            "upper_bound": 865.5972616917965
          },
          "point_estimate": 675.665661849348,
          "standard_error": 139.3188715421754
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323860.61924526794,
            "upper_bound": 324683.30190195254
          },
          "point_estimate": 324248.4306429976,
          "standard_error": 210.7604899710435
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323789.2908554572,
            "upper_bound": 324781.2376738306
          },
          "point_estimate": 324051.1101892822,
          "standard_error": 251.890819085516
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148.12042960485212,
            "upper_bound": 1093.8824331239896
          },
          "point_estimate": 478.3961060126894,
          "standard_error": 283.2587801581274
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323843.0790848262,
            "upper_bound": 324400.4186253904
          },
          "point_estimate": 324061.41455005173,
          "standard_error": 142.7903388551441
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 313.21529953755424,
            "upper_bound": 897.676044338497
          },
          "point_estimate": 702.3829476749272,
          "standard_error": 145.33922579760275
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153042.36824699878,
            "upper_bound": 153303.00196078428
          },
          "point_estimate": 153170.71132519675,
          "standard_error": 66.56518426405492
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152988.85037348274,
            "upper_bound": 153311.22328931573
          },
          "point_estimate": 153162.23844537814,
          "standard_error": 75.64975019549311
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.363780549430075,
            "upper_bound": 376.984370660086
          },
          "point_estimate": 181.68839967355547,
          "standard_error": 90.50275527323926
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153069.68957351727,
            "upper_bound": 153225.9605332226
          },
          "point_estimate": 153141.8260395067,
          "standard_error": 39.875812922235866
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.68875659965308,
            "upper_bound": 292.5700287744309
          },
          "point_estimate": 222.15478670629585,
          "standard_error": 46.62829215278721
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223380.008250073,
            "upper_bound": 224054.5880651475
          },
          "point_estimate": 223707.5015834064,
          "standard_error": 172.84637811079455
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223167.1209465381,
            "upper_bound": 224432.6789366053
          },
          "point_estimate": 223524.0783231084,
          "standard_error": 311.60986292118133
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.595983062067475,
            "upper_bound": 967.4744027012204
          },
          "point_estimate": 603.685754619864,
          "standard_error": 237.72026553452267
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223329.56835156505,
            "upper_bound": 223794.6892137223
          },
          "point_estimate": 223517.97002629272,
          "standard_error": 118.6351701184894
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 346.4182142574818,
            "upper_bound": 668.4877392299773
          },
          "point_estimate": 577.7700321023366,
          "standard_error": 80.97403753161656
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/libc_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84674.97029669619,
            "upper_bound": 84731.62007364343
          },
          "point_estimate": 84702.26121714656,
          "standard_error": 14.453324119436427
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84665.69431524548,
            "upper_bound": 84755.07023255814
          },
          "point_estimate": 84683.7868770764,
          "standard_error": 20.215396679530112
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6611224688800378,
            "upper_bound": 80.98199536460493
          },
          "point_estimate": 42.17134948387324,
          "standard_error": 23.569334210555944
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84673.2313793347,
            "upper_bound": 84727.32904109602
          },
          "point_estimate": 84696.51888855331,
          "standard_error": 13.92409162698758
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.61865781597254,
            "upper_bound": 58.88265463447222
          },
          "point_estimate": 48.109031502844395,
          "standard_error": 8.27608015948602
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/libc_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1070795.6154295343,
            "upper_bound": 1073911.0945588234
          },
          "point_estimate": 1072191.824062792,
          "standard_error": 805.4045183780036
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1070058.3823529412,
            "upper_bound": 1073664.4735294117
          },
          "point_estimate": 1071587.6731617646,
          "standard_error": 778.6523920825742
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 374.1945286508252,
            "upper_bound": 3842.375861196079
          },
          "point_estimate": 1873.1560520388891,
          "standard_error": 909.469077203712
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1070539.031836227,
            "upper_bound": 1071750.2548926466
          },
          "point_estimate": 1071168.7501909854,
          "standard_error": 310.2009254509213
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 909.0135443554254,
            "upper_bound": 3652.089163112695
          },
          "point_estimate": 2687.284257027177,
          "standard_error": 754.6188077405293
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/libc_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2176.400950641473,
            "upper_bound": 2183.7621449581925
          },
          "point_estimate": 2180.207354569368,
          "standard_error": 1.8865248624011584
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2174.278214178881,
            "upper_bound": 2185.9372804581
          },
          "point_estimate": 2181.687493269518,
          "standard_error": 3.4776455261360515
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6062406194094957,
            "upper_bound": 10.397878623856135
          },
          "point_estimate": 6.889047352418357,
          "standard_error": 2.9782603783654613
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2180.567327174402,
            "upper_bound": 2185.4142412845445
          },
          "point_estimate": 2183.842368818737,
          "standard_error": 1.255945851129771
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.7417119097017113,
            "upper_bound": 7.413947825484435
          },
          "point_estimate": 6.289418457706451,
          "standard_error": 0.9372748487118326
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51378.33191708883,
            "upper_bound": 51722.743138804464
          },
          "point_estimate": 51525.97398395811,
          "standard_error": 87.9130965572253
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51371.226501182035,
            "upper_bound": 51728.69148936171
          },
          "point_estimate": 51406.349024822695,
          "standard_error": 72.85832286032549
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.633073484959557,
            "upper_bound": 427.00070929865126
          },
          "point_estimate": 51.28033412505783,
          "standard_error": 78.87487023962453
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51384.6569415488,
            "upper_bound": 51443.075724298105
          },
          "point_estimate": 51410.41978815511,
          "standard_error": 15.045746260533182
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.742438309876626,
            "upper_bound": 373.3248304801276
          },
          "point_estimate": 293.5321054464437,
          "standard_error": 90.79130495786428
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52668.170865251894,
            "upper_bound": 52813.002344202905
          },
          "point_estimate": 52740.169066080045,
          "standard_error": 37.129533561949344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52609.27801932367,
            "upper_bound": 52831.16742149758
          },
          "point_estimate": 52775.52884057971,
          "standard_error": 66.30996648658896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.804166312168013,
            "upper_bound": 235.10543952169635
          },
          "point_estimate": 184.0015792695501,
          "standard_error": 65.31770495350847
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52625.09920241222,
            "upper_bound": 52771.0008736286
          },
          "point_estimate": 52701.59164313947,
          "standard_error": 37.7197121374991
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.847928142446,
            "upper_bound": 149.10130262302647
          },
          "point_estimate": 123.48046430611876,
          "standard_error": 18.391799636650024
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52793.585425737794,
            "upper_bound": 52898.02648829308
          },
          "point_estimate": 52844.988308837215,
          "standard_error": 26.764978483559677
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52763.42815249267,
            "upper_bound": 52923.438599706744
          },
          "point_estimate": 52833.103901922455,
          "standard_error": 38.40542424647122
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.75739403749755,
            "upper_bound": 153.042707326938
          },
          "point_estimate": 105.93972700972073,
          "standard_error": 32.15163011916222
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52782.83820568386,
            "upper_bound": 52877.14120921921
          },
          "point_estimate": 52826.805857485626,
          "standard_error": 24.51208548400628
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.4659702006108,
            "upper_bound": 110.90083727645396
          },
          "point_estimate": 88.99175365644484,
          "standard_error": 15.14619023489293
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15231.521560402683,
            "upper_bound": 15262.400237113165
          },
          "point_estimate": 15245.586805206003,
          "standard_error": 7.949713688949929
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15226.846266778524,
            "upper_bound": 15268.521812080537
          },
          "point_estimate": 15235.43875838926,
          "standard_error": 10.34376630284184
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.643183151552905,
            "upper_bound": 45.23919986798557
          },
          "point_estimate": 17.05944580032163,
          "standard_error": 9.946348207566928
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15225.260684533696,
            "upper_bound": 15247.152346904675
          },
          "point_estimate": 15234.724930271072,
          "standard_error": 5.6551098167942655
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.789194460868764,
            "upper_bound": 34.76298277467116
          },
          "point_estimate": 26.456486872575972,
          "standard_error": 6.26569689236526
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26699.880170217977,
            "upper_bound": 26713.837050300022
          },
          "point_estimate": 26706.83479916728,
          "standard_error": 3.579796654685925
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26696.221895664952,
            "upper_bound": 26719.547391623804
          },
          "point_estimate": 26704.308596620132,
          "standard_error": 5.9599186391789365
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3429110164509517,
            "upper_bound": 20.433571929664357
          },
          "point_estimate": 14.59029248527423,
          "standard_error": 4.719003905156978
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26696.736890411335,
            "upper_bound": 26718.331296823257
          },
          "point_estimate": 26709.50470147046,
          "standard_error": 5.575166985090599
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.580624130449547,
            "upper_bound": 14.344235609918211
          },
          "point_estimate": 11.987380449934168,
          "standard_error": 1.7360788688336604
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16366.132978762162,
            "upper_bound": 16386.159100874374
          },
          "point_estimate": 16376.632850124295,
          "standard_error": 5.123272098463969
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16365.014817731771,
            "upper_bound": 16388.704207920793
          },
          "point_estimate": 16377.44958290472,
          "standard_error": 4.667029914284306
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.2741345444601935,
            "upper_bound": 29.698505872385667
          },
          "point_estimate": 9.706310319353609,
          "standard_error": 7.861178280890124
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16359.453964814466,
            "upper_bound": 16380.499620251368
          },
          "point_estimate": 16369.849475726793,
          "standard_error": 5.540580431168429
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.957339723258622,
            "upper_bound": 22.534957743812605
          },
          "point_estimate": 17.130941653736038,
          "standard_error": 3.834559401727093
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16368.722629977285,
            "upper_bound": 16390.89934938494
          },
          "point_estimate": 16379.518150422187,
          "standard_error": 5.671443382746212
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16362.374164202134,
            "upper_bound": 16394.987473747373
          },
          "point_estimate": 16377.551665166517,
          "standard_error": 9.447647321825132
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.8393078413803026,
            "upper_bound": 31.587919274934784
          },
          "point_estimate": 22.35067834982091,
          "standard_error": 7.076043855967276
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16362.404233941912,
            "upper_bound": 16378.985778853115
          },
          "point_estimate": 16369.044512373315,
          "standard_error": 4.247574954264407
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.46095047196048,
            "upper_bound": 23.13961432593038
          },
          "point_estimate": 18.879484880031715,
          "standard_error": 3.0389038939909603
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19101.049810625987,
            "upper_bound": 19147.398915249287
          },
          "point_estimate": 19122.81124863272,
          "standard_error": 11.86177589784415
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19094.96991057338,
            "upper_bound": 19165.70994213572
          },
          "point_estimate": 19106.946826231808,
          "standard_error": 17.277972050608707
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.069560996925535,
            "upper_bound": 62.21487849041634
          },
          "point_estimate": 20.790825951248557,
          "standard_error": 14.905649487081993
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19094.637875138866,
            "upper_bound": 19121.232896400623
          },
          "point_estimate": 19104.499585317364,
          "standard_error": 6.784139799041618
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.490297678110174,
            "upper_bound": 47.03784654180899
          },
          "point_estimate": 39.60797028236007,
          "standard_error": 7.4105008755608734
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22995.82541424946,
            "upper_bound": 23056.02445310612
          },
          "point_estimate": 23018.801681321005,
          "standard_error": 16.82052171222641
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22991.16021573604,
            "upper_bound": 23014.790371192896
          },
          "point_estimate": 23006.377326565143,
          "standard_error": 7.496791197328076
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0916457610769692,
            "upper_bound": 31.39341944583236
          },
          "point_estimate": 14.485822870996426,
          "standard_error": 8.022067718192712
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22987.24071710506,
            "upper_bound": 23007.8704502124
          },
          "point_estimate": 22996.271145098555,
          "standard_error": 5.427076162225911
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.75024323393546,
            "upper_bound": 86.0667594874833
          },
          "point_estimate": 56.08963073963809,
          "standard_error": 26.558501389280956
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15980.320677775331,
            "upper_bound": 16012.08129915188
          },
          "point_estimate": 15994.867839929047,
          "standard_error": 8.155617932526189
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15970.827320721512,
            "upper_bound": 16010.859583516643
          },
          "point_estimate": 15989.49648042235,
          "standard_error": 11.54941024572691
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.2135061168747994,
            "upper_bound": 47.717748668897855
          },
          "point_estimate": 28.363332074541137,
          "standard_error": 10.495342077036828
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15972.170403551574,
            "upper_bound": 15992.158556009776
          },
          "point_estimate": 15980.029545026026,
          "standard_error": 5.0983784067929845
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.499512633086448,
            "upper_bound": 36.834236055221304
          },
          "point_estimate": 27.203197839653225,
          "standard_error": 6.704192995411898
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20105.727678334253,
            "upper_bound": 20124.34814153293
          },
          "point_estimate": 20113.937273565763,
          "standard_error": 4.820895872312716
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20101.705589374655,
            "upper_bound": 20121.566131710017
          },
          "point_estimate": 20109.501844678103,
          "standard_error": 4.435636749103618
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.410694976572531,
            "upper_bound": 20.839062579676735
          },
          "point_estimate": 10.965250331060115,
          "standard_error": 5.036456043514173
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20102.529939473676,
            "upper_bound": 20113.596176841544
          },
          "point_estimate": 20107.255875060193,
          "standard_error": 2.8074568998669367
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.262252057652828,
            "upper_bound": 22.566766739679096
          },
          "point_estimate": 16.143758637391084,
          "standard_error": 4.900004613358671
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16739.62254450583,
            "upper_bound": 16763.561803026983
          },
          "point_estimate": 16750.380790069867,
          "standard_error": 6.173918197266669
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16740.427782893392,
            "upper_bound": 16759.17337542752
          },
          "point_estimate": 16744.179001688153,
          "standard_error": 5.394493074336495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7963002137615283,
            "upper_bound": 28.623109590059137
          },
          "point_estimate": 7.628965871312151,
          "standard_error": 7.554994440978873
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16730.34971008522,
            "upper_bound": 16751.18154919964
          },
          "point_estimate": 16739.51143837746,
          "standard_error": 5.456712726771359
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.183034404513116,
            "upper_bound": 29.21756927525457
          },
          "point_estimate": 20.629388768207267,
          "standard_error": 6.357817489708669
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19251.255276894542,
            "upper_bound": 19285.419048184216
          },
          "point_estimate": 19263.60177545192,
          "standard_error": 10.02393088227141
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19250.09303421068,
            "upper_bound": 19257.70174880763
          },
          "point_estimate": 19254.600920773715,
          "standard_error": 3.10283200361549
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3616820426016296,
            "upper_bound": 10.43040465495489
          },
          "point_estimate": 5.039710478888655,
          "standard_error": 3.5059478193008053
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19250.300994909416,
            "upper_bound": 19256.93685765432
          },
          "point_estimate": 19253.843518537637,
          "standard_error": 1.7487418929135885
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.905581052843801,
            "upper_bound": 51.27304250684736
          },
          "point_estimate": 33.35077652376366,
          "standard_error": 17.35276219087941
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16358.552885633848,
            "upper_bound": 16392.954825825822
          },
          "point_estimate": 16373.90848294723,
          "standard_error": 8.87425496743074
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16347.520077220075,
            "upper_bound": 16385.14496996997
          },
          "point_estimate": 16372.985480480482,
          "standard_error": 8.129821545436736
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.812817952050196,
            "upper_bound": 42.12794150433092
          },
          "point_estimate": 21.75949204612311,
          "standard_error": 11.779946283870224
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16351.402773285774,
            "upper_bound": 16373.259590843423
          },
          "point_estimate": 16361.103285363286,
          "standard_error": 5.663943857517035
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.938557924061287,
            "upper_bound": 41.79365208713414
          },
          "point_estimate": 29.60882953048179,
          "standard_error": 8.905345049832833
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16602.255440389137,
            "upper_bound": 16624.96017835379
          },
          "point_estimate": 16613.51153189475,
          "standard_error": 5.804981818669028
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16597.562100091407,
            "upper_bound": 16625.757693479587
          },
          "point_estimate": 16615.45364868982,
          "standard_error": 7.040211476246675
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.007001665603477,
            "upper_bound": 34.91991614878912
          },
          "point_estimate": 17.280277366891237,
          "standard_error": 7.748536365881455
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16605.597218667826,
            "upper_bound": 16620.802050721155
          },
          "point_estimate": 16613.20651843586,
          "standard_error": 3.861323047587311
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.597873827695498,
            "upper_bound": 25.068131640956384
          },
          "point_estimate": 19.345184352592835,
          "standard_error": 3.7612143217016265
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16517.67141970695,
            "upper_bound": 16563.568454150234
          },
          "point_estimate": 16538.408295564506,
          "standard_error": 11.785152216049054
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16504.238853503186,
            "upper_bound": 16557.438087655442
          },
          "point_estimate": 16530.346971994746,
          "standard_error": 12.950479403049822
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.752346730912238,
            "upper_bound": 58.61911937650003
          },
          "point_estimate": 37.89752846731121,
          "standard_error": 13.78607998475019
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16512.58720540024,
            "upper_bound": 16543.400325809387
          },
          "point_estimate": 16527.07406260709,
          "standard_error": 8.03356086560474
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.342062260869525,
            "upper_bound": 53.62029110444452
          },
          "point_estimate": 39.17845420619864,
          "standard_error": 10.490692591528212
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19694.850200459303,
            "upper_bound": 19725.751248319633
          },
          "point_estimate": 19709.078340162523,
          "standard_error": 7.922284702260981
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19688.0235703945,
            "upper_bound": 19724.174049945712
          },
          "point_estimate": 19704.887425352877,
          "standard_error": 10.204446761463174
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.250833946210658,
            "upper_bound": 43.96064024602702
          },
          "point_estimate": 25.836321227887726,
          "standard_error": 9.036728894754969
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19694.02513646219,
            "upper_bound": 19714.174931145302
          },
          "point_estimate": 19702.494172060295,
          "standard_error": 5.088733275412787
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.700530396045336,
            "upper_bound": 35.76357439478493
          },
          "point_estimate": 26.355401822353517,
          "standard_error": 6.616569777162559
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17004.349332979524,
            "upper_bound": 17021.24636613707
          },
          "point_estimate": 17012.798354301212,
          "standard_error": 4.324852912796825
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16999.522524636322,
            "upper_bound": 17025.916798879727
          },
          "point_estimate": 17014.014453308308,
          "standard_error": 8.724562834309554
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7685725907650464,
            "upper_bound": 22.532870857586698
          },
          "point_estimate": 20.263649077132815,
          "standard_error": 5.746886141874618
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17008.023022800564,
            "upper_bound": 17022.71395982448
          },
          "point_estimate": 17017.848451126538,
          "standard_error": 3.709267414602048
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.776908916141302,
            "upper_bound": 16.789524884852423
          },
          "point_estimate": 14.459246596078616,
          "standard_error": 1.805744387545846
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19812.871687557912,
            "upper_bound": 19839.88527987772
          },
          "point_estimate": 19827.375185769703,
          "standard_error": 6.901363141836433
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19813.686852154937,
            "upper_bound": 19844.373073026265
          },
          "point_estimate": 19828.738634297144,
          "standard_error": 7.265214313554977
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6797475519858533,
            "upper_bound": 35.56673936674214
          },
          "point_estimate": 20.52107833862432,
          "standard_error": 7.84981694232507
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19820.7009372678,
            "upper_bound": 19848.711371282312
          },
          "point_estimate": 19834.90024160237,
          "standard_error": 7.43686572572853
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.289124994696213,
            "upper_bound": 31.74425528032967
          },
          "point_estimate": 23.04234319014088,
          "standard_error": 5.989853284630194
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71541.28876531059,
            "upper_bound": 71632.53092519686
          },
          "point_estimate": 71585.02413823272,
          "standard_error": 23.483019759522897
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71515.33513779528,
            "upper_bound": 71668.28897637795
          },
          "point_estimate": 71568.50393700787,
          "standard_error": 32.937158022720624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.8654155769237235,
            "upper_bound": 117.38028059324856
          },
          "point_estimate": 90.35126216563124,
          "standard_error": 34.04426929845047
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71515.13068816086,
            "upper_bound": 71593.97218719416
          },
          "point_estimate": 71543.5253655793,
          "standard_error": 20.35271680029195
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.94305074536877,
            "upper_bound": 93.8091767232688
          },
          "point_estimate": 78.07106693828574,
          "standard_error": 13.09800149843814
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231605.7803998676,
            "upper_bound": 1234644.1986878307
          },
          "point_estimate": 1233012.392169312,
          "standard_error": 781.2235744471934
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1230889.5733333332,
            "upper_bound": 1234991.23
          },
          "point_estimate": 1231865.2238095235,
          "standard_error": 1011.0286775089456
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.51713679506676,
            "upper_bound": 4159.169829159787
          },
          "point_estimate": 1822.6558498636157,
          "standard_error": 1022.0192272519894
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231291.4356258886,
            "upper_bound": 1235140.6841947564
          },
          "point_estimate": 1233123.4945454546,
          "standard_error": 1030.1213390354494
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1083.058804810345,
            "upper_bound": 3378.19549093108
          },
          "point_estimate": 2608.72806985798,
          "standard_error": 598.9697619913851
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203088.094924182,
            "upper_bound": 203452.91569607944
          },
          "point_estimate": 203254.41727210252,
          "standard_error": 93.8689889449342
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203026.72402234637,
            "upper_bound": 203447.3844273743
          },
          "point_estimate": 203159.1401081848,
          "standard_error": 117.47535129296314
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.645782314659016,
            "upper_bound": 479.3382970766541
          },
          "point_estimate": 282.59059526233995,
          "standard_error": 109.4137782316234
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203105.43251078928,
            "upper_bound": 203471.89471995473
          },
          "point_estimate": 203252.2588260901,
          "standard_error": 94.00777202766312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.42113082821035,
            "upper_bound": 422.1830083895789
          },
          "point_estimate": 311.79711547424074,
          "standard_error": 79.71751399407937
        }
      }
    },
    "memmem/regex/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/regex_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6021.559629636217,
            "upper_bound": 6043.961366439006
          },
          "point_estimate": 6032.469076294031,
          "standard_error": 5.724961203294302
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6016.5392596281545,
            "upper_bound": 6052.265438247012
          },
          "point_estimate": 6031.840755035413,
          "standard_error": 8.98289579628105
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9054674794524316,
            "upper_bound": 30.899673966694195
          },
          "point_estimate": 23.042120005096073,
          "standard_error": 7.663245788256159
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6015.314605293564,
            "upper_bound": 6044.371288572632
          },
          "point_estimate": 6029.238199582622,
          "standard_error": 7.719872180371556
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.983448931595287,
            "upper_bound": 23.39765253789615
          },
          "point_estimate": 19.033156421123476,
          "standard_error": 3.1297741988834114
        }
      }
    },
    "memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/regex_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6065.487973730979,
            "upper_bound": 6086.509854034766
          },
          "point_estimate": 6074.781864114077,
          "standard_error": 5.451791679060391
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6061.935740467463,
            "upper_bound": 6082.457597467511
          },
          "point_estimate": 6070.605906364544,
          "standard_error": 5.227718853949606
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6087868069743303,
            "upper_bound": 23.110418887164936
          },
          "point_estimate": 12.19162504735646,
          "standard_error": 5.4830085811364135
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6065.853060406783,
            "upper_bound": 6078.180125239066
          },
          "point_estimate": 6071.753311233918,
          "standard_error": 3.133770017058753
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.329141925221101,
            "upper_bound": 25.681400899303373
          },
          "point_estimate": 18.2086271867483,
          "standard_error": 5.66272977216795
        }
      }
    },
    "memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/regex_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15577.114259165828,
            "upper_bound": 15626.382692971469
          },
          "point_estimate": 15601.205933843125,
          "standard_error": 12.595302708790198
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15561.058060207051,
            "upper_bound": 15637.4336625161
          },
          "point_estimate": 15598.941921732196,
          "standard_error": 24.86589576588346
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.9117268914742045,
            "upper_bound": 67.53503878813116
          },
          "point_estimate": 55.249182445494505,
          "standard_error": 19.06619634820477
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15570.36986990154,
            "upper_bound": 15616.160880478492
          },
          "point_estimate": 15589.86395030474,
          "standard_error": 11.602943030531444
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.292488463088706,
            "upper_bound": 50.284908597710725
          },
          "point_estimate": 42.002224276889685,
          "standard_error": 6.02464876092602
        }
      }
    },
    "memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/regex_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.87672183956936,
            "upper_bound": 44.11122045901196
          },
          "point_estimate": 43.98384692720922,
          "standard_error": 0.060533709512654695
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.86328082685314,
            "upper_bound": 44.11774104517619
          },
          "point_estimate": 43.924874217917136,
          "standard_error": 0.05149406664942551
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014181546225068488,
            "upper_bound": 0.29904601778743806
          },
          "point_estimate": 0.09706781780698528,
          "standard_error": 0.07048713125636148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.90736831602928,
            "upper_bound": 43.999775280755976
          },
          "point_estimate": 43.9462005934042,
          "standard_error": 0.0234866925201439
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06156083402226144,
            "upper_bound": 0.2643767993900785
          },
          "point_estimate": 0.2025073413259172,
          "standard_error": 0.05271147804012134
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.004983088594987,
            "upper_bound": 16.030493763901188
          },
          "point_estimate": 16.016490832447698,
          "standard_error": 0.006557288744857161
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.00048768005581,
            "upper_bound": 16.033605548817086
          },
          "point_estimate": 16.011955428341906,
          "standard_error": 0.0060267209473022386
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00036895745713035543,
            "upper_bound": 0.03085290768188314
          },
          "point_estimate": 0.01047247441710356,
          "standard_error": 0.007795428766493219
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.002443882370194,
            "upper_bound": 16.013208774720137
          },
          "point_estimate": 16.007721512513136,
          "standard_error": 0.002819562177748984
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006143199796097359,
            "upper_bound": 0.027651550219138215
          },
          "point_estimate": 0.021842298502493963,
          "standard_error": 0.005698399868971292
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.954651012575509,
            "upper_bound": 15.985396017507885
          },
          "point_estimate": 15.972752221527989,
          "standard_error": 0.008114793059748356
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.97220514335478,
            "upper_bound": 15.987135688770511
          },
          "point_estimate": 15.97703647909806,
          "standard_error": 0.0037600522961733097
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012141187781239977,
            "upper_bound": 0.024687774323754127
          },
          "point_estimate": 0.005443716726500323,
          "standard_error": 0.0062214460616523875
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.934212403341968,
            "upper_bound": 15.98905900274503
          },
          "point_estimate": 15.964225207611332,
          "standard_error": 0.014886909821915428
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00393677255747852,
            "upper_bound": 0.040648448301442104
          },
          "point_estimate": 0.027055509516143203,
          "standard_error": 0.01102471992680438
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.977484367213654,
            "upper_bound": 15.990392693959327
          },
          "point_estimate": 15.98410932200901,
          "standard_error": 0.0033109357090704986
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.973138214936505,
            "upper_bound": 15.995276236788088
          },
          "point_estimate": 15.986082907077968,
          "standard_error": 0.005251777035283955
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013565926274287014,
            "upper_bound": 0.017771110734280712
          },
          "point_estimate": 0.014089305826921538,
          "standard_error": 0.004293667223360836
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.98046333558989,
            "upper_bound": 15.990356158563788
          },
          "point_estimate": 15.985059003942624,
          "standard_error": 0.002537939965623841
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006282235124063187,
            "upper_bound": 0.013516208534356746
          },
          "point_estimate": 0.011027373885933292,
          "standard_error": 0.0018234908559504528
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.959406463113524,
            "upper_bound": 15.982851548539257
          },
          "point_estimate": 15.969698041253183,
          "standard_error": 0.006089631292960093
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.956420255076775,
            "upper_bound": 15.977453959860211
          },
          "point_estimate": 15.96274586452932,
          "standard_error": 0.006521036858770707
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014240956356779434,
            "upper_bound": 0.02735888349352243
          },
          "point_estimate": 0.010744905291510091,
          "standard_error": 0.006874488817318786
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.95816410737934,
            "upper_bound": 15.981663545335516
          },
          "point_estimate": 15.969621403106864,
          "standard_error": 0.006141206499499475
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0069772714818630115,
            "upper_bound": 0.02866206943066796
          },
          "point_estimate": 0.020272053228900792,
          "standard_error": 0.006299285262149729
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.43560655644713,
            "upper_bound": 17.447329897493137
          },
          "point_estimate": 17.440277024115765,
          "standard_error": 0.003192814592267558
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.43515239412315,
            "upper_bound": 17.441712398729724
          },
          "point_estimate": 17.43627693045366,
          "standard_error": 0.001703390663395814
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00029690686011925766,
            "upper_bound": 0.008121367858086448
          },
          "point_estimate": 0.002878334255655354,
          "standard_error": 0.00209250392700918
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.43577876204739,
            "upper_bound": 17.441036381155005
          },
          "point_estimate": 17.437740223894707,
          "standard_error": 0.0014520254973243434
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001635767742706334,
            "upper_bound": 0.015964037776948523
          },
          "point_estimate": 0.010611598000186016,
          "standard_error": 0.00452166987113262
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.062680503607208,
            "upper_bound": 18.08191727601279
          },
          "point_estimate": 18.07080553786505,
          "standard_error": 0.005030523686730772
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.061092903895485,
            "upper_bound": 18.075456709623232
          },
          "point_estimate": 18.0643481842444,
          "standard_error": 0.004346849587869816
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007888592233108918,
            "upper_bound": 0.01811840173637311
          },
          "point_estimate": 0.01133556451440024,
          "standard_error": 0.004843315627702907
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.06092904304292,
            "upper_bound": 18.094129062123372
          },
          "point_estimate": 18.07517138402299,
          "standard_error": 0.0091898514519856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005374411596482999,
            "upper_bound": 0.024577887384885824
          },
          "point_estimate": 0.01675689573987485,
          "standard_error": 0.006070564492358906
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.10200168184448,
            "upper_bound": 15.148988052957645
          },
          "point_estimate": 15.124830050981547,
          "standard_error": 0.01208802939827963
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.089912712316066,
            "upper_bound": 15.172199528819696
          },
          "point_estimate": 15.11741977054783,
          "standard_error": 0.020291712703140546
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006424390911438247,
            "upper_bound": 0.06503778060054659
          },
          "point_estimate": 0.04300758196485231,
          "standard_error": 0.016364558315692928
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.095450627849992,
            "upper_bound": 15.149884949153517
          },
          "point_estimate": 15.123814458505285,
          "standard_error": 0.013555794909851876
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021961582571756563,
            "upper_bound": 0.04763700589453871
          },
          "point_estimate": 0.0403658071384954,
          "standard_error": 0.006151783923298954
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.407890098300786,
            "upper_bound": 17.447048297653673
          },
          "point_estimate": 17.424581864509694,
          "standard_error": 0.01020137188085145
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.402413000583532,
            "upper_bound": 17.440206915495125
          },
          "point_estimate": 17.411248950777754,
          "standard_error": 0.010059123021841433
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008523551035140639,
            "upper_bound": 0.04370164873014192
          },
          "point_estimate": 0.013435098767071088,
          "standard_error": 0.011441947799361682
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.405259930326192,
            "upper_bound": 17.416985392740784
          },
          "point_estimate": 17.40950531495226,
          "standard_error": 0.0030337358776410736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006914499217368969,
            "upper_bound": 0.04878401506833534
          },
          "point_estimate": 0.03393837323310883,
          "standard_error": 0.011467420929206391
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.6069135485315,
            "upper_bound": 20.62585706402684
          },
          "point_estimate": 20.616254675856233,
          "standard_error": 0.004865101440664224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.59799839535935,
            "upper_bound": 20.63276931129492
          },
          "point_estimate": 20.613869914853808,
          "standard_error": 0.00802605860651821
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000695637711154122,
            "upper_bound": 0.028951782033218472
          },
          "point_estimate": 0.02361600392708304,
          "standard_error": 0.007197774987149096
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.600557389073547,
            "upper_bound": 20.61463594280633
          },
          "point_estimate": 20.605657765381764,
          "standard_error": 0.0035842420688595084
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009774412224035484,
            "upper_bound": 0.019126456005014662
          },
          "point_estimate": 0.016198402132892194,
          "standard_error": 0.00232357688019291
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.940530503266436,
            "upper_bound": 16.1193176396051
          },
          "point_estimate": 16.020946425279945,
          "standard_error": 0.045858554590602445
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.924662843515335,
            "upper_bound": 16.134104690681028
          },
          "point_estimate": 15.982569955540196,
          "standard_error": 0.03815389462062787
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00589665460418992,
            "upper_bound": 0.21574230694426333
          },
          "point_estimate": 0.0317844560176054,
          "standard_error": 0.05573470478663956
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.937468946177534,
            "upper_bound": 16.194600388087867
          },
          "point_estimate": 16.056897218458325,
          "standard_error": 0.07282033946653114
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04023979323579302,
            "upper_bound": 0.19440409364840272
          },
          "point_estimate": 0.15240496862961117,
          "standard_error": 0.04051914889884505
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.321274883364133,
            "upper_bound": 18.39018980927093
          },
          "point_estimate": 18.357597376878093,
          "standard_error": 0.017741404635889045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.315697216801425,
            "upper_bound": 18.391667778079263
          },
          "point_estimate": 18.366036092403245,
          "standard_error": 0.013838959288911164
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015321705079463788,
            "upper_bound": 0.09993298374749286
          },
          "point_estimate": 0.015808703071307835,
          "standard_error": 0.02914821901417692
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.296262050249094,
            "upper_bound": 18.37709999608125
          },
          "point_estimate": 18.341920999976292,
          "standard_error": 0.022310586661530945
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023443887978887364,
            "upper_bound": 0.07689649720731305
          },
          "point_estimate": 0.05898835734089998,
          "standard_error": 0.01384511309373553
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.237099964929104,
            "upper_bound": 27.662255114505832
          },
          "point_estimate": 27.444611682371395,
          "standard_error": 0.1046539124721362
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.212187624993152,
            "upper_bound": 27.93542434239792
          },
          "point_estimate": 27.246518514682997,
          "standard_error": 0.1770989040159452
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011491328301072412,
            "upper_bound": 0.5443171913828346
          },
          "point_estimate": 0.05442980257914684,
          "standard_error": 0.1478594227931195
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.231175014600733,
            "upper_bound": 27.70678024443551
          },
          "point_estimate": 27.43167069592864,
          "standard_error": 0.12969309772985188
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026640157285429008,
            "upper_bound": 0.3859004761434677
          },
          "point_estimate": 0.3486345399960382,
          "standard_error": 0.06969176427669113
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138784.3893956743,
            "upper_bound": 139024.3741836302
          },
          "point_estimate": 138900.2323303647,
          "standard_error": 61.40258481134831
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138694.01526717556,
            "upper_bound": 139122.1106870229
          },
          "point_estimate": 138853.80995547073,
          "standard_error": 97.62940508367598
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.49524652410749,
            "upper_bound": 332.31294782165736
          },
          "point_estimate": 243.10966077934873,
          "standard_error": 84.41169774765497
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138757.21028430606,
            "upper_bound": 139024.6507898643
          },
          "point_estimate": 138897.72436799842,
          "standard_error": 68.79310322044284
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107.95079854689426,
            "upper_bound": 242.85604331151848
          },
          "point_estimate": 204.57537425237956,
          "standard_error": 32.0436539783813
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57326.568288033275,
            "upper_bound": 57447.34760220915
          },
          "point_estimate": 57383.80803421212,
          "standard_error": 30.914931038641143
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57284.600045065345,
            "upper_bound": 57467.0327681388
          },
          "point_estimate": 57353.35304942166,
          "standard_error": 42.793514867205694
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.064611893448095,
            "upper_bound": 171.47925679790345
          },
          "point_estimate": 104.98937366839831,
          "standard_error": 40.4310637259442
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57306.93193197367,
            "upper_bound": 57458.10763716619
          },
          "point_estimate": 57379.33621614978,
          "standard_error": 41.20748837608285
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.39102682585463,
            "upper_bound": 129.41865404614518
          },
          "point_estimate": 103.31793514991628,
          "standard_error": 19.645337458881773
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204779.1736329588,
            "upper_bound": 205236.3445626003
          },
          "point_estimate": 204992.098788122,
          "standard_error": 116.79538343640752
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204654.5098314607,
            "upper_bound": 205147.53785446764
          },
          "point_estimate": 205070.27879213484,
          "standard_error": 162.51817363784838
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8623936570422313,
            "upper_bound": 659.6641274739995
          },
          "point_estimate": 388.1952036699378,
          "standard_error": 184.46853541486053
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204861.44466503337,
            "upper_bound": 205148.10773561645
          },
          "point_estimate": 205040.91244710344,
          "standard_error": 72.62523984994861
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199.2004429938125,
            "upper_bound": 533.0158686875387
          },
          "point_estimate": 389.8507932044305,
          "standard_error": 99.30532554402254
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 670763.2452563131,
            "upper_bound": 678065.7094219697
          },
          "point_estimate": 673959.9432020201,
          "standard_error": 1888.304816054199
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 669865.6101010102,
            "upper_bound": 678103.9018181819
          },
          "point_estimate": 670820.9022727273,
          "standard_error": 2153.560319173966
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153.4896815779438,
            "upper_bound": 9475.53005359392
          },
          "point_estimate": 2089.168687909928,
          "standard_error": 2409.7087746049874
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 670207.3141446251,
            "upper_bound": 678123.4174294671
          },
          "point_estimate": 672615.3159386069,
          "standard_error": 2100.24169197707
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1386.4269412749788,
            "upper_bound": 8761.165003473598
          },
          "point_estimate": 6299.63428147642,
          "standard_error": 1882.0927674724503
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1378513.9320488686,
            "upper_bound": 1381584.463452601
          },
          "point_estimate": 1379704.578309818,
          "standard_error": 843.6353133283577
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1378355.7962962964,
            "upper_bound": 1379675.3962962965
          },
          "point_estimate": 1378959.1975308645,
          "standard_error": 400.0028274926531
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240.0831401821505,
            "upper_bound": 1738.172753585712
          },
          "point_estimate": 978.2194626330294,
          "standard_error": 419.52432600793554
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1378372.7899106003,
            "upper_bound": 1379544.6098811016
          },
          "point_estimate": 1378979.4277056276,
          "standard_error": 303.5019299365849
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 496.81778511063374,
            "upper_bound": 4284.860077219685
          },
          "point_estimate": 2806.7306845045537,
          "standard_error": 1275.0143048277805
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57116.31461340721,
            "upper_bound": 57242.16292420073
          },
          "point_estimate": 57175.06996487222,
          "standard_error": 32.17697299040011
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57100.8356743536,
            "upper_bound": 57298.34158805032
          },
          "point_estimate": 57124.358640311475,
          "standard_error": 47.13065190240493
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.2053623289728606,
            "upper_bound": 154.52185509866652
          },
          "point_estimate": 40.45605687217441,
          "standard_error": 39.8267409189916
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57099.30523900866,
            "upper_bound": 57153.767291602664
          },
          "point_estimate": 57116.70405946255,
          "standard_error": 14.146133048630915
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.949537369828995,
            "upper_bound": 128.26414434752698
          },
          "point_estimate": 107.13535413454208,
          "standard_error": 22.07749831276586
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166156.06373468874,
            "upper_bound": 166425.73927303037
          },
          "point_estimate": 166279.70620044213,
          "standard_error": 68.41376969908302
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166127.46626078134,
            "upper_bound": 166338.71765601216
          },
          "point_estimate": 166310.31539465103,
          "standard_error": 66.23407730354344
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.546715583932494,
            "upper_bound": 304.8209749536406
          },
          "point_estimate": 138.59333834997506,
          "standard_error": 90.99980911096564
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166150.8747583823,
            "upper_bound": 166397.74391020392
          },
          "point_estimate": 166231.96070687304,
          "standard_error": 64.32010758505321
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.81294211803946,
            "upper_bound": 325.7396639847238
          },
          "point_estimate": 228.76066077298736,
          "standard_error": 68.03447782385612
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111001.4569821973,
            "upper_bound": 111139.83360482
          },
          "point_estimate": 111064.29384618178,
          "standard_error": 35.61131698158245
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110985.15250254064,
            "upper_bound": 111160.10228658536
          },
          "point_estimate": 111014.84781504064,
          "standard_error": 40.0772996163665
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.093240046956206,
            "upper_bound": 166.27693947178358
          },
          "point_estimate": 67.29429371382592,
          "standard_error": 39.888026543569936
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110987.8861375476,
            "upper_bound": 111052.73560255856
          },
          "point_estimate": 111017.69010928096,
          "standard_error": 17.39855867629331
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.458837409737896,
            "upper_bound": 150.24104929145372
          },
          "point_estimate": 118.32881244861936,
          "standard_error": 30.31672075340058
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 687744.3796731805,
            "upper_bound": 689278.336051213
          },
          "point_estimate": 688461.449442947,
          "standard_error": 394.1695553532458
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 687533.7641509434,
            "upper_bound": 689499.5235849057
          },
          "point_estimate": 688175.4283468104,
          "standard_error": 364.9117489758873
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161.98903297319012,
            "upper_bound": 2198.723734549828
          },
          "point_estimate": 555.2839759908625,
          "standard_error": 556.2817978738374
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 687879.8543082828,
            "upper_bound": 689648.06126643
          },
          "point_estimate": 688660.9117863269,
          "standard_error": 460.0380174427637
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 516.3487026369884,
            "upper_bound": 1679.968990118159
          },
          "point_estimate": 1307.8794351059294,
          "standard_error": 299.35141593421105
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48492.338288750085,
            "upper_bound": 48568.63329704156
          },
          "point_estimate": 48528.71329704157,
          "standard_error": 19.56566163448792
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48477.169309078774,
            "upper_bound": 48579.353471295064
          },
          "point_estimate": 48511.49580392905,
          "standard_error": 27.55516797903384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.838633212159175,
            "upper_bound": 108.3642762303548
          },
          "point_estimate": 60.20897381425517,
          "standard_error": 24.530732041461725
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48467.62099346463,
            "upper_bound": 48533.31340723626
          },
          "point_estimate": 48490.54778145753,
          "standard_error": 16.942959086867138
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.04800561333809,
            "upper_bound": 81.52093694143971
          },
          "point_estimate": 65.02611277555862,
          "standard_error": 12.34117804481714
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101715.9269263261,
            "upper_bound": 101923.732155829
          },
          "point_estimate": 101813.60871204038,
          "standard_error": 53.45803236116182
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101694.83893557424,
            "upper_bound": 101935.4411764706
          },
          "point_estimate": 101795.85146558622,
          "standard_error": 44.9613393572298
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.490588784343084,
            "upper_bound": 329.28473892526284
          },
          "point_estimate": 99.25091281543348,
          "standard_error": 80.89443082444815
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101740.64717430144,
            "upper_bound": 102020.1689911873
          },
          "point_estimate": 101869.68987595038,
          "standard_error": 81.85330838088493
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.74679353471885,
            "upper_bound": 235.720067987457
          },
          "point_estimate": 178.1398815534158,
          "standard_error": 43.06972356707961
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318659.90825779724,
            "upper_bound": 319002.9799164317
          },
          "point_estimate": 318813.22361702874,
          "standard_error": 88.54379844806415
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318615.9264619883,
            "upper_bound": 318978.7055921053
          },
          "point_estimate": 318737.5104949875,
          "standard_error": 85.52528748078635
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.72280479704036,
            "upper_bound": 437.556652319525
          },
          "point_estimate": 169.536752019337,
          "standard_error": 97.03448814564968
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318689.9451201533,
            "upper_bound": 318899.6525590458
          },
          "point_estimate": 318783.7246753247,
          "standard_error": 53.52147301074328
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.27494616609496,
            "upper_bound": 400.1440682256848
          },
          "point_estimate": 295.15734243368496,
          "standard_error": 82.67734846320269
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50001.986280459765,
            "upper_bound": 50081.163087233166
          },
          "point_estimate": 50042.834946141214,
          "standard_error": 20.284392901262397
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50002.360137931035,
            "upper_bound": 50083.574942528736
          },
          "point_estimate": 50053.75577011494,
          "standard_error": 22.301615090464423
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.470881157911345,
            "upper_bound": 114.17362657990758
          },
          "point_estimate": 48.27035361199879,
          "standard_error": 25.693390396560083
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49998.31449541285,
            "upper_bound": 50087.250762430376
          },
          "point_estimate": 50036.68567845947,
          "standard_error": 22.714647777499295
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.80969219945349,
            "upper_bound": 90.43562671249296
          },
          "point_estimate": 67.7100867347249,
          "standard_error": 15.396953693221432
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/regex_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19578.77252717847,
            "upper_bound": 19636.63073919967
          },
          "point_estimate": 19608.03753291896,
          "standard_error": 14.82230113376847
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19566.5274811219,
            "upper_bound": 19652.28721682848
          },
          "point_estimate": 19603.63431918974,
          "standard_error": 18.25745303049679
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.148289489909832,
            "upper_bound": 90.63100052690196
          },
          "point_estimate": 51.6087196698826,
          "standard_error": 24.17589778869521
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19571.302900885024,
            "upper_bound": 19608.746507509582
          },
          "point_estimate": 19592.200958265035,
          "standard_error": 9.430940748798305
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.277295057865555,
            "upper_bound": 61.82906778759903
          },
          "point_estimate": 49.291474018773386,
          "standard_error": 8.95273538813653
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/regex_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1091881.0372163863,
            "upper_bound": 1094568.8597130312
          },
          "point_estimate": 1093116.4824451446,
          "standard_error": 693.7859202176658
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1091491.9205882354,
            "upper_bound": 1094264.5210084035
          },
          "point_estimate": 1092545.758374183,
          "standard_error": 787.7020509522904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 411.8537359969114,
            "upper_bound": 3557.1279868483543
          },
          "point_estimate": 1826.1475187312824,
          "standard_error": 774.5445450996874
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1091624.8253351108,
            "upper_bound": 1092916.729896905
          },
          "point_estimate": 1092221.191367456,
          "standard_error": 334.4151005878855
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 968.3077236834685,
            "upper_bound": 3116.146847908564
          },
          "point_estimate": 2308.4320390858443,
          "standard_error": 589.6003555288088
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/regex_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2183.7298035853937,
            "upper_bound": 2190.1616393512913
          },
          "point_estimate": 2187.2489631596695,
          "standard_error": 1.6625922716366393
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2183.240968838357,
            "upper_bound": 2191.1526649822536
          },
          "point_estimate": 2189.96690696291,
          "standard_error": 2.1469134292489556
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.22463906236619957,
            "upper_bound": 8.137320370783538
          },
          "point_estimate": 2.524048559331289,
          "standard_error": 2.4404976965897647
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2181.2620878147222,
            "upper_bound": 2190.123610245564
          },
          "point_estimate": 2186.7147696314996,
          "standard_error": 2.364943282592408
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9957501167471705,
            "upper_bound": 7.308304694742089
          },
          "point_estimate": 5.540179115373363,
          "standard_error": 1.4030273922065457
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52394.94772755547,
            "upper_bound": 52508.21359216255
          },
          "point_estimate": 52448.01653638814,
          "standard_error": 29.12194469196102
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52371.54746008708,
            "upper_bound": 52533.30527334301
          },
          "point_estimate": 52416.48466376391,
          "standard_error": 38.042796380135705
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.580972980040567,
            "upper_bound": 166.2118658445042
          },
          "point_estimate": 82.09247967464063,
          "standard_error": 37.17389744047818
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52372.64663483173,
            "upper_bound": 52435.49382648113
          },
          "point_estimate": 52402.118055529376,
          "standard_error": 16.160017186671535
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.78272696764717,
            "upper_bound": 120.9464788291796
          },
          "point_estimate": 96.94890437180048,
          "standard_error": 20.222741726554172
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48412.3608954727,
            "upper_bound": 48580.68439873819
          },
          "point_estimate": 48483.443342368904,
          "standard_error": 44.16348242785419
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48374.353750554816,
            "upper_bound": 48533.56820429903
          },
          "point_estimate": 48445.69573901464,
          "standard_error": 31.68119075195387
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.56997554247832,
            "upper_bound": 160.70488758633434
          },
          "point_estimate": 75.54942528988879,
          "standard_error": 44.17938756377542
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48423.83510140749,
            "upper_bound": 48508.875357544275
          },
          "point_estimate": 48467.102135680565,
          "standard_error": 21.15147984725216
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.702927519504165,
            "upper_bound": 213.86090923762956
          },
          "point_estimate": 147.06531094159664,
          "standard_error": 52.20965247260109
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50890.22093127731,
            "upper_bound": 51137.80530608224
          },
          "point_estimate": 50996.1622105181,
          "standard_error": 64.54382974638463
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50829.6693196005,
            "upper_bound": 51041.15056179775
          },
          "point_estimate": 50959.48735955056,
          "standard_error": 51.66935958097981
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.093706595514224,
            "upper_bound": 255.0052519896074
          },
          "point_estimate": 140.920086346482,
          "standard_error": 59.47527506396221
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50831.16986384901,
            "upper_bound": 50968.45159429092
          },
          "point_estimate": 50874.97614548373,
          "standard_error": 33.47266874894528
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.48050617680528,
            "upper_bound": 316.19058211469746
          },
          "point_estimate": 215.2972460697361,
          "standard_error": 76.39233307667979
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14141.363893863896,
            "upper_bound": 14173.93056555622
          },
          "point_estimate": 14157.45351577095,
          "standard_error": 8.329147091786032
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14133.305871654977,
            "upper_bound": 14179.060994738897
          },
          "point_estimate": 14153.42209788566,
          "standard_error": 10.753309029673826
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.260681226426588,
            "upper_bound": 50.57632550797437
          },
          "point_estimate": 28.715817512732407,
          "standard_error": 11.934720504274926
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14141.828863229097,
            "upper_bound": 14164.954997220551
          },
          "point_estimate": 14152.560561184724,
          "standard_error": 5.800477544643453
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.736215625938634,
            "upper_bound": 34.709372330173494
          },
          "point_estimate": 27.70474814401386,
          "standard_error": 4.854115628560735
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23189.502604922585,
            "upper_bound": 23241.867170282916
          },
          "point_estimate": 23215.136639392837,
          "standard_error": 13.301096573471009
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23186.982423192632,
            "upper_bound": 23233.21229525633
          },
          "point_estimate": 23218.75114869177,
          "standard_error": 13.081853150149112
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.788646532728423,
            "upper_bound": 71.80974459301854
          },
          "point_estimate": 27.92574615386859,
          "standard_error": 16.81562701930126
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23198.716488322505,
            "upper_bound": 23224.435493399895
          },
          "point_estimate": 23212.05886340845,
          "standard_error": 6.52769653185924
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.811523010042883,
            "upper_bound": 61.60648615641522
          },
          "point_estimate": 44.34802858493705,
          "standard_error": 11.509339927122314
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17833.62857540072,
            "upper_bound": 17862.525862365066
          },
          "point_estimate": 17846.659946025517,
          "standard_error": 7.4506715705698605
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17826.9675171737,
            "upper_bound": 17861.202142623486
          },
          "point_estimate": 17843.313256460584,
          "standard_error": 8.479648578885701
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.588851377005967,
            "upper_bound": 35.307230850110365
          },
          "point_estimate": 22.54789884896079,
          "standard_error": 10.013850776757392
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17836.99008626669,
            "upper_bound": 17864.948778694074
          },
          "point_estimate": 17852.18580349974,
          "standard_error": 7.22842875826018
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.700520565596037,
            "upper_bound": 33.70909190066168
          },
          "point_estimate": 24.736231601704823,
          "standard_error": 6.696792984205442
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17468.687809127747,
            "upper_bound": 17493.822841568322
          },
          "point_estimate": 17480.186829013026,
          "standard_error": 6.465530422452094
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17465.514327042292,
            "upper_bound": 17494.9889371049
          },
          "point_estimate": 17470.988771697204,
          "standard_error": 8.196738708180442
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.0299648446274148,
            "upper_bound": 33.1884502276472
          },
          "point_estimate": 14.982525193267955,
          "standard_error": 8.77171558387451
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17470.063522523764,
            "upper_bound": 17495.34974824619
          },
          "point_estimate": 17480.95712532405,
          "standard_error": 6.681635079163249
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.761633274697736,
            "upper_bound": 28.369815118386008
          },
          "point_estimate": 21.5318901904619,
          "standard_error": 5.231112322784722
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17496.60240691703,
            "upper_bound": 17523.703378403683
          },
          "point_estimate": 17510.538188225542,
          "standard_error": 6.962171277568807
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17486.290322580644,
            "upper_bound": 17524.82045418071
          },
          "point_estimate": 17518.774440298508,
          "standard_error": 9.441915259405375
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4309830013982008,
            "upper_bound": 41.677093397063594
          },
          "point_estimate": 13.257551252353284,
          "standard_error": 11.480640235255809
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17501.949335255707,
            "upper_bound": 17520.5232289153
          },
          "point_estimate": 17512.64096817963,
          "standard_error": 4.723740501961848
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.936828684782885,
            "upper_bound": 29.1179495625434
          },
          "point_estimate": 23.29875867179072,
          "standard_error": 4.204138574197437
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30754.069520919853,
            "upper_bound": 30830.277756254793
          },
          "point_estimate": 30790.520926508336,
          "standard_error": 19.51879199902976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30727.606900931412,
            "upper_bound": 30829.26394097012
          },
          "point_estimate": 30785.21532599492,
          "standard_error": 23.30617293717256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.370899438487742,
            "upper_bound": 109.06242672590663
          },
          "point_estimate": 75.35836244281732,
          "standard_error": 27.51081600968652
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30741.909001671,
            "upper_bound": 30802.75785578761
          },
          "point_estimate": 30770.363082133787,
          "standard_error": 16.030421051101754
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.33088332666502,
            "upper_bound": 84.69840491277118
          },
          "point_estimate": 65.32866228799777,
          "standard_error": 13.394941051349695
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32207.91815453344,
            "upper_bound": 32322.9770451446
          },
          "point_estimate": 32253.30414508555,
          "standard_error": 31.374519719519128
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32199.740003796025,
            "upper_bound": 32253.860545221927
          },
          "point_estimate": 32227.129949069975,
          "standard_error": 16.94793431357227
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.69881709087943,
            "upper_bound": 73.27608654010106
          },
          "point_estimate": 36.6874550616896,
          "standard_error": 16.904497508974003
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32205.605415738315,
            "upper_bound": 32246.543175604616
          },
          "point_estimate": 32226.38788952412,
          "standard_error": 10.498950642656506
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.00447368573505,
            "upper_bound": 158.98976058397596
          },
          "point_estimate": 104.48864387209234,
          "standard_error": 46.26793875471021
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53145.67648208184,
            "upper_bound": 53271.00886267255
          },
          "point_estimate": 53205.91433934091,
          "standard_error": 32.07098278949822
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53107.00073206442,
            "upper_bound": 53322.55534407028
          },
          "point_estimate": 53177.55128517976,
          "standard_error": 49.54103099342438
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.152127964705898,
            "upper_bound": 175.31440788314788
          },
          "point_estimate": 106.47010430792255,
          "standard_error": 41.367069508279286
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53139.84604312976,
            "upper_bound": 53276.70197081133
          },
          "point_estimate": 53207.31373809207,
          "standard_error": 36.45558470322549
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.43128597341059,
            "upper_bound": 129.55673072943682
          },
          "point_estimate": 106.77009773173582,
          "standard_error": 18.31349104581111
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17599.76132148056,
            "upper_bound": 17619.877218773825
          },
          "point_estimate": 17609.883042867157,
          "standard_error": 5.1252960226282935
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17597.975315380885,
            "upper_bound": 17623.4477080012
          },
          "point_estimate": 17610.933586716266,
          "standard_error": 4.9808919329399295
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3414581502463885,
            "upper_bound": 32.45909019278041
          },
          "point_estimate": 9.810589185359516,
          "standard_error": 8.415613425581762
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17605.467148645286,
            "upper_bound": 17618.363643721736
          },
          "point_estimate": 17612.23044417979,
          "standard_error": 3.2462602371803264
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.58555989806217,
            "upper_bound": 22.22391899683397
          },
          "point_estimate": 17.045942985383192,
          "standard_error": 3.5066889148941573
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18160.386288323047,
            "upper_bound": 18187.979225621457
          },
          "point_estimate": 18173.604701646087,
          "standard_error": 7.0800382722330575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18155.55677900122,
            "upper_bound": 18193.94874874875
          },
          "point_estimate": 18165.708667000337,
          "standard_error": 9.795328142565095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.811437044443152,
            "upper_bound": 37.32958679472546
          },
          "point_estimate": 20.12677697000668,
          "standard_error": 9.453935725316583
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18152.495670266344,
            "upper_bound": 18176.959821065084
          },
          "point_estimate": 18161.198533598537,
          "standard_error": 6.303819619527419
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.3451136172452,
            "upper_bound": 28.59403351819078
          },
          "point_estimate": 23.558502828762897,
          "standard_error": 4.153048482304151
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86248.06995778454,
            "upper_bound": 86403.52608840857
          },
          "point_estimate": 86324.48997223162,
          "standard_error": 39.78898360961153
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86200.59405606214,
            "upper_bound": 86458.9206724455
          },
          "point_estimate": 86326.27620173365,
          "standard_error": 67.84521416250651
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.53387509634787,
            "upper_bound": 217.12413742187516
          },
          "point_estimate": 188.7733462743014,
          "standard_error": 56.95673614083853
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86237.95295905162,
            "upper_bound": 86445.82939576502
          },
          "point_estimate": 86359.81307297903,
          "standard_error": 53.1888327419041
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.42952370137587,
            "upper_bound": 157.3166135803527
          },
          "point_estimate": 132.4742031874633,
          "standard_error": 19.22812232545585
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32645.361287633223,
            "upper_bound": 32696.889747652447
          },
          "point_estimate": 32670.013528960037,
          "standard_error": 13.22703092803386
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32634.03116561656,
            "upper_bound": 32706.79174167417
          },
          "point_estimate": 32651.543654365436,
          "standard_error": 24.2011263096294
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.950970539164816,
            "upper_bound": 72.99897049968945
          },
          "point_estimate": 35.82401319537989,
          "standard_error": 19.684008058734364
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32638.968787429923,
            "upper_bound": 32722.302543471775
          },
          "point_estimate": 32678.8941751318,
          "standard_error": 22.676595673906952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.371591870220573,
            "upper_bound": 54.59062660883187
          },
          "point_estimate": 44.088091648562575,
          "standard_error": 7.768320043000008
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73688.02939864452,
            "upper_bound": 73838.21460253067
          },
          "point_estimate": 73762.52804179143,
          "standard_error": 38.419737536183575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73652.08265720081,
            "upper_bound": 73879.86409736308
          },
          "point_estimate": 73773.62305611899,
          "standard_error": 53.0164479928527
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.780965194016396,
            "upper_bound": 242.3068738658519
          },
          "point_estimate": 125.94848597896492,
          "standard_error": 57.291963122633504
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73638.91818568061,
            "upper_bound": 73789.91940901951
          },
          "point_estimate": 73707.12569215774,
          "standard_error": 38.37917406142415
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.55482344517435,
            "upper_bound": 159.07708044581148
          },
          "point_estimate": 128.24353306522278,
          "standard_error": 21.80210885310878
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18508.37305550899,
            "upper_bound": 18541.001332199543
          },
          "point_estimate": 18523.810914804017,
          "standard_error": 8.359255122735718
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18498.70642857143,
            "upper_bound": 18548.245663265305
          },
          "point_estimate": 18518.29584953029,
          "standard_error": 11.017361750101266
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.771336667741516,
            "upper_bound": 44.98140241570676
          },
          "point_estimate": 30.79104100266924,
          "standard_error": 11.10420722163818
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18502.4413842239,
            "upper_bound": 18531.76062481865
          },
          "point_estimate": 18515.053293135436,
          "standard_error": 7.582807530060101
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.38892956771963,
            "upper_bound": 35.70049454452205
          },
          "point_estimate": 27.799589805096907,
          "standard_error": 5.705602043251524
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22410.559437233904,
            "upper_bound": 22449.945735475896
          },
          "point_estimate": 22429.305655473152,
          "standard_error": 10.104415232858718
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22405.85435723115,
            "upper_bound": 22449.935929130617
          },
          "point_estimate": 22422.85235544568,
          "standard_error": 10.083257266508488
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8768130046249468,
            "upper_bound": 58.94396295229964
          },
          "point_estimate": 23.984276953674183,
          "standard_error": 15.490709584768071
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22407.750453381697,
            "upper_bound": 22438.18763189495
          },
          "point_estimate": 22420.44682548601,
          "standard_error": 7.657206961581124
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.670392955504946,
            "upper_bound": 43.36019328538326
          },
          "point_estimate": 33.677320110499956,
          "standard_error": 7.139467425733016
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35447.40006986022,
            "upper_bound": 35525.06660403904
          },
          "point_estimate": 35484.852135740584,
          "standard_error": 19.833490349992687
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35433.17042606516,
            "upper_bound": 35533.58382066277
          },
          "point_estimate": 35476.56997238466,
          "standard_error": 22.814852917126597
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.328486085081199,
            "upper_bound": 110.9993771960326
          },
          "point_estimate": 70.7018302993689,
          "standard_error": 24.883863981959134
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35421.664039065494,
            "upper_bound": 35498.09593164475
          },
          "point_estimate": 35461.443107769424,
          "standard_error": 20.166763128612754
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.876469302927255,
            "upper_bound": 88.25005512876822
          },
          "point_estimate": 66.1711387713639,
          "standard_error": 14.551808078128955
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector-freq_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 380623.5825868056,
            "upper_bound": 381394.5022560763
          },
          "point_estimate": 380948.4465219907,
          "standard_error": 201.39542535572457
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 380512.91041666665,
            "upper_bound": 381050.8302083333
          },
          "point_estimate": 380915.40625,
          "standard_error": 171.12292918280926
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.33447511417832,
            "upper_bound": 756.9831146859065
          },
          "point_estimate": 379.36155524416233,
          "standard_error": 178.0535690330412
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 380548.32005378854,
            "upper_bound": 381011.4855072464
          },
          "point_estimate": 380740.3937229437,
          "standard_error": 120.61289855444107
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.87674925201057,
            "upper_bound": 991.1750660770095
          },
          "point_estimate": 671.2441711785066,
          "standard_error": 246.39961970182816
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector-repeated_ra"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2312031.287952443,
            "upper_bound": 2315122.5887133554
          },
          "point_estimate": 2313620.518201885,
          "standard_error": 793.4092267019122
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2311628.6875,
            "upper_bound": 2315871.0
          },
          "point_estimate": 2313786.479464286,
          "standard_error": 1158.171266901708
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 833.5094685356589,
            "upper_bound": 4573.677291926273
          },
          "point_estimate": 3074.165254016407,
          "standard_error": 974.591365663032
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2311589.979262673,
            "upper_bound": 2315681.697422986
          },
          "point_estimate": 2313720.1435064934,
          "standard_error": 1057.625083974211
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1479.7164871149985,
            "upper_bound": 3238.071834016117
          },
          "point_estimate": 2638.4906222216655,
          "standard_error": 443.0301836897072
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120213.46780632823,
            "upper_bound": 120498.7118445178
          },
          "point_estimate": 120341.18460500814,
          "standard_error": 73.95469797102204
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120164.46732673269,
            "upper_bound": 120486.77062706272
          },
          "point_estimate": 120281.21471004243,
          "standard_error": 58.4950894133148
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.892950697708892,
            "upper_bound": 309.8060380190851
          },
          "point_estimate": 111.1834721295079,
          "standard_error": 86.00937325833539
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120227.18412758004,
            "upper_bound": 120326.03991677154
          },
          "point_estimate": 120279.92474390296,
          "standard_error": 24.823937447745184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.61673219430251,
            "upper_bound": 330.33018303026626
          },
          "point_estimate": 246.70495381241437,
          "standard_error": 70.48336108151356
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/sliceslice_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10941.39324933982,
            "upper_bound": 10969.848532549728
          },
          "point_estimate": 10954.66842029287,
          "standard_error": 7.311007465024555
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10942.70185352622,
            "upper_bound": 10977.775835510012
          },
          "point_estimate": 10944.610106489854,
          "standard_error": 8.699854756315041
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3026778125890073,
            "upper_bound": 38.12178252292282
          },
          "point_estimate": 3.0287835658794164,
          "standard_error": 11.836552396720643
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10937.002222353976,
            "upper_bound": 10970.316110935091
          },
          "point_estimate": 10952.51170240248,
          "standard_error": 8.499215916733112
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.578322140375327,
            "upper_bound": 31.686322122112824
          },
          "point_estimate": 24.4786048080929,
          "standard_error": 5.615554344083949
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/sliceslice_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10694.161061903827,
            "upper_bound": 10718.431138651822
          },
          "point_estimate": 10705.824545133924,
          "standard_error": 6.226019298693949
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10690.678597704533,
            "upper_bound": 10721.631288713248
          },
          "point_estimate": 10699.12095350206,
          "standard_error": 9.854577165891149
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8824477829416832,
            "upper_bound": 35.884547832610025
          },
          "point_estimate": 25.066032986419813,
          "standard_error": 8.830219636811899
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10689.286284078626,
            "upper_bound": 10722.43496697296
          },
          "point_estimate": 10701.9898656964,
          "standard_error": 8.316611321824997
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.700008597350156,
            "upper_bound": 26.536191076763085
          },
          "point_estimate": 20.760107581216104,
          "standard_error": 3.941669835729466
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/sliceslice_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14231.371768205452,
            "upper_bound": 14257.75082518846
          },
          "point_estimate": 14243.712372306813,
          "standard_error": 6.798532036784205
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14223.98584987809,
            "upper_bound": 14257.79154304747
          },
          "point_estimate": 14239.945699451411,
          "standard_error": 9.207368794560956
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.418229255418591,
            "upper_bound": 36.64692880667586
          },
          "point_estimate": 23.110901751403045,
          "standard_error": 8.513170561476361
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14232.64879695729,
            "upper_bound": 14257.84097769756
          },
          "point_estimate": 14242.207839026178,
          "standard_error": 6.39279515702389
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.097884160145172,
            "upper_bound": 29.64479129807209
          },
          "point_estimate": 22.681110646420635,
          "standard_error": 4.986114236098451
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/sliceslice_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.312183071446576,
            "upper_bound": 35.346539451550704
          },
          "point_estimate": 35.32636698282505,
          "standard_error": 0.0090328693264132
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.314449211771354,
            "upper_bound": 35.32918847192325
          },
          "point_estimate": 35.31758559457421,
          "standard_error": 0.004917010063746321
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006488764893353474,
            "upper_bound": 0.02670068906284249
          },
          "point_estimate": 0.009447821444618537,
          "standard_error": 0.0068609407201603925
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.31590704994431,
            "upper_bound": 35.32559046864729
          },
          "point_estimate": 35.32060833786692,
          "standard_error": 0.0025519601887407824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005930348926728736,
            "upper_bound": 0.04520837712344771
          },
          "point_estimate": 0.03025444029116664,
          "standard_error": 0.012327695130270886
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.218197547166316,
            "upper_bound": 15.230830062209742
          },
          "point_estimate": 15.223697545684184,
          "standard_error": 0.0032791447120020154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.216873774018495,
            "upper_bound": 15.228734565292172
          },
          "point_estimate": 15.219508249769206,
          "standard_error": 0.003177700481580809
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011025146932513226,
            "upper_bound": 0.013925975825845772
          },
          "point_estimate": 0.004722305445608965,
          "standard_error": 0.0035722068552056455
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.216937661353011,
            "upper_bound": 15.223144172417692
          },
          "point_estimate": 15.220146024126134,
          "standard_error": 0.0015542874479734922
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003625788740873378,
            "upper_bound": 0.01554165708809449
          },
          "point_estimate": 0.010943406810750178,
          "standard_error": 0.003474002311433472
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.68996993064671,
            "upper_bound": 16.69713917587072
          },
          "point_estimate": 16.693438342604715,
          "standard_error": 0.0018400829227821652
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.68814032821383,
            "upper_bound": 16.69893717804171
          },
          "point_estimate": 16.691985091241335,
          "standard_error": 0.0027178972941064444
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001410283214095182,
            "upper_bound": 0.010413556208622929
          },
          "point_estimate": 0.006737779065960848,
          "standard_error": 0.0023301964842937437
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.689669480193434,
            "upper_bound": 16.699130940103355
          },
          "point_estimate": 16.694852733772624,
          "standard_error": 0.002395849082586564
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0033829412846418587,
            "upper_bound": 0.007456966197734077
          },
          "point_estimate": 0.006136226254342773,
          "standard_error": 0.0010155622190537515
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.537323280782855,
            "upper_bound": 11.54388680655854
          },
          "point_estimate": 11.540256071822276,
          "standard_error": 0.001695505337476522
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.537952514439509,
            "upper_bound": 11.542136823745595
          },
          "point_estimate": 11.538451395911787,
          "standard_error": 0.0011482015545775943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00014978022832728353,
            "upper_bound": 0.008199461474205008
          },
          "point_estimate": 0.0015442959736834778,
          "standard_error": 0.0020105498656169353
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.538014862763507,
            "upper_bound": 11.541059306751734
          },
          "point_estimate": 11.539313651535103,
          "standard_error": 0.0007910165818527125
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001381423996494274,
            "upper_bound": 0.008060862332574336
          },
          "point_estimate": 0.005633262170904357,
          "standard_error": 0.0018461733694256984
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.483377006916909,
            "upper_bound": 10.497949221490456
          },
          "point_estimate": 10.489444993750908,
          "standard_error": 0.003817804447358165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.481879683807728,
            "upper_bound": 10.490144797729563
          },
          "point_estimate": 10.48819340600858,
          "standard_error": 0.002078006068712531
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005783503117958689,
            "upper_bound": 0.012014716359022489
          },
          "point_estimate": 0.003310418617358748,
          "standard_error": 0.0031389719192229286
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.483614418229374,
            "upper_bound": 10.489645969572466
          },
          "point_estimate": 10.48670746865842,
          "standard_error": 0.0015300724893147033
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028501986139502894,
            "upper_bound": 0.018983600306855815
          },
          "point_estimate": 0.01273886356133278,
          "standard_error": 0.0050166537065716435
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.285022302547697,
            "upper_bound": 11.31750138611827
          },
          "point_estimate": 11.302134149158729,
          "standard_error": 0.008336432928527714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.278637821964162,
            "upper_bound": 11.32548895853893
          },
          "point_estimate": 11.315683391494334,
          "standard_error": 0.014235752190045884
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002020828096785519,
            "upper_bound": 0.04810999210618188
          },
          "point_estimate": 0.01655856153900753,
          "standard_error": 0.012880534114473034
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.292291115567297,
            "upper_bound": 11.321866673380176
          },
          "point_estimate": 11.309240151095718,
          "standard_error": 0.007549049698867437
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01469594438788206,
            "upper_bound": 0.03514043381398561
          },
          "point_estimate": 0.02777458800989731,
          "standard_error": 0.005450846693179092
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.086949039081713,
            "upper_bound": 26.10970234545144
          },
          "point_estimate": 26.097321799379916,
          "standard_error": 0.0058138911947011465
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.084728066608857,
            "upper_bound": 26.10535322487799
          },
          "point_estimate": 26.093690490478117,
          "standard_error": 0.0054045105613232045
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001836458071807583,
            "upper_bound": 0.027956967366636266
          },
          "point_estimate": 0.015198663763189424,
          "standard_error": 0.006617186316245393
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.08580174927432,
            "upper_bound": 26.095905345212493
          },
          "point_estimate": 26.09102302555154,
          "standard_error": 0.0025340047709957883
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007765500795182493,
            "upper_bound": 0.027060498665357854
          },
          "point_estimate": 0.01941145999352275,
          "standard_error": 0.005436072545901082
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.067995040854576,
            "upper_bound": 14.075897229404855
          },
          "point_estimate": 14.071659300653982,
          "standard_error": 0.0020282869280333826
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.06738310567361,
            "upper_bound": 14.07568994597982
          },
          "point_estimate": 14.07045028660785,
          "standard_error": 0.0017347195112780538
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007145388120726633,
            "upper_bound": 0.010338689144221364
          },
          "point_estimate": 0.0045874861889302,
          "standard_error": 0.0028044154622992815
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.069712403482129,
            "upper_bound": 14.072883092624188
          },
          "point_estimate": 14.071139925965785,
          "standard_error": 0.0007986555201485863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0026597317935924017,
            "upper_bound": 0.009359517791126057
          },
          "point_estimate": 0.006753847749765448,
          "standard_error": 0.0018285518566953055
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.310313003618482,
            "upper_bound": 17.33569268425428
          },
          "point_estimate": 17.320401030484778,
          "standard_error": 0.006865816398014963
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.30669624294766,
            "upper_bound": 17.322892541253207
          },
          "point_estimate": 17.314059034360703,
          "standard_error": 0.004466241794856807
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001597516622604242,
            "upper_bound": 0.01871211030689148
          },
          "point_estimate": 0.010796055733060752,
          "standard_error": 0.004370621756101556
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.308907421474842,
            "upper_bound": 17.31965506904456
          },
          "point_estimate": 17.31363221705835,
          "standard_error": 0.0027678912265191834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00514783776671226,
            "upper_bound": 0.03461936555297426
          },
          "point_estimate": 0.022893399907587632,
          "standard_error": 0.009691017609601064
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.728450866130874,
            "upper_bound": 16.783041254642054
          },
          "point_estimate": 16.754508609649008,
          "standard_error": 0.013803940923901643
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.720976992766552,
            "upper_bound": 16.796873697196286
          },
          "point_estimate": 16.723945192530493,
          "standard_error": 0.022590452801851076
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009676672887269596,
            "upper_bound": 0.08031129192746914
          },
          "point_estimate": 0.008039783995117582,
          "standard_error": 0.024872821477585932
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.721649358725628,
            "upper_bound": 16.741711221105327
          },
          "point_estimate": 16.727251505808894,
          "standard_error": 0.005380584624779307
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019952530248322248,
            "upper_bound": 0.05573514604945415
          },
          "point_estimate": 0.045935492702387073,
          "standard_error": 0.009024862480009598
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.337340654597234,
            "upper_bound": 17.40468598864417
          },
          "point_estimate": 17.370439528943464,
          "standard_error": 0.01728240886948149
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.314607465590353,
            "upper_bound": 17.43014625783884
          },
          "point_estimate": 17.358586650150656,
          "standard_error": 0.030404596799127843
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009081436600245098,
            "upper_bound": 0.09322232223001324
          },
          "point_estimate": 0.07348975430756577,
          "standard_error": 0.02201899702179317
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.334307625056834,
            "upper_bound": 17.409450591512023
          },
          "point_estimate": 17.376391775887733,
          "standard_error": 0.018932836783620975
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.036454642761168214,
            "upper_bound": 0.06811925644126422
          },
          "point_estimate": 0.05785632599979832,
          "standard_error": 0.008118495861835353
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.176476580147382,
            "upper_bound": 14.218055016186272
          },
          "point_estimate": 14.19747285119836,
          "standard_error": 0.010667867611226033
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.165522800534726,
            "upper_bound": 14.235042783513622
          },
          "point_estimate": 14.199914422356915,
          "standard_error": 0.017401281952076693
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008351098581179188,
            "upper_bound": 0.05966097594901866
          },
          "point_estimate": 0.0515351624673237,
          "standard_error": 0.013271834446907488
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.185173029454768,
            "upper_bound": 14.234700690929651
          },
          "point_estimate": 14.218531375970093,
          "standard_error": 0.012723850002501652
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0220500470471369,
            "upper_bound": 0.042735547846278234
          },
          "point_estimate": 0.03563700582865573,
          "standard_error": 0.005259488687132065
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.702393629105604,
            "upper_bound": 22.75539992356082
          },
          "point_estimate": 22.725016591426037,
          "standard_error": 0.013807443157343294
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.69609756142046,
            "upper_bound": 22.74083982746245
          },
          "point_estimate": 22.711344053578298,
          "standard_error": 0.013923898118602555
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004249414390322491,
            "upper_bound": 0.05742679051725404
          },
          "point_estimate": 0.02890579968598592,
          "standard_error": 0.012640524134206485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.696411807475116,
            "upper_bound": 22.722396238305173
          },
          "point_estimate": 22.706413603905343,
          "standard_error": 0.006633727263529287
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016092475328028006,
            "upper_bound": 0.0672916211033044
          },
          "point_estimate": 0.04612276537936631,
          "standard_error": 0.016006076754553455
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51902.15758714284,
            "upper_bound": 51981.14443333333
          },
          "point_estimate": 51941.75103707482,
          "standard_error": 20.18389549677451
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51894.90238095238,
            "upper_bound": 51979.18321428572
          },
          "point_estimate": 51951.62335714286,
          "standard_error": 22.3290485696634
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.917015038421916,
            "upper_bound": 123.47939780780708
          },
          "point_estimate": 43.16753968361856,
          "standard_error": 27.792087191977977
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51902.71031152068,
            "upper_bound": 52003.49462406015
          },
          "point_estimate": 51947.89312801484,
          "standard_error": 25.758685102634743
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.288207554065714,
            "upper_bound": 89.3871106393486
          },
          "point_estimate": 67.27988071964033,
          "standard_error": 14.542177676904643
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48892.38966292334,
            "upper_bound": 49291.55932009511
          },
          "point_estimate": 49067.98954276718,
          "standard_error": 101.76709141312364
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48871.563941906046,
            "upper_bound": 49282.00944669366
          },
          "point_estimate": 48909.681376518216,
          "standard_error": 96.33963160872332
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.913464384680356,
            "upper_bound": 484.26997277901006
          },
          "point_estimate": 61.48545315942776,
          "standard_error": 104.304432451977
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48852.34889472569,
            "upper_bound": 48960.19714660303
          },
          "point_estimate": 48891.11405787195,
          "standard_error": 27.971856994205996
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.08925169293929,
            "upper_bound": 428.3823942604579
          },
          "point_estimate": 338.6901001199928,
          "standard_error": 98.44169579760124
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50515.37158403109,
            "upper_bound": 50644.53512610259
          },
          "point_estimate": 50581.082757936514,
          "standard_error": 33.00673982474419
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50493.19497833488,
            "upper_bound": 50667.59145775302
          },
          "point_estimate": 50595.43257063271,
          "standard_error": 49.83168016765262
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.04032160876939,
            "upper_bound": 179.71999102939137
          },
          "point_estimate": 125.06606149133265,
          "standard_error": 37.85076452477773
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50462.18530895585,
            "upper_bound": 50581.907881579886
          },
          "point_estimate": 50513.332644792536,
          "standard_error": 30.201261967742465
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.89547992935486,
            "upper_bound": 136.4418852685623
          },
          "point_estimate": 110.01578309589418,
          "standard_error": 18.348708773949813
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13990.27642150699,
            "upper_bound": 14019.600590817374
          },
          "point_estimate": 14003.735642273215,
          "standard_error": 7.535433477703829
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13985.769782036466,
            "upper_bound": 14021.095377503852
          },
          "point_estimate": 13993.64285408565,
          "standard_error": 8.60363388262632
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.736832679648442,
            "upper_bound": 39.0442197228524
          },
          "point_estimate": 18.57737935046342,
          "standard_error": 9.304444222512586
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13989.50761560668,
            "upper_bound": 14022.117558867354
          },
          "point_estimate": 14004.542200788426,
          "standard_error": 9.203151781175704
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.559735114318643,
            "upper_bound": 32.687771197819394
          },
          "point_estimate": 25.121721892734012,
          "standard_error": 6.010994623680296
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23317.56876033479,
            "upper_bound": 23354.341280239776
          },
          "point_estimate": 23334.285564781683,
          "standard_error": 9.490843417556794
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23309.30109184329,
            "upper_bound": 23351.159464171025
          },
          "point_estimate": 23326.43188824663,
          "standard_error": 9.872173993626708
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.179618570442772,
            "upper_bound": 46.12361021303917
          },
          "point_estimate": 26.699754950949146,
          "standard_error": 10.774508228156495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23313.98319588267,
            "upper_bound": 23329.801714454978
          },
          "point_estimate": 23321.33041563446,
          "standard_error": 4.015194863224595
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.592674650033326,
            "upper_bound": 42.82790442748722
          },
          "point_estimate": 31.74925314095513,
          "standard_error": 8.444993243380633
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17638.767054143013,
            "upper_bound": 17665.987734198432
          },
          "point_estimate": 17652.078173830767,
          "standard_error": 6.980529936777647
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17630.788807785888,
            "upper_bound": 17673.3100243309
          },
          "point_estimate": 17648.721870775888,
          "standard_error": 11.05278521899566
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.268099760486336,
            "upper_bound": 40.789607918176344
          },
          "point_estimate": 29.03127346269478,
          "standard_error": 8.965181436911518
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17637.22799814746,
            "upper_bound": 17664.354312331
          },
          "point_estimate": 17649.82209624925,
          "standard_error": 6.866160293284634
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.813525096572477,
            "upper_bound": 28.03226420902808
          },
          "point_estimate": 23.374855427497536,
          "standard_error": 3.5875564145853143
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17461.027417593417,
            "upper_bound": 17498.445243365903
          },
          "point_estimate": 17478.885569590322,
          "standard_error": 9.58398460759772
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17454.34698862726,
            "upper_bound": 17496.096897858937
          },
          "point_estimate": 17476.079056543327,
          "standard_error": 10.556711025101295
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.1893492639500565,
            "upper_bound": 54.79007499716514
          },
          "point_estimate": 26.458282495191995,
          "standard_error": 12.409156995778034
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17457.224812777964,
            "upper_bound": 17491.672292580064
          },
          "point_estimate": 17478.34609484701,
          "standard_error": 8.862755216868113
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.170719394599583,
            "upper_bound": 43.05665275873398
          },
          "point_estimate": 32.02007262585159,
          "standard_error": 7.449648598319683
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17262.64124655984,
            "upper_bound": 17325.898437020944
          },
          "point_estimate": 17291.060037871932,
          "standard_error": 16.337128491112313
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17252.36515480717,
            "upper_bound": 17325.040684410647
          },
          "point_estimate": 17271.738302703845,
          "standard_error": 15.940072204309011
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.7116237946788875,
            "upper_bound": 80.5041685612649
          },
          "point_estimate": 35.25850341871421,
          "standard_error": 18.1820329834211
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17253.890405500784,
            "upper_bound": 17277.155164622975
          },
          "point_estimate": 17265.14840378253,
          "standard_error": 5.9025077643415305
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.609942756808124,
            "upper_bound": 72.96115202594729
          },
          "point_estimate": 54.30802628344589,
          "standard_error": 14.906989483963605
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30600.456984334254,
            "upper_bound": 30647.20457776959
          },
          "point_estimate": 30621.27027149276,
          "standard_error": 12.091848172846015
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30592.479921370403,
            "upper_bound": 30639.179191238414
          },
          "point_estimate": 30611.550463352993,
          "standard_error": 12.31566035918011
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3642375317427624,
            "upper_bound": 53.65410022268009
          },
          "point_estimate": 29.114374832740513,
          "standard_error": 13.086523550462498
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30592.780901329443,
            "upper_bound": 30618.336009223854
          },
          "point_estimate": 30603.809744089103,
          "standard_error": 6.502102203906311
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.450896994416068,
            "upper_bound": 56.814024907235385
          },
          "point_estimate": 40.56965846789057,
          "standard_error": 12.119023583766468
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32140.96863002879,
            "upper_bound": 32199.264402654862
          },
          "point_estimate": 32169.09677672426,
          "standard_error": 14.895597145583098
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32129.672308259585,
            "upper_bound": 32199.329469026547
          },
          "point_estimate": 32162.13796179239,
          "standard_error": 18.14631677911221
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.206616703916703,
            "upper_bound": 81.3084253706482
          },
          "point_estimate": 50.65139222169907,
          "standard_error": 20.36360340667371
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32147.15497156701,
            "upper_bound": 32183.148591844056
          },
          "point_estimate": 32166.960135616595,
          "standard_error": 9.325081403580924
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.31369083854974,
            "upper_bound": 65.62572529911782
          },
          "point_estimate": 49.512378730039345,
          "standard_error": 10.709656426111206
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51305.02498975754,
            "upper_bound": 51396.28498119417
          },
          "point_estimate": 51345.22009184633,
          "standard_error": 23.620762378383738
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51293.107663375646,
            "upper_bound": 51374.79689703808
          },
          "point_estimate": 51329.82667489422,
          "standard_error": 17.17061567168696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.106184182851641,
            "upper_bound": 103.71478377223158
          },
          "point_estimate": 43.300353392052926,
          "standard_error": 25.44798346102581
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51297.61420430468,
            "upper_bound": 51372.01440806074
          },
          "point_estimate": 51336.44798051032,
          "standard_error": 18.906813454456604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.035814563891563,
            "upper_bound": 111.61835094900349
          },
          "point_estimate": 78.58886989483528,
          "standard_error": 25.14984998843684
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17743.310008742377,
            "upper_bound": 17760.085519772903
          },
          "point_estimate": 17751.080827922076,
          "standard_error": 4.318181365788893
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17741.042471081786,
            "upper_bound": 17761.592823395244
          },
          "point_estimate": 17747.3137829912,
          "standard_error": 4.084572547214994
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9845293880315615,
            "upper_bound": 23.861363249395932
          },
          "point_estimate": 7.797538668995696,
          "standard_error": 5.402762760517435
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17741.101474563093,
            "upper_bound": 17748.816532095145
          },
          "point_estimate": 17745.235243934952,
          "standard_error": 1.964225076445335
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.285336938003232,
            "upper_bound": 18.738180235086645
          },
          "point_estimate": 14.402754786577985,
          "standard_error": 3.476936591652667
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18155.066992719894,
            "upper_bound": 18210.053151728527
          },
          "point_estimate": 18176.88712986832,
          "standard_error": 15.000724427383313
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18153.654887218043,
            "upper_bound": 18178.889373433583
          },
          "point_estimate": 18159.527656442697,
          "standard_error": 7.203316833541833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6705528592300694,
            "upper_bound": 35.99895793983582
          },
          "point_estimate": 9.079150254438552,
          "standard_error": 9.893128876910025
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18154.487483535264,
            "upper_bound": 18162.736020114746
          },
          "point_estimate": 18157.550007486247,
          "standard_error": 2.1357070511427936
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.953739327458757,
            "upper_bound": 75.82228206763006
          },
          "point_estimate": 50.20712943960324,
          "standard_error": 21.947522343565463
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85358.26710447243,
            "upper_bound": 85479.20890457516
          },
          "point_estimate": 85420.68432754434,
          "standard_error": 30.95748796449901
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85330.39882352941,
            "upper_bound": 85497.48611764706
          },
          "point_estimate": 85433.88549019607,
          "standard_error": 37.50968053871967
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.592381362822907,
            "upper_bound": 172.55804178354913
          },
          "point_estimate": 123.63127916981558,
          "standard_error": 44.19905715742382
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85355.89937335397,
            "upper_bound": 85439.63994227587
          },
          "point_estimate": 85399.07309396486,
          "standard_error": 21.15058630623364
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.5905409345057,
            "upper_bound": 132.72642236380997
          },
          "point_estimate": 103.1901959152937,
          "standard_error": 20.041067210689505
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32721.886017655157,
            "upper_bound": 32763.80384820535
          },
          "point_estimate": 32741.351628986136,
          "standard_error": 10.752307418657583
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32715.341666666667,
            "upper_bound": 32765.361055341054
          },
          "point_estimate": 32729.4253003003,
          "standard_error": 12.814397390847764
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.820256695736288,
            "upper_bound": 57.35600505380297
          },
          "point_estimate": 27.392866297462625,
          "standard_error": 12.942154995151409
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32716.085494942497,
            "upper_bound": 32746.125680106335
          },
          "point_estimate": 32727.03572715573,
          "standard_error": 7.698850735658381
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.02272767855623,
            "upper_bound": 46.39381342323185
          },
          "point_estimate": 35.83453828744669,
          "standard_error": 8.10060337567337
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71800.00353465349,
            "upper_bound": 71907.29779699433
          },
          "point_estimate": 71852.92368387553,
          "standard_error": 27.45629371768933
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71772.26297029703,
            "upper_bound": 71936.77178217823
          },
          "point_estimate": 71852.72253653937,
          "standard_error": 41.584605325832186
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.814729181843695,
            "upper_bound": 161.62431500188734
          },
          "point_estimate": 100.02675626376912,
          "standard_error": 34.11197942182662
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71818.97817296293,
            "upper_bound": 71926.69245083412
          },
          "point_estimate": 71879.18364407869,
          "standard_error": 28.345526975910666
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.847422010001566,
            "upper_bound": 112.0282938935314
          },
          "point_estimate": 91.692800875101,
          "standard_error": 14.556985758406428
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18698.35061692914,
            "upper_bound": 18726.661680091707
          },
          "point_estimate": 18710.430279555523,
          "standard_error": 7.406804280983963
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18695.500600755237,
            "upper_bound": 18723.486032440785
          },
          "point_estimate": 18699.80430827326,
          "standard_error": 6.878513228261569
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2825787619865958,
            "upper_bound": 32.799282525315334
          },
          "point_estimate": 8.003829518048972,
          "standard_error": 7.464447961803714
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18695.41677618222,
            "upper_bound": 18713.226720867897
          },
          "point_estimate": 18702.397597870717,
          "standard_error": 4.656755791731834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.894967105309584,
            "upper_bound": 35.690419245033574
          },
          "point_estimate": 24.719009461083125,
          "standard_error": 8.490839532361537
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22400.72717193972,
            "upper_bound": 22448.724858210197
          },
          "point_estimate": 22424.68225693526,
          "standard_error": 12.296808645000723
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22388.381743094113,
            "upper_bound": 22459.376927822337
          },
          "point_estimate": 22420.263065127347,
          "standard_error": 19.943378597846618
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.8899808663013475,
            "upper_bound": 66.2453111938441
          },
          "point_estimate": 53.86565404060989,
          "standard_error": 15.95350837975552
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22405.657872985423,
            "upper_bound": 22453.207756166008
          },
          "point_estimate": 22424.31467989136,
          "standard_error": 12.167577176675788
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.78637039243511,
            "upper_bound": 50.54829830770571
          },
          "point_estimate": 40.973515176072524,
          "standard_error": 6.586231222321266
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35313.75800175853,
            "upper_bound": 35419.127644742926
          },
          "point_estimate": 35357.9257795363,
          "standard_error": 27.665184816210623
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35297.33576287658,
            "upper_bound": 35380.21720116618
          },
          "point_estimate": 35332.74379656625,
          "standard_error": 22.93675344307884
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.586897717965796,
            "upper_bound": 99.92397237565748
          },
          "point_estimate": 61.04275204552909,
          "standard_error": 21.351398262157623
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35302.66611769559,
            "upper_bound": 35368.301743295444
          },
          "point_estimate": 35335.13925005995,
          "standard_error": 17.3527608445955
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.528435298755397,
            "upper_bound": 136.40457941421877
          },
          "point_estimate": 92.19355613941708,
          "standard_error": 34.20523303634901
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 423381.19190349296,
            "upper_bound": 423907.7504069768
          },
          "point_estimate": 423612.4773440383,
          "standard_error": 136.55614520153796
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 423297.14244186046,
            "upper_bound": 423854.1613372093
          },
          "point_estimate": 423496.5789036545,
          "standard_error": 116.84139319093552
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.58672430387619,
            "upper_bound": 542.8857777261654
          },
          "point_estimate": 236.4248858607594,
          "standard_error": 136.02996179289565
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 423365.2759621476,
            "upper_bound": 423576.80453824473
          },
          "point_estimate": 423470.3209302326,
          "standard_error": 54.07382760582387
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121.87605711584978,
            "upper_bound": 623.2923152862554
          },
          "point_estimate": 454.0181511128512,
          "standard_error": 138.16298889890794
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2461065.2373761903,
            "upper_bound": 2465915.896397619
          },
          "point_estimate": 2463299.5694047627,
          "standard_error": 1245.8189451174303
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2460031.56,
            "upper_bound": 2466985.6083333334
          },
          "point_estimate": 2462235.773611111,
          "standard_error": 1548.7140020212971
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397.7041482729387,
            "upper_bound": 6502.121332064294
          },
          "point_estimate": 3047.662157893001,
          "standard_error": 1526.5548735603331
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2460434.225273632,
            "upper_bound": 2463280.681530537
          },
          "point_estimate": 2461753.7903030305,
          "standard_error": 733.9728929445262
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1608.4010602195522,
            "upper_bound": 5185.102174686911
          },
          "point_estimate": 4140.654228145199,
          "standard_error": 928.0658210137852
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120103.46580098484,
            "upper_bound": 120353.5474306669
          },
          "point_estimate": 120222.57748336738,
          "standard_error": 64.15862680625493
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120025.85665566556,
            "upper_bound": 120364.33190319032
          },
          "point_estimate": 120236.75991408664,
          "standard_error": 98.59560462185856
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.08736139680871,
            "upper_bound": 388.647653441152
          },
          "point_estimate": 285.31250123831916,
          "standard_error": 95.77578242436851
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120096.66289486091,
            "upper_bound": 120293.90366491913
          },
          "point_estimate": 120212.9329218636,
          "standard_error": 50.59660071495736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120.25583932841532,
            "upper_bound": 273.55584302100164
          },
          "point_estimate": 213.5674910441295,
          "standard_error": 41.6290764574115
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10735.77400985088,
            "upper_bound": 10759.238950516983
          },
          "point_estimate": 10746.942484502122,
          "standard_error": 6.009945106363981
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10726.964135893648,
            "upper_bound": 10762.861546036434
          },
          "point_estimate": 10744.953940177253,
          "standard_error": 7.832860420618785
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5088087334581101,
            "upper_bound": 33.57278100613157
          },
          "point_estimate": 20.660094506918274,
          "standard_error": 8.897228714685305
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10729.269217415476,
            "upper_bound": 10746.313938287256
          },
          "point_estimate": 10735.30749179919,
          "standard_error": 4.297688314911416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.651745318582822,
            "upper_bound": 25.48208796554047
          },
          "point_estimate": 20.078242800905567,
          "standard_error": 4.133559428332962
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10616.050792107197,
            "upper_bound": 10638.29342250254
          },
          "point_estimate": 10628.006115377304,
          "standard_error": 5.730611689300533
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10612.671360405104,
            "upper_bound": 10643.026351153958
          },
          "point_estimate": 10635.415043658908,
          "standard_error": 8.15693613674324
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8361173157327164,
            "upper_bound": 32.977367696744
          },
          "point_estimate": 14.61644358849642,
          "standard_error": 7.805768551496331
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10622.697642170044,
            "upper_bound": 10637.233936135804
          },
          "point_estimate": 10629.719057862969,
          "standard_error": 3.767800943260123
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.433357770510698,
            "upper_bound": 24.597140148920296
          },
          "point_estimate": 19.086093552613388,
          "standard_error": 4.207355641287353
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14223.65252883318,
            "upper_bound": 14251.633381424905
          },
          "point_estimate": 14236.952687097037,
          "standard_error": 7.162084746480608
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14216.47277712495,
            "upper_bound": 14253.367746628615
          },
          "point_estimate": 14233.28365865866,
          "standard_error": 11.623874488493495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.418977083838702,
            "upper_bound": 44.69603166829808
          },
          "point_estimate": 25.87340047746397,
          "standard_error": 9.994060756644366
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14224.763190444071,
            "upper_bound": 14248.879011620316
          },
          "point_estimate": 14238.78621433404,
          "standard_error": 6.137926787856904
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.317843087022997,
            "upper_bound": 30.72788241021276
          },
          "point_estimate": 23.81463508717523,
          "standard_error": 4.785246447215482
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.029901576430795,
            "upper_bound": 28.208985534185313
          },
          "point_estimate": 28.124172581231164,
          "standard_error": 0.04595302390063027
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.973623581733463,
            "upper_bound": 28.243628154330736
          },
          "point_estimate": 28.20205139539283,
          "standard_error": 0.07520808820990009
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010385638638769044,
            "upper_bound": 0.2496531202516896
          },
          "point_estimate": 0.10171335731469462,
          "standard_error": 0.06785735528319514
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.10486408617943,
            "upper_bound": 28.24069178216715
          },
          "point_estimate": 28.17739623101823,
          "standard_error": 0.037033626011219824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07672175325597351,
            "upper_bound": 0.18230357242717715
          },
          "point_estimate": 0.1532058950037685,
          "standard_error": 0.02660138882680227
        }
      }
    },
    "memmem/sliceslice/prebuilt/sliceslice-haystack/words": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/sliceslice-haystack/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/sliceslice-haystack/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/sliceslice-haystack/words",
        "directory_name": "memmem/sliceslice_prebuilt_sliceslice-haystack_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170517.47375382853,
            "upper_bound": 170843.0949154371
          },
          "point_estimate": 170680.3433201058,
          "standard_error": 83.01498273068269
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170494.2581824279,
            "upper_bound": 170879.75899843505
          },
          "point_estimate": 170655.41105894628,
          "standard_error": 105.76612646301405
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.00644360357675,
            "upper_bound": 466.3570131407045
          },
          "point_estimate": 301.2224835997815,
          "standard_error": 108.96735446407712
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170450.90441654436,
            "upper_bound": 170760.6644843938
          },
          "point_estimate": 170594.83407109324,
          "standard_error": 79.21549216013118
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.11166736362796,
            "upper_bound": 364.6594331645911
          },
          "point_estimate": 277.07862291578715,
          "standard_error": 57.84698537757416
        }
      }
    },
    "memmem/sliceslice/prebuilt/sliceslice-i386/words": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/sliceslice_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26205416.73998611,
            "upper_bound": 26255462.14928571
          },
          "point_estimate": 26226243.554920636,
          "standard_error": 13203.370426498272
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26204674.275,
            "upper_bound": 26236971.416666668
          },
          "point_estimate": 26211554.285714284,
          "standard_error": 8529.042568153196
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3201.5687431615816,
            "upper_bound": 43876.19246603971
          },
          "point_estimate": 14202.714707850855,
          "standard_error": 11418.460010781124
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26207281.932741117,
            "upper_bound": 26233671.12401621
          },
          "point_estimate": 26220316.025974028,
          "standard_error": 7146.4062518378605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11157.061627929295,
            "upper_bound": 65596.26825357058
          },
          "point_estimate": 44166.91428592592,
          "standard_error": 17016.912313816716
        }
      }
    },
    "memmem/sliceslice/prebuilt/sliceslice-words/words": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/sliceslice_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83604554.97,
            "upper_bound": 83826895.06666666
          },
          "point_estimate": 83716727.13333334,
          "standard_error": 57004.49071631322
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83486416.0,
            "upper_bound": 83900958.33333333
          },
          "point_estimate": 83718784.83333334,
          "standard_error": 88025.80064811424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18466.523972153664,
            "upper_bound": 334861.26555501675
          },
          "point_estimate": 286547.7802127416,
          "standard_error": 93319.8979610161
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112532.93512381992,
            "upper_bound": 226671.81055710572
          },
          "point_estimate": 189956.09573041028,
          "standard_error": 28387.93991288106
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.667783281998391,
            "upper_bound": 5.671370723433195
          },
          "point_estimate": 5.6694758304559025,
          "standard_error": 0.0009210499442731492
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.667503238266199,
            "upper_bound": 5.67237429282516
          },
          "point_estimate": 5.668481460180748,
          "standard_error": 0.0011770688098849808
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003448560103614194,
            "upper_bound": 0.004887469258111237
          },
          "point_estimate": 0.0015931003388872267,
          "standard_error": 0.0012854509534520485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.667686967750926,
            "upper_bound": 5.670818255177138
          },
          "point_estimate": 5.668980865410537,
          "standard_error": 0.0008017193521199308
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013815023061162313,
            "upper_bound": 0.003884796223341798
          },
          "point_estimate": 0.003065369012934921,
          "standard_error": 0.0006282054084817205
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.135818665947934,
            "upper_bound": 6.139711440722735
          },
          "point_estimate": 6.1376221691975035,
          "standard_error": 0.0010004003269291311
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.135156502862786,
            "upper_bound": 6.140419900496841
          },
          "point_estimate": 6.136551516493978,
          "standard_error": 0.001160681235834891
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005972917065647621,
            "upper_bound": 0.00555722603469361
          },
          "point_estimate": 0.002658583607069233,
          "standard_error": 0.0012163050654771077
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.135934307990407,
            "upper_bound": 6.138040920422992
          },
          "point_estimate": 6.136998871457074,
          "standard_error": 0.0005523699827731778
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013157712820243636,
            "upper_bound": 0.004217195897364477
          },
          "point_estimate": 0.0033431250701158174,
          "standard_error": 0.0007439434675526336
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.135863823322084,
            "upper_bound": 6.139748951217834
          },
          "point_estimate": 6.1377035403822955,
          "standard_error": 0.0009975384631889276
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.134925107108552,
            "upper_bound": 6.140063086612616
          },
          "point_estimate": 6.137100232256232,
          "standard_error": 0.001317966996940763
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007302493701752294,
            "upper_bound": 0.005461027676106247
          },
          "point_estimate": 0.003695537241978583,
          "standard_error": 0.0012494848342999383
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.13561886095656,
            "upper_bound": 6.139219291967907
          },
          "point_estimate": 6.137115948141857,
          "standard_error": 0.0009354069677091194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015725933877056425,
            "upper_bound": 0.004223100903340231
          },
          "point_estimate": 0.003319790182921582,
          "standard_error": 0.0006752041946724135
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.136227069696659,
            "upper_bound": 6.142146518539701
          },
          "point_estimate": 6.138902425516487,
          "standard_error": 0.0015237526103659426
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.135355197083908,
            "upper_bound": 6.1425460340451465
          },
          "point_estimate": 6.136260451074255,
          "standard_error": 0.002140507809452756
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00034464289855357555,
            "upper_bound": 0.009132719384490142
          },
          "point_estimate": 0.002603067346868624,
          "standard_error": 0.0022236895057980567
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.135916941029684,
            "upper_bound": 6.139858468087527
          },
          "point_estimate": 6.137796869638342,
          "standard_error": 0.0010374882057623352
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022854868850793893,
            "upper_bound": 0.006862910534498502
          },
          "point_estimate": 0.005068468534731607,
          "standard_error": 0.0012987571620621942
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.153677555034554,
            "upper_bound": 5.156394846256103
          },
          "point_estimate": 5.154824952070463,
          "standard_error": 0.0007124379113121591
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.15345053589506,
            "upper_bound": 5.1557266923244915
          },
          "point_estimate": 5.154147694797389,
          "standard_error": 0.0005267636762083201
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00020751441871627623,
            "upper_bound": 0.002604855423195763
          },
          "point_estimate": 0.0013380340005806125,
          "standard_error": 0.0006689962300384213
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.153912215628873,
            "upper_bound": 5.154896769614385
          },
          "point_estimate": 5.154304766263281,
          "standard_error": 0.00025274032997857533
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006522744857152702,
            "upper_bound": 0.0034893636005090527
          },
          "point_estimate": 0.0023735315468934167,
          "standard_error": 0.0008644392681973155
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.598713074749838,
            "upper_bound": 14.630456456353825
          },
          "point_estimate": 14.61289518946574,
          "standard_error": 0.008098495394334196
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.601790024991791,
            "upper_bound": 14.619667250129035
          },
          "point_estimate": 14.606911374230195,
          "standard_error": 0.005819093750846525
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012777578789015167,
            "upper_bound": 0.03590566973516201
          },
          "point_estimate": 0.012940987029854264,
          "standard_error": 0.008060353680917546
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.586312864263716,
            "upper_bound": 14.61004940556556
          },
          "point_estimate": 14.599566725899711,
          "standard_error": 0.006584080295605029
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007082622373177009,
            "upper_bound": 0.0394707971190165
          },
          "point_estimate": 0.02700168707524936,
          "standard_error": 0.009123214309642646
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.872013139286571,
            "upper_bound": 6.876629287715132
          },
          "point_estimate": 6.8739752172347774,
          "standard_error": 0.0012061995920346648
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.871582656868521,
            "upper_bound": 6.87522418007997
          },
          "point_estimate": 6.87302830251647,
          "standard_error": 0.0010821906090548654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002059744037473805,
            "upper_bound": 0.004444325073429003
          },
          "point_estimate": 0.0021494014225347737,
          "standard_error": 0.0010563847713073336
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.87196476891044,
            "upper_bound": 6.874118275784289
          },
          "point_estimate": 6.873005302007687,
          "standard_error": 0.0005514514082983838
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012951884896256207,
            "upper_bound": 0.005916404640345535
          },
          "point_estimate": 0.004035507956090424,
          "standard_error": 0.001460393585190623
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.626301158277025,
            "upper_bound": 6.630582506132558
          },
          "point_estimate": 6.628183863175717,
          "standard_error": 0.0010914480329868815
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.625879426340349,
            "upper_bound": 6.630901418036526
          },
          "point_estimate": 6.626895205267678,
          "standard_error": 0.0010076146766603326
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004109221174388427,
            "upper_bound": 0.00519478519186538
          },
          "point_estimate": 0.0014838009764658712,
          "standard_error": 0.0010269810950904958
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.625801347797738,
            "upper_bound": 6.627369296812276
          },
          "point_estimate": 6.626459709608682,
          "standard_error": 0.0004039122719982889
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007847063242336373,
            "upper_bound": 0.004615854378215371
          },
          "point_estimate": 0.0036434521729257385,
          "standard_error": 0.0010256152700300403
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.895228752830247,
            "upper_bound": 8.902951095483855
          },
          "point_estimate": 8.89875137724089,
          "standard_error": 0.0019927473053567473
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.893822155275732,
            "upper_bound": 8.903464236234985
          },
          "point_estimate": 8.897134423261036,
          "standard_error": 0.0024592708274339507
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005877295639124779,
            "upper_bound": 0.01076031685051721
          },
          "point_estimate": 0.005030646975076064,
          "standard_error": 0.0024836141874082025
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.894465508721138,
            "upper_bound": 8.898586009610117
          },
          "point_estimate": 8.896281642800645,
          "standard_error": 0.0010466792410575207
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0026841398794792467,
            "upper_bound": 0.008965039582904594
          },
          "point_estimate": 0.006632149244650302,
          "standard_error": 0.001692137415552403
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.135059139436489,
            "upper_bound": 6.136961007399832
          },
          "point_estimate": 6.135929121854991,
          "standard_error": 0.0004900307878736886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.134638457989661,
            "upper_bound": 6.136743256092241
          },
          "point_estimate": 6.135609898428726,
          "standard_error": 0.0005847537123785192
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00032633911006972236,
            "upper_bound": 0.0025037186565479955
          },
          "point_estimate": 0.001456581852646905,
          "standard_error": 0.0005373429145145866
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.135104801706048,
            "upper_bound": 6.136130107883479
          },
          "point_estimate": 6.135608104930702,
          "standard_error": 0.00026181922187126154
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007466742502318115,
            "upper_bound": 0.002237234674219316
          },
          "point_estimate": 0.001631797791394586,
          "standard_error": 0.0004310032004870129
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.66471573431592,
            "upper_bound": 4.669075621572765
          },
          "point_estimate": 4.666639784314016,
          "standard_error": 0.001133450165810876
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.6642328020602495,
            "upper_bound": 4.668497574176561
          },
          "point_estimate": 4.665300792895152,
          "standard_error": 0.0009548061125390528
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003304028630346726,
            "upper_bound": 0.004966109920135993
          },
          "point_estimate": 0.0016094029291874952,
          "standard_error": 0.001085816505941368
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.664734107534162,
            "upper_bound": 4.666479156830286
          },
          "point_estimate": 4.6654369404306975,
          "standard_error": 0.000442281364377024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008175712770044428,
            "upper_bound": 0.005153727863105481
          },
          "point_estimate": 0.003783891824976331,
          "standard_error": 0.0011698860933153498
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.44472501866589,
            "upper_bound": 16.459647064632946
          },
          "point_estimate": 16.450279653149884,
          "standard_error": 0.004265337721271079
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.443732151499376,
            "upper_bound": 16.448846029981482
          },
          "point_estimate": 16.44664433333965,
          "standard_error": 0.0017341320113563115
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005969568796110286,
            "upper_bound": 0.0062168086678648276
          },
          "point_estimate": 0.0037830089080929143,
          "standard_error": 0.0017262331466525531
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.444736024842378,
            "upper_bound": 16.44773414191891
          },
          "point_estimate": 16.44621830195042,
          "standard_error": 0.0007722821294685626
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018086644221344404,
            "upper_bound": 0.021828203245683506
          },
          "point_estimate": 0.01420775051843897,
          "standard_error": 0.007044055878266701
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1195566.4973022274,
            "upper_bound": 1199448.531044547
          },
          "point_estimate": 1197129.355919099,
          "standard_error": 1040.2649771124966
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1195304.3978494625,
            "upper_bound": 1197427.741935484
          },
          "point_estimate": 1196487.2974910396,
          "standard_error": 573.7988410277367
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206.25277214474593,
            "upper_bound": 2676.320125066426
          },
          "point_estimate": 1491.8197262027757,
          "standard_error": 632.2416241402982
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1195893.0851036515,
            "upper_bound": 1199004.799695499
          },
          "point_estimate": 1197049.9381650607,
          "standard_error": 803.8060157807336
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 731.6534272145572,
            "upper_bound": 5234.876665715782
          },
          "point_estimate": 3463.949587355137,
          "standard_error": 1471.7729359550246
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1330639.8689413264,
            "upper_bound": 1334591.7254591838
          },
          "point_estimate": 1332601.7555087863,
          "standard_error": 1009.9884380447093
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1329404.49872449,
            "upper_bound": 1335501.284523809
          },
          "point_estimate": 1332923.2063492064,
          "standard_error": 1698.0151149262106
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 923.4725675336948,
            "upper_bound": 5590.621641818089
          },
          "point_estimate": 4549.242243639638,
          "standard_error": 1293.141022093214
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1329474.15759025,
            "upper_bound": 1333163.2226412464
          },
          "point_estimate": 1330988.4042671614,
          "standard_error": 946.5900898063782
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2110.2892304124252,
            "upper_bound": 4023.77805584289
          },
          "point_estimate": 3365.9008635849823,
          "standard_error": 489.0452803577776
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1236084.097120139,
            "upper_bound": 1239577.3119722218
          },
          "point_estimate": 1237705.2188055555,
          "standard_error": 900.1394037339755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1235460.94,
            "upper_bound": 1240038.052777778
          },
          "point_estimate": 1236539.941666667,
          "standard_error": 1150.598474475412
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 234.90478716289675,
            "upper_bound": 4790.807661612821
          },
          "point_estimate": 2112.6720158259727,
          "standard_error": 1145.9096266564698
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1236042.753820598,
            "upper_bound": 1238982.8489321056
          },
          "point_estimate": 1237177.514112554,
          "standard_error": 755.817297763446
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1176.5226250494504,
            "upper_bound": 3726.777779065628
          },
          "point_estimate": 2999.2592048846304,
          "standard_error": 637.3309584270422
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/stud_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325288.35736713436,
            "upper_bound": 326097.91199354274
          },
          "point_estimate": 325605.0773628826,
          "standard_error": 221.53289586734564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325237.1994047619,
            "upper_bound": 325632.7833333333
          },
          "point_estimate": 325373.23748405615,
          "standard_error": 125.43497192890452
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.448004335162146,
            "upper_bound": 506.8876383447025
          },
          "point_estimate": 309.25049825973326,
          "standard_error": 127.70587562596388
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325316.235636646,
            "upper_bound": 325581.72398062964
          },
          "point_estimate": 325475.49972170684,
          "standard_error": 67.91077366398589
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146.78165864323603,
            "upper_bound": 1115.534710058092
          },
          "point_estimate": 731.8268067642646,
          "standard_error": 326.42495048150624
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/stud_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246557.90205598457,
            "upper_bound": 247261.4962388795
          },
          "point_estimate": 246903.092872426,
          "standard_error": 180.51277090611737
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246349.7483108108,
            "upper_bound": 247454.1576576577
          },
          "point_estimate": 246845.35033783785,
          "standard_error": 311.97431237052683
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.5579973255045,
            "upper_bound": 997.7209115099012
          },
          "point_estimate": 773.6333134443041,
          "standard_error": 242.93334710273385
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246473.180586324,
            "upper_bound": 247159.4095547133
          },
          "point_estimate": 246833.9387855388,
          "standard_error": 175.12348481287142
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 370.9954618974816,
            "upper_bound": 709.4691796649996
          },
          "point_estimate": 602.1969972350291,
          "standard_error": 85.8583051051387
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 429737.0570728291,
            "upper_bound": 430455.5791769608
          },
          "point_estimate": 430108.7945070028,
          "standard_error": 183.73718374068255
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 429752.06764705887,
            "upper_bound": 430601.2914705882
          },
          "point_estimate": 430147.56470588234,
          "standard_error": 198.5355968215328
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167.34978020540916,
            "upper_bound": 1056.274717423821
          },
          "point_estimate": 481.00065698994,
          "standard_error": 240.29212772168992
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 429918.67779923504,
            "upper_bound": 430361.0342471415
          },
          "point_estimate": 430153.12100840337,
          "standard_error": 112.63119367188662
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308.5176922791701,
            "upper_bound": 807.1743485167408
          },
          "point_estimate": 613.1000749217608,
          "standard_error": 130.51487096469253
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/stud_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237155.40677404296,
            "upper_bound": 237742.86856520383
          },
          "point_estimate": 237441.6282492997,
          "standard_error": 150.27927158288244
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237070.95578898225,
            "upper_bound": 237861.0043572985
          },
          "point_estimate": 237387.1437908497,
          "standard_error": 182.7300481015928
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.57299823913483,
            "upper_bound": 938.0949223650688
          },
          "point_estimate": 360.9335532326588,
          "standard_error": 209.45898714318184
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237144.03973441597,
            "upper_bound": 237632.10195690944
          },
          "point_estimate": 237363.03021814788,
          "standard_error": 123.84613120385792
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257.8388626005247,
            "upper_bound": 629.8188399958836
          },
          "point_estimate": 498.81158573682575,
          "standard_error": 94.74582891053088
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/stud_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 529121.2328029675,
            "upper_bound": 529677.6714653785
          },
          "point_estimate": 529404.957331493,
          "standard_error": 142.09461199811116
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 529111.0144927537,
            "upper_bound": 529781.3031400966
          },
          "point_estimate": 529403.7101449275,
          "standard_error": 166.46570436179587
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.4273355571658,
            "upper_bound": 807.5005824756283
          },
          "point_estimate": 390.2472911393336,
          "standard_error": 184.49078535914148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 528947.7304519895,
            "upper_bound": 529560.1126821414
          },
          "point_estimate": 529247.0609072087,
          "standard_error": 155.43849081275832
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 242.99032616636575,
            "upper_bound": 621.3071043553995
          },
          "point_estimate": 474.05233627393784,
          "standard_error": 96.92875720536328
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112285.22701862874,
            "upper_bound": 112583.80973655204
          },
          "point_estimate": 112417.25070399763,
          "standard_error": 77.53277571257372
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112249.25555555556,
            "upper_bound": 112564.1183127572
          },
          "point_estimate": 112306.54276895944,
          "standard_error": 67.24811569571436
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.939940745639524,
            "upper_bound": 279.58906911037775
          },
          "point_estimate": 86.56259309283438,
          "standard_error": 73.70736172774838
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112273.81170879876,
            "upper_bound": 112376.50441261574
          },
          "point_estimate": 112316.62804232804,
          "standard_error": 26.63089033396647
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.35446194098749,
            "upper_bound": 341.03306543666923
          },
          "point_estimate": 258.42423827318896,
          "standard_error": 79.47269334000651
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118563.22500775554,
            "upper_bound": 118799.8611563518
          },
          "point_estimate": 118664.61689855743,
          "standard_error": 61.9344853827291
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118541.42643865364,
            "upper_bound": 118712.35966340934
          },
          "point_estimate": 118630.14101520088,
          "standard_error": 33.52482921205131
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.822284388822105,
            "upper_bound": 240.969179109577
          },
          "point_estimate": 54.46474273014153,
          "standard_error": 67.78057615916573
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118517.50316638831,
            "upper_bound": 118645.99913816016
          },
          "point_estimate": 118573.34756969416,
          "standard_error": 32.325370166444
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.4398416318127,
            "upper_bound": 301.1562541781475
          },
          "point_estimate": 206.3067241393631,
          "standard_error": 74.08861502265094
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193585.4155092716,
            "upper_bound": 193917.42106319655
          },
          "point_estimate": 193744.12184291624,
          "standard_error": 85.43789803884195
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193472.32021276595,
            "upper_bound": 193980.2671542553
          },
          "point_estimate": 193690.0988105792,
          "standard_error": 136.41141774823456
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.032846630226086,
            "upper_bound": 481.66028607652214
          },
          "point_estimate": 330.8949667007903,
          "standard_error": 124.08612738509144
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193610.24902853867,
            "upper_bound": 193935.80324827743
          },
          "point_estimate": 193767.34997236804,
          "standard_error": 81.63513969025311
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.8574454723367,
            "upper_bound": 343.6226872980782
          },
          "point_estimate": 284.82963797712574,
          "standard_error": 47.74819612875719
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271943.70720260305,
            "upper_bound": 272304.36521973467
          },
          "point_estimate": 272110.87963604595,
          "standard_error": 92.9037002337327
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271867.9552238806,
            "upper_bound": 272308.6688225539
          },
          "point_estimate": 272009.4501599147,
          "standard_error": 123.22097979929303
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.49108112218596,
            "upper_bound": 513.4155195417678
          },
          "point_estimate": 288.3884379149103,
          "standard_error": 113.1189087107382
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271955.92760311684,
            "upper_bound": 272341.0188551921
          },
          "point_estimate": 272167.82804807136,
          "standard_error": 98.81017347698482
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.5003763719783,
            "upper_bound": 407.8289830664174
          },
          "point_estimate": 309.28896789341474,
          "standard_error": 71.3787628208498
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289741.6998066894,
            "upper_bound": 290537.38939843164
          },
          "point_estimate": 290119.42317082395,
          "standard_error": 204.35693050368084
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289545.5912698413,
            "upper_bound": 290669.97883597884
          },
          "point_estimate": 289898.38265306124,
          "standard_error": 326.09657059436176
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.74164806949092,
            "upper_bound": 1186.7022114715107
          },
          "point_estimate": 745.188870103589,
          "standard_error": 267.872254351536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289702.3833270841,
            "upper_bound": 290933.6560830067
          },
          "point_estimate": 290388.4184291899,
          "standard_error": 326.7214156347895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.6258937422639,
            "upper_bound": 860.3592831915005
          },
          "point_estimate": 682.3380258725929,
          "standard_error": 127.66817633852057
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316308.28650241543,
            "upper_bound": 317319.9368599034
          },
          "point_estimate": 316784.1835603865,
          "standard_error": 259.55116818175816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316054.85869565216,
            "upper_bound": 317263.2439613526
          },
          "point_estimate": 316766.65579710144,
          "standard_error": 304.48979319277527
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 210.4335793075408,
            "upper_bound": 1456.354301361954
          },
          "point_estimate": 806.068655906836,
          "standard_error": 321.72459863894426
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316232.6510049893,
            "upper_bound": 317126.9746143058
          },
          "point_estimate": 316660.2981592321,
          "standard_error": 232.54497736042865
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 432.25501364912344,
            "upper_bound": 1159.1531928309266
          },
          "point_estimate": 865.4549874959624,
          "standard_error": 201.6666200161524
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395282.96587667137,
            "upper_bound": 395876.8489919772
          },
          "point_estimate": 395567.9839859386,
          "standard_error": 151.69224786819237
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395052.36352657,
            "upper_bound": 396018.0621118013
          },
          "point_estimate": 395503.6705163043,
          "standard_error": 215.8811863322555
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.12673559049087,
            "upper_bound": 809.2256269377099
          },
          "point_estimate": 670.8759285333872,
          "standard_error": 214.14286811894016
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395195.3939406841,
            "upper_bound": 395686.8922101449
          },
          "point_estimate": 395441.1013833992,
          "standard_error": 124.71611384602971
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258.4414887585086,
            "upper_bound": 629.3234606585902
          },
          "point_estimate": 505.8212885466099,
          "standard_error": 91.40907321192174
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272812.8495073622,
            "upper_bound": 273294.9215528106
          },
          "point_estimate": 273064.9480758443,
          "standard_error": 123.73373635185318
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272715.8515037594,
            "upper_bound": 273362.3984962406
          },
          "point_estimate": 273146.9548872181,
          "standard_error": 142.16187096362282
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.28875748645088,
            "upper_bound": 694.5559753884654
          },
          "point_estimate": 334.5851607265673,
          "standard_error": 165.26020432589047
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272949.9947477687,
            "upper_bound": 273377.0836690481
          },
          "point_estimate": 273194.1248706181,
          "standard_error": 107.4627996261787
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 185.48377886541272,
            "upper_bound": 526.217436918807
          },
          "point_estimate": 412.1225732142509,
          "standard_error": 84.41555989040258
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123982.3486602419,
            "upper_bound": 124926.67418259368
          },
          "point_estimate": 124475.73035079904,
          "standard_error": 235.214226569856
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123523.00793650794,
            "upper_bound": 124987.55123906703
          },
          "point_estimate": 124898.1808956916,
          "standard_error": 367.61225458432017
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.7731667753832,
            "upper_bound": 1236.9445044684044
          },
          "point_estimate": 156.57847023606098,
          "standard_error": 318.54441362438126
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124743.64610247494,
            "upper_bound": 125016.89443998072
          },
          "point_estimate": 124936.0198780811,
          "standard_error": 71.88271026210408
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.55437293195698,
            "upper_bound": 892.2389692465182
          },
          "point_estimate": 781.6209196300198,
          "standard_error": 155.93482951576766
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203460.36868814845,
            "upper_bound": 203977.24629738624
          },
          "point_estimate": 203713.97829431584,
          "standard_error": 131.79450638784235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203333.65083798883,
            "upper_bound": 204069.96787709495
          },
          "point_estimate": 203736.5810055866,
          "standard_error": 162.49461238512137
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.42443521762702,
            "upper_bound": 840.1593113411391
          },
          "point_estimate": 427.9847848598663,
          "standard_error": 207.0044865087592
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203362.4704832122,
            "upper_bound": 204060.2913178134
          },
          "point_estimate": 203685.88484364795,
          "standard_error": 179.99866949929455
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248.23847039799185,
            "upper_bound": 551.3284028057017
          },
          "point_estimate": 440.2159233118706,
          "standard_error": 78.74457696410006
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150714.55877238294,
            "upper_bound": 151622.82383168538
          },
          "point_estimate": 151117.9076895579,
          "standard_error": 233.7093106647092
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150584.9085005903,
            "upper_bound": 151670.80681818182
          },
          "point_estimate": 150742.34785353538,
          "standard_error": 266.6873666379136
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.08265793330685,
            "upper_bound": 1088.084875971907
          },
          "point_estimate": 345.6550533331355,
          "standard_error": 299.3424258867091
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150641.6922754355,
            "upper_bound": 150982.16724977246
          },
          "point_estimate": 150756.33127616186,
          "standard_error": 87.7998109925387
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230.06372786458525,
            "upper_bound": 1034.0871841277208
          },
          "point_estimate": 775.2047171657309,
          "standard_error": 212.32384143303364
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47575.51156768106,
            "upper_bound": 47682.09105344676
          },
          "point_estimate": 47617.98772905759,
          "standard_error": 28.70651705620775
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47573.86491055846,
            "upper_bound": 47636.05977312391
          },
          "point_estimate": 47586.4862565445,
          "standard_error": 17.349986809049312
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.214843939740563,
            "upper_bound": 75.88803106110127
          },
          "point_estimate": 19.47683240945073,
          "standard_error": 21.412338350577745
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47578.54867769507,
            "upper_bound": 47621.24969585904
          },
          "point_estimate": 47599.23350445366,
          "standard_error": 11.78056376319558
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.74280918508186,
            "upper_bound": 144.45612836668212
          },
          "point_estimate": 95.58686764486134,
          "standard_error": 40.50323374173776
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1112246.2863041128,
            "upper_bound": 1116603.8335804471
          },
          "point_estimate": 1114497.1721981722,
          "standard_error": 1112.0149608515765
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1112548.5909090908,
            "upper_bound": 1118370.2626262626
          },
          "point_estimate": 1114304.9996632996,
          "standard_error": 1254.578672704924
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.74825963040175,
            "upper_bound": 6875.022172893663
          },
          "point_estimate": 2951.074313870634,
          "standard_error": 1813.3803985615036
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1112231.3293690789,
            "upper_bound": 1114405.4340021363
          },
          "point_estimate": 1113246.726249508,
          "standard_error": 555.0709626181307
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1903.7457745349232,
            "upper_bound": 4949.510689483488
          },
          "point_estimate": 3710.4316811587,
          "standard_error": 835.934877591421
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125061.64562911144,
            "upper_bound": 125238.78618488516
          },
          "point_estimate": 125144.26664307536,
          "standard_error": 45.46437936174068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125033.9085910653,
            "upper_bound": 125226.17031786942
          },
          "point_estimate": 125130.13043146238,
          "standard_error": 41.88361251062723
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.06705870158746,
            "upper_bound": 236.38859291666753
          },
          "point_estimate": 74.23893132803602,
          "standard_error": 58.24762710896455
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125079.3150570174,
            "upper_bound": 125204.3410808823
          },
          "point_estimate": 125133.60463248091,
          "standard_error": 32.400476303461254
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.29782550147422,
            "upper_bound": 206.826843797335
          },
          "point_estimate": 151.82945196558114,
          "standard_error": 38.66497756517821
        }
      }
    },
    "memmem/stud/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/stud_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24077.721648566287,
            "upper_bound": 24119.012289590253
          },
          "point_estimate": 24095.82019994199,
          "standard_error": 10.690126652118998
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24069.838482834995,
            "upper_bound": 24113.43953488372
          },
          "point_estimate": 24084.89632178453,
          "standard_error": 12.22728333347238
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.546911099820879,
            "upper_bound": 50.25552137522656
          },
          "point_estimate": 24.794582738100857,
          "standard_error": 11.56543419163865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24076.64645356288,
            "upper_bound": 24111.0235941922
          },
          "point_estimate": 24094.71533157872,
          "standard_error": 8.771614136914492
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.841652410800547,
            "upper_bound": 50.95452867494003
          },
          "point_estimate": 35.78642169290738,
          "standard_error": 11.16725492992735
        }
      }
    },
    "memmem/stud/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/stud_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24541.991661001583,
            "upper_bound": 24629.520959947884
          },
          "point_estimate": 24586.73529409861,
          "standard_error": 22.462525634548623
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24507.89629503739,
            "upper_bound": 24646.505268524812
          },
          "point_estimate": 24616.99750736461,
          "standard_error": 43.06761594485148
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.818981860952276,
            "upper_bound": 117.10179659539862
          },
          "point_estimate": 76.71697749592042,
          "standard_error": 30.43541031703714
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24541.481429224976,
            "upper_bound": 24642.492075303784
          },
          "point_estimate": 24595.26267138708,
          "standard_error": 26.65572239838045
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.5254393363695,
            "upper_bound": 86.76268158409445
          },
          "point_estimate": 74.98893200253492,
          "standard_error": 9.955777005907514
        }
      }
    },
    "memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/stud_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 606132.1490337467,
            "upper_bound": 607510.2992908896
          },
          "point_estimate": 606792.8027294974,
          "standard_error": 353.9154392272546
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 605951.5770833334,
            "upper_bound": 607855.9138888889
          },
          "point_estimate": 606250.3175,
          "standard_error": 543.7872751302709
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.10714152462924,
            "upper_bound": 1929.089074085204
          },
          "point_estimate": 1093.6226618343412,
          "standard_error": 523.912034440413
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 606248.6712187872,
            "upper_bound": 607766.8720942982
          },
          "point_estimate": 606874.5046753247,
          "standard_error": 394.4481238155888
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 628.5613192389828,
            "upper_bound": 1425.241151410579
          },
          "point_estimate": 1183.7660530404482,
          "standard_error": 196.33497733784108
        }
      }
    },
    "memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/stud_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1339.1534307033269,
            "upper_bound": 1340.5264750753918
          },
          "point_estimate": 1339.74873064862,
          "standard_error": 0.35738038505117925
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1338.9622180866745,
            "upper_bound": 1340.2487628453505
          },
          "point_estimate": 1339.3426297169813,
          "standard_error": 0.32314801539749716
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1726346690497272,
            "upper_bound": 1.461677836023113
          },
          "point_estimate": 0.6466316677471321,
          "standard_error": 0.35812440647311894
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1339.0498653941415,
            "upper_bound": 1339.7265283336274
          },
          "point_estimate": 1339.3364301603467,
          "standard_error": 0.17243177198174572
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3382288932052752,
            "upper_bound": 1.6991313426896824
          },
          "point_estimate": 1.196305587223909,
          "standard_error": 0.38980564914532045
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.525027916834382,
            "upper_bound": 29.54917103487476
          },
          "point_estimate": 29.535268251368052,
          "standard_error": 0.00631269306135599
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.51938123917436,
            "upper_bound": 29.54130317897799
          },
          "point_estimate": 29.53000029165431,
          "standard_error": 0.005104841847075869
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003454248693846425,
            "upper_bound": 0.022851791931568433
          },
          "point_estimate": 0.013819597223094685,
          "standard_error": 0.005920532733137741
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.527255322620505,
            "upper_bound": 29.541312420334837
          },
          "point_estimate": 29.533514937593235,
          "standard_error": 0.003807909853132022
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005925467315078623,
            "upper_bound": 0.03078795973104131
          },
          "point_estimate": 0.021150642194388274,
          "standard_error": 0.007505018813139355
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.2846862932765,
            "upper_bound": 32.399413093636596
          },
          "point_estimate": 32.34349809933336,
          "standard_error": 0.029429804655336934
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.245024528908246,
            "upper_bound": 32.43488174470997
          },
          "point_estimate": 32.372116593149975,
          "standard_error": 0.06058783746073948
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011163160293492827,
            "upper_bound": 0.1528137418183963
          },
          "point_estimate": 0.10697762110559612,
          "standard_error": 0.04229521185261673
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.29493805583714,
            "upper_bound": 32.428741922155595
          },
          "point_estimate": 32.35962444924933,
          "standard_error": 0.03560061325853694
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06368488893923843,
            "upper_bound": 0.11058421217444896
          },
          "point_estimate": 0.09802564388031965,
          "standard_error": 0.011984926507973469
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.07201306583422,
            "upper_bound": 23.089330845519868
          },
          "point_estimate": 23.078966350484308,
          "standard_error": 0.0046708455225497585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.070014235248912,
            "upper_bound": 23.081598752414756
          },
          "point_estimate": 23.07344216274224,
          "standard_error": 0.002728890310851288
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005177482020932407,
            "upper_bound": 0.012472103144508275
          },
          "point_estimate": 0.00599064941072849,
          "standard_error": 0.0031920290465311456
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.07224542855547,
            "upper_bound": 23.07940734025008
          },
          "point_estimate": 23.075318667489956,
          "standard_error": 0.0018371691712660524
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002993213052158712,
            "upper_bound": 0.023345374931062736
          },
          "point_estimate": 0.015547082397331043,
          "standard_error": 0.006471610537678036
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.486334785591826,
            "upper_bound": 27.51803588467407
          },
          "point_estimate": 27.49865017307372,
          "standard_error": 0.008754282642571543
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.484183061345757,
            "upper_bound": 27.500096700085912
          },
          "point_estimate": 27.487809922829168,
          "standard_error": 0.005464729573614919
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006710258416411572,
            "upper_bound": 0.02091225373107696
          },
          "point_estimate": 0.00875413103878159,
          "standard_error": 0.005290676018745578
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.4866663594349,
            "upper_bound": 27.496910660560697
          },
          "point_estimate": 27.49195686203441,
          "standard_error": 0.0025972232482052555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005521455642337575,
            "upper_bound": 0.044579566125251296
          },
          "point_estimate": 0.02923011993804296,
          "standard_error": 0.01325718597546838
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.92059016558605,
            "upper_bound": 35.949990165210195
          },
          "point_estimate": 35.93485367729853,
          "standard_error": 0.007492213949784663
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.906635647690216,
            "upper_bound": 35.95268691507979
          },
          "point_estimate": 35.93373671068195,
          "standard_error": 0.010917871214710372
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012579115327441726,
            "upper_bound": 0.04594152596431951
          },
          "point_estimate": 0.029775002895311296,
          "standard_error": 0.010912712083706337
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.912878823188436,
            "upper_bound": 35.947989087827516
          },
          "point_estimate": 35.931727768455346,
          "standard_error": 0.00905363120883719
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014327893303367645,
            "upper_bound": 0.03166888694746225
          },
          "point_estimate": 0.024840965229806687,
          "standard_error": 0.0046830628647218184
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.38703664512391,
            "upper_bound": 65.45949545946215
          },
          "point_estimate": 65.42554384133004,
          "standard_error": 0.018550447657964052
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.39300554037331,
            "upper_bound": 65.46161685000286
          },
          "point_estimate": 65.43737271286895,
          "standard_error": 0.01610651775762309
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006310703939451389,
            "upper_bound": 0.10244802450618697
          },
          "point_estimate": 0.04016299941436868,
          "standard_error": 0.025559977765901395
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.43162232630536,
            "upper_bound": 65.48963688387552
          },
          "point_estimate": 65.46222987979841,
          "standard_error": 0.01581279968410688
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02521071963851845,
            "upper_bound": 0.08272295065763968
          },
          "point_estimate": 0.06199523933877395,
          "standard_error": 0.015037854297682935
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.54825112633983,
            "upper_bound": 54.58596136136572
          },
          "point_estimate": 54.56555808029767,
          "standard_error": 0.009673576516661647
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.543490281555805,
            "upper_bound": 54.58830688418138
          },
          "point_estimate": 54.55568015534478,
          "standard_error": 0.010608310644149284
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00431280309343449,
            "upper_bound": 0.05029425492493385
          },
          "point_estimate": 0.01953357393464286,
          "standard_error": 0.01148379110909794
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.55074789799218,
            "upper_bound": 54.57509193421631
          },
          "point_estimate": 54.562843396672704,
          "standard_error": 0.006008750644584386
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011433964508993423,
            "upper_bound": 0.04099652345079741
          },
          "point_estimate": 0.03211376151090086,
          "standard_error": 0.007541051969682551
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.09201910497839,
            "upper_bound": 50.16702030920339
          },
          "point_estimate": 50.12027111235888,
          "standard_error": 0.021138015098076755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.08781299750456,
            "upper_bound": 50.11393016698447
          },
          "point_estimate": 50.10264413057091,
          "standard_error": 0.008448837956888394
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035732860970533274,
            "upper_bound": 0.03416128265918772
          },
          "point_estimate": 0.01753262535596918,
          "standard_error": 0.008746862277705053
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.09196956492524,
            "upper_bound": 50.111780185483205
          },
          "point_estimate": 50.10297350736918,
          "standard_error": 0.005110527740411044
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009714337173046863,
            "upper_bound": 0.10861018940218488
          },
          "point_estimate": 0.07077900632665363,
          "standard_error": 0.03430993669232761
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.7171488172286,
            "upper_bound": 95.8042059598892
          },
          "point_estimate": 95.760663427661,
          "standard_error": 0.02220355572617192
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.7237092380566,
            "upper_bound": 95.82207955066364
          },
          "point_estimate": 95.75702719524476,
          "standard_error": 0.02345578700926644
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006844314183439651,
            "upper_bound": 0.1350462908934936
          },
          "point_estimate": 0.04492961341175733,
          "standard_error": 0.033013841971147234
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.7436696203505,
            "upper_bound": 95.78538828167548
          },
          "point_estimate": 95.76410165330871,
          "standard_error": 0.01044624386595346
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027048552520346797,
            "upper_bound": 0.09993966696164408
          },
          "point_estimate": 0.07425047379055744,
          "standard_error": 0.01678525491104347
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.901203646400305,
            "upper_bound": 39.01470328948318
          },
          "point_estimate": 38.96172816887574,
          "standard_error": 0.028762209940744413
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.859297337508025,
            "upper_bound": 39.023993494997264
          },
          "point_estimate": 39.00862693052349,
          "standard_error": 0.04162098911542437
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003732119519177585,
            "upper_bound": 0.15054990902052617
          },
          "point_estimate": 0.023416116902280456,
          "standard_error": 0.038202970360711026
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.859057540006305,
            "upper_bound": 39.02083502182595
          },
          "point_estimate": 38.935108999892265,
          "standard_error": 0.04399201920495488
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018811687965010065,
            "upper_bound": 0.1126217151439709
          },
          "point_estimate": 0.09598840271164628,
          "standard_error": 0.019327289128845147
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.8466905519028,
            "upper_bound": 44.01345572205739
          },
          "point_estimate": 43.93349912747957,
          "standard_error": 0.0427138993908385
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.78468627998837,
            "upper_bound": 44.037309601076984
          },
          "point_estimate": 44.008970254244225,
          "standard_error": 0.0710711680820269
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005647262945082543,
            "upper_bound": 0.22421299597345865
          },
          "point_estimate": 0.09697874798647604,
          "standard_error": 0.06188937412118584
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.77674236026685,
            "upper_bound": 44.00753652812463
          },
          "point_estimate": 43.90774641910353,
          "standard_error": 0.059068138268527325
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07547136898318517,
            "upper_bound": 0.1672427252732065
          },
          "point_estimate": 0.1426259785018335,
          "standard_error": 0.0228914615498541
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.54487622749203,
            "upper_bound": 79.61683640188969
          },
          "point_estimate": 79.5742130204132,
          "standard_error": 0.019126212107398663
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.53411281392819,
            "upper_bound": 79.5857237003465
          },
          "point_estimate": 79.55735263958263,
          "standard_error": 0.01582801068932912
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002982810630774202,
            "upper_bound": 0.06391529754188716
          },
          "point_estimate": 0.03383946870232032,
          "standard_error": 0.015145281668205848
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.53871649054453,
            "upper_bound": 79.57853629389489
          },
          "point_estimate": 79.5589719802733,
          "standard_error": 0.01018537331008144
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017217348159780082,
            "upper_bound": 0.09554258660288788
          },
          "point_estimate": 0.06379537319510035,
          "standard_error": 0.02561038003085094
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1081992.3766981792,
            "upper_bound": 1085870.455203081
          },
          "point_estimate": 1084163.0292471987,
          "standard_error": 1000.2686338460446
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1082145.3394607843,
            "upper_bound": 1085884.9294117647
          },
          "point_estimate": 1085485.9582457982,
          "standard_error": 819.9465938651248
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.59536965957135,
            "upper_bound": 4684.719760211999
          },
          "point_estimate": 667.5024830024337,
          "standard_error": 1062.2189604462935
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1079864.7263200977,
            "upper_bound": 1085480.438194471
          },
          "point_estimate": 1082841.3827349122,
          "standard_error": 1456.818962815299
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 625.844398324652,
            "upper_bound": 4330.471015197109
          },
          "point_estimate": 3333.084922997918,
          "standard_error": 972.4874256248158
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1152588.0342411334,
            "upper_bound": 1155638.1156510415
          },
          "point_estimate": 1153929.7313008432,
          "standard_error": 794.8474023861164
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1152360.9578125,
            "upper_bound": 1155241.28125
          },
          "point_estimate": 1152976.8704427085,
          "standard_error": 673.5411664504928
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.0627640251286,
            "upper_bound": 3400.246074946149
          },
          "point_estimate": 979.7573383612648,
          "standard_error": 793.1491469375646
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1152213.3450149642,
            "upper_bound": 1153121.7666073164
          },
          "point_estimate": 1152566.0715097403,
          "standard_error": 232.2727345163546
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 610.3944263272213,
            "upper_bound": 3639.2924838850154
          },
          "point_estimate": 2651.211865090834,
          "standard_error": 816.7802763967306
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1314644.1287095023,
            "upper_bound": 1317007.6517750851
          },
          "point_estimate": 1315836.9386522109,
          "standard_error": 603.7925791623225
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1314573.5109693878,
            "upper_bound": 1316901.8452380951
          },
          "point_estimate": 1316192.6800595238,
          "standard_error": 576.7883083604638
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.25868579091056,
            "upper_bound": 3432.2763015649475
          },
          "point_estimate": 1466.5372132494424,
          "standard_error": 783.6455864509416
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1313616.9660276142,
            "upper_bound": 1316490.07440323
          },
          "point_estimate": 1314907.5966604825,
          "standard_error": 732.8409497171589
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 889.626483016745,
            "upper_bound": 2714.2785169288427
          },
          "point_estimate": 2013.924854144993,
          "standard_error": 467.1684449064786
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1284705.7954679802,
            "upper_bound": 1285376.8976566775
          },
          "point_estimate": 1285002.9731499727,
          "standard_error": 173.48821672099544
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1284611.124137931,
            "upper_bound": 1285319.418103448
          },
          "point_estimate": 1284786.577586207,
          "standard_error": 171.65484220567996
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.83088333200142,
            "upper_bound": 829.0388047644245
          },
          "point_estimate": 290.6819834603251,
          "standard_error": 191.837126534902
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1284680.2981706515,
            "upper_bound": 1285690.7457645482
          },
          "point_estimate": 1285178.343304971,
          "standard_error": 268.0512547578942
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162.72690667443553,
            "upper_bound": 781.4842617629179
          },
          "point_estimate": 577.5117316101234,
          "standard_error": 165.66286979903603
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1097026.1676797387,
            "upper_bound": 1097843.8264052286
          },
          "point_estimate": 1097387.0024276376,
          "standard_error": 211.41866709713972
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1096863.8349673203,
            "upper_bound": 1097829.855882353
          },
          "point_estimate": 1097175.9443277312,
          "standard_error": 236.45113074060325
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.0083585078239,
            "upper_bound": 1029.7671865218902
          },
          "point_estimate": 463.33654422523216,
          "standard_error": 246.75550937705495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1096923.9528592357,
            "upper_bound": 1097606.7268424612
          },
          "point_estimate": 1097248.3614973263,
          "standard_error": 184.32523993272343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 211.6315209546393,
            "upper_bound": 985.4431911844348
          },
          "point_estimate": 705.7758923731222,
          "standard_error": 208.49510018237817
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335850.56087474275,
            "upper_bound": 336897.8245524691
          },
          "point_estimate": 336349.211654174,
          "standard_error": 268.59061731695874
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335801.1759259259,
            "upper_bound": 337006.2364969136
          },
          "point_estimate": 336140.4972222222,
          "standard_error": 300.95577036456
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.03103411124414,
            "upper_bound": 1496.371794498904
          },
          "point_estimate": 559.4760309932376,
          "standard_error": 376.9952817988317
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335992.89285152854,
            "upper_bound": 336562.4791026204
          },
          "point_estimate": 336215.17053872056,
          "standard_error": 145.14611723341497
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 373.4659285578893,
            "upper_bound": 1143.1635217858393
          },
          "point_estimate": 898.3447021883122,
          "standard_error": 186.6973380998583
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 419236.6733391603,
            "upper_bound": 419541.0386909322
          },
          "point_estimate": 419384.3232215837,
          "standard_error": 78.2362823667826
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 419127.81992337166,
            "upper_bound": 419622.0561941251
          },
          "point_estimate": 419346.2103448276,
          "standard_error": 120.18657863120411
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.475787927343056,
            "upper_bound": 460.1619668305352
          },
          "point_estimate": 317.47521045562087,
          "standard_error": 100.72753571338072
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 419298.664004814,
            "upper_bound": 419596.50161176315
          },
          "point_estimate": 419441.78017614566,
          "standard_error": 76.48789105526177
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150.9505391143848,
            "upper_bound": 315.8502988691558
          },
          "point_estimate": 260.78880064223137,
          "standard_error": 41.60286727193508
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 641521.7921553885,
            "upper_bound": 643580.3721737678
          },
          "point_estimate": 642476.7856251741,
          "standard_error": 530.2540105301017
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 641309.0949874687,
            "upper_bound": 643734.5131578947
          },
          "point_estimate": 642070.1669590643,
          "standard_error": 492.4696919087205
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 209.5643296128584,
            "upper_bound": 2949.5762914941274
          },
          "point_estimate": 1058.1255320917162,
          "standard_error": 686.9862862981399
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 640966.517773841,
            "upper_bound": 642387.6568288777
          },
          "point_estimate": 641641.8374572796,
          "standard_error": 367.48201706756646
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 680.5710585935142,
            "upper_bound": 2301.287096635957
          },
          "point_estimate": 1775.33150935888,
          "standard_error": 417.7130897770267
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 691364.1450943395,
            "upper_bound": 691668.7574221698
          },
          "point_estimate": 691503.399297694,
          "standard_error": 78.38525617551426
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 691304.2150943396,
            "upper_bound": 691633.8501048218
          },
          "point_estimate": 691432.1650943395,
          "standard_error": 90.11876275610774
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.479071647441597,
            "upper_bound": 395.3394790190678
          },
          "point_estimate": 249.34254462987897,
          "standard_error": 91.27284069168252
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 691338.362813707,
            "upper_bound": 691596.587081149
          },
          "point_estimate": 691466.5376133301,
          "standard_error": 66.64262016266673
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.97165829352552,
            "upper_bound": 357.23462386138704
          },
          "point_estimate": 260.7471589091128,
          "standard_error": 68.32529745744095
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 588770.420065028,
            "upper_bound": 590468.2206745392
          },
          "point_estimate": 589512.4781387609,
          "standard_error": 441.38492774848913
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 588400.9498207886,
            "upper_bound": 590195.9274193548
          },
          "point_estimate": 589056.5846774194,
          "standard_error": 431.53544088433057
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.76670437857835,
            "upper_bound": 1869.896004436977
          },
          "point_estimate": 986.2727779202904,
          "standard_error": 459.2955968266966
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 588549.6722523674,
            "upper_bound": 589218.9663820962
          },
          "point_estimate": 588785.3054461668,
          "standard_error": 172.26610452211344
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 469.9233561862643,
            "upper_bound": 2082.986629651292
          },
          "point_estimate": 1466.9772410633702,
          "standard_error": 468.70833778479215
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291590.8795031666,
            "upper_bound": 291954.47188011906
          },
          "point_estimate": 291755.62784539687,
          "standard_error": 93.51067050495377
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291546.7184,
            "upper_bound": 291987.7865
          },
          "point_estimate": 291654.09212698415,
          "standard_error": 99.24838350269177
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.65828725102003,
            "upper_bound": 433.6964700603601
          },
          "point_estimate": 161.6008178777178,
          "standard_error": 97.61496793471146
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291602.6567883876,
            "upper_bound": 291773.8272572161
          },
          "point_estimate": 291695.6417038961,
          "standard_error": 43.67487672997477
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.19974532536386,
            "upper_bound": 391.7755746171323
          },
          "point_estimate": 311.5791577898858,
          "standard_error": 79.25445933557774
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527483.3646135265,
            "upper_bound": 528009.1857487924
          },
          "point_estimate": 527709.7324516907,
          "standard_error": 136.03896412582813
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527428.2753623188,
            "upper_bound": 527877.5028985507
          },
          "point_estimate": 527541.0268115941,
          "standard_error": 109.05271894324989
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.611595179789944,
            "upper_bound": 583.3314664554947
          },
          "point_estimate": 187.58864753911297,
          "standard_error": 135.49944096050422
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527454.944077473,
            "upper_bound": 527969.4902953682
          },
          "point_estimate": 527696.2221343874,
          "standard_error": 145.14036465454663
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.86297528266832,
            "upper_bound": 633.9271426101477
          },
          "point_estimate": 452.5361669220333,
          "standard_error": 148.59377939419116
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189073.33043922065,
            "upper_bound": 189746.776539016
          },
          "point_estimate": 189405.2484728423,
          "standard_error": 173.84673655232473
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188752.14657738095,
            "upper_bound": 189897.4694010417
          },
          "point_estimate": 189506.31458333333,
          "standard_error": 234.72081744862004
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.37190945725169,
            "upper_bound": 1116.31718311111
          },
          "point_estimate": 740.8974841120653,
          "standard_error": 349.1203612085058
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189162.2682646704,
            "upper_bound": 189700.27555522777
          },
          "point_estimate": 189428.30742694804,
          "standard_error": 134.27085266425206
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 343.3240141900118,
            "upper_bound": 719.6301756495045
          },
          "point_estimate": 580.7715032081244,
          "standard_error": 99.3899635124284
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/stud_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148091.66153644313,
            "upper_bound": 148801.17290525994
          },
          "point_estimate": 148449.84662536444,
          "standard_error": 181.16647969565727
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147768.6787755102,
            "upper_bound": 148922.95482993196
          },
          "point_estimate": 148885.18911564624,
          "standard_error": 423.43696746137334
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.515365527586331,
            "upper_bound": 874.4496681897131
          },
          "point_estimate": 99.20268052455478,
          "standard_error": 315.83764207739586
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147812.35136265412,
            "upper_bound": 148521.3771807457
          },
          "point_estimate": 148029.95069175723,
          "standard_error": 181.17700591129903
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 365.80142397652133,
            "upper_bound": 638.9797003416287
          },
          "point_estimate": 605.4252393041647,
          "standard_error": 72.70551301117314
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/stud_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391951.6675277351,
            "upper_bound": 393099.1316232719
          },
          "point_estimate": 392558.6133789896,
          "standard_error": 294.7241263463134
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391524.1330645161,
            "upper_bound": 393308.24301075266
          },
          "point_estimate": 393043.23387096776,
          "standard_error": 464.6348908988568
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.51725520721077,
            "upper_bound": 1562.3842509807134
          },
          "point_estimate": 636.1722236518807,
          "standard_error": 393.6797680988864
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391341.104686886,
            "upper_bound": 392784.22746743594
          },
          "point_estimate": 391824.926686217,
          "standard_error": 361.3508736044312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397.82476663037744,
            "upper_bound": 1155.823983592891
          },
          "point_estimate": 982.556707529916,
          "standard_error": 172.56230368210595
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/stud_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829.3413264936204,
            "upper_bound": 829.8258346826428
          },
          "point_estimate": 829.5616323033203,
          "standard_error": 0.12503812352633584
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829.2532747217089,
            "upper_bound": 829.7830279799524
          },
          "point_estimate": 829.4511978370485,
          "standard_error": 0.12446328196809968
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04163839860817503,
            "upper_bound": 0.5981416524350397
          },
          "point_estimate": 0.3095560431250827,
          "standard_error": 0.15357670212418756
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829.2279601756718,
            "upper_bound": 829.6405972772232
          },
          "point_estimate": 829.3981052692088,
          "standard_error": 0.1058292316477426
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.16238365851747066,
            "upper_bound": 0.568072262324003
          },
          "point_estimate": 0.41568648880591935,
          "standard_error": 0.11259747283336054
        }
      }
    },
    "memmem/stud/prebuilt/sliceslice-haystack/words": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/prebuilt/sliceslice-haystack/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/prebuilt/sliceslice-haystack/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/stud/prebuilt/sliceslice-haystack/words",
        "directory_name": "memmem/stud_prebuilt_sliceslice-haystack_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1616401.2358628365,
            "upper_bound": 1619551.7836835752
          },
          "point_estimate": 1618054.0243288476,
          "standard_error": 810.0975090483946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1615953.880978261,
            "upper_bound": 1620122.4438405796
          },
          "point_estimate": 1618933.052173913,
          "standard_error": 1093.8416807838605
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498.0798192732873,
            "upper_bound": 4377.404048372497
          },
          "point_estimate": 2549.9927215763105,
          "standard_error": 1037.4869241227611
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1615281.5314106192,
            "upper_bound": 1619692.2526781878
          },
          "point_estimate": 1617245.117108978,
          "standard_error": 1117.3917518517048
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1329.236710921327,
            "upper_bound": 3427.584599703401
          },
          "point_estimate": 2695.7677615579723,
          "standard_error": 541.1462460682131
        }
      }
    },
    "memmem/stud/prebuilt/sliceslice-i386/words": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/stud/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/stud_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 329426581.7325,
            "upper_bound": 329742785.9
          },
          "point_estimate": 329583770.5,
          "standard_error": 80418.89734463554
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 329432994.5,
            "upper_bound": 329669736.0
          },
          "point_estimate": 329633087.5,
          "standard_error": 60065.93203665487
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4502.656120061874,
            "upper_bound": 440100.1652866602
          },
          "point_estimate": 117427.10701525211,
          "standard_error": 108287.14330917282
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73226.396563506,
            "upper_bound": 376656.0912797452
          },
          "point_estimate": 267805.2815947728,
          "standard_error": 75039.73435865264
        }
      }
    },
    "memmem/stud/prebuilt/sliceslice-words/words": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/stud/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/stud/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/stud_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248203978.7575,
            "upper_bound": 248999062.1125
          },
          "point_estimate": 248587353.3,
          "standard_error": 203707.9594029198
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248023305.5,
            "upper_bound": 249114488.5
          },
          "point_estimate": 248445032.0,
          "standard_error": 321240.6274062552
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132249.4002521038,
            "upper_bound": 1173152.4484723806
          },
          "point_estimate": 769365.6043410301,
          "standard_error": 258115.05106944797
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 384096.846589512,
            "upper_bound": 854648.9914762343
          },
          "point_estimate": 677335.250986959,
          "standard_error": 123562.03759131052
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160297.66431718064,
            "upper_bound": 160565.22778671072
          },
          "point_estimate": 160424.45132892803,
          "standard_error": 68.7476993475838
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160261.88766519824,
            "upper_bound": 160583.57004405287
          },
          "point_estimate": 160369.68656387666,
          "standard_error": 83.7203933632889
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.91481881470681,
            "upper_bound": 381.1539203256806
          },
          "point_estimate": 209.94791257224043,
          "standard_error": 83.08624550097396
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160327.49489171169,
            "upper_bound": 160628.66808205965
          },
          "point_estimate": 160501.2230447966,
          "standard_error": 73.57125737497888
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.63136032906448,
            "upper_bound": 303.1548659734038
          },
          "point_estimate": 230.26468351056897,
          "standard_error": 50.60548111629937
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217569.87282647204,
            "upper_bound": 218259.86952095805
          },
          "point_estimate": 217904.3325556031,
          "standard_error": 176.65582626636007
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217413.60239520957,
            "upper_bound": 218637.15568862276
          },
          "point_estimate": 217765.6591103507,
          "standard_error": 294.25893725099763
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.64607068842047,
            "upper_bound": 942.7267294908048
          },
          "point_estimate": 556.8311525094696,
          "standard_error": 237.8199464932156
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217434.96349042503,
            "upper_bound": 217828.6958697989
          },
          "point_estimate": 217575.76312310444,
          "standard_error": 100.65229857784212
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298.3621145542326,
            "upper_bound": 689.8488138981988
          },
          "point_estimate": 588.4274646585209,
          "standard_error": 90.90430034545666
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217571.59230824065,
            "upper_bound": 218072.84127555604
          },
          "point_estimate": 217800.77969347025,
          "standard_error": 128.9430471018025
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217472.3532934132,
            "upper_bound": 218039.673502994
          },
          "point_estimate": 217683.26937553467,
          "standard_error": 156.76293315353698
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.306296587707905,
            "upper_bound": 663.1077825987555
          },
          "point_estimate": 319.1361758911166,
          "standard_error": 158.63932591269196
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217489.90098828776,
            "upper_bound": 218101.3765468115
          },
          "point_estimate": 217783.78396453845,
          "standard_error": 163.0240323597617
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.36224916291724,
            "upper_bound": 571.6200295580812
          },
          "point_estimate": 428.2627132670501,
          "standard_error": 106.48184782652228
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128117.50662767226,
            "upper_bound": 128517.21117664322
          },
          "point_estimate": 128303.15579434944,
          "standard_error": 103.09108257220664
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128082.38274647888,
            "upper_bound": 128560.61950871898
          },
          "point_estimate": 128136.1071742958,
          "standard_error": 127.55631578719448
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.799030317822815,
            "upper_bound": 564.4689437198758
          },
          "point_estimate": 128.0222839947507,
          "standard_error": 131.50107478247608
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128081.60735861873,
            "upper_bound": 128405.79198748044
          },
          "point_estimate": 128184.50638375708,
          "standard_error": 86.83596030235665
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.90479141768019,
            "upper_bound": 421.63729405252445
          },
          "point_estimate": 343.8829854291241,
          "standard_error": 78.36099349087331
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92580.7266771679,
            "upper_bound": 92785.63451579728
          },
          "point_estimate": 92678.41902843412,
          "standard_error": 52.63242980690489
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92542.37408821034,
            "upper_bound": 92830.86488549618
          },
          "point_estimate": 92661.74268447838,
          "standard_error": 64.32220837701674
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.878921276743554,
            "upper_bound": 328.43542178571494
          },
          "point_estimate": 137.09695640829077,
          "standard_error": 71.50482597335724
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92546.73560685987,
            "upper_bound": 92704.30316815135
          },
          "point_estimate": 92643.88447176231,
          "standard_error": 40.580390250051884
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.53810120680673,
            "upper_bound": 220.7350465422217
          },
          "point_estimate": 175.4953775424122,
          "standard_error": 34.40358516475661
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81350.99251891817,
            "upper_bound": 81491.0887983464
          },
          "point_estimate": 81422.35284940209,
          "standard_error": 35.89743879127568
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81330.73617339312,
            "upper_bound": 81496.68385650223
          },
          "point_estimate": 81441.07585949177,
          "standard_error": 43.187496746787744
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.649297031582288,
            "upper_bound": 197.2285265152941
          },
          "point_estimate": 99.23163234702533,
          "standard_error": 47.56196539502772
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81342.38136341952,
            "upper_bound": 81456.12185782683
          },
          "point_estimate": 81409.22782598567,
          "standard_error": 28.75759207334444
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.60592220837955,
            "upper_bound": 156.7567340705643
          },
          "point_estimate": 119.46754252458778,
          "standard_error": 25.01330829080947
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57723.75264109347,
            "upper_bound": 57779.10886904763
          },
          "point_estimate": 57745.40113315696,
          "standard_error": 15.203043018297889
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57721.71653439153,
            "upper_bound": 57748.12698412698
          },
          "point_estimate": 57729.947222222225,
          "standard_error": 6.309383550430605
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7628722086728256,
            "upper_bound": 35.15396196848226
          },
          "point_estimate": 9.00914817338463,
          "standard_error": 9.11917649587709
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57717.25199955129,
            "upper_bound": 57734.39049516221
          },
          "point_estimate": 57726.01056689343,
          "standard_error": 4.51450990377093
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.34490172419588,
            "upper_bound": 77.19325203777967
          },
          "point_estimate": 50.81905724627306,
          "standard_error": 22.77159015868059
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123985.34598010438,
            "upper_bound": 124265.27155679226
          },
          "point_estimate": 124140.03925636008,
          "standard_error": 72.18704466839564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124006.77482876711,
            "upper_bound": 124313.8698630137
          },
          "point_estimate": 124241.2708537182,
          "standard_error": 99.73778463681695
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.787560882831116,
            "upper_bound": 403.2935106825864
          },
          "point_estimate": 154.89730997264178,
          "standard_error": 94.68669842302822
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124041.33571736146,
            "upper_bound": 124265.22921903477
          },
          "point_estimate": 124115.29645970468,
          "standard_error": 54.77357460959066
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.78849490827112,
            "upper_bound": 332.0808451652437
          },
          "point_estimate": 239.85079310085263,
          "standard_error": 66.28357945288755
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76544.93056842106,
            "upper_bound": 76635.14387802841
          },
          "point_estimate": 76589.79917159566,
          "standard_error": 23.199030116279744
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76511.63968421053,
            "upper_bound": 76659.13082706767
          },
          "point_estimate": 76595.14263157896,
          "standard_error": 44.98739858594396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.624153317275101,
            "upper_bound": 126.8658629932862
          },
          "point_estimate": 110.2791674105673,
          "standard_error": 34.506252728321684
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76536.93570065577,
            "upper_bound": 76669.88258329309
          },
          "point_estimate": 76607.31234996582,
          "standard_error": 35.19253579419438
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.658708628411354,
            "upper_bound": 90.7938608814593
          },
          "point_estimate": 77.23168583797049,
          "standard_error": 10.351978643529243
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76147.22414799253,
            "upper_bound": 76312.12854470123
          },
          "point_estimate": 76226.43729925304,
          "standard_error": 42.45747430618467
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76100.6474789916,
            "upper_bound": 76372.57166199814
          },
          "point_estimate": 76192.49772408964,
          "standard_error": 72.69091867758878
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.667162520194774,
            "upper_bound": 241.3231507156707
          },
          "point_estimate": 145.35434021357307,
          "standard_error": 61.64650181797016
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76121.86601850831,
            "upper_bound": 76296.29337439622
          },
          "point_estimate": 76204.841449307,
          "standard_error": 46.04379333737797
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.91126589188328,
            "upper_bound": 171.15817226821562
          },
          "point_estimate": 141.3519758117757,
          "standard_error": 23.392753798783488
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80471.1348051904,
            "upper_bound": 80614.15790993559
          },
          "point_estimate": 80539.52778939219,
          "standard_error": 36.70895123714944
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80451.21532397142,
            "upper_bound": 80639.82154735508
          },
          "point_estimate": 80524.07233924612,
          "standard_error": 36.227245206864126
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.548731686334758,
            "upper_bound": 224.2543995663537
          },
          "point_estimate": 82.7111989380804,
          "standard_error": 55.17382872104386
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80470.64937861693,
            "upper_bound": 80609.85207267014
          },
          "point_estimate": 80532.41284879201,
          "standard_error": 35.14049717049481
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.77106063506911,
            "upper_bound": 156.8942905577949
          },
          "point_estimate": 122.3518548230879,
          "standard_error": 26.076246936102432
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71912.65016791018,
            "upper_bound": 72036.49698774163
          },
          "point_estimate": 71973.79277769919,
          "standard_error": 31.748846432515087
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71877.54843234323,
            "upper_bound": 72067.216718529
          },
          "point_estimate": 71967.96874587459,
          "standard_error": 53.65635641878702
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.027685884789683,
            "upper_bound": 175.79011737415178
          },
          "point_estimate": 136.1106899845485,
          "standard_error": 37.66397989823935
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71929.66442569673,
            "upper_bound": 72039.63648771252
          },
          "point_estimate": 71981.58757875787,
          "standard_error": 27.962898689155384
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.03092981660711,
            "upper_bound": 126.70153236622896
          },
          "point_estimate": 106.02709636145732,
          "standard_error": 15.274555873774764
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 259249.6312063492,
            "upper_bound": 260053.14470790816
          },
          "point_estimate": 259635.6532026644,
          "standard_error": 204.97229429067937
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258922.39428571428,
            "upper_bound": 259997.22946428572
          },
          "point_estimate": 259652.03270975055,
          "standard_error": 268.5639570820808
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.25836234433736,
            "upper_bound": 1323.5898468765645
          },
          "point_estimate": 554.9343440467372,
          "standard_error": 302.17070453605686
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 259394.35315432743,
            "upper_bound": 259932.64381350868
          },
          "point_estimate": 259730.6110018553,
          "standard_error": 138.526882989855
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 365.9713557356265,
            "upper_bound": 901.3063310126662
          },
          "point_estimate": 681.9445678593922,
          "standard_error": 146.9737711355024
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145511.31391555557,
            "upper_bound": 145961.307431
          },
          "point_estimate": 145726.8940588889,
          "standard_error": 115.10852648889232
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145376.34,
            "upper_bound": 146061.738
          },
          "point_estimate": 145728.64177777778,
          "standard_error": 159.8397211372962
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.782319738070626,
            "upper_bound": 631.0079910373973
          },
          "point_estimate": 508.0855283796903,
          "standard_error": 155.39465837716818
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145439.280772182,
            "upper_bound": 145740.47184071422
          },
          "point_estimate": 145608.59206233767,
          "standard_error": 76.79862134725613
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 210.10540815970953,
            "upper_bound": 485.13175359631975
          },
          "point_estimate": 382.960838753345,
          "standard_error": 70.84465679164386
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71761.33047363303,
            "upper_bound": 71915.80924103815
          },
          "point_estimate": 71838.33769989983,
          "standard_error": 39.530593070524645
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71731.80977427131,
            "upper_bound": 71974.23570019724
          },
          "point_estimate": 71816.6651627219,
          "standard_error": 71.12340144222722
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.69704842842114,
            "upper_bound": 207.32843090169783
          },
          "point_estimate": 168.1162934321696,
          "standard_error": 49.84320057600963
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71743.70840173781,
            "upper_bound": 71922.64061268956
          },
          "point_estimate": 71808.37378518916,
          "standard_error": 45.77978980000948
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.9012110667066,
            "upper_bound": 156.48588067971758
          },
          "point_estimate": 131.48390909223497,
          "standard_error": 18.321826358394063
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233097.08222542735,
            "upper_bound": 234414.5816843712
          },
          "point_estimate": 233725.63133089137,
          "standard_error": 337.9799681727263
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232877.74679487175,
            "upper_bound": 234646.0608974359
          },
          "point_estimate": 233595.9564102564,
          "standard_error": 376.0959221903633
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122.20303129199988,
            "upper_bound": 2088.1340358769594
          },
          "point_estimate": 815.9890153210035,
          "standard_error": 482.6504420923862
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232744.6119919388,
            "upper_bound": 233746.6758020932
          },
          "point_estimate": 233224.83786213785,
          "standard_error": 257.94172662124066
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 518.3375069092551,
            "upper_bound": 1427.9522720677005
          },
          "point_estimate": 1127.7475265143748,
          "standard_error": 233.21179771975983
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59254.59241517372,
            "upper_bound": 59348.82321089266
          },
          "point_estimate": 59301.80959412646,
          "standard_error": 24.160365809428463
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59230.47864639884,
            "upper_bound": 59373.73638901815
          },
          "point_estimate": 59307.92969598263,
          "standard_error": 40.66830044046325
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.256092166285967,
            "upper_bound": 133.0260066774067
          },
          "point_estimate": 98.37349974646698,
          "standard_error": 28.47386387756756
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59247.594128827724,
            "upper_bound": 59338.622641244525
          },
          "point_estimate": 59282.902698929734,
          "standard_error": 23.078092469898653
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.73210925207758,
            "upper_bound": 97.44809020146698
          },
          "point_estimate": 80.73398604704339,
          "standard_error": 11.918320754990429
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65215.60852072628,
            "upper_bound": 65372.28784972022
          },
          "point_estimate": 65284.023184595186,
          "standard_error": 40.84530992236625
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65187.220173860915,
            "upper_bound": 65332.239208633095
          },
          "point_estimate": 65247.54626298961,
          "standard_error": 40.12228251087252
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.471668462885724,
            "upper_bound": 174.03661561527144
          },
          "point_estimate": 99.18184953413584,
          "standard_error": 36.762317813507565
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65220.37972954969,
            "upper_bound": 65299.93568941638
          },
          "point_estimate": 65264.10035971223,
          "standard_error": 19.97681331460377
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.58845896079986,
            "upper_bound": 196.41581984722845
          },
          "point_estimate": 136.79884470205934,
          "standard_error": 44.66569047080377
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68437.56788980264,
            "upper_bound": 68594.741381523
          },
          "point_estimate": 68511.65496822415,
          "standard_error": 40.26466154332432
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68414.73809523809,
            "upper_bound": 68585.13480128894
          },
          "point_estimate": 68491.42130325815,
          "standard_error": 40.521517446629545
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.636202268370831,
            "upper_bound": 221.8659304032039
          },
          "point_estimate": 126.31507595294973,
          "standard_error": 54.794966155964374
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68417.42438653941,
            "upper_bound": 68526.35132154365
          },
          "point_estimate": 68467.03217947466,
          "standard_error": 27.5563661991098
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.94742129324821,
            "upper_bound": 178.1194503559754
          },
          "point_estimate": 133.71430311045964,
          "standard_error": 30.649046431370262
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92621.64635768808,
            "upper_bound": 92732.52203592632
          },
          "point_estimate": 92673.13449442624,
          "standard_error": 28.55639219486702
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92595.83605961468,
            "upper_bound": 92744.70186598811
          },
          "point_estimate": 92642.17748091604,
          "standard_error": 38.920372131621065
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.162978562682476,
            "upper_bound": 160.53870468805616
          },
          "point_estimate": 82.44048082646732,
          "standard_error": 37.42264640793877
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92605.70878834464,
            "upper_bound": 92717.51058688531
          },
          "point_estimate": 92651.20800370112,
          "standard_error": 28.67319713560455
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.32770016966825,
            "upper_bound": 123.6597763656636
          },
          "point_estimate": 95.1707123891857,
          "standard_error": 21.34822827605576
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66302.19177988138,
            "upper_bound": 66381.3182535229
          },
          "point_estimate": 66337.83206001623,
          "standard_error": 20.37613684879336
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66296.27166970803,
            "upper_bound": 66384.71218572586
          },
          "point_estimate": 66321.08321167884,
          "standard_error": 17.059274794903
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.255925125681456,
            "upper_bound": 93.56143731704942
          },
          "point_estimate": 22.621080227966345,
          "standard_error": 22.5896270287145
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66299.17319310989,
            "upper_bound": 66332.21348062354
          },
          "point_estimate": 66317.82256138022,
          "standard_error": 8.359138351364992
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.12908435045119,
            "upper_bound": 87.99172378172739
          },
          "point_estimate": 67.96566530690305,
          "standard_error": 18.32263748314199
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92874.01407366707,
            "upper_bound": 93163.89017094015
          },
          "point_estimate": 93014.39786284088,
          "standard_error": 74.380814166877
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92752.00427350428,
            "upper_bound": 93199.6955128205
          },
          "point_estimate": 93018.50082417583,
          "standard_error": 90.35642884106996
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.936481263394263,
            "upper_bound": 449.4049437137746
          },
          "point_estimate": 250.87809118703916,
          "standard_error": 131.89606038028026
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92836.55916589434,
            "upper_bound": 93066.27499214932
          },
          "point_estimate": 92964.8447019647,
          "standard_error": 58.91571812471623
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.49162025760188,
            "upper_bound": 315.13055341457704
          },
          "point_estimate": 247.53153713008663,
          "standard_error": 47.18789359327216
        }
      }
    },
    "memmem/twoway/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/twoway_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19202.86367376665,
            "upper_bound": 19297.206348119707
          },
          "point_estimate": 19250.128311327186,
          "standard_error": 24.14129123636291
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19182.794160487287,
            "upper_bound": 19311.80790960452
          },
          "point_estimate": 19252.65214512712,
          "standard_error": 39.05856178850847
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.137008062831123,
            "upper_bound": 129.45771305786175
          },
          "point_estimate": 92.04571360023692,
          "standard_error": 30.654987662315655
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19175.37844209149,
            "upper_bound": 19240.1854225972
          },
          "point_estimate": 19207.054830233326,
          "standard_error": 16.33584188992094
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.79269192213026,
            "upper_bound": 99.409004193796
          },
          "point_estimate": 80.21891186098087,
          "standard_error": 12.97650676414854
        }
      }
    },
    "memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/twoway_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19129.25137430027,
            "upper_bound": 19175.69380745287
          },
          "point_estimate": 19151.717096480657,
          "standard_error": 11.88774464666078
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19115.85061939905,
            "upper_bound": 19174.269811983835
          },
          "point_estimate": 19153.690959409592,
          "standard_error": 14.54009859893505
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.556949347840944,
            "upper_bound": 70.1029591244298
          },
          "point_estimate": 41.309726572880585,
          "standard_error": 15.271158813744826
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19109.56275869051,
            "upper_bound": 19160.680574822985
          },
          "point_estimate": 19131.33239222559,
          "standard_error": 13.217099257630515
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.78218642715544,
            "upper_bound": 51.97651394360154
          },
          "point_estimate": 39.621231215880215,
          "standard_error": 8.263031078246119
        }
      }
    },
    "memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/twoway_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247321.52716846857,
            "upper_bound": 247661.1961331592
          },
          "point_estimate": 247500.91848531476,
          "standard_error": 87.52366978589116
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247259.25,
            "upper_bound": 247685.73847316703
          },
          "point_estimate": 247589.3559280855,
          "standard_error": 108.68817541150833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.786705573916606,
            "upper_bound": 464.1786024734783
          },
          "point_estimate": 152.11734276424326,
          "standard_error": 121.33618019728128
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247297.3401832955,
            "upper_bound": 247628.2377012581
          },
          "point_estimate": 247471.7290573372,
          "standard_error": 89.92825836470958
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.4836221500473,
            "upper_bound": 375.3550580712481
          },
          "point_estimate": 290.9266802978923,
          "standard_error": 63.35732915090411
        }
      }
    },
    "memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/twoway_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527.0369714082882,
            "upper_bound": 528.6529885167056
          },
          "point_estimate": 527.7952611865132,
          "standard_error": 0.4154556212148926
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 526.7504388332143,
            "upper_bound": 529.2251202232823
          },
          "point_estimate": 527.2230048283809,
          "standard_error": 0.5979561860730229
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06421435125884419,
            "upper_bound": 2.080550387551901
          },
          "point_estimate": 0.8318687152650506,
          "standard_error": 0.5405020745692607
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 526.9613160511732,
            "upper_bound": 529.156023639913
          },
          "point_estimate": 528.0524763378643,
          "standard_error": 0.5635532037834868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5036523675309893,
            "upper_bound": 1.6499145766000167
          },
          "point_estimate": 1.3830865629239364,
          "standard_error": 0.25361560518700327
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.48681140590254,
            "upper_bound": 44.622730414770174
          },
          "point_estimate": 44.55255138300405,
          "standard_error": 0.03485934145271725
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.44524821900048,
            "upper_bound": 44.67078654452021
          },
          "point_estimate": 44.530903931553006,
          "standard_error": 0.058751782802818765
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018906075674709328,
            "upper_bound": 0.18778463720126723
          },
          "point_estimate": 0.13192082151141313,
          "standard_error": 0.04865084833203897
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.46627945614956,
            "upper_bound": 44.627742555740774
          },
          "point_estimate": 44.5443461526968,
          "standard_error": 0.04283838365893674
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06334248942671188,
            "upper_bound": 0.13678095179485836
          },
          "point_estimate": 0.11596492640265406,
          "standard_error": 0.01774272588891521
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.53924510717846,
            "upper_bound": 47.702673915184214
          },
          "point_estimate": 47.62632470536454,
          "standard_error": 0.04169524379962678
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.51100015603058,
            "upper_bound": 47.71762735103924
          },
          "point_estimate": 47.69926591638371,
          "standard_error": 0.05800448481108324
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000707622111966585,
            "upper_bound": 0.19641690550773644
          },
          "point_estimate": 0.03055925413609076,
          "standard_error": 0.05399615909178668
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.586517022283886,
            "upper_bound": 47.718693378725035
          },
          "point_estimate": 47.67858632580538,
          "standard_error": 0.03567989066965275
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023396061605040805,
            "upper_bound": 0.16663789526504377
          },
          "point_estimate": 0.139217130758347,
          "standard_error": 0.030445495650335972
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.74933434860121,
            "upper_bound": 26.762629736484417
          },
          "point_estimate": 26.754585825039506,
          "standard_error": 0.0036238734272560423
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.748624544028942,
            "upper_bound": 26.75455253573996
          },
          "point_estimate": 26.752113944503193,
          "standard_error": 0.0016299677915821672
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008769065634425284,
            "upper_bound": 0.008482927704234668
          },
          "point_estimate": 0.003872620833024886,
          "standard_error": 0.0020925907827117687
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.749474677784743,
            "upper_bound": 26.75270142336463
          },
          "point_estimate": 26.75107450902723,
          "standard_error": 0.0008124437026058132
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022995279308807236,
            "upper_bound": 0.018316049808565063
          },
          "point_estimate": 0.01205558091917417,
          "standard_error": 0.005342043407274663
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.751892122708693,
            "upper_bound": 26.77284863324156
          },
          "point_estimate": 26.76091618523954,
          "standard_error": 0.005419279438086151
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.74950629195341,
            "upper_bound": 26.765728096229516
          },
          "point_estimate": 26.75843616311169,
          "standard_error": 0.003867304464762988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019088858474371991,
            "upper_bound": 0.02022915996307397
          },
          "point_estimate": 0.01134289729118667,
          "standard_error": 0.004550098706789258
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.754272761469043,
            "upper_bound": 26.760828920667084
          },
          "point_estimate": 26.75754026022388,
          "standard_error": 0.001644397911511305
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005278350731693945,
            "upper_bound": 0.02633687059047571
          },
          "point_estimate": 0.018055007008264,
          "standard_error": 0.006368275446513858
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.144918843172725,
            "upper_bound": 38.2120601456029
          },
          "point_estimate": 38.18538983025261,
          "standard_error": 0.01800138364059806
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.191318513725854,
            "upper_bound": 38.20856191998685
          },
          "point_estimate": 38.19630358838407,
          "standard_error": 0.005372051142196847
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002114915648100656,
            "upper_bound": 0.04115679039436176
          },
          "point_estimate": 0.007794059200102225,
          "standard_error": 0.010021385979163704
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.17888107456199,
            "upper_bound": 38.20043221615687
          },
          "point_estimate": 38.193491764182845,
          "standard_error": 0.005753864599469793
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004731276127226123,
            "upper_bound": 0.09149592863978052
          },
          "point_estimate": 0.0602254568887509,
          "standard_error": 0.027245399842348406
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.42545215386898,
            "upper_bound": 53.4552407259828
          },
          "point_estimate": 53.43862330562665,
          "standard_error": 0.00763775028387479
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.42891635977026,
            "upper_bound": 53.443400082383256
          },
          "point_estimate": 53.434419646202606,
          "standard_error": 0.004049289748934389
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001950970662492833,
            "upper_bound": 0.030852180468163983
          },
          "point_estimate": 0.008442970775480278,
          "standard_error": 0.006717960431648133
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.42426244784222,
            "upper_bound": 53.4349628389114
          },
          "point_estimate": 53.43091411313653,
          "standard_error": 0.002746002473453907
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005302309496646222,
            "upper_bound": 0.03729262547622064
          },
          "point_estimate": 0.025405039193086388,
          "standard_error": 0.008950967386841547
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.67915259318765,
            "upper_bound": 68.80218520287924
          },
          "point_estimate": 68.73716771770408,
          "standard_error": 0.031201413270628853
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.66512907895977,
            "upper_bound": 68.86701798009985
          },
          "point_estimate": 68.68734365474964,
          "standard_error": 0.04931077745089247
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006172819180713214,
            "upper_bound": 0.16100761305786723
          },
          "point_estimate": 0.034822399225140355,
          "standard_error": 0.040283784211386474
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.66703573648864,
            "upper_bound": 68.79991779852817
          },
          "point_estimate": 68.71231064031647,
          "standard_error": 0.03771969543430047
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01823995820801507,
            "upper_bound": 0.11927985926127056
          },
          "point_estimate": 0.10345036514423156,
          "standard_error": 0.020221759664071443
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.051957650712815,
            "upper_bound": 46.08991014118732
          },
          "point_estimate": 46.06851039329028,
          "standard_error": 0.009836251510413547
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.046110682795465,
            "upper_bound": 46.08098266317016
          },
          "point_estimate": 46.06249134319786,
          "standard_error": 0.0098724310590328
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003647389339315431,
            "upper_bound": 0.04051833840982357
          },
          "point_estimate": 0.0221288190304633,
          "standard_error": 0.009523811305218975
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.05025352184048,
            "upper_bound": 46.07153247700957
          },
          "point_estimate": 46.06121444102669,
          "standard_error": 0.005501791808942853
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011890898009932297,
            "upper_bound": 0.04692358759772178
          },
          "point_estimate": 0.03278336691916422,
          "standard_error": 0.010547070350442687
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.48834326708838,
            "upper_bound": 64.54525900064398
          },
          "point_estimate": 64.51715489436245,
          "standard_error": 0.014636285502161084
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.47120044608769,
            "upper_bound": 64.56339228547444
          },
          "point_estimate": 64.52002143714222,
          "standard_error": 0.0239573541081466
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01197125172606568,
            "upper_bound": 0.08406214468803797
          },
          "point_estimate": 0.06338549262850712,
          "standard_error": 0.018233609063856175
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.46452267254558,
            "upper_bound": 64.52319661811475
          },
          "point_estimate": 64.48868141341748,
          "standard_error": 0.01504125187938108
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030333666734374622,
            "upper_bound": 0.05818021258482672
          },
          "point_estimate": 0.048846174128916536,
          "standard_error": 0.007048079417319916
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.32660585458317,
            "upper_bound": 53.58065369423472
          },
          "point_estimate": 53.44525351196016,
          "standard_error": 0.06409440797992086
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.305465874893,
            "upper_bound": 53.646980817998376
          },
          "point_estimate": 53.33096100238579,
          "standard_error": 0.08816184304526924
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010058578019762506,
            "upper_bound": 0.3167004393033383
          },
          "point_estimate": 0.04022106148455075,
          "standard_error": 0.085368486378358
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.300315284619614,
            "upper_bound": 53.36944216640387
          },
          "point_estimate": 53.32071318654544,
          "standard_error": 0.018247680342153877
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027921802886014792,
            "upper_bound": 0.2640329410320704
          },
          "point_estimate": 0.21309752437664223,
          "standard_error": 0.048631283806117954
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.86529046621648,
            "upper_bound": 39.94686904114148
          },
          "point_estimate": 39.911388918938016,
          "standard_error": 0.021196621225046352
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.88019111342363,
            "upper_bound": 39.945227301979465
          },
          "point_estimate": 39.93078947527493,
          "standard_error": 0.013839391429068195
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00328300573859909,
            "upper_bound": 0.09161027066388466
          },
          "point_estimate": 0.017961403117921836,
          "standard_error": 0.02240702910246973
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.84926977571298,
            "upper_bound": 39.94203554843813
          },
          "point_estimate": 39.90824349309808,
          "standard_error": 0.02418544131498276
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010119221607269588,
            "upper_bound": 0.09820061781371668
          },
          "point_estimate": 0.07063906783449826,
          "standard_error": 0.0227900728817642
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.610342783137206,
            "upper_bound": 62.65394590741248
          },
          "point_estimate": 62.631527125436115,
          "standard_error": 0.011180671299644429
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.598559551680594,
            "upper_bound": 62.65405196152045
          },
          "point_estimate": 62.63005156086611,
          "standard_error": 0.013435475192848906
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007526215916691037,
            "upper_bound": 0.06370072024725473
          },
          "point_estimate": 0.04113652268396692,
          "standard_error": 0.01388964227713808
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.62248035143483,
            "upper_bound": 62.649459365122624
          },
          "point_estimate": 62.63540017320671,
          "standard_error": 0.006888340321473202
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019429739416491867,
            "upper_bound": 0.04899498550274378
          },
          "point_estimate": 0.03723883171792124,
          "standard_error": 0.007722008908320984
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230804.22103870305,
            "upper_bound": 231807.80695329088
          },
          "point_estimate": 231334.99750353865,
          "standard_error": 255.54775328987415
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230158.7571656051,
            "upper_bound": 231861.35536093416
          },
          "point_estimate": 231737.2383227176,
          "standard_error": 407.7919777648647
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.09784681135021,
            "upper_bound": 1222.2594054767767
          },
          "point_estimate": 266.36868014362494,
          "standard_error": 345.2243470444159
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230296.79144795536,
            "upper_bound": 231667.38338685344
          },
          "point_estimate": 230706.44526428985,
          "standard_error": 311.07711768528003
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.8353549015182,
            "upper_bound": 991.8602651185004
          },
          "point_estimate": 851.6902704311859,
          "standard_error": 152.23778024689193
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169691.42585538584,
            "upper_bound": 170292.0536493102
          },
          "point_estimate": 169970.53658414923,
          "standard_error": 153.72272551225527
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169596.9308411215,
            "upper_bound": 170473.54049844237
          },
          "point_estimate": 169746.23720794392,
          "standard_error": 214.5101215399837
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.53927976098283,
            "upper_bound": 824.6888124616545
          },
          "point_estimate": 239.06364146837825,
          "standard_error": 201.84518445739255
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169656.60206175662,
            "upper_bound": 169828.4033786622
          },
          "point_estimate": 169742.4676417041,
          "standard_error": 43.07250900977352
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.4497224773742,
            "upper_bound": 606.2048557840742
          },
          "point_estimate": 511.78424555132545,
          "standard_error": 101.28277059855336
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300894.3542318067,
            "upper_bound": 302021.9486914601
          },
          "point_estimate": 301491.70350091823,
          "standard_error": 289.94052865663735
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300724.826446281,
            "upper_bound": 302175.85537190083
          },
          "point_estimate": 301847.280130854,
          "standard_error": 387.3954054531148
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.53727910351414,
            "upper_bound": 1562.8686714270748
          },
          "point_estimate": 580.3031078793747,
          "standard_error": 377.1301450952347
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300476.2502649966,
            "upper_bound": 301953.003820999
          },
          "point_estimate": 301324.8041214983,
          "standard_error": 394.59358586212625
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387.4847993780101,
            "upper_bound": 1181.109930891195
          },
          "point_estimate": 966.7578360236912,
          "standard_error": 192.30256759315796
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 360063.55003418203,
            "upper_bound": 360707.1916474147
          },
          "point_estimate": 360344.2488397769,
          "standard_error": 166.72178981068865
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 359952.82574257423,
            "upper_bound": 360578.2194306931
          },
          "point_estimate": 360175.1080858086,
          "standard_error": 171.88576353219912
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.2098165200304,
            "upper_bound": 723.5702534246673
          },
          "point_estimate": 419.3077921597439,
          "standard_error": 158.48583280707751
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 360005.6522918288,
            "upper_bound": 360498.5681061697
          },
          "point_estimate": 360320.8347434743,
          "standard_error": 122.41629009633682
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208.21759023897852,
            "upper_bound": 795.9137379691309
          },
          "point_estimate": 555.6683187705421,
          "standard_error": 177.0968921584592
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 501052.0169243313,
            "upper_bound": 501792.00194797775
          },
          "point_estimate": 501423.5625375082,
          "standard_error": 190.0006375192845
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500775.0616438356,
            "upper_bound": 501948.39954337897
          },
          "point_estimate": 501499.01621004567,
          "standard_error": 301.72993129690815
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.18207767116274,
            "upper_bound": 1085.1560234744002
          },
          "point_estimate": 803.5878028567143,
          "standard_error": 242.3249873798691
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 501081.7200509762,
            "upper_bound": 501950.2617951628
          },
          "point_estimate": 501516.3409001957,
          "standard_error": 224.34453154399873
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392.1554379104411,
            "upper_bound": 758.862918220693
          },
          "point_estimate": 632.7052244770596,
          "standard_error": 93.95352331272994
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104179.00216785204,
            "upper_bound": 104279.60611760642
          },
          "point_estimate": 104218.65895040252,
          "standard_error": 27.408456897886825
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104169.15959885386,
            "upper_bound": 104228.2870105062
          },
          "point_estimate": 104185.0182425979,
          "standard_error": 18.258563957626333
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.058997084397263,
            "upper_bound": 70.7792979626184
          },
          "point_estimate": 31.567140757629208,
          "standard_error": 17.56823251347079
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104174.38872302482,
            "upper_bound": 104211.25312257488
          },
          "point_estimate": 104188.90220667583,
          "standard_error": 9.442353303142244
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.26579581026059,
            "upper_bound": 138.85881543836726
          },
          "point_estimate": 91.60853418958176,
          "standard_error": 39.997713843429345
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159901.52401524648,
            "upper_bound": 159982.7865225564
          },
          "point_estimate": 159936.25523235174,
          "standard_error": 21.087623690444023
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159899.28305921052,
            "upper_bound": 159950.92285401002
          },
          "point_estimate": 159921.23209064326,
          "standard_error": 13.030140150666051
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.92029006354052,
            "upper_bound": 76.6209400431897
          },
          "point_estimate": 31.329028128003568,
          "standard_error": 17.316712668960253
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159898.76468971564,
            "upper_bound": 159944.57725859416
          },
          "point_estimate": 159920.72313738894,
          "standard_error": 12.010009075830077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.433147127088898,
            "upper_bound": 103.33125062156714
          },
          "point_estimate": 70.44687845712416,
          "standard_error": 25.509543683483574
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295511.99579878047,
            "upper_bound": 295845.202346206
          },
          "point_estimate": 295670.49700251647,
          "standard_error": 85.20993458597681
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295506.9825058072,
            "upper_bound": 295795.50033875334
          },
          "point_estimate": 295638.5739837398,
          "standard_error": 58.55760625791883
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.134584789948452,
            "upper_bound": 442.0381579245807
          },
          "point_estimate": 119.0904455686522,
          "standard_error": 114.19106954286065
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295535.54671492486,
            "upper_bound": 295736.2561109275
          },
          "point_estimate": 295641.9142645972,
          "standard_error": 51.009899887076735
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.13407982730936,
            "upper_bound": 397.50839305173054
          },
          "point_estimate": 283.4932886600426,
          "standard_error": 75.9889964742807
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271610.26179120026,
            "upper_bound": 272336.78169838316
          },
          "point_estimate": 271895.8645211443,
          "standard_error": 198.81299853996785
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271557.7938432836,
            "upper_bound": 271930.3078358209
          },
          "point_estimate": 271720.2235696517,
          "standard_error": 91.33439301226925
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.06587056636948,
            "upper_bound": 473.5075333286023
          },
          "point_estimate": 190.259389999062,
          "standard_error": 118.33806931738616
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271638.61024902127,
            "upper_bound": 271821.7114429673
          },
          "point_estimate": 271725.50947858114,
          "standard_error": 46.191754132406366
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.76757023892156,
            "upper_bound": 1007.022905198008
          },
          "point_estimate": 662.7582940852342,
          "standard_error": 294.2873729622704
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141588.34330150206,
            "upper_bound": 142193.47708007813
          },
          "point_estimate": 141911.97724624877,
          "standard_error": 154.99947396104074
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141753.41650390625,
            "upper_bound": 142272.30078125
          },
          "point_estimate": 141883.77259114583,
          "standard_error": 135.05856909767627
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.001449591713026,
            "upper_bound": 761.2269743761324
          },
          "point_estimate": 259.89017819852086,
          "standard_error": 184.7627002542538
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141203.39452239918,
            "upper_bound": 141921.49138271363
          },
          "point_estimate": 141618.9884841721,
          "standard_error": 203.3265828528316
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198.324666191258,
            "upper_bound": 728.7167829192717
          },
          "point_estimate": 515.8080801962495,
          "standard_error": 145.77569989932897
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151881.4848181217,
            "upper_bound": 152051.9524655754
          },
          "point_estimate": 151956.90669775134,
          "standard_error": 44.07058271832626
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151867.59699074074,
            "upper_bound": 152042.4640625
          },
          "point_estimate": 151885.82351190475,
          "standard_error": 44.57978910247168
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.718992211760107,
            "upper_bound": 210.62649188561505
          },
          "point_estimate": 58.26019757681587,
          "standard_error": 53.54170314687539
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151878.02281715293,
            "upper_bound": 151949.53920066234
          },
          "point_estimate": 151903.63231601732,
          "standard_error": 18.564466767755928
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.059017825571125,
            "upper_bound": 198.84266552611103
          },
          "point_estimate": 146.66897828800774,
          "standard_error": 42.48986813783275
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152103.55230125523,
            "upper_bound": 152248.98857740586
          },
          "point_estimate": 152176.80315501097,
          "standard_error": 37.2849240427962
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152052.00976290097,
            "upper_bound": 152302.4769874477
          },
          "point_estimate": 152177.38045427375,
          "standard_error": 61.98870546752548
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.168117958122203,
            "upper_bound": 210.4704712424942
          },
          "point_estimate": 185.67135026017095,
          "standard_error": 49.52461159809383
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152069.2727441782,
            "upper_bound": 152254.06765534944
          },
          "point_estimate": 152164.85173069607,
          "standard_error": 48.684605817822145
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.16686210110498,
            "upper_bound": 147.44796186558136
          },
          "point_estimate": 124.32372263383282,
          "standard_error": 17.8377483036875
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85041.41586964564,
            "upper_bound": 85098.12586233264
          },
          "point_estimate": 85068.94611639593,
          "standard_error": 14.468175497376476
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85034.9327978972,
            "upper_bound": 85101.67679127726
          },
          "point_estimate": 85065.8765186916,
          "standard_error": 17.210948995572462
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.1568712496008,
            "upper_bound": 81.83419261840608
          },
          "point_estimate": 48.49587593030684,
          "standard_error": 17.376647761991997
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85043.78854205608,
            "upper_bound": 85086.26739710398
          },
          "point_estimate": 85061.46695594126,
          "standard_error": 10.778944971276273
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.040095028137856,
            "upper_bound": 63.19324077834523
          },
          "point_estimate": 48.25064380628375,
          "standard_error": 9.944324544232202
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/twoway_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28835.0931987916,
            "upper_bound": 28858.9687230665
          },
          "point_estimate": 28847.203767703068,
          "standard_error": 6.1174017566473635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28832.42267564712,
            "upper_bound": 28862.875
          },
          "point_estimate": 28848.653559165345,
          "standard_error": 7.33587642330564
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.672627630991475,
            "upper_bound": 34.275809890696586
          },
          "point_estimate": 17.634477849893155,
          "standard_error": 7.814208026367834
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28832.43697533751,
            "upper_bound": 28857.670925089293
          },
          "point_estimate": 28843.826103690288,
          "standard_error": 6.445533695007143
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.775183198355109,
            "upper_bound": 26.50286428042274
          },
          "point_estimate": 20.368167369103855,
          "standard_error": 4.031413163457613
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/twoway_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2042364.810771605,
            "upper_bound": 2043357.1304146824
          },
          "point_estimate": 2042784.317888007,
          "standard_error": 262.9863027926839
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2042352.7793209876,
            "upper_bound": 2043123.3333333335
          },
          "point_estimate": 2042431.4712301583,
          "standard_error": 165.52597085596116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.22214417394349,
            "upper_bound": 964.195949548986
          },
          "point_estimate": 165.93222298001666,
          "standard_error": 220.55124730473523
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2042358.7049539955,
            "upper_bound": 2043116.0548341656
          },
          "point_estimate": 2042660.4415584416,
          "standard_error": 205.8526333113495
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.43012207371454,
            "upper_bound": 1242.3287117483192
          },
          "point_estimate": 877.3987419330228,
          "standard_error": 316.85241056718786
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/twoway_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4091.320169302414,
            "upper_bound": 4095.575859085434
          },
          "point_estimate": 4093.0027818255576,
          "standard_error": 1.164617134072569
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4091.3159339112817,
            "upper_bound": 4093.266522179689
          },
          "point_estimate": 4091.847133314032,
          "standard_error": 0.4596798086523095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.18833654529396393,
            "upper_bound": 2.8422994262848564
          },
          "point_estimate": 0.5990612319525184,
          "standard_error": 0.7468973429521731
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4091.6153325151104,
            "upper_bound": 4092.5856791363663
          },
          "point_estimate": 4092.032750901429,
          "standard_error": 0.2455022592500168
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4875858299355424,
            "upper_bound": 5.876544382185329
          },
          "point_estimate": 3.882026280546762,
          "standard_error": 1.7110787961666265
        }
      }
    },
    "memrchr1/krate/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr1/krate/empty/never",
        "directory_name": "memrchr1/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4949182267633893,
            "upper_bound": 0.5070862972721881
          },
          "point_estimate": 0.5003581527806937,
          "standard_error": 0.003146363376940327
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4934747750011958,
            "upper_bound": 0.5072934546762237
          },
          "point_estimate": 0.4969148290808296,
          "standard_error": 0.0027895159315211257
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011739763740725429,
            "upper_bound": 0.014435735423303493
          },
          "point_estimate": 0.004657926826004898,
          "standard_error": 0.0033025887330548585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.49541652876640657,
            "upper_bound": 0.4983867754536534
          },
          "point_estimate": 0.4966555054600723,
          "standard_error": 0.0007524907828136948
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028219184282549607,
            "upper_bound": 0.013668269082907336
          },
          "point_estimate": 0.0104569928828253,
          "standard_error": 0.0028673427494665662
        }
      }
    },
    "memrchr1/krate/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/common",
        "directory_name": "memrchr1/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224869.95038016856,
            "upper_bound": 225359.98914199983
          },
          "point_estimate": 225104.786388399,
          "standard_error": 125.2129400913958
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224893.10150891633,
            "upper_bound": 225357.27160493823
          },
          "point_estimate": 225024.35730452675,
          "standard_error": 125.02075708827785
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.807452348269598,
            "upper_bound": 708.8726883409109
          },
          "point_estimate": 205.211970430828,
          "standard_error": 164.89857071709446
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224976.1713191926,
            "upper_bound": 225386.55829150273
          },
          "point_estimate": 225147.4719737053,
          "standard_error": 105.07104922444336
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165.78030175409594,
            "upper_bound": 576.7696814017605
          },
          "point_estimate": 418.3603494341263,
          "standard_error": 109.24002528896
        }
      }
    },
    "memrchr1/krate/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/never",
        "directory_name": "memrchr1/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9825.203190300554,
            "upper_bound": 9843.13235430041
          },
          "point_estimate": 9833.591494250652,
          "standard_error": 4.606966266762714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9821.62582566324,
            "upper_bound": 9848.365367262228
          },
          "point_estimate": 9827.512718501044,
          "standard_error": 6.981661403204063
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7149277147030514,
            "upper_bound": 24.517560095316483
          },
          "point_estimate": 10.287574186606088,
          "standard_error": 6.586742103183586
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9822.299286023634,
            "upper_bound": 9840.459475857928
          },
          "point_estimate": 9829.3862465634,
          "standard_error": 4.778649547626661
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.068272225675412,
            "upper_bound": 19.26125922529657
          },
          "point_estimate": 15.337660949163435,
          "standard_error": 3.131425599187292
        }
      }
    },
    "memrchr1/krate/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/rare",
        "directory_name": "memrchr1/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9847.843354243309,
            "upper_bound": 9865.887050321742
          },
          "point_estimate": 9856.588060880264,
          "standard_error": 4.6070876993915
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9843.330498328363,
            "upper_bound": 9868.182976416374
          },
          "point_estimate": 9855.43745885451,
          "standard_error": 6.557778398894221
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.1885193892263555,
            "upper_bound": 26.780863642191747
          },
          "point_estimate": 15.55833740879331,
          "standard_error": 5.439747318948129
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9842.137375950491,
            "upper_bound": 9857.176011661031
          },
          "point_estimate": 9847.79505866863,
          "standard_error": 3.8060989820380633
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.755414427376785,
            "upper_bound": 19.43387965428901
          },
          "point_estimate": 15.32026906297228,
          "standard_error": 2.78622253216831
        }
      }
    },
    "memrchr1/krate/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/uncommon",
        "directory_name": "memrchr1/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79377.39916119621,
            "upper_bound": 79669.54844866887
          },
          "point_estimate": 79518.08377644056,
          "standard_error": 74.67620750824899
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79329.22811816193,
            "upper_bound": 79709.40791393144
          },
          "point_estimate": 79479.22450765864,
          "standard_error": 84.74405089619931
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.526713709824023,
            "upper_bound": 446.5797127499647
          },
          "point_estimate": 207.02451268125196,
          "standard_error": 104.76950032431152
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79263.60969331753,
            "upper_bound": 79514.77084343348
          },
          "point_estimate": 79360.36130608997,
          "standard_error": 64.85909842361474
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.79382343032223,
            "upper_bound": 321.0797302560146
          },
          "point_estimate": 249.66999431010663,
          "standard_error": 50.38311150020963
        }
      }
    },
    "memrchr1/krate/huge/verycommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/verycommon",
        "directory_name": "memrchr1/krate_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462854.6226789733,
            "upper_bound": 463366.4221187463
          },
          "point_estimate": 463109.67791239714,
          "standard_error": 131.39768439785814
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462772.88481012656,
            "upper_bound": 463597.305907173
          },
          "point_estimate": 462999.78069620254,
          "standard_error": 220.8399385469056
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.5919978475957,
            "upper_bound": 720.0615411404151
          },
          "point_estimate": 526.6022032247971,
          "standard_error": 183.28583409891803
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462962.0384318993,
            "upper_bound": 463396.36694376846
          },
          "point_estimate": 463182.0437941805,
          "standard_error": 109.86147586020697
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274.2106122987113,
            "upper_bound": 523.62018785871
          },
          "point_estimate": 435.79130137937346,
          "standard_error": 64.28926052060133
        }
      }
    },
    "memrchr1/krate/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/common",
        "directory_name": "memrchr1/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 225.49176354104307,
            "upper_bound": 225.71245958650312
          },
          "point_estimate": 225.59611092368615,
          "standard_error": 0.05657218198230389
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 225.4608836561826,
            "upper_bound": 225.7308268013285
          },
          "point_estimate": 225.57072715835324,
          "standard_error": 0.051901747632729034
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025368631899173585,
            "upper_bound": 0.31061294543355683
          },
          "point_estimate": 0.11913774468280912,
          "standard_error": 0.08046882418978837
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 225.40633008969817,
            "upper_bound": 225.61436865197095
          },
          "point_estimate": 225.4945443504423,
          "standard_error": 0.05423213419448105
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06982038723055546,
            "upper_bound": 0.24408664566489296
          },
          "point_estimate": 0.1879185887241601,
          "standard_error": 0.043097740408115634
        }
      }
    },
    "memrchr1/krate/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/never",
        "directory_name": "memrchr1/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.069973595900903,
            "upper_bound": 8.096470131497512
          },
          "point_estimate": 8.083024570452462,
          "standard_error": 0.006804311646385276
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.06319280445559,
            "upper_bound": 8.106327789800503
          },
          "point_estimate": 8.077425241762644,
          "standard_error": 0.013256946321355646
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004759093762958356,
            "upper_bound": 0.03506002017233325
          },
          "point_estimate": 0.031577571672181046,
          "standard_error": 0.008693132654784748
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.062973914201551,
            "upper_bound": 8.097940906322753
          },
          "point_estimate": 8.077515725482753,
          "standard_error": 0.008892394690252024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015004575632052677,
            "upper_bound": 0.025968479273374423
          },
          "point_estimate": 0.02269487081394309,
          "standard_error": 0.0028112990057832547
        }
      }
    },
    "memrchr1/krate/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/rare",
        "directory_name": "memrchr1/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.103941092084948,
            "upper_bound": 13.124154147860276
          },
          "point_estimate": 13.112642668931963,
          "standard_error": 0.00525844665607968
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.10326140870775,
            "upper_bound": 13.118485394223946
          },
          "point_estimate": 13.108392851657436,
          "standard_error": 0.003465148455461603
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024014508278703112,
            "upper_bound": 0.02115526733129253
          },
          "point_estimate": 0.006633317753995771,
          "standard_error": 0.004965345168603415
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.106377664229385,
            "upper_bound": 13.11225255160856
          },
          "point_estimate": 13.10919936249434,
          "standard_error": 0.001483416934708063
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004783908487243492,
            "upper_bound": 0.02533513654962776
          },
          "point_estimate": 0.017503519198920257,
          "standard_error": 0.006062122590652364
        }
      }
    },
    "memrchr1/krate/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/uncommon",
        "directory_name": "memrchr1/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.758051305995515,
            "upper_bound": 43.91545867183389
          },
          "point_estimate": 43.84021459955327,
          "standard_error": 0.040444364768074226
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.724279088307135,
            "upper_bound": 43.963260656514336
          },
          "point_estimate": 43.87913919403617,
          "standard_error": 0.07200313843365772
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0067728628901634605,
            "upper_bound": 0.238524311007232
          },
          "point_estimate": 0.1306762654358573,
          "standard_error": 0.06432509624317767
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.80558090168404,
            "upper_bound": 43.94764088975826
          },
          "point_estimate": 43.87189421011228,
          "standard_error": 0.03722183328269932
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.075380767751608,
            "upper_bound": 0.16661864163192658
          },
          "point_estimate": 0.13533839840688974,
          "standard_error": 0.02346791356873413
        }
      }
    },
    "memrchr1/krate/small/verycommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/verycommon",
        "directory_name": "memrchr1/krate_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495.3740611142322,
            "upper_bound": 497.2260590101407
          },
          "point_estimate": 496.3585937406532,
          "standard_error": 0.4778025793355392
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 494.2211617237424,
            "upper_bound": 497.47847324630874
          },
          "point_estimate": 497.1627109618221,
          "standard_error": 0.8027257374883413
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12313904172666042,
            "upper_bound": 2.3813690180205267
          },
          "point_estimate": 0.8145569569052648,
          "standard_error": 0.6008826009061705
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 494.86677250999304,
            "upper_bound": 497.4062881841709
          },
          "point_estimate": 496.3547610535731,
          "standard_error": 0.6558168542755888
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5492699184301656,
            "upper_bound": 1.8345371696459305
          },
          "point_estimate": 1.5982410747089526,
          "standard_error": 0.27662260488235674
        }
      }
    },
    "memrchr1/krate/tiny/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/common",
        "directory_name": "memrchr1/krate_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.00047305257715,
            "upper_bound": 50.03859828319294
          },
          "point_estimate": 50.01905359278088,
          "standard_error": 0.00982262756666673
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.98810062460604,
            "upper_bound": 50.04726216262679
          },
          "point_estimate": 50.01070418123126,
          "standard_error": 0.015236647527900356
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006790400200349916,
            "upper_bound": 0.05535163697502425
          },
          "point_estimate": 0.03579255684550557,
          "standard_error": 0.012343706834120474
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.99610010598217,
            "upper_bound": 50.04863591389223
          },
          "point_estimate": 50.023187845745454,
          "standard_error": 0.014108826426938254
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018265290052815044,
            "upper_bound": 0.03910120164442955
          },
          "point_estimate": 0.03265441872976038,
          "standard_error": 0.005187556228400585
        }
      }
    },
    "memrchr1/krate/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/never",
        "directory_name": "memrchr1/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.333286914361392,
            "upper_bound": 4.37515023502806
          },
          "point_estimate": 4.354262859251013,
          "standard_error": 0.01069574540056603
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.318323852202565,
            "upper_bound": 4.373450026757338
          },
          "point_estimate": 4.363371824606272,
          "standard_error": 0.013808829862583055
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003172805726982942,
            "upper_bound": 0.0687916772166597
          },
          "point_estimate": 0.025976079235812546,
          "standard_error": 0.016286019907722042
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.342225617623625,
            "upper_bound": 4.365500546829646
          },
          "point_estimate": 4.3566008129251,
          "standard_error": 0.005956691931106977
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018844178159813648,
            "upper_bound": 0.04642096698929603
          },
          "point_estimate": 0.03564315869176903,
          "standard_error": 0.007028288068431056
        }
      }
    },
    "memrchr1/krate/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/rare",
        "directory_name": "memrchr1/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.027156010349364,
            "upper_bound": 7.0631509870738185
          },
          "point_estimate": 7.046149333113361,
          "standard_error": 0.009263299933047116
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.023258135015159,
            "upper_bound": 7.069568093420731
          },
          "point_estimate": 7.0590537287092445,
          "standard_error": 0.014440227370298231
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025753624902001547,
            "upper_bound": 0.05583828898057168
          },
          "point_estimate": 0.029029489012895936,
          "standard_error": 0.012675707001371854
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.017760983875731,
            "upper_bound": 7.060502147207642
          },
          "point_estimate": 7.035132364785636,
          "standard_error": 0.009830124178307968
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016503277651497405,
            "upper_bound": 0.03961841800812551
          },
          "point_estimate": 0.030863572190429657,
          "standard_error": 0.006217551283651486
        }
      }
    },
    "memrchr1/krate/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/uncommon",
        "directory_name": "memrchr1/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.24240705551975,
            "upper_bound": 27.262694788752768
          },
          "point_estimate": 27.25148477112694,
          "standard_error": 0.005190100206115409
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.2406621553772,
            "upper_bound": 27.265816818791546
          },
          "point_estimate": 27.24500410912025,
          "standard_error": 0.005109045596759859
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019221234068490013,
            "upper_bound": 0.023099108243042028
          },
          "point_estimate": 0.007757814567800551,
          "standard_error": 0.004910370425048878
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.242161688846508,
            "upper_bound": 27.262577877854973
          },
          "point_estimate": 27.24937386661808,
          "standard_error": 0.005485397890577062
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004273739529450336,
            "upper_bound": 0.02184597248742107
          },
          "point_estimate": 0.017339276430606437,
          "standard_error": 0.004664106031136614
        }
      }
    },
    "memrchr1/libc/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr1/libc/empty/never",
        "directory_name": "memrchr1/libc_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4914139358654963,
            "upper_bound": 0.49496001290948743
          },
          "point_estimate": 0.4930730477536313,
          "standard_error": 0.000916728326790228
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4908058839546086,
            "upper_bound": 0.49575762264984075
          },
          "point_estimate": 0.4911372098991835,
          "standard_error": 0.0015246794598492272
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000041345115025575495,
            "upper_bound": 0.004926509559231617
          },
          "point_estimate": 0.0005843068836613658,
          "standard_error": 0.0015094158606179897
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4916338094159347,
            "upper_bound": 0.4964528286612824
          },
          "point_estimate": 0.49417996500376343,
          "standard_error": 0.0012545279769111011
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013111548158467217,
            "upper_bound": 0.003605758461073859
          },
          "point_estimate": 0.003057105042948414,
          "standard_error": 0.0005713582302080665
        }
      }
    },
    "memrchr1/libc/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/common",
        "directory_name": "memrchr1/libc_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263407.75886502187,
            "upper_bound": 264334.6642038259
          },
          "point_estimate": 263829.5647345871,
          "standard_error": 238.68217029651663
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263275.3703703704,
            "upper_bound": 264238.8240165631
          },
          "point_estimate": 263663.49577294686,
          "standard_error": 238.0183111392604
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131.56664684900394,
            "upper_bound": 1145.6148052167632
          },
          "point_estimate": 714.2081752429444,
          "standard_error": 274.58156861055295
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263145.3610719792,
            "upper_bound": 263852.46692956507
          },
          "point_estimate": 263430.89121023903,
          "standard_error": 182.5311045590624
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.85141493103436,
            "upper_bound": 1089.2331053760752
          },
          "point_estimate": 794.2670528501094,
          "standard_error": 213.56505082486424
        }
      }
    },
    "memrchr1/libc/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/never",
        "directory_name": "memrchr1/libc_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9720.015352607104,
            "upper_bound": 9743.819775696496
          },
          "point_estimate": 9732.04935135498,
          "standard_error": 6.050957326950344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9719.904614971829,
            "upper_bound": 9744.694571147482
          },
          "point_estimate": 9732.731173419194,
          "standard_error": 5.819033210858407
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.8694652653519817,
            "upper_bound": 33.34187634728972
          },
          "point_estimate": 14.516118640194428,
          "standard_error": 7.81322492185375
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9725.849635410912,
            "upper_bound": 9743.08443769904
          },
          "point_estimate": 9734.629462086075,
          "standard_error": 4.339070153948689
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.297312663885346,
            "upper_bound": 26.855409969777774
          },
          "point_estimate": 20.127884738475732,
          "standard_error": 4.439654058990069
        }
      }
    },
    "memrchr1/libc/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/rare",
        "directory_name": "memrchr1/libc_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10844.224178215069,
            "upper_bound": 10862.178950966174
          },
          "point_estimate": 10853.264071897796,
          "standard_error": 4.582412705582268
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10844.66640788295,
            "upper_bound": 10861.478698865332
          },
          "point_estimate": 10854.485912639168,
          "standard_error": 3.731836234614973
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8794348186654408,
            "upper_bound": 26.08359405394303
          },
          "point_estimate": 10.28195698676316,
          "standard_error": 7.068891321368563
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10844.597144784417,
            "upper_bound": 10861.639781263357
          },
          "point_estimate": 10853.166256257926,
          "standard_error": 4.340799251829911
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.635817785264283,
            "upper_bound": 20.5498513532676
          },
          "point_estimate": 15.232266756547418,
          "standard_error": 3.553527983590657
        }
      }
    },
    "memrchr1/libc/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/uncommon",
        "directory_name": "memrchr1/libc_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77729.6602386485,
            "upper_bound": 77862.85960950855
          },
          "point_estimate": 77799.05952457266,
          "standard_error": 34.087879438753056
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77704.0267094017,
            "upper_bound": 77884.77831196581
          },
          "point_estimate": 77830.81452991453,
          "standard_error": 52.860171944981815
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.29639273042588,
            "upper_bound": 193.38857443524844
          },
          "point_estimate": 106.3317216378722,
          "standard_error": 45.88205510813121
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77665.58167692681,
            "upper_bound": 77844.10362351451
          },
          "point_estimate": 77755.50722610722,
          "standard_error": 46.60047271526968
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.357275437407814,
            "upper_bound": 145.13213812726318
          },
          "point_estimate": 113.51148058300691,
          "standard_error": 22.170548862802303
        }
      }
    },
    "memrchr1/libc/huge/verycommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/verycommon",
        "directory_name": "memrchr1/libc_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 548918.4096947405,
            "upper_bound": 550136.29825
          },
          "point_estimate": 549439.5747121534,
          "standard_error": 318.3602866593249
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 548781.4004975124,
            "upper_bound": 549731.2377398721
          },
          "point_estimate": 549303.3828358208,
          "standard_error": 231.5679747935708
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.15724315773986,
            "upper_bound": 1178.208184455808
          },
          "point_estimate": 654.0818021188753,
          "standard_error": 303.17612222510195
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 548858.7053317792,
            "upper_bound": 549355.0594219982
          },
          "point_estimate": 549091.6598953286,
          "standard_error": 126.37157299630492
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 317.41017778142935,
            "upper_bound": 1546.5556602293643
          },
          "point_estimate": 1058.0374010157425,
          "standard_error": 373.5500092322157
        }
      }
    },
    "memrchr1/libc/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/common",
        "directory_name": "memrchr1/libc_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.86433652076548,
            "upper_bound": 238.5767374548581
          },
          "point_estimate": 238.19189266991316,
          "standard_error": 0.18319197647020813
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.79190629200957,
            "upper_bound": 238.78976687760257
          },
          "point_estimate": 237.8887049848629,
          "standard_error": 0.23805068555367467
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028899494552354344,
            "upper_bound": 1.00337927022869
          },
          "point_estimate": 0.21095207402790217,
          "standard_error": 0.23307924443567388
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.82727047645773,
            "upper_bound": 238.46200297967164
          },
          "point_estimate": 238.0485943290455,
          "standard_error": 0.16262469213400224
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13236548525372993,
            "upper_bound": 0.7940935831858834
          },
          "point_estimate": 0.6117437276118061,
          "standard_error": 0.14863406570775278
        }
      }
    },
    "memrchr1/libc/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/never",
        "directory_name": "memrchr1/libc_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.105288184004277,
            "upper_bound": 8.129934544813745
          },
          "point_estimate": 8.117715596541561,
          "standard_error": 0.006283762944298069
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.097709437005566,
            "upper_bound": 8.134828047106847
          },
          "point_estimate": 8.115585233899676,
          "standard_error": 0.00917881287735651
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013202460490477253,
            "upper_bound": 0.03602826728937789
          },
          "point_estimate": 0.027215861691713885,
          "standard_error": 0.008448520487550706
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.100869251309964,
            "upper_bound": 8.127156182757826
          },
          "point_estimate": 8.111679401379915,
          "standard_error": 0.006663696587573003
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012445627935930155,
            "upper_bound": 0.025895825818264547
          },
          "point_estimate": 0.020927959074125452,
          "standard_error": 0.0034362896162616965
        }
      }
    },
    "memrchr1/libc/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/rare",
        "directory_name": "memrchr1/libc_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.351650628797552,
            "upper_bound": 14.407592146303276
          },
          "point_estimate": 14.380928369399276,
          "standard_error": 0.014310546733054296
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.335004081679976,
            "upper_bound": 14.409132925283911
          },
          "point_estimate": 14.403282027560882,
          "standard_error": 0.020384441397715437
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007708229853441069,
            "upper_bound": 0.07564449706791423
          },
          "point_estimate": 0.03589859153881164,
          "standard_error": 0.024588484141593574
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.35061084387156,
            "upper_bound": 14.405895094315806
          },
          "point_estimate": 14.388188991518868,
          "standard_error": 0.014235428356421217
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023111740500757236,
            "upper_bound": 0.05988792001642638
          },
          "point_estimate": 0.0476692571962288,
          "standard_error": 0.009017351470517389
        }
      }
    },
    "memrchr1/libc/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/uncommon",
        "directory_name": "memrchr1/libc_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.87125094194264,
            "upper_bound": 47.899456476118
          },
          "point_estimate": 47.8850868146632,
          "standard_error": 0.007195369477697005
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.86065926715395,
            "upper_bound": 47.905212324995006
          },
          "point_estimate": 47.88933872314438,
          "standard_error": 0.010200489410178468
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023788974130805043,
            "upper_bound": 0.04435633930614141
          },
          "point_estimate": 0.03391929507633662,
          "standard_error": 0.012693570050134606
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.8666901762941,
            "upper_bound": 47.89175001549918
          },
          "point_estimate": 47.88052919343915,
          "standard_error": 0.0064817050776612576
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013988797126688438,
            "upper_bound": 0.029695443107625626
          },
          "point_estimate": 0.023975079948720703,
          "standard_error": 0.004081101997871204
        }
      }
    },
    "memrchr1/libc/small/verycommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/verycommon",
        "directory_name": "memrchr1/libc_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 582.5675234685377,
            "upper_bound": 583.6462834851624
          },
          "point_estimate": 583.1379367132655,
          "standard_error": 0.27544651572709855
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 582.8218473663976,
            "upper_bound": 583.7891756375753
          },
          "point_estimate": 583.1490332244009,
          "standard_error": 0.2230379569753056
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08642799731226555,
            "upper_bound": 1.4341608693866643
          },
          "point_estimate": 0.6569842978566492,
          "standard_error": 0.3452360934362952
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 583.0169447981831,
            "upper_bound": 584.0634849641398
          },
          "point_estimate": 583.5808588022256,
          "standard_error": 0.2694516751705982
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3490342077479722,
            "upper_bound": 1.2934342793605529
          },
          "point_estimate": 0.9176644475832092,
          "standard_error": 0.2512803544725119
        }
      }
    },
    "memrchr1/libc/tiny/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/common",
        "directory_name": "memrchr1/libc_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.17838325138834,
            "upper_bound": 50.29647556129532
          },
          "point_estimate": 50.23768407907957,
          "standard_error": 0.02980837767214136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.150724614703776,
            "upper_bound": 50.30175352653022
          },
          "point_estimate": 50.24481630272792,
          "standard_error": 0.04219657779666494
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006400960396054233,
            "upper_bound": 0.15773225868413712
          },
          "point_estimate": 0.0728073418695524,
          "standard_error": 0.04749612985037408
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.15031091182896,
            "upper_bound": 50.27866623026103
          },
          "point_estimate": 50.227174716505814,
          "standard_error": 0.03278965548970172
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05041860127831144,
            "upper_bound": 0.1265500402562531
          },
          "point_estimate": 0.09895746118319598,
          "standard_error": 0.018573839650337307
        }
      }
    },
    "memrchr1/libc/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/never",
        "directory_name": "memrchr1/libc_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.942160468104696,
            "upper_bound": 2.946782683548785
          },
          "point_estimate": 2.9445687114829275,
          "standard_error": 0.0011881517846717023
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.940313527420195,
            "upper_bound": 2.9474288861560547
          },
          "point_estimate": 2.9455093387458717,
          "standard_error": 0.0016895000030390844
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00044431622822874606,
            "upper_bound": 0.0063680843255323315
          },
          "point_estimate": 0.0033707665651177966,
          "standard_error": 0.001625141294228974
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.941679361089499,
            "upper_bound": 2.9471001612322945
          },
          "point_estimate": 2.9442851735847446,
          "standard_error": 0.0014128514445979634
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018581164463420056,
            "upper_bound": 0.004757546566615672
          },
          "point_estimate": 0.0039623157923238655,
          "standard_error": 0.0006773278194833371
        }
      }
    },
    "memrchr1/libc/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/rare",
        "directory_name": "memrchr1/libc_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.865168847795884,
            "upper_bound": 5.8835764317677315
          },
          "point_estimate": 5.874235508107954,
          "standard_error": 0.004724024098607237
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.861231328958688,
            "upper_bound": 5.88820720409794
          },
          "point_estimate": 5.87318526704493,
          "standard_error": 0.006495074695130669
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004490473387219028,
            "upper_bound": 0.028748791511770788
          },
          "point_estimate": 0.0179507343158134,
          "standard_error": 0.006446442262629281
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.862732399946684,
            "upper_bound": 5.888120804193312
          },
          "point_estimate": 5.873491915139575,
          "standard_error": 0.006400967511176747
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009131527746550152,
            "upper_bound": 0.01931786857129283
          },
          "point_estimate": 0.015769102470134824,
          "standard_error": 0.002577605574143198
        }
      }
    },
    "memrchr1/libc/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr1/libc/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/uncommon",
        "directory_name": "memrchr1/libc_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.660901504562883,
            "upper_bound": 18.703201235904437
          },
          "point_estimate": 18.679471861365265,
          "standard_error": 0.010991104144679863
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.65530239350581,
            "upper_bound": 18.6955534569438
          },
          "point_estimate": 18.664865715730713,
          "standard_error": 0.01049374199389226
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004964377581675548,
            "upper_bound": 0.04743228014319047
          },
          "point_estimate": 0.018051917850127178,
          "standard_error": 0.01177930487808422
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.659530274574493,
            "upper_bound": 18.67910176671193
          },
          "point_estimate": 18.6684303322247,
          "standard_error": 0.004968306644976377
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010558405441698952,
            "upper_bound": 0.05136979139837799
          },
          "point_estimate": 0.03669213632280611,
          "standard_error": 0.011351450752438088
        }
      }
    },
    "memrchr2/krate/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr2/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr2/krate/empty/never",
        "directory_name": "memrchr2/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.986914881041454,
            "upper_bound": 0.9888221027774806
          },
          "point_estimate": 0.987841720711805,
          "standard_error": 0.0004895280274558068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9865603555404904,
            "upper_bound": 0.9893744789618316
          },
          "point_estimate": 0.9873947069620131,
          "standard_error": 0.0007728305830219013
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003182696266092017,
            "upper_bound": 0.0026534361354582266
          },
          "point_estimate": 0.0015862053541904654,
          "standard_error": 0.000631524488663854
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9871640833500008,
            "upper_bound": 0.989281718023632
          },
          "point_estimate": 0.9882354685831216,
          "standard_error": 0.0005466666086709116
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009334559004908948,
            "upper_bound": 0.001959901605834456
          },
          "point_estimate": 0.001629777303246772,
          "standard_error": 0.0002561019891644568
        }
      }
    },
    "memrchr2/krate/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr2/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/common",
        "directory_name": "memrchr2/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 445494.4661665941,
            "upper_bound": 445926.6547357724
          },
          "point_estimate": 445704.8207955866,
          "standard_error": 110.20899127339445
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 445413.68597560975,
            "upper_bound": 445915.5597560976
          },
          "point_estimate": 445726.3262195122,
          "standard_error": 120.55332185181956
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.48644817131026,
            "upper_bound": 628.2280620174362
          },
          "point_estimate": 309.1963536570102,
          "standard_error": 140.2912241522459
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 445493.4602905751,
            "upper_bound": 445799.0648954704
          },
          "point_estimate": 445660.8358251505,
          "standard_error": 77.8886915574686
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.6257908516851,
            "upper_bound": 495.7561971541043
          },
          "point_estimate": 368.0968990293672,
          "standard_error": 83.71633668037941
        }
      }
    },
    "memrchr2/krate/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr2/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/never",
        "directory_name": "memrchr2/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12781.962875220692,
            "upper_bound": 12824.942354058769
          },
          "point_estimate": 12801.144353276568,
          "standard_error": 11.095454772772074
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12775.338310062964,
            "upper_bound": 12823.135163674762
          },
          "point_estimate": 12786.152141264814,
          "standard_error": 12.349660857059012
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.437551200110106,
            "upper_bound": 55.491072496883774
          },
          "point_estimate": 17.13347895869919,
          "standard_error": 13.43634810456774
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12775.164552868584,
            "upper_bound": 12819.16352846761
          },
          "point_estimate": 12794.27277024278,
          "standard_error": 12.672909862202845
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.675510860698822,
            "upper_bound": 50.22824898274097
          },
          "point_estimate": 36.96905567298503,
          "standard_error": 10.258023600334717
        }
      }
    },
    "memrchr2/krate/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr2/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/rare",
        "directory_name": "memrchr2/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17307.637893972034,
            "upper_bound": 17334.141660422338
          },
          "point_estimate": 17321.021815173845,
          "standard_error": 6.819162130934853
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17296.805904761906,
            "upper_bound": 17342.916825396824
          },
          "point_estimate": 17326.491904761904,
          "standard_error": 12.78646599246052
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.327372012063999,
            "upper_bound": 34.426299174525056
          },
          "point_estimate": 31.90495133357287,
          "standard_error": 8.96622059556145
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17297.7255500821,
            "upper_bound": 17337.867811263637
          },
          "point_estimate": 17316.676707482995,
          "standard_error": 10.622506373957089
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.069515413003431,
            "upper_bound": 26.163000132892968
          },
          "point_estimate": 22.815673417578864,
          "standard_error": 2.879182820139342
        }
      }
    },
    "memrchr2/krate/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr2/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/uncommon",
        "directory_name": "memrchr2/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168391.33564480452,
            "upper_bound": 168777.94595701975
          },
          "point_estimate": 168582.20948228982,
          "standard_error": 98.90214596587612
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168317.14780092594,
            "upper_bound": 168858.0212962963
          },
          "point_estimate": 168575.24819958847,
          "standard_error": 142.60192348795195
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.44556528705489,
            "upper_bound": 577.1192757417265
          },
          "point_estimate": 370.5296465699328,
          "standard_error": 119.19372064951511
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168421.1392624541,
            "upper_bound": 168860.1949248777
          },
          "point_estimate": 168652.3487012987,
          "standard_error": 110.38865071715048
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.9935265957307,
            "upper_bound": 419.7513856939387
          },
          "point_estimate": 329.7631268720126,
          "standard_error": 59.89871826642583
        }
      }
    },
    "memrchr2/krate/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr2/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/common",
        "directory_name": "memrchr2/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462.0545288424707,
            "upper_bound": 464.3489825221454
          },
          "point_estimate": 463.1548345891975,
          "standard_error": 0.5885259103565481
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 461.38626655698175,
            "upper_bound": 464.7804004630513
          },
          "point_estimate": 462.63508800943066,
          "standard_error": 1.0847793682397011
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2520329425163567,
            "upper_bound": 3.48314522686923
          },
          "point_estimate": 2.128501153508572,
          "standard_error": 0.9168539306544182
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 461.4973482975942,
            "upper_bound": 462.7830705352449
          },
          "point_estimate": 461.9228167753833,
          "standard_error": 0.3321185475995156
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1716731673422518,
            "upper_bound": 2.414312104541138
          },
          "point_estimate": 1.9616630738133456,
          "standard_error": 0.3278549470472817
        }
      }
    },
    "memrchr2/krate/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr2/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/never",
        "directory_name": "memrchr2/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.986299115274106,
            "upper_bound": 13.027555825060755
          },
          "point_estimate": 13.00528391932994,
          "standard_error": 0.01053896424989484
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.978496488882204,
            "upper_bound": 13.020006668748175
          },
          "point_estimate": 13.006900672460496,
          "standard_error": 0.010562971592386469
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0037600255213401696,
            "upper_bound": 0.05348936553924187
          },
          "point_estimate": 0.03011813975532332,
          "standard_error": 0.01272368928511257
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.972775806610304,
            "upper_bound": 13.006513789409205
          },
          "point_estimate": 12.98774064177195,
          "standard_error": 0.008770040292102588
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015490021229517124,
            "upper_bound": 0.04852279255594473
          },
          "point_estimate": 0.03514477599736584,
          "standard_error": 0.009344514084608006
        }
      }
    },
    "memrchr2/krate/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr2/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/rare",
        "directory_name": "memrchr2/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.09666522887385,
            "upper_bound": 24.19710699662543
          },
          "point_estimate": 24.14783469398554,
          "standard_error": 0.02560982073026355
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.08829267627091,
            "upper_bound": 24.200279274397293
          },
          "point_estimate": 24.152204104337613,
          "standard_error": 0.02369606289178972
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014103173802692655,
            "upper_bound": 0.14656087099194864
          },
          "point_estimate": 0.049941562103703915,
          "standard_error": 0.037717996756668905
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.051680609792054,
            "upper_bound": 24.191704780072257
          },
          "point_estimate": 24.12533596272789,
          "standard_error": 0.036405968636002775
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03980088524499604,
            "upper_bound": 0.11272619283653385
          },
          "point_estimate": 0.08565137459494367,
          "standard_error": 0.018565663116373383
        }
      }
    },
    "memrchr2/krate/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr2/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/uncommon",
        "directory_name": "memrchr2/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.11643206970292,
            "upper_bound": 98.38785688091336
          },
          "point_estimate": 98.24133678303572,
          "standard_error": 0.07003901663482703
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.08346192817176,
            "upper_bound": 98.3926770273049
          },
          "point_estimate": 98.14800538130643,
          "standard_error": 0.08299510356616836
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024160038017630357,
            "upper_bound": 0.37467577103864
          },
          "point_estimate": 0.10748760939327529,
          "standard_error": 0.08998609945507917
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.08845006619998,
            "upper_bound": 98.18554677176355
          },
          "point_estimate": 98.1285573524321,
          "standard_error": 0.02469864538147559
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07528775396937436,
            "upper_bound": 0.2953743949164031
          },
          "point_estimate": 0.2331441160108688,
          "standard_error": 0.05312962404200728
        }
      }
    },
    "memrchr2/krate/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr2/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/never",
        "directory_name": "memrchr2/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.906567117309021,
            "upper_bound": 4.911401023756183
          },
          "point_estimate": 4.909038769757864,
          "standard_error": 0.001235719132180412
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.906744453532221,
            "upper_bound": 4.912139820626367
          },
          "point_estimate": 4.9094344739151055,
          "standard_error": 0.0010966742118585547
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003582140359954361,
            "upper_bound": 0.007283413754692612
          },
          "point_estimate": 0.0024995268895160335,
          "standard_error": 0.001919697987135034
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.909316948024105,
            "upper_bound": 4.9122800664911646
          },
          "point_estimate": 4.910540199127992,
          "standard_error": 0.0007576959510771084
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018755585059454984,
            "upper_bound": 0.005522150931178418
          },
          "point_estimate": 0.004141570533767904,
          "standard_error": 0.000941173513781204
        }
      }
    },
    "memrchr2/krate/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr2/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/rare",
        "directory_name": "memrchr2/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.8114668881029,
            "upper_bound": 15.912150282310996
          },
          "point_estimate": 15.855226172841938,
          "standard_error": 0.026147346056581956
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.79449977142394,
            "upper_bound": 15.885256467444922
          },
          "point_estimate": 15.836502928635314,
          "standard_error": 0.023629197932206147
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01691888616596622,
            "upper_bound": 0.107287267461154
          },
          "point_estimate": 0.0608183515536334,
          "standard_error": 0.02284796600408313
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.797488623341629,
            "upper_bound": 15.87321944356225
          },
          "point_estimate": 15.83532128334457,
          "standard_error": 0.019676960123116205
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03103458150975829,
            "upper_bound": 0.12541054804082918
          },
          "point_estimate": 0.08712237957094879,
          "standard_error": 0.028662258177482
        }
      }
    },
    "memrchr2/krate/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr2/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/uncommon",
        "directory_name": "memrchr2/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.872786613916816,
            "upper_bound": 53.02421269872163
          },
          "point_estimate": 52.94625054184659,
          "standard_error": 0.03875056329785621
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.83449796941167,
            "upper_bound": 53.085103373638944
          },
          "point_estimate": 52.8941763032747,
          "standard_error": 0.07322631564302269
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013803023277164908,
            "upper_bound": 0.21566087416683125
          },
          "point_estimate": 0.11494380308919486,
          "standard_error": 0.052791052570223054
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.8516404012385,
            "upper_bound": 53.062297064846966
          },
          "point_estimate": 52.9687365654642,
          "standard_error": 0.05542773806848666
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07452474888451688,
            "upper_bound": 0.14882785503984536
          },
          "point_estimate": 0.12942845065472203,
          "standard_error": 0.01814433754861356
        }
      }
    },
    "memrchr3/krate/empty/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr3/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr3/krate/empty/never",
        "directory_name": "memrchr3/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.228127713760901,
            "upper_bound": 1.2286317458797138
          },
          "point_estimate": 1.228373697199476,
          "standard_error": 0.00012890453091356048
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.227996038840822,
            "upper_bound": 1.2287323837872508
          },
          "point_estimate": 1.2283557819148023,
          "standard_error": 0.0001593669855018261
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00010595371672670108,
            "upper_bound": 0.0007788832995304941
          },
          "point_estimate": 0.0004431811885395697,
          "standard_error": 0.00018174133819797233
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.227945972347716,
            "upper_bound": 1.228531182574568
          },
          "point_estimate": 1.2282226860211802,
          "standard_error": 0.00015119290865472747
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00023157015228230296,
            "upper_bound": 0.0005395900011112783
          },
          "point_estimate": 0.0004303794804506172,
          "standard_error": 0.00007825558121236372
        }
      }
    },
    "memrchr3/krate/huge/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr3/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/common",
        "directory_name": "memrchr3/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 682114.7394290123,
            "upper_bound": 683478.8273074846
          },
          "point_estimate": 682749.8653549382,
          "standard_error": 350.64465680768006
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 681833.4796296296,
            "upper_bound": 683502.2067901234
          },
          "point_estimate": 682398.7972222222,
          "standard_error": 494.73647304494506
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.45486184960973,
            "upper_bound": 1965.480954735388
          },
          "point_estimate": 1078.342092892592,
          "standard_error": 451.2513084491506
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 681835.1185679106,
            "upper_bound": 682742.3911879151
          },
          "point_estimate": 682208.0403078403,
          "standard_error": 242.12769903007592
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 565.4335768233259,
            "upper_bound": 1531.7322282088412
          },
          "point_estimate": 1164.4422966790369,
          "standard_error": 263.0502484019873
        }
      }
    },
    "memrchr3/krate/huge/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr3/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/never",
        "directory_name": "memrchr3/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16716.033715323185,
            "upper_bound": 16756.438813082947
          },
          "point_estimate": 16737.032359854406,
          "standard_error": 10.346632046042425
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16714.4356089793,
            "upper_bound": 16768.018307905688
          },
          "point_estimate": 16741.105332100477,
          "standard_error": 18.07411965183865
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5126745184978407,
            "upper_bound": 66.31374868245976
          },
          "point_estimate": 39.72085400894517,
          "standard_error": 16.680011003952373
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16719.756502957414,
            "upper_bound": 16762.34555576989
          },
          "point_estimate": 16740.254772412052,
          "standard_error": 11.069139355099376
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.992256151484543,
            "upper_bound": 44.07415507686495
          },
          "point_estimate": 34.48922605681248,
          "standard_error": 6.694137071866507
        }
      }
    },
    "memrchr3/krate/huge/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr3/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/rare",
        "directory_name": "memrchr3/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22183.57430979514,
            "upper_bound": 22249.22065308303
          },
          "point_estimate": 22213.010276421108,
          "standard_error": 17.019842964798663
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22180.74120879121,
            "upper_bound": 22238.120370370372
          },
          "point_estimate": 22195.78884310134,
          "standard_error": 14.586885835809596
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.668003232704585,
            "upper_bound": 81.68764306542094
          },
          "point_estimate": 23.07273580832308,
          "standard_error": 20.390542209490288
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22167.379477316805,
            "upper_bound": 22228.50995951417
          },
          "point_estimate": 22198.257662971948,
          "standard_error": 15.796242733011413
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.123683790428935,
            "upper_bound": 77.48302933526989
          },
          "point_estimate": 56.7237869388307,
          "standard_error": 16.210824097132956
        }
      }
    },
    "memrchr3/krate/huge/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr3/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/uncommon",
        "directory_name": "memrchr3/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228982.49500224617,
            "upper_bound": 229376.6291823899
          },
          "point_estimate": 229183.94748502545,
          "standard_error": 101.21016060248694
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228857.75561545373,
            "upper_bound": 229468.21907756815
          },
          "point_estimate": 229218.0529350105,
          "standard_error": 130.28509017721692
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.69829021121416,
            "upper_bound": 599.0822837037706
          },
          "point_estimate": 400.1806907727316,
          "standard_error": 157.84468997656876
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 229027.03609863232,
            "upper_bound": 229465.57439050244
          },
          "point_estimate": 229295.02630074328,
          "standard_error": 112.79925429445332
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 185.6093539746979,
            "upper_bound": 416.907075278281
          },
          "point_estimate": 336.53950516183784,
          "standard_error": 58.31501841706464
        }
      }
    },
    "memrchr3/krate/small/common": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr3/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/common",
        "directory_name": "memrchr3/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 717.8629713138685,
            "upper_bound": 720.1741403330437
          },
          "point_estimate": 719.0060624470523,
          "standard_error": 0.5921538374025217
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 717.163249820166,
            "upper_bound": 721.0854781830623
          },
          "point_estimate": 718.8580200767145,
          "standard_error": 1.0817023050929666
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4504829374862994,
            "upper_bound": 3.3160782588189734
          },
          "point_estimate": 2.9075478337956966,
          "standard_error": 0.7442394058463604
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 717.0123586918592,
            "upper_bound": 719.225207445091
          },
          "point_estimate": 717.783976993321,
          "standard_error": 0.5662539803033086
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2913191292158344,
            "upper_bound": 2.300384055260529
          },
          "point_estimate": 1.9772211099346475,
          "standard_error": 0.2573951931275468
        }
      }
    },
    "memrchr3/krate/small/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr3/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/never",
        "directory_name": "memrchr3/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.913456713456643,
            "upper_bound": 15.971857676798
          },
          "point_estimate": 15.945417121973016,
          "standard_error": 0.01501064417355602
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.904884309666883,
            "upper_bound": 15.979153928427849
          },
          "point_estimate": 15.958581195316192,
          "standard_error": 0.015279318833860044
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004167606530563583,
            "upper_bound": 0.06547560903016898
          },
          "point_estimate": 0.027415342491501096,
          "standard_error": 0.01450001657053568
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.898922184764356,
            "upper_bound": 15.982004192250246
          },
          "point_estimate": 15.951615750148216,
          "standard_error": 0.021300301022320833
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014132543052712773,
            "upper_bound": 0.06289914579588982
          },
          "point_estimate": 0.05003043402125413,
          "standard_error": 0.01285550589975249
        }
      }
    },
    "memrchr3/krate/small/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr3/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/rare",
        "directory_name": "memrchr3/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.5716432603589,
            "upper_bound": 35.74381711837009
          },
          "point_estimate": 35.651765351813324,
          "standard_error": 0.04415507920703051
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.533497024130966,
            "upper_bound": 35.77336893874978
          },
          "point_estimate": 35.59487010137785,
          "standard_error": 0.06235719815769621
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017664865171940165,
            "upper_bound": 0.2493872666279315
          },
          "point_estimate": 0.11939414383962602,
          "standard_error": 0.05931826003716494
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.54242108652261,
            "upper_bound": 35.639338187551864
          },
          "point_estimate": 35.57543569150025,
          "standard_error": 0.025044730385292854
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.064095976688773,
            "upper_bound": 0.18782629719608787
          },
          "point_estimate": 0.1471940442734371,
          "standard_error": 0.031567853320038655
        }
      }
    },
    "memrchr3/krate/small/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr3/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/uncommon",
        "directory_name": "memrchr3/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.6042157561559,
            "upper_bound": 155.98218904737004
          },
          "point_estimate": 155.80854319282008,
          "standard_error": 0.0975390295045198
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.6605575460943,
            "upper_bound": 156.0613928818715
          },
          "point_estimate": 155.8674582833828,
          "standard_error": 0.11687778405781996
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01327233439489,
            "upper_bound": 0.49778327440505865
          },
          "point_estimate": 0.2934957134405601,
          "standard_error": 0.1153607977120354
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.7921776783792,
            "upper_bound": 156.05582326681662
          },
          "point_estimate": 155.96860847166514,
          "standard_error": 0.06838780321980255
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14460848599120796,
            "upper_bound": 0.4507681884991544
          },
          "point_estimate": 0.3252652313601537,
          "standard_error": 0.08842315544292308
        }
      }
    },
    "memrchr3/krate/tiny/never": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr3/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/never",
        "directory_name": "memrchr3/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.636576012604085,
            "upper_bound": 5.647997216952208
          },
          "point_estimate": 5.643093659075498,
          "standard_error": 0.0029395593652990714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.642382048451154,
            "upper_bound": 5.6463494886187755
          },
          "point_estimate": 5.644757846340305,
          "standard_error": 0.0011686826592899864
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003006027182766553,
            "upper_bound": 0.009878783072877394
          },
          "point_estimate": 0.0017860591537624297,
          "standard_error": 0.002207868464667948
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.635004199205205,
            "upper_bound": 5.645234641516382
          },
          "point_estimate": 5.641974099648209,
          "standard_error": 0.0027837354283581175
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014034490882993448,
            "upper_bound": 0.014428748745571969
          },
          "point_estimate": 0.00979197882154488,
          "standard_error": 0.003828778920876798
        }
      }
    },
    "memrchr3/krate/tiny/rare": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr3/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/rare",
        "directory_name": "memrchr3/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.692935752104603,
            "upper_bound": 16.717397267750695
          },
          "point_estimate": 16.70490937316702,
          "standard_error": 0.006285292943108292
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.68824468434993,
            "upper_bound": 16.72682804491032
          },
          "point_estimate": 16.697000206535726,
          "standard_error": 0.010917017010045502
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003615731555276359,
            "upper_bound": 0.03460977443870509
          },
          "point_estimate": 0.02312821646156193,
          "standard_error": 0.008378119529454414
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.68899760922151,
            "upper_bound": 16.705963550314067
          },
          "point_estimate": 16.695905924561583,
          "standard_error": 0.004298117485988311
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01247437259338862,
            "upper_bound": 0.024773299444934702
          },
          "point_estimate": 0.0209104662154793,
          "standard_error": 0.0030971131329535348
        }
      }
    },
    "memrchr3/krate/tiny/uncommon": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrchr3/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/uncommon",
        "directory_name": "memrchr3/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.20667930507796,
            "upper_bound": 86.36709678624526
          },
          "point_estimate": 86.30363297990417,
          "standard_error": 0.04338855827063541
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.28873659730007,
            "upper_bound": 86.3848747277592
          },
          "point_estimate": 86.34462015841906,
          "standard_error": 0.026114112546005337
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015477970675139346,
            "upper_bound": 0.111211116642002
          },
          "point_estimate": 0.058744010992549944,
          "standard_error": 0.02563089298954218
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.22253585558153,
            "upper_bound": 86.37261183942812
          },
          "point_estimate": 86.32039245209988,
          "standard_error": 0.038415731366999405
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0305834609822917,
            "upper_bound": 0.21895923351053412
          },
          "point_estimate": 0.14468563803896448,
          "standard_error": 0.06209299851979549
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1384729.522465021,
            "upper_bound": 1385503.6869853027
          },
          "point_estimate": 1385105.5428159907,
          "standard_error": 198.34610123595135
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1384498.7716049382,
            "upper_bound": 1385794.3148148148
          },
          "point_estimate": 1384957.6543209876,
          "standard_error": 335.2114513455444
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.8678239526748,
            "upper_bound": 1055.3938247816393
          },
          "point_estimate": 703.4985902881191,
          "standard_error": 241.95813342551347
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1384474.583756345,
            "upper_bound": 1385549.784631607
          },
          "point_estimate": 1384977.9782587783,
          "standard_error": 282.3178332420089
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399.2331437340929,
            "upper_bound": 782.1912553523023
          },
          "point_estimate": 656.4348042130928,
          "standard_error": 97.689174158462
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1539373.8514635002,
            "upper_bound": 1541135.914060888
          },
          "point_estimate": 1540150.5066484786,
          "standard_error": 455.50025961624766
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1538997.7800925926,
            "upper_bound": 1540741.71875
          },
          "point_estimate": 1539855.9296875,
          "standard_error": 559.368710942071
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.983881231423,
            "upper_bound": 2263.204217771545
          },
          "point_estimate": 1279.7959616541127,
          "standard_error": 513.5769039961035
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1538980.9104591836,
            "upper_bound": 1539892.1892156862
          },
          "point_estimate": 1539273.143073593,
          "standard_error": 236.8942456150502
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 639.2681792157007,
            "upper_bound": 2161.397467281965
          },
          "point_estimate": 1519.1438550321384,
          "standard_error": 465.799541140906
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1443814.344102564,
            "upper_bound": 1445253.2408136444
          },
          "point_estimate": 1444452.9949938948,
          "standard_error": 373.0293696867069
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1443619.521794872,
            "upper_bound": 1444911.8516483516
          },
          "point_estimate": 1444264.658119658,
          "standard_error": 293.6252378791321
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.70575751169943,
            "upper_bound": 1556.2937458317404
          },
          "point_estimate": 730.4839312621231,
          "standard_error": 368.6972546546855
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1443999.8459272396,
            "upper_bound": 1444681.8906404998
          },
          "point_estimate": 1444389.303396603,
          "standard_error": 172.65761463178325
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 426.8524982662837,
            "upper_bound": 1782.4707544700962
          },
          "point_estimate": 1243.3783226946698,
          "standard_error": 399.3255157015823
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 785734.9550759878,
            "upper_bound": 786518.1825852329
          },
          "point_estimate": 786118.5193009118,
          "standard_error": 200.8027781789792
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 785448.4589665653,
            "upper_bound": 786668.914893617
          },
          "point_estimate": 786064.1473404255,
          "standard_error": 322.9926514351086
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.24189346795328,
            "upper_bound": 1119.1553109821025
          },
          "point_estimate": 794.1177986675793,
          "standard_error": 258.9244732597002
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 785556.7110004527,
            "upper_bound": 786445.4001326825
          },
          "point_estimate": 786034.6459242884,
          "standard_error": 225.3929335741575
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 403.73679708294515,
            "upper_bound": 820.5758166798194
          },
          "point_estimate": 668.068720103392,
          "standard_error": 107.75993815797648
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 283284.733044789,
            "upper_bound": 284068.00887220073
          },
          "point_estimate": 283635.11721453175,
          "standard_error": 202.2310156946736
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 283182.25086132647,
            "upper_bound": 284051.33284883725
          },
          "point_estimate": 283484.0276854928,
          "standard_error": 169.48331602016535
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.56631078042643,
            "upper_bound": 907.1384261431352
          },
          "point_estimate": 279.97935453847845,
          "standard_error": 226.25494729610497
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 283362.24627011025,
            "upper_bound": 283769.21799307957
          },
          "point_estimate": 283495.885653881,
          "standard_error": 106.66217406139576
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199.93953610857773,
            "upper_bound": 899.4987879711761
          },
          "point_estimate": 672.1685576849949,
          "standard_error": 186.95007414048965
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498578.4805403349,
            "upper_bound": 499108.294311807
          },
          "point_estimate": 498825.25295553374,
          "standard_error": 135.95970595113832
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498438.27910958906,
            "upper_bound": 499163.7055881714
          },
          "point_estimate": 498661.6205479452,
          "standard_error": 209.8588311210953
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.39771668898023,
            "upper_bound": 830.7587886186518
          },
          "point_estimate": 387.7567599652557,
          "standard_error": 191.9693892108932
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498553.65536183026,
            "upper_bound": 499046.0553803172
          },
          "point_estimate": 498787.3832058352,
          "standard_error": 125.51039384485182
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 225.0330688814977,
            "upper_bound": 586.1354940463254
          },
          "point_estimate": 452.0157531666918,
          "standard_error": 96.44270808529932
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257699.32595782672,
            "upper_bound": 258149.7023546099
          },
          "point_estimate": 257917.497933975,
          "standard_error": 115.06062587645116
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257585.21580547112,
            "upper_bound": 258189.3439716312
          },
          "point_estimate": 257880.159929078,
          "standard_error": 138.75989939603653
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.713647876217486,
            "upper_bound": 672.7362222409449
          },
          "point_estimate": 447.8402016237148,
          "standard_error": 169.13430226475683
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257734.64797235865,
            "upper_bound": 258174.770109732
          },
          "point_estimate": 257952.99237358387,
          "standard_error": 113.0172783986183
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205.83673098124055,
            "upper_bound": 483.4007235726862
          },
          "point_estimate": 383.3042957959101,
          "standard_error": 70.91736479906842
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 615891.4223702583,
            "upper_bound": 616685.5190744551
          },
          "point_estimate": 616261.2564083937,
          "standard_error": 205.6671725621537
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 615764.3898305085,
            "upper_bound": 617108.3389830509
          },
          "point_estimate": 615903.7330508474,
          "standard_error": 337.9457776598887
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.14756649627294,
            "upper_bound": 1053.6260050233122
          },
          "point_estimate": 340.21002616342923,
          "standard_error": 265.46733447054766
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 615798.7073446327,
            "upper_bound": 616746.0319769885
          },
          "point_estimate": 616201.1797050408,
          "standard_error": 259.6171652930616
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 210.91257025674423,
            "upper_bound": 784.2425778209014
          },
          "point_estimate": 683.8603880749637,
          "standard_error": 122.42748198307056
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2068.4999968939965,
            "upper_bound": 2069.849274567164
          },
          "point_estimate": 2069.125387013506,
          "standard_error": 0.3463703983862149
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2068.1547918093934,
            "upper_bound": 2069.917430594069
          },
          "point_estimate": 2068.825033540763,
          "standard_error": 0.49545556186945217
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07409755234983512,
            "upper_bound": 1.9612519233906296
          },
          "point_estimate": 1.0168430738351653,
          "standard_error": 0.508862708703074
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2068.4844824982724,
            "upper_bound": 2069.5994458353066
          },
          "point_estimate": 2069.0342422364115,
          "standard_error": 0.28330265060762283
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.49557594399927896,
            "upper_bound": 1.495280561791552
          },
          "point_estimate": 1.157008611518838,
          "standard_error": 0.25470644232092937
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648.160697310752,
            "upper_bound": 656.925627009038
          },
          "point_estimate": 652.5164684457584,
          "standard_error": 2.2943948874274316
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 645.2926783598105,
            "upper_bound": 659.6809055726147
          },
          "point_estimate": 652.1228791540236,
          "standard_error": 6.0739231560106495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0316442021260866,
            "upper_bound": 10.6839483708358
          },
          "point_estimate": 10.172017623993517,
          "standard_error": 4.186972961307184
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 645.6226112585904,
            "upper_bound": 651.5435209709029
          },
          "point_estimate": 647.3220836797158,
          "standard_error": 1.559652682657719
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.862888587681974,
            "upper_bound": 7.81423855015145
          },
          "point_estimate": 7.606588747307851,
          "standard_error": 0.6687353559698396
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.11933602454756,
            "upper_bound": 125.18761527422346
          },
          "point_estimate": 125.14616660740958,
          "standard_error": 0.01869887115535303
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.11776439071215,
            "upper_bound": 125.1448543589784
          },
          "point_estimate": 125.12901381950005,
          "standard_error": 0.007595153218810618
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002769611311884864,
            "upper_bound": 0.04358045514558162
          },
          "point_estimate": 0.01429185206071788,
          "standard_error": 0.01188193651784877
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.1251617151919,
            "upper_bound": 125.13584295724164
          },
          "point_estimate": 125.12882370189914,
          "standard_error": 0.002748893598985734
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010317080034338174,
            "upper_bound": 0.09482983436804446
          },
          "point_estimate": 0.06227140466133291,
          "standard_error": 0.027922671040516777
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.8416083097253,
            "upper_bound": 48.865678195510064
          },
          "point_estimate": 48.85294775997031,
          "standard_error": 0.0061595655464341615
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.83742933101402,
            "upper_bound": 48.86330745535612
          },
          "point_estimate": 48.85257361386691,
          "standard_error": 0.007366014951731285
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002148592558584981,
            "upper_bound": 0.03380855281122988
          },
          "point_estimate": 0.02048154610635826,
          "standard_error": 0.00742515580341539
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.8382982843193,
            "upper_bound": 48.85563866414377
          },
          "point_estimate": 48.84664063661419,
          "standard_error": 0.004474967793694159
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009746972162691018,
            "upper_bound": 0.02767944464953752
          },
          "point_estimate": 0.020485663972026,
          "standard_error": 0.004893875209521968
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.53321798018153,
            "upper_bound": 79.59283850493648
          },
          "point_estimate": 79.56140261251016,
          "standard_error": 0.015295299613110325
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.5120610893231,
            "upper_bound": 79.59266836255941
          },
          "point_estimate": 79.55375054044734,
          "standard_error": 0.01848195515176635
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00878664479799503,
            "upper_bound": 0.08469110577733813
          },
          "point_estimate": 0.053433452148922866,
          "standard_error": 0.01926000330191278
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.52213464325816,
            "upper_bound": 79.5717257196296
          },
          "point_estimate": 79.54195418237563,
          "standard_error": 0.01263448108392686
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025623140050749577,
            "upper_bound": 0.06790730502743447
          },
          "point_estimate": 0.05112890576058893,
          "standard_error": 0.0116332139571747
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300358.45610070264,
            "upper_bound": 300812.2398241121
          },
          "point_estimate": 300576.20899752795,
          "standard_error": 116.54960363101524
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300227.8325526932,
            "upper_bound": 300904.9472905282
          },
          "point_estimate": 300514.6706284153,
          "standard_error": 176.4898299984187
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.51381446546328,
            "upper_bound": 653.0677686106455
          },
          "point_estimate": 434.0527852448666,
          "standard_error": 151.0722880103512
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300369.6374346319,
            "upper_bound": 300669.4897722452
          },
          "point_estimate": 300502.7342984884,
          "standard_error": 76.01029053956825
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207.7918660838401,
            "upper_bound": 469.0915626863719
          },
          "point_estimate": 387.2443252514954,
          "standard_error": 65.07841826921253
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.25109173111175,
            "upper_bound": 59.287548583984105
          },
          "point_estimate": 59.26781286161512,
          "standard_error": 0.009385375442396909
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.24222683288974,
            "upper_bound": 59.284064294364136
          },
          "point_estimate": 59.25947655160462,
          "standard_error": 0.013102385799560823
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019545793985374146,
            "upper_bound": 0.05207185581815127
          },
          "point_estimate": 0.02583600966596384,
          "standard_error": 0.013737131669623974
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.245864649902565,
            "upper_bound": 59.27517625525506
          },
          "point_estimate": 59.26003084736527,
          "standard_error": 0.007415470171768389
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013956779968965483,
            "upper_bound": 0.04204926311748639
          },
          "point_estimate": 0.031249999365047652,
          "standard_error": 0.007788837971652029
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.3222501191308,
            "upper_bound": 97.36276512198802
          },
          "point_estimate": 97.34202655757528,
          "standard_error": 0.010406797340956286
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.3108884307491,
            "upper_bound": 97.37470147933276
          },
          "point_estimate": 97.33616433592412,
          "standard_error": 0.019639956904791565
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004094694628389293,
            "upper_bound": 0.05660569052547592
          },
          "point_estimate": 0.03923134028283299,
          "standard_error": 0.014393029103420897
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.3138771230402,
            "upper_bound": 97.36601377945904
          },
          "point_estimate": 97.34165503831248,
          "standard_error": 0.013365356972562808
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021944506822029856,
            "upper_bound": 0.04044166238574408
          },
          "point_estimate": 0.03469228086828245,
          "standard_error": 0.004726592576217207
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135411.87557119253,
            "upper_bound": 135745.1114725062
          },
          "point_estimate": 135574.82010341066,
          "standard_error": 85.18981444165257
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135301.74488847583,
            "upper_bound": 135787.88289962825
          },
          "point_estimate": 135547.61007847998,
          "standard_error": 114.79831153069156
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.40869269732245,
            "upper_bound": 486.2090686915101
          },
          "point_estimate": 360.37410126936936,
          "standard_error": 110.02976283301554
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135382.17736050638,
            "upper_bound": 135630.8673275506
          },
          "point_estimate": 135505.63376623378,
          "standard_error": 62.068767928750134
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160.3249863140859,
            "upper_bound": 364.6168192677904
          },
          "point_estimate": 284.1269535256869,
          "standard_error": 53.28984772655255
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.95228368409902,
            "upper_bound": 58.01619374819808
          },
          "point_estimate": 57.97765965223874,
          "standard_error": 0.017248945526108353
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.9541876583848,
            "upper_bound": 57.97167792604217
          },
          "point_estimate": 57.96775448820078,
          "standard_error": 0.005356335149381578
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010161304726970214,
            "upper_bound": 0.03733318921716093
          },
          "point_estimate": 0.00575657846569986,
          "standard_error": 0.010406731831114615
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.95898447828597,
            "upper_bound": 57.96936558161803
          },
          "point_estimate": 57.96501593849293,
          "standard_error": 0.0026097441463939423
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0033923482339215647,
            "upper_bound": 0.0872563467215824
          },
          "point_estimate": 0.05737467790905469,
          "standard_error": 0.02594471175129589
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.60441745494852,
            "upper_bound": 92.6666376999975
          },
          "point_estimate": 92.6275231443074,
          "standard_error": 0.017816381847889078
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.60251917055513,
            "upper_bound": 92.62392878792512
          },
          "point_estimate": 92.60768038405637,
          "standard_error": 0.0065609837604987745
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002433923879910063,
            "upper_bound": 0.02559763339134142
          },
          "point_estimate": 0.01080986956632268,
          "standard_error": 0.0075562219888737105
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.60293228615396,
            "upper_bound": 92.61623172673684
          },
          "point_estimate": 92.60911089491228,
          "standard_error": 0.003393002236032356
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0055936929498318026,
            "upper_bound": 0.09110413635880604
          },
          "point_estimate": 0.05937872229194222,
          "standard_error": 0.029376793181157615
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.78119519304164,
            "upper_bound": 239.94779151118132
          },
          "point_estimate": 239.8501187441373,
          "standard_error": 0.04381972974021227
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.74913558740465,
            "upper_bound": 239.88450490933565
          },
          "point_estimate": 239.8131417108771,
          "standard_error": 0.03265973691954482
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014377534365255788,
            "upper_bound": 0.14013649382020887
          },
          "point_estimate": 0.08094535270814857,
          "standard_error": 0.03289918921321136
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.76536460702192,
            "upper_bound": 239.84593673211128
          },
          "point_estimate": 239.80526168212447,
          "standard_error": 0.020860492655204708
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04065853694865393,
            "upper_bound": 0.21629871656755623
          },
          "point_estimate": 0.145777835788446,
          "standard_error": 0.05585174977860014
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482.4335214435848,
            "upper_bound": 482.9897998577862
          },
          "point_estimate": 482.6569351330869,
          "standard_error": 0.14971897555908054
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482.38217195711024,
            "upper_bound": 482.7136119810558
          },
          "point_estimate": 482.5183479085939,
          "standard_error": 0.08133591038919576
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04414244872266985,
            "upper_bound": 0.3995782295428223
          },
          "point_estimate": 0.15460309189932597,
          "standard_error": 0.09996720366652614
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482.4354795902118,
            "upper_bound": 482.5729172486144
          },
          "point_estimate": 482.502421579164,
          "standard_error": 0.03495849535770965
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10053502964730524,
            "upper_bound": 0.7530067249448277
          },
          "point_estimate": 0.5001886941757406,
          "standard_error": 0.21066562158526153
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.60904602513372,
            "upper_bound": 31.617445256603222
          },
          "point_estimate": 31.613079861120173,
          "standard_error": 0.0021501319019495107
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.60841797790731,
            "upper_bound": 31.61826754151332
          },
          "point_estimate": 31.611820549153872,
          "standard_error": 0.002463076498632517
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010169563524036114,
            "upper_bound": 0.012220822069885395
          },
          "point_estimate": 0.005418006760045175,
          "standard_error": 0.002739078114259869
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.60644237191678,
            "upper_bound": 31.616535601997235
          },
          "point_estimate": 31.61168799650547,
          "standard_error": 0.0026368085875156135
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003413541431887135,
            "upper_bound": 0.009443517714641893
          },
          "point_estimate": 0.00718531262960714,
          "standard_error": 0.0015512524745845827
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/bstr_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33537.823732551944,
            "upper_bound": 33550.769973166694
          },
          "point_estimate": 33544.20405680753,
          "standard_error": 3.3200356848278303
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33536.4557195572,
            "upper_bound": 33555.01586715867
          },
          "point_estimate": 33542.27329335794,
          "standard_error": 4.075915186332454
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3675411888194489,
            "upper_bound": 19.89120771881924
          },
          "point_estimate": 9.81749482109774,
          "standard_error": 5.193944673480315
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33541.137286791054,
            "upper_bound": 33555.824095592776
          },
          "point_estimate": 33548.66346863469,
          "standard_error": 3.772327656481661
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.918675811989208,
            "upper_bound": 13.960071053033657
          },
          "point_estimate": 11.036874014532785,
          "standard_error": 2.030074552573342
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/bstr_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.81763776072356,
            "upper_bound": 126.33974216320313
          },
          "point_estimate": 125.40642796385492,
          "standard_error": 0.4214120566802988
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.65259329520087,
            "upper_bound": 125.32346986289558
          },
          "point_estimate": 125.11059975280963,
          "standard_error": 0.17842689727417554
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06533028139600273,
            "upper_bound": 0.8769521101703992
          },
          "point_estimate": 0.3689323746521791,
          "standard_error": 0.2347974251001078
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.6812488432274,
            "upper_bound": 125.11207858048182
          },
          "point_estimate": 124.88493111854947,
          "standard_error": 0.10991889628054756
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2431920253540906,
            "upper_bound": 2.1527745585747247
          },
          "point_estimate": 1.4065991774668565,
          "standard_error": 0.6532367498619974
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/bstr_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104746.2671657044,
            "upper_bound": 1105365.0575378789
          },
          "point_estimate": 1105021.6675300626,
          "standard_error": 160.86670408999896
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104704.2222222222,
            "upper_bound": 1105396.2727272727
          },
          "point_estimate": 1104802.1038961038,
          "standard_error": 168.5203107179032
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.29888314096756,
            "upper_bound": 860.1900281628851
          },
          "point_estimate": 182.1019997972412,
          "standard_error": 199.2862169820309
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104716.3513759505,
            "upper_bound": 1105197.1548134403
          },
          "point_estimate": 1104864.2476190475,
          "standard_error": 127.47382884864685
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.65809379921117,
            "upper_bound": 739.2914358773644
          },
          "point_estimate": 536.3686034467393,
          "standard_error": 156.1929310779184
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/bstr_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2231.3310265103846,
            "upper_bound": 2232.2485501812394
          },
          "point_estimate": 2231.750142664698,
          "standard_error": 0.23624601890604469
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2231.2807056135325,
            "upper_bound": 2232.1195109474115
          },
          "point_estimate": 2231.503155602327,
          "standard_error": 0.23972781751897793
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03188831619259838,
            "upper_bound": 1.2192154683293763
          },
          "point_estimate": 0.3333760978260485,
          "standard_error": 0.3023190285710408
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2231.2948727953412,
            "upper_bound": 2231.854900271561
          },
          "point_estimate": 2231.52168440522,
          "standard_error": 0.14456577671214474
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2762533327692291,
            "upper_bound": 1.063191271671289
          },
          "point_estimate": 0.7861291533618155,
          "standard_error": 0.20812456188612635
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.73025403279024,
            "upper_bound": 41.765693512409406
          },
          "point_estimate": 41.7463148157409,
          "standard_error": 0.009157648592456977
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.72506973866582,
            "upper_bound": 41.76575354650629
          },
          "point_estimate": 41.73379371442016,
          "standard_error": 0.010157640619072546
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003468958869523684,
            "upper_bound": 0.04550777969891799
          },
          "point_estimate": 0.02333376125534888,
          "standard_error": 0.010929920593355043
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.7260450515697,
            "upper_bound": 41.74622665753164
          },
          "point_estimate": 41.73299904484461,
          "standard_error": 0.005212844918511518
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011314693299069064,
            "upper_bound": 0.041041967739907405
          },
          "point_estimate": 0.03046540952231697,
          "standard_error": 0.008036061909973101
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.652666870972936,
            "upper_bound": 45.67845208747987
          },
          "point_estimate": 45.66451013228324,
          "standard_error": 0.006637514284180989
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.64978708513999,
            "upper_bound": 45.67459890445303
          },
          "point_estimate": 45.662808697911544,
          "standard_error": 0.005963531371323184
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004348176782919127,
            "upper_bound": 0.032694428955104674
          },
          "point_estimate": 0.014409076988912433,
          "standard_error": 0.007281353726722482
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.65264548983154,
            "upper_bound": 45.66636143731656
          },
          "point_estimate": 45.65885033683313,
          "standard_error": 0.0035370144002462387
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008651270783239493,
            "upper_bound": 0.031076205425209134
          },
          "point_estimate": 0.02221506704338019,
          "standard_error": 0.006286185346053694
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.87795929741516,
            "upper_bound": 33.90467221872694
          },
          "point_estimate": 33.88865193854962,
          "standard_error": 0.007191626989265843
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.87660039613189,
            "upper_bound": 33.89299993897127
          },
          "point_estimate": 33.8774067342421,
          "standard_error": 0.004520990497739333
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00026380364439032664,
            "upper_bound": 0.01925388886241644
          },
          "point_estimate": 0.0015937848953861277,
          "standard_error": 0.005650627299633577
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.87760778885759,
            "upper_bound": 33.88921570439294
          },
          "point_estimate": 33.88253758032717,
          "standard_error": 0.0029575401033579006
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0031283509979622742,
            "upper_bound": 0.03568178826405894
          },
          "point_estimate": 0.023876507154559978,
          "standard_error": 0.0100275214946706
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.043185740275945,
            "upper_bound": 38.068337008331625
          },
          "point_estimate": 38.05483378201253,
          "standard_error": 0.006460814245083578
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.03737225836506,
            "upper_bound": 38.07288433819333
          },
          "point_estimate": 38.04710507983485,
          "standard_error": 0.009264715620034836
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003173610898952994,
            "upper_bound": 0.0352928557444739
          },
          "point_estimate": 0.017603491753881456,
          "standard_error": 0.009131789660861148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.03821642621986,
            "upper_bound": 38.05930028081079
          },
          "point_estimate": 38.04704232846717,
          "standard_error": 0.005536807208369829
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009586781136616594,
            "upper_bound": 0.02691233227777995
          },
          "point_estimate": 0.021572338994484547,
          "standard_error": 0.0044409587472128715
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.44427040156871,
            "upper_bound": 44.470726725755405
          },
          "point_estimate": 44.45618815573839,
          "standard_error": 0.006862649994697576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.44472173218675,
            "upper_bound": 44.47136929562036
          },
          "point_estimate": 44.44959259670654,
          "standard_error": 0.005348518106882286
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001376705283710414,
            "upper_bound": 0.03244073321499535
          },
          "point_estimate": 0.0061490681816043615,
          "standard_error": 0.007723769393681353
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.44496627134763,
            "upper_bound": 44.463388166925974
          },
          "point_estimate": 44.452447430071246,
          "standard_error": 0.004670075974408895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00445777643074709,
            "upper_bound": 0.029819215912630724
          },
          "point_estimate": 0.022869141388534263,
          "standard_error": 0.0063026967796070196
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.78198613767104,
            "upper_bound": 72.8584439586185
          },
          "point_estimate": 72.8138922542308,
          "standard_error": 0.020168736695968116
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.77389115830155,
            "upper_bound": 72.83181535372086
          },
          "point_estimate": 72.79021517146859,
          "standard_error": 0.015219669051130189
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0057437222420571995,
            "upper_bound": 0.07018199280682658
          },
          "point_estimate": 0.03189645584598219,
          "standard_error": 0.016675119567239098
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.78730891405628,
            "upper_bound": 72.81666456123307
          },
          "point_estimate": 72.7986207830956,
          "standard_error": 0.00747399920310882
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017096246975648547,
            "upper_bound": 0.0986401845874779
          },
          "point_estimate": 0.06685515099501438,
          "standard_error": 0.025054834862972927
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.81349442861888,
            "upper_bound": 79.85773723679286
          },
          "point_estimate": 79.8343340016184,
          "standard_error": 0.0113565297096144
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.80518047930245,
            "upper_bound": 79.85511795236538
          },
          "point_estimate": 79.82830891265223,
          "standard_error": 0.014133746817394197
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007247551244772333,
            "upper_bound": 0.06279803455511652
          },
          "point_estimate": 0.03377433410865251,
          "standard_error": 0.013369440663880367
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.80401475653441,
            "upper_bound": 79.84146511525627
          },
          "point_estimate": 79.82107531773048,
          "standard_error": 0.009541025779570922
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0188024994844775,
            "upper_bound": 0.05048571173889243
          },
          "point_estimate": 0.037816454675245345,
          "standard_error": 0.008754282250504172
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.02610010682598,
            "upper_bound": 59.0686381830518
          },
          "point_estimate": 59.04613063494048,
          "standard_error": 0.01091139280577699
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.01787510290639,
            "upper_bound": 59.07483120917235
          },
          "point_estimate": 59.0380493910955,
          "standard_error": 0.01608031977100541
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00213504321119353,
            "upper_bound": 0.06035279756894337
          },
          "point_estimate": 0.02996023232744645,
          "standard_error": 0.01515915125538372
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.02853327579004,
            "upper_bound": 59.06474083312064
          },
          "point_estimate": 59.04729599483363,
          "standard_error": 0.00910195066884414
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017997275692807707,
            "upper_bound": 0.04632251686169976
          },
          "point_estimate": 0.0363140134929393,
          "standard_error": 0.007355762875809453
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.28815646532048,
            "upper_bound": 97.40652368233714
          },
          "point_estimate": 97.33604500292732,
          "standard_error": 0.03161034452564857
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.27507176871296,
            "upper_bound": 97.35098959643592
          },
          "point_estimate": 97.31207547624273,
          "standard_error": 0.0191979731312604
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0053645221633078,
            "upper_bound": 0.09012026902473425
          },
          "point_estimate": 0.05627788469190924,
          "standard_error": 0.02112652981944488
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.28935167267016,
            "upper_bound": 97.33082185345776
          },
          "point_estimate": 97.3113006622767,
          "standard_error": 0.010675389963983787
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025554122019606152,
            "upper_bound": 0.1590879281011152
          },
          "point_estimate": 0.1055583006807086,
          "standard_error": 0.04382658636512764
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.352213075063176,
            "upper_bound": 59.416587118449776
          },
          "point_estimate": 59.37809786634871,
          "standard_error": 0.017283705600057923
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.342139171200536,
            "upper_bound": 59.39060346295894
          },
          "point_estimate": 59.361545899161456,
          "standard_error": 0.0124466228896366
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025489532882311343,
            "upper_bound": 0.05380849591602742
          },
          "point_estimate": 0.029622169188902245,
          "standard_error": 0.012737968392196236
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.34921268808392,
            "upper_bound": 59.3766635698506
          },
          "point_estimate": 59.363874445128985,
          "standard_error": 0.006845919790915854
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013231728594805694,
            "upper_bound": 0.08650932045634736
          },
          "point_estimate": 0.057493038914010954,
          "standard_error": 0.023687087172240055
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.95165986333454,
            "upper_bound": 57.994519610115745
          },
          "point_estimate": 57.971392848556754,
          "standard_error": 0.011040154470213708
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.94552767786388,
            "upper_bound": 58.00083781512739
          },
          "point_estimate": 57.96797683073248,
          "standard_error": 0.011391663372951192
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009536097924739076,
            "upper_bound": 0.05385404629128675
          },
          "point_estimate": 0.02849663870660262,
          "standard_error": 0.01428660761952427
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.95731186673188,
            "upper_bound": 57.9762305469483
          },
          "point_estimate": 57.966898544949366,
          "standard_error": 0.004721888357865721
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012641874010369624,
            "upper_bound": 0.04665566326216216
          },
          "point_estimate": 0.03673097579612052,
          "standard_error": 0.008697655431972467
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.62280190174404,
            "upper_bound": 92.67222502984517
          },
          "point_estimate": 92.64657332695148,
          "standard_error": 0.01268197309706349
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.61596221369746,
            "upper_bound": 92.67992928865816
          },
          "point_estimate": 92.63780985034124,
          "standard_error": 0.015439546240385486
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005784124233115227,
            "upper_bound": 0.07291017762361292
          },
          "point_estimate": 0.034962017403556596,
          "standard_error": 0.01890768039373101
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.62637181983676,
            "upper_bound": 92.67356172341928
          },
          "point_estimate": 92.65070537004858,
          "standard_error": 0.01216172421623551
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021011043224593823,
            "upper_bound": 0.05290526989182548
          },
          "point_estimate": 0.04241704172825103,
          "standard_error": 0.007841655841041507
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1155543.5037331968,
            "upper_bound": 1156767.5842957592
          },
          "point_estimate": 1156093.1044469248,
          "standard_error": 317.48351059951966
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1155404.949652778,
            "upper_bound": 1156790.58203125
          },
          "point_estimate": 1155777.7285714285,
          "standard_error": 274.6006847948609
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.18798709577993,
            "upper_bound": 1441.753314820421
          },
          "point_estimate": 446.50428676055634,
          "standard_error": 326.17935615112805
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1155302.4571799308,
            "upper_bound": 1155990.9029017857
          },
          "point_estimate": 1155632.066801948,
          "standard_error": 180.66242476309364
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 284.29831242341237,
            "upper_bound": 1386.9907335949208
          },
          "point_estimate": 1059.375281909773,
          "standard_error": 290.88422662810103
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1649031.2358592134,
            "upper_bound": 1654179.4930331262
          },
          "point_estimate": 1651622.2103157348,
          "standard_error": 1318.5847434557054
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1647962.5782608695,
            "upper_bound": 1654496.233695652
          },
          "point_estimate": 1652261.4963768115,
          "standard_error": 1652.1652165630255
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 567.0075828688382,
            "upper_bound": 7794.728536398479
          },
          "point_estimate": 3340.563833628043,
          "standard_error": 1846.47551449248
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1648631.258156055,
            "upper_bound": 1653729.8113454003
          },
          "point_estimate": 1651189.7444381705,
          "standard_error": 1296.8073425223872
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2265.24390759986,
            "upper_bound": 5696.6086531485635
          },
          "point_estimate": 4398.343174375473,
          "standard_error": 862.2203318673811
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1412921.2677793042,
            "upper_bound": 1417950.7725449482
          },
          "point_estimate": 1415038.1329929796,
          "standard_error": 1321.7880068821712
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1412224.983974359,
            "upper_bound": 1415966.6935897437
          },
          "point_estimate": 1414205.485958486,
          "standard_error": 820.3779082855579
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 213.44054108257103,
            "upper_bound": 4641.015010682478
          },
          "point_estimate": 1710.9617113552515,
          "standard_error": 1224.505885493563
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1412656.9194652906,
            "upper_bound": 1415167.9671945702
          },
          "point_estimate": 1414082.7666333667,
          "standard_error": 649.8408667597829
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1223.147666527952,
            "upper_bound": 6478.72729236528
          },
          "point_estimate": 4400.8021431887155,
          "standard_error": 1625.4030145516754
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398465.7763793996,
            "upper_bound": 400246.4383540373
          },
          "point_estimate": 399226.2701462216,
          "standard_error": 465.7479658274224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398027.3711180124,
            "upper_bound": 399503.5673913043
          },
          "point_estimate": 399047.13491847826,
          "standard_error": 321.9117036161262
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.89296102792322,
            "upper_bound": 1766.4196905148517
          },
          "point_estimate": 770.9284178078682,
          "standard_error": 512.2558917448274
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398523.31842556474,
            "upper_bound": 399185.3861992683
          },
          "point_estimate": 398920.4469226426,
          "standard_error": 168.82042930893934
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 503.5879775759684,
            "upper_bound": 2293.698810006221
          },
          "point_estimate": 1559.5374532364815,
          "standard_error": 564.0453848789737
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 620709.5473634651,
            "upper_bound": 621877.3040435498
          },
          "point_estimate": 621289.0630555556,
          "standard_error": 299.02075503396816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 620453.2288135593,
            "upper_bound": 622187.1620762711
          },
          "point_estimate": 621167.0200564972,
          "standard_error": 509.373643431299
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 229.3078186973552,
            "upper_bound": 1629.8188703587907
          },
          "point_estimate": 1260.3310147432103,
          "standard_error": 349.2893980582687
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 620650.9762544051,
            "upper_bound": 621837.3143858011
          },
          "point_estimate": 621185.3864847018,
          "standard_error": 304.1889771988752
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 625.3019207385877,
            "upper_bound": 1194.3846023850226
          },
          "point_estimate": 994.1476739659888,
          "standard_error": 145.65025396742442
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 379168.7381925637,
            "upper_bound": 380153.2188657407
          },
          "point_estimate": 379652.748587963,
          "standard_error": 252.35633511783507
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378770.6119791667,
            "upper_bound": 380327.8489583333
          },
          "point_estimate": 379584.2179398148,
          "standard_error": 374.06059758436584
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.54639494790187,
            "upper_bound": 1501.7212663078535
          },
          "point_estimate": 1154.3797521618835,
          "standard_error": 361.2026459143521
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 379097.2139423077,
            "upper_bound": 380093.6324652778
          },
          "point_estimate": 379587.1712121212,
          "standard_error": 250.89169609845985
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 490.96366108945864,
            "upper_bound": 1016.668508986724
          },
          "point_estimate": 841.2210558942111,
          "standard_error": 132.90627404907565
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464861.327964449,
            "upper_bound": 465256.9673392606
          },
          "point_estimate": 465069.3594911593,
          "standard_error": 101.26917168999331
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464899.31392405066,
            "upper_bound": 465349.59493670886
          },
          "point_estimate": 465071.65796413505,
          "standard_error": 105.81202836611614
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.957744791306855,
            "upper_bound": 553.6861091257739
          },
          "point_estimate": 333.7933087575111,
          "standard_error": 125.01792166265857
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464929.80070267746,
            "upper_bound": 465137.5760746961
          },
          "point_estimate": 465013.100575374,
          "standard_error": 52.78265066869216
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.83600920261233,
            "upper_bound": 457.0498186689753
          },
          "point_estimate": 338.58062077376354,
          "standard_error": 80.64538912219707
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 985568.4431381382,
            "upper_bound": 987307.9333976835
          },
          "point_estimate": 986392.9517009868,
          "standard_error": 445.037815008872
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 985243.9687687688,
            "upper_bound": 986905.5765765766
          },
          "point_estimate": 986515.2277992278,
          "standard_error": 409.05830849769814
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.74224472447855,
            "upper_bound": 2507.968118988279
          },
          "point_estimate": 716.4987161084388,
          "standard_error": 622.1536986098567
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 985732.2321904718,
            "upper_bound": 986680.2288717288
          },
          "point_estimate": 986172.6563706564,
          "standard_error": 239.78522965855623
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 509.5450578636505,
            "upper_bound": 2062.3118490883357
          },
          "point_estimate": 1485.6555086737403,
          "standard_error": 393.6121909694592
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 327170.1652852853,
            "upper_bound": 328178.16782282275
          },
          "point_estimate": 327599.532042042,
          "standard_error": 264.34354726032427
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326991.91126126127,
            "upper_bound": 327804.55495495495
          },
          "point_estimate": 327429.3843843844,
          "standard_error": 213.44440482589368
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146.18970010732525,
            "upper_bound": 979.2178801830132
          },
          "point_estimate": 536.9589758724634,
          "standard_error": 211.65931594004908
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326955.5286807787,
            "upper_bound": 327575.5714869316
          },
          "point_estimate": 327183.97564057563,
          "standard_error": 158.01347977225942
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 281.8033177217963,
            "upper_bound": 1295.7628014264656
          },
          "point_estimate": 883.6472517003477,
          "standard_error": 316.43624670064895
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829536.4370570887,
            "upper_bound": 831885.840988456
          },
          "point_estimate": 830577.1149089106,
          "standard_error": 604.9851380023172
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829167.3442234849,
            "upper_bound": 831325.9738636364
          },
          "point_estimate": 830319.0023989899,
          "standard_error": 533.6055735803186
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328.94157332679595,
            "upper_bound": 2679.305814023733
          },
          "point_estimate": 1330.2483854742063,
          "standard_error": 600.9947203511296
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829253.0274100899,
            "upper_bound": 830512.74141735
          },
          "point_estimate": 829919.0969303424,
          "standard_error": 319.7399322452213
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 772.1139902282958,
            "upper_bound": 2855.378835861262
          },
          "point_estimate": 2022.0701082208984,
          "standard_error": 615.2178878236451
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387715.9657598784,
            "upper_bound": 388119.4612370399
          },
          "point_estimate": 387913.9080720196,
          "standard_error": 103.23406928062612
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387644.29640957445,
            "upper_bound": 388150.8994089834
          },
          "point_estimate": 387921.7021276596,
          "standard_error": 142.44186627702203
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.58173064419465,
            "upper_bound": 591.7611155579913
          },
          "point_estimate": 352.44586510805567,
          "standard_error": 130.3042463813693
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387596.7455897257,
            "upper_bound": 388066.9653749696
          },
          "point_estimate": 387798.6731693838,
          "standard_error": 122.656107939113
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193.21130190250213,
            "upper_bound": 442.4495405123663
          },
          "point_estimate": 344.27972653457454,
          "standard_error": 64.56138874539626
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174041.13895665298,
            "upper_bound": 174358.13963755983
          },
          "point_estimate": 174193.93219867855,
          "standard_error": 81.20835955835247
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173947.5857826384,
            "upper_bound": 174420.4110047847
          },
          "point_estimate": 174155.5844497608,
          "standard_error": 106.94433383136416
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.80689047711152,
            "upper_bound": 442.8313307984613
          },
          "point_estimate": 337.8408059160576,
          "standard_error": 116.04078090783683
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174050.0415685679,
            "upper_bound": 174375.03266918255
          },
          "point_estimate": 174217.4110979929,
          "standard_error": 83.80046675338336
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.00547559794325,
            "upper_bound": 331.2392543169678
          },
          "point_estimate": 270.7299004219975,
          "standard_error": 47.16483128518475
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230589.19445411392,
            "upper_bound": 230978.28074015467
          },
          "point_estimate": 230772.6147727045,
          "standard_error": 99.97834000838236
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230491.8506329114,
            "upper_bound": 230954.45253164557
          },
          "point_estimate": 230781.63014240505,
          "standard_error": 112.76603754402376
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.26074867005351,
            "upper_bound": 552.887938158973
          },
          "point_estimate": 317.6637262168792,
          "standard_error": 131.4650857804554
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230549.54769634845,
            "upper_bound": 230812.01388407729
          },
          "point_estimate": 230696.70244944928,
          "standard_error": 67.49270230959473
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163.6688402980519,
            "upper_bound": 448.2535869746961
          },
          "point_estimate": 334.26781585688826,
          "standard_error": 79.69478690395019
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171250.81598719678,
            "upper_bound": 171510.02050014972
          },
          "point_estimate": 171376.76939446692,
          "standard_error": 66.15807628926953
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171187.326819407,
            "upper_bound": 171604.36438679244
          },
          "point_estimate": 171279.18710691825,
          "standard_error": 133.01456308983447
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.230229411064453,
            "upper_bound": 348.06167367915486
          },
          "point_estimate": 168.25834200104126,
          "standard_error": 102.6987983119649
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171228.33868966988,
            "upper_bound": 171606.6649011139
          },
          "point_estimate": 171438.70709384957,
          "standard_error": 98.07452110169775
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138.38086095252336,
            "upper_bound": 253.0160887760986
          },
          "point_estimate": 221.403898685125,
          "standard_error": 29.926982143121172
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 671904.1796204546,
            "upper_bound": 673697.5759977272
          },
          "point_estimate": 672704.4571601732,
          "standard_error": 459.7705066194043
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 671773.5818181818,
            "upper_bound": 673447.6381818182
          },
          "point_estimate": 671956.6977272728,
          "standard_error": 481.75169450769187
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.647874945809065,
            "upper_bound": 2182.225423075966
          },
          "point_estimate": 305.69796248183206,
          "standard_error": 560.4407422227926
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 671785.9267260099,
            "upper_bound": 672778.2587553386
          },
          "point_estimate": 672199.307390791,
          "standard_error": 272.7026048917806
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246.7683869916903,
            "upper_bound": 2030.0105527406831
          },
          "point_estimate": 1531.182330848226,
          "standard_error": 431.22195469551986
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1453.655404505684,
            "upper_bound": 1459.153412644505
          },
          "point_estimate": 1456.4952075928927,
          "standard_error": 1.3969385869379476
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1451.0213760588142,
            "upper_bound": 1460.2457147994246
          },
          "point_estimate": 1459.263996324117,
          "standard_error": 2.8329955968326765
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3363464559911984,
            "upper_bound": 7.148070984784683
          },
          "point_estimate": 2.160112856008586,
          "standard_error": 2.0692775477807825
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1451.189731047156,
            "upper_bound": 1457.0872131278404
          },
          "point_estimate": 1453.055371749874,
          "standard_error": 1.5233559562032994
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.883190995467592,
            "upper_bound": 5.159736332346117
          },
          "point_estimate": 4.654085035834578,
          "standard_error": 0.6247834338897384
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1382982.4717979496,
            "upper_bound": 1384027.5696237506
          },
          "point_estimate": 1383470.9054776602,
          "standard_error": 267.89014019246935
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1382859.961728395,
            "upper_bound": 1384055.2757201646
          },
          "point_estimate": 1383263.0582010583,
          "standard_error": 298.1159112087915
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165.26184984378065,
            "upper_bound": 1437.1388041152147
          },
          "point_estimate": 638.0682764498528,
          "standard_error": 350.90656004235944
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1382947.957037037,
            "upper_bound": 1384072.2136819172
          },
          "point_estimate": 1383497.8708032707,
          "standard_error": 293.0011423395592
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 382.5626040009682,
            "upper_bound": 1170.5039309821725
          },
          "point_estimate": 893.5729545742839,
          "standard_error": 201.80399463612
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1538394.97375,
            "upper_bound": 1539260.7104761903
          },
          "point_estimate": 1538825.9952380953,
          "standard_error": 221.3254368831525
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1538252.25,
            "upper_bound": 1539467.6527777778
          },
          "point_estimate": 1538713.269791667,
          "standard_error": 307.7174753833685
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.22211198003596,
            "upper_bound": 1301.804628763647
          },
          "point_estimate": 900.9780631710894,
          "standard_error": 305.9386939499254
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1538799.5321138212,
            "upper_bound": 1539605.1412348049
          },
          "point_estimate": 1539310.3732683982,
          "standard_error": 206.9501574821731
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 437.4592844283767,
            "upper_bound": 898.9102370631225
          },
          "point_estimate": 736.4856517008178,
          "standard_error": 117.94631202794756
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1444066.558408043,
            "upper_bound": 1445118.1629120875
          },
          "point_estimate": 1444607.4360637972,
          "standard_error": 268.01375262904867
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1443869.4307692307,
            "upper_bound": 1445449.1923076925
          },
          "point_estimate": 1444798.967948718,
          "standard_error": 409.4240258917867
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.20926280655598,
            "upper_bound": 1457.010461056154
          },
          "point_estimate": 1134.570321684154,
          "standard_error": 345.5923381127862
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1443591.0234761648,
            "upper_bound": 1444656.5997208436
          },
          "point_estimate": 1444006.3905094906,
          "standard_error": 268.1453247478685
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 530.2499953528059,
            "upper_bound": 1095.9030188958548
          },
          "point_estimate": 889.9310483603791,
          "standard_error": 145.35589841262748
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 784886.8272948328,
            "upper_bound": 785319.3507779257
          },
          "point_estimate": 785104.2632987166,
          "standard_error": 110.68945418344894
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 784888.9184397163,
            "upper_bound": 785409.4042553192
          },
          "point_estimate": 785067.9969604863,
          "standard_error": 120.25539065169124
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.697474206255485,
            "upper_bound": 651.4265639668354
          },
          "point_estimate": 314.26660000567927,
          "standard_error": 146.98002259560033
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 784851.4449873499,
            "upper_bound": 785289.0633212075
          },
          "point_estimate": 785082.6445426913,
          "standard_error": 113.35299697666356
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179.17821873775864,
            "upper_bound": 492.02307220953
          },
          "point_estimate": 367.7625248604738,
          "standard_error": 80.65101378141057
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282589.75059428456,
            "upper_bound": 282960.03719575185
          },
          "point_estimate": 282779.06016488245,
          "standard_error": 95.36507657167482
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282523.22286821704,
            "upper_bound": 283109.0568475452
          },
          "point_estimate": 282787.8936046512,
          "standard_error": 129.5470847456181
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.23454387065913,
            "upper_bound": 558.6681885311896
          },
          "point_estimate": 434.2787211659787,
          "standard_error": 148.01931107190967
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282675.39238900633,
            "upper_bound": 283092.6194457111
          },
          "point_estimate": 282928.52364844456,
          "standard_error": 108.12338241036296
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179.72042348524508,
            "upper_bound": 388.6112580897416
          },
          "point_estimate": 317.0533240637236,
          "standard_error": 52.59801497660905
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498042.2035909981,
            "upper_bound": 499098.0682254567
          },
          "point_estimate": 498513.50767449447,
          "standard_error": 272.43399589319637
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 497839.04419439007,
            "upper_bound": 498928.98287671234
          },
          "point_estimate": 498238.4365296804,
          "standard_error": 313.2016278923778
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.8924373741702,
            "upper_bound": 1303.7596593879127
          },
          "point_estimate": 607.3183073001553,
          "standard_error": 330.92261181473424
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 497921.7945964406,
            "upper_bound": 498557.1893954883
          },
          "point_estimate": 498213.4239459171,
          "standard_error": 164.24436159896052
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323.54038066121836,
            "upper_bound": 1254.103595098345
          },
          "point_estimate": 911.5564087379676,
          "standard_error": 256.8193339634677
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258051.7537639311,
            "upper_bound": 258434.2174879264
          },
          "point_estimate": 258240.06829533944,
          "standard_error": 97.25625255862224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257909.30023640665,
            "upper_bound": 258536.02127659577
          },
          "point_estimate": 258245.68498817965,
          "standard_error": 184.5161398702788
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.975546666924031,
            "upper_bound": 616.505288522932
          },
          "point_estimate": 458.795406216458,
          "standard_error": 156.94999294716146
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258009.6467076701,
            "upper_bound": 258455.39059488388
          },
          "point_estimate": 258246.28672745693,
          "standard_error": 118.19118136852067
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.94111102009967,
            "upper_bound": 389.1757166011947
          },
          "point_estimate": 324.2745273370703,
          "standard_error": 47.697025132552795
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 612815.9749102184,
            "upper_bound": 616154.8656388888
          },
          "point_estimate": 614676.879005291,
          "standard_error": 852.0148725663554
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 612706.8851851851,
            "upper_bound": 616145.3366666667
          },
          "point_estimate": 615875.374702381,
          "standard_error": 810.1271593772171
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.98180750766662,
            "upper_bound": 3769.2676573321983
          },
          "point_estimate": 445.263102011591,
          "standard_error": 958.1849615421596
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 610661.5078891258,
            "upper_bound": 615937.2312328233
          },
          "point_estimate": 612667.9137662337,
          "standard_error": 1340.290526741591
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 544.0847081812382,
            "upper_bound": 3570.7312672682197
          },
          "point_estimate": 2838.5605250344624,
          "standard_error": 779.2795645614298
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376.6089349163135,
            "upper_bound": 376.8377173197874
          },
          "point_estimate": 376.71383162713744,
          "standard_error": 0.0588676047992036
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376.5633088456238,
            "upper_bound": 376.8738718902508
          },
          "point_estimate": 376.6215103287018,
          "standard_error": 0.08170665314846187
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02168727412250518,
            "upper_bound": 0.34030432377575126
          },
          "point_estimate": 0.10797679140190963,
          "standard_error": 0.08112690107401377
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376.5906081324542,
            "upper_bound": 376.776714892575
          },
          "point_estimate": 376.66481061912464,
          "standard_error": 0.04905867371373178
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07851584122270913,
            "upper_bound": 0.25789329522756205
          },
          "point_estimate": 0.196158046070402,
          "standard_error": 0.04651448222836821
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.78955523402493,
            "upper_bound": 187.89575190690408
          },
          "point_estimate": 187.84175536778915,
          "standard_error": 0.027242575385008076
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.76412209007967,
            "upper_bound": 187.92126527934204
          },
          "point_estimate": 187.83930891652287,
          "standard_error": 0.03332229130960746
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01950583840301132,
            "upper_bound": 0.1750630996704784
          },
          "point_estimate": 0.06997200591822608,
          "standard_error": 0.039089291410610294
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.80825712373615,
            "upper_bound": 187.8775213516379
          },
          "point_estimate": 187.842641647624,
          "standard_error": 0.01765814259258921
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.050583477494601174,
            "upper_bound": 0.11407088811661371
          },
          "point_estimate": 0.09084341948128276,
          "standard_error": 0.016441932792482374
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.99437218345709,
            "upper_bound": 27.0102125030148
          },
          "point_estimate": 27.001303156826737,
          "standard_error": 0.004078247726036528
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.992470399223684,
            "upper_bound": 27.004837439301653
          },
          "point_estimate": 26.99987532769415,
          "standard_error": 0.0035080195533197943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017257893984845543,
            "upper_bound": 0.017387065772282245
          },
          "point_estimate": 0.00837034519460887,
          "standard_error": 0.0037969963401570726
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.99276925301533,
            "upper_bound": 27.00341748069736
          },
          "point_estimate": 26.998477431311898,
          "standard_error": 0.0027833255472754477
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004904279140854604,
            "upper_bound": 0.01956971563968076
          },
          "point_estimate": 0.013591031080905891,
          "standard_error": 0.004438150290749536
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.162338113652226,
            "upper_bound": 18.17311745598781
          },
          "point_estimate": 18.167323995217735,
          "standard_error": 0.002770494723212347
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.159651845892597,
            "upper_bound": 18.173168852389388
          },
          "point_estimate": 18.16458019168782,
          "standard_error": 0.003859855645395376
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000031644400796701224,
            "upper_bound": 0.01575008503227076
          },
          "point_estimate": 0.007479658604647198,
          "standard_error": 0.0041212435803388734
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.16204037146614,
            "upper_bound": 18.170468637786445
          },
          "point_estimate": 18.166901429833956,
          "standard_error": 0.002141151677118423
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004133509996970953,
            "upper_bound": 0.012247504916074048
          },
          "point_estimate": 0.0092153791234452,
          "standard_error": 0.0022271188014058784
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.54972481418371,
            "upper_bound": 26.58805533435862
          },
          "point_estimate": 26.565974110320933,
          "standard_error": 0.00995722811007786
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.547124701145087,
            "upper_bound": 26.57283040485855
          },
          "point_estimate": 26.56008115540115,
          "standard_error": 0.0069833325103704135
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035422315627830097,
            "upper_bound": 0.034911420054253316
          },
          "point_estimate": 0.019055637824484863,
          "standard_error": 0.00777699828564945
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.553251058341008,
            "upper_bound": 26.57151208203818
          },
          "point_estimate": 26.564032302944184,
          "standard_error": 0.004711473968853407
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009281808298994464,
            "upper_bound": 0.04883038701593043
          },
          "point_estimate": 0.03316640684966567,
          "standard_error": 0.012177232365869931
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300315.8783606232,
            "upper_bound": 300814.3516751317
          },
          "point_estimate": 300545.98234159517,
          "standard_error": 128.14073240131285
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300213.4533811476,
            "upper_bound": 300819.21926229505
          },
          "point_estimate": 300434.86224954465,
          "standard_error": 140.35899726621736
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.6248439183914,
            "upper_bound": 680.0333657959176
          },
          "point_estimate": 355.7647745581758,
          "standard_error": 154.10991958579604
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300348.3251228476,
            "upper_bound": 300604.05139767803
          },
          "point_estimate": 300463.08482009795,
          "standard_error": 63.733775191061426
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 185.8540353791052,
            "upper_bound": 571.6096519264919
          },
          "point_estimate": 427.3537991329543,
          "standard_error": 104.22831178515791
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.19845996670865,
            "upper_bound": 16.208757868819017
          },
          "point_estimate": 16.202960807709864,
          "standard_error": 0.002673957049238985
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.196695486443545,
            "upper_bound": 16.20501267734921
          },
          "point_estimate": 16.202139013038288,
          "standard_error": 0.0021631826855160797
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014768457289831211,
            "upper_bound": 0.011502029716421336
          },
          "point_estimate": 0.004999792854918052,
          "standard_error": 0.0025671530954756135
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.199077054916863,
            "upper_bound": 16.20391603367579
          },
          "point_estimate": 16.201731113692382,
          "standard_error": 0.001237652591405272
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003195269029282799,
            "upper_bound": 0.013022178583001932
          },
          "point_estimate": 0.008977179154898634,
          "standard_error": 0.0030319576260087852
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.997684468247854,
            "upper_bound": 24.018112335487896
          },
          "point_estimate": 24.006865799750095,
          "standard_error": 0.005267239807371169
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.995260519560095,
            "upper_bound": 24.018868523693147
          },
          "point_estimate": 24.001762779033776,
          "standard_error": 0.004843892826718408
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017666245249282528,
            "upper_bound": 0.025360976581736736
          },
          "point_estimate": 0.009598106823800145,
          "standard_error": 0.005756981627884641
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.99567144974464,
            "upper_bound": 24.01891617030395
          },
          "point_estimate": 24.006721139715793,
          "standard_error": 0.006161049807571791
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005388276329722581,
            "upper_bound": 0.02294359553758692
          },
          "point_estimate": 0.017524751601822607,
          "standard_error": 0.004605885116002904
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134806.3681536555,
            "upper_bound": 135362.32334218448
          },
          "point_estimate": 135127.40078952027,
          "standard_error": 144.5354527974681
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135074.29739776952,
            "upper_bound": 135377.2246415295
          },
          "point_estimate": 135194.2229244114,
          "standard_error": 73.8345331484692
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.82873867702936,
            "upper_bound": 493.1445999809913
          },
          "point_estimate": 163.47869319432337,
          "standard_error": 113.25009676642118
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134356.62938137195,
            "upper_bound": 135269.4703197026
          },
          "point_estimate": 134862.09037802345,
          "standard_error": 271.07182139316535
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.28255044648488,
            "upper_bound": 711.5560945838148
          },
          "point_estimate": 480.909895670801,
          "standard_error": 182.5081750212309
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.41150052983971,
            "upper_bound": 24.442868990487835
          },
          "point_estimate": 24.427493606687875,
          "standard_error": 0.007974854374571043
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.410769011474592,
            "upper_bound": 24.441514605281277
          },
          "point_estimate": 24.434330063140987,
          "standard_error": 0.009679367624387664
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028288343482573315,
            "upper_bound": 0.04588721330098013
          },
          "point_estimate": 0.022887841544971026,
          "standard_error": 0.011599718800803484
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.41424265848006,
            "upper_bound": 24.43674778629174
          },
          "point_estimate": 24.42355848316657,
          "standard_error": 0.005796620627893842
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01240273625011172,
            "upper_bound": 0.035480218251636456
          },
          "point_estimate": 0.026549380046959573,
          "standard_error": 0.0061899352213551345
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.35468036713712,
            "upper_bound": 21.368435248528268
          },
          "point_estimate": 21.36083515322505,
          "standard_error": 0.003561858364468513
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.353237665899464,
            "upper_bound": 21.369150366037196
          },
          "point_estimate": 21.35677139586471,
          "standard_error": 0.0031373949019188703
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001014030526806408,
            "upper_bound": 0.016100606662698427
          },
          "point_estimate": 0.004760471547454865,
          "standard_error": 0.0037205096402402873
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.351655013775396,
            "upper_bound": 21.35904787333236
          },
          "point_estimate": 21.35517524173872,
          "standard_error": 0.0019570662464229093
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003092865709728669,
            "upper_bound": 0.015402466498261071
          },
          "point_estimate": 0.011875738169584708,
          "standard_error": 0.003236564992613173
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.38189025801898,
            "upper_bound": 47.76668371639282
          },
          "point_estimate": 47.58659844887895,
          "standard_error": 0.098469464464972
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.40886538318965,
            "upper_bound": 47.80642702738841
          },
          "point_estimate": 47.633015607352746,
          "standard_error": 0.0961400702743624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05455231910868938,
            "upper_bound": 0.5298200936690262
          },
          "point_estimate": 0.2546121378921089,
          "standard_error": 0.1183210203978856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.11733782820342,
            "upper_bound": 47.654697180934505
          },
          "point_estimate": 47.36289401221072,
          "standard_error": 0.1443049596392514
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14848328769359517,
            "upper_bound": 0.4440631861550424
          },
          "point_estimate": 0.3282084695728885,
          "standard_error": 0.08072667179751086
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.75152836796366,
            "upper_bound": 89.85128552203798
          },
          "point_estimate": 89.79804340375347,
          "standard_error": 0.02550441092828147
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.7288223670772,
            "upper_bound": 89.85662732721386
          },
          "point_estimate": 89.78504187071076,
          "standard_error": 0.03015197175922848
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015539210761695394,
            "upper_bound": 0.13553908014235164
          },
          "point_estimate": 0.0871045967614789,
          "standard_error": 0.033544036009263443
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.73098101566907,
            "upper_bound": 89.84301183870537
          },
          "point_estimate": 89.78487673679815,
          "standard_error": 0.029041909595035196
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.037017199086746895,
            "upper_bound": 0.11011238759096348
          },
          "point_estimate": 0.08512400281055815,
          "standard_error": 0.018602881816303457
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.797903943664409,
            "upper_bound": 10.806666977331451
          },
          "point_estimate": 10.801494065885194,
          "standard_error": 0.002324189945940452
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.79749642581046,
            "upper_bound": 10.802283492651668
          },
          "point_estimate": 10.799716604388928,
          "standard_error": 0.0013526744709044772
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009853381162869032,
            "upper_bound": 0.006962078448850146
          },
          "point_estimate": 0.003148514022092106,
          "standard_error": 0.0015559984988853073
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.7991998892288,
            "upper_bound": 10.80208726098941
          },
          "point_estimate": 10.80089010442347,
          "standard_error": 0.0007247449395115214
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018434019316319607,
            "upper_bound": 0.01156915046925468
          },
          "point_estimate": 0.007727090368530056,
          "standard_error": 0.003099953823779865
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/bstr_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33382.35328868045,
            "upper_bound": 33408.60653151282
          },
          "point_estimate": 33395.21447662045,
          "standard_error": 6.5560428065207965
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33388.514340373426,
            "upper_bound": 33400.864830119375
          },
          "point_estimate": 33395.14082382264,
          "standard_error": 3.8701674203459886
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.2019657705251467,
            "upper_bound": 36.32186142127312
          },
          "point_estimate": 8.593150052521985,
          "standard_error": 6.812068749005541
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33390.63892185563,
            "upper_bound": 33398.517840627166
          },
          "point_estimate": 33394.0581803871,
          "standard_error": 1.9968278165858524
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.8508986095464985,
            "upper_bound": 30.916057045072662
          },
          "point_estimate": 21.842608515144786,
          "standard_error": 6.95053118996622
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/bstr_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.219812998446628,
            "upper_bound": 29.73296958289607
          },
          "point_estimate": 29.46481811449782,
          "standard_error": 0.13168688421928665
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.141490600158537,
            "upper_bound": 29.76120855588468
          },
          "point_estimate": 29.36936264674788,
          "standard_error": 0.14503855089957293
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07374456323806171,
            "upper_bound": 0.7744241976499072
          },
          "point_estimate": 0.3675868276786699,
          "standard_error": 0.1858904134436535
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.247512594231083,
            "upper_bound": 29.472659335386343
          },
          "point_estimate": 29.362683013245753,
          "standard_error": 0.05653460785664027
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2119525500413066,
            "upper_bound": 0.5607526982590608
          },
          "point_estimate": 0.4393766048643102,
          "standard_error": 0.08866776804999862
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/bstr_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104794.3089478114,
            "upper_bound": 1105406.27490861
          },
          "point_estimate": 1105053.610543531,
          "standard_error": 159.75657171980387
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104756.106060606,
            "upper_bound": 1105284.1682900432
          },
          "point_estimate": 1104813.9326599326,
          "standard_error": 138.38940226242977
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.536200250004748,
            "upper_bound": 616.4750885501354
          },
          "point_estimate": 202.71834044141335,
          "standard_error": 171.5159010174059
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104780.4126816627,
            "upper_bound": 1105156.6287683917
          },
          "point_estimate": 1104909.248170012,
          "standard_error": 99.38977760596418
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131.0856779999248,
            "upper_bound": 765.6901335353323
          },
          "point_estimate": 532.9577066231012,
          "standard_error": 183.57572989435351
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/bstr_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2205.322308916261,
            "upper_bound": 2206.959597146551
          },
          "point_estimate": 2206.081574984069,
          "standard_error": 0.41647240902569954
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2205.17626216545,
            "upper_bound": 2206.729346432916
          },
          "point_estimate": 2205.986396999189,
          "standard_error": 0.4181806700412419
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3115699476317091,
            "upper_bound": 2.1509592432909663
          },
          "point_estimate": 1.016645886756314,
          "standard_error": 0.44725192382345935
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2205.4658014544016,
            "upper_bound": 2206.5300972089185
          },
          "point_estimate": 2206.040498941448,
          "standard_error": 0.26939452010920284
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5910023044131657,
            "upper_bound": 1.9217031915729783
          },
          "point_estimate": 1.388267761012449,
          "standard_error": 0.3685058301262562
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.607836085846758,
            "upper_bound": 7.609934413537904
          },
          "point_estimate": 7.608737663327524,
          "standard_error": 0.0005474824134490286
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.607452238708384,
            "upper_bound": 7.609421116126575
          },
          "point_estimate": 7.608234296093776,
          "standard_error": 0.000533269429327268
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00012068424486361124,
            "upper_bound": 0.002284176027580256
          },
          "point_estimate": 0.0011715565116129426,
          "standard_error": 0.0005441319639801612
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.607714064108998,
            "upper_bound": 7.608554364177919
          },
          "point_estimate": 7.608123460421773,
          "standard_error": 0.00021458498272632632
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005779667045359306,
            "upper_bound": 0.00262891858037895
          },
          "point_estimate": 0.0018209291461011548,
          "standard_error": 0.0006123196581840932
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.870917977874957,
            "upper_bound": 6.875508163828366
          },
          "point_estimate": 6.873054046316176,
          "standard_error": 0.001184118895160103
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.870140785760241,
            "upper_bound": 6.875738429231756
          },
          "point_estimate": 6.872114079029636,
          "standard_error": 0.0013458976855008355
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008745961619296617,
            "upper_bound": 0.006502194105331231
          },
          "point_estimate": 0.003494055275875328,
          "standard_error": 0.00143966989625679
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.871240374611937,
            "upper_bound": 6.875507509094484
          },
          "point_estimate": 6.873130273775006,
          "standard_error": 0.0011060787584470512
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016309944673543772,
            "upper_bound": 0.005022294572910733
          },
          "point_estimate": 0.003926517605028811,
          "standard_error": 0.0008587197636776326
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.097764461172483,
            "upper_bound": 8.104515429551176
          },
          "point_estimate": 8.100581429928313,
          "standard_error": 0.0017797086609933309
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.097571752854291,
            "upper_bound": 8.10191370117874
          },
          "point_estimate": 8.098165180228328,
          "standard_error": 0.001192560316747303
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00018214480691662057,
            "upper_bound": 0.0060380916333072215
          },
          "point_estimate": 0.001352207063468139,
          "standard_error": 0.0016757716493817645
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.097543624546189,
            "upper_bound": 8.100230344384217
          },
          "point_estimate": 8.098760995830373,
          "standard_error": 0.0007558012965453529
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011697838858097049,
            "upper_bound": 0.008681303603648036
          },
          "point_estimate": 0.005937415205162842,
          "standard_error": 0.0022141374304192984
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.390695011420135,
            "upper_bound": 19.40061185345119
          },
          "point_estimate": 19.3950846245606,
          "standard_error": 0.0025702886089678956
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.388942533420977,
            "upper_bound": 19.39857782114869
          },
          "point_estimate": 19.393391567499272,
          "standard_error": 0.002391770799097678
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014462612426981792,
            "upper_bound": 0.011132309168345029
          },
          "point_estimate": 0.006550682313112676,
          "standard_error": 0.002477767330437035
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.390469850247683,
            "upper_bound": 19.396142500129233
          },
          "point_estimate": 19.392955439030928,
          "standard_error": 0.001436596268807136
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0031934387132814593,
            "upper_bound": 0.012192710337487032
          },
          "point_estimate": 0.008595938640829557,
          "standard_error": 0.0026654048892323155
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.744168919746002,
            "upper_bound": 13.7526300205967
          },
          "point_estimate": 13.748157477046275,
          "standard_error": 0.0021712342371378573
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.74176742018918,
            "upper_bound": 13.753663182022825
          },
          "point_estimate": 13.745769064316228,
          "standard_error": 0.0032114136325896176
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005534962499167752,
            "upper_bound": 0.011823480375033312
          },
          "point_estimate": 0.006432100844940638,
          "standard_error": 0.002889011169263933
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.743016524496626,
            "upper_bound": 13.750027753330604
          },
          "point_estimate": 13.746125673337236,
          "standard_error": 0.0017712308786160095
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003529602542456978,
            "upper_bound": 0.008997729610871724
          },
          "point_estimate": 0.007243142474824098,
          "standard_error": 0.0013785202372037055
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.421355871072116,
            "upper_bound": 19.435397805225545
          },
          "point_estimate": 19.428005992612064,
          "standard_error": 0.003598551937541156
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.417789529779192,
            "upper_bound": 19.43615621894855
          },
          "point_estimate": 19.426657515926937,
          "standard_error": 0.0042476178515383186
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0026901978904409546,
            "upper_bound": 0.020316333260744373
          },
          "point_estimate": 0.013615226439529391,
          "standard_error": 0.004777416001467586
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.422918893054984,
            "upper_bound": 19.436417307134725
          },
          "point_estimate": 19.429699262241048,
          "standard_error": 0.0035094049945165896
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005975769306156282,
            "upper_bound": 0.015816765567614088
          },
          "point_estimate": 0.011997019801473958,
          "standard_error": 0.002651317740549542
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.799892438296798,
            "upper_bound": 10.809671492108864
          },
          "point_estimate": 10.80426558258694,
          "standard_error": 0.0025285684206292633
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.799794027143625,
            "upper_bound": 10.809520927454129
          },
          "point_estimate": 10.80082667518656,
          "standard_error": 0.002302627452649042
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006067650568977429,
            "upper_bound": 0.012063240218003157
          },
          "point_estimate": 0.0019354238214796895,
          "standard_error": 0.0029615761178076
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.799710550497164,
            "upper_bound": 10.807016494560218
          },
          "point_estimate": 10.802463986528448,
          "standard_error": 0.0018994107153754904
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001977964799725178,
            "upper_bound": 0.010926883501402012
          },
          "point_estimate": 0.008417711889638437,
          "standard_error": 0.0023042112777912713
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.197694311713306,
            "upper_bound": 16.207190048244165
          },
          "point_estimate": 16.202112205241747,
          "standard_error": 0.0024439058730398302
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.195465279442605,
            "upper_bound": 16.209867708151688
          },
          "point_estimate": 16.19971609838813,
          "standard_error": 0.003358012418148219
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011101021360785306,
            "upper_bound": 0.012967428032771023
          },
          "point_estimate": 0.006953873571699316,
          "standard_error": 0.00307676536695192
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.19676137403544,
            "upper_bound": 16.20610864851584
          },
          "point_estimate": 16.201431890540352,
          "standard_error": 0.00238330993715724
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0031003727673691492,
            "upper_bound": 0.01044441599936032
          },
          "point_estimate": 0.008130263631575499,
          "standard_error": 0.0017735365135757503
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.99622995597292,
            "upper_bound": 24.01157158037494
          },
          "point_estimate": 24.00298329886254,
          "standard_error": 0.003976386327210587
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.99365830393008,
            "upper_bound": 24.00805865907958
          },
          "point_estimate": 24.00031195295395,
          "standard_error": 0.003911140692686938
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023759348327774127,
            "upper_bound": 0.01785774200813317
          },
          "point_estimate": 0.009636884332708442,
          "standard_error": 0.0037122596142767617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.99363684147081,
            "upper_bound": 24.00085673554776
          },
          "point_estimate": 23.99715548104192,
          "standard_error": 0.0018165311728310536
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005141364989863684,
            "upper_bound": 0.01894501745729016
          },
          "point_estimate": 0.013271418995202237,
          "standard_error": 0.0041831939334707885
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.798301507537651,
            "upper_bound": 10.805514255093987
          },
          "point_estimate": 10.801416537954056,
          "standard_error": 0.001870884739079581
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.797203750630093,
            "upper_bound": 10.805101080294817
          },
          "point_estimate": 10.799035648375268,
          "standard_error": 0.0018737740851059183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006310909234869456,
            "upper_bound": 0.008477993942010764
          },
          "point_estimate": 0.0027779927793103896,
          "standard_error": 0.0019427433481010745
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.797336398984182,
            "upper_bound": 10.803097830298592
          },
          "point_estimate": 10.800039801449955,
          "standard_error": 0.001583677861057864
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001543226347025946,
            "upper_bound": 0.00887670825630691
          },
          "point_estimate": 0.00622985568554396,
          "standard_error": 0.0020187391864145247
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.396661512153816,
            "upper_bound": 24.423805866787557
          },
          "point_estimate": 24.411405268221085,
          "standard_error": 0.006999126305841498
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.398263667256643,
            "upper_bound": 24.428173835302267
          },
          "point_estimate": 24.418616079588713,
          "standard_error": 0.007740315466083477
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0034419023439696446,
            "upper_bound": 0.036905601290705206
          },
          "point_estimate": 0.014579150673483646,
          "standard_error": 0.008481206712828706
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.3997558039838,
            "upper_bound": 24.425345083151942
          },
          "point_estimate": 24.41403277099277,
          "standard_error": 0.006586594468746024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008551158810630722,
            "upper_bound": 0.031126358657492713
          },
          "point_estimate": 0.02340828774556539,
          "standard_error": 0.00588441191315644
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.353499034349177,
            "upper_bound": 21.36657119057592
          },
          "point_estimate": 21.35943623944704,
          "standard_error": 0.0033625719132301416
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.351338424203547,
            "upper_bound": 21.36493185835561
          },
          "point_estimate": 21.357399352770074,
          "standard_error": 0.00313676801919852
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002093614237868179,
            "upper_bound": 0.016706974844724255
          },
          "point_estimate": 0.008137997321246898,
          "standard_error": 0.0037602897894491766
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.35383859096125,
            "upper_bound": 21.363489197635392
          },
          "point_estimate": 21.35810158625282,
          "standard_error": 0.0025051470959936415
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004342318297450537,
            "upper_bound": 0.015464465029153286
          },
          "point_estimate": 0.01124121076379183,
          "standard_error": 0.0030907878057889757
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1152450.3145611978,
            "upper_bound": 1154795.3654088543
          },
          "point_estimate": 1153408.4309895833,
          "standard_error": 626.1218680596953
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1152226.421875,
            "upper_bound": 1153868.828125
          },
          "point_estimate": 1152747.581770833,
          "standard_error": 383.79237276488135
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165.1878914423517,
            "upper_bound": 1911.621549655683
          },
          "point_estimate": 797.0557842869887,
          "standard_error": 465.0764265320222
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1152398.133504747,
            "upper_bound": 1153189.6524038462
          },
          "point_estimate": 1152820.3112824676,
          "standard_error": 200.29584887936588
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 449.8099569878436,
            "upper_bound": 3107.434872380462
          },
          "point_estimate": 2085.588378159297,
          "standard_error": 830.7705595061825
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1550088.4385416666,
            "upper_bound": 1551702.2149192707
          },
          "point_estimate": 1550776.7437847222,
          "standard_error": 421.7545220286051
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1549922.1822916667,
            "upper_bound": 1551325.3472222222
          },
          "point_estimate": 1550405.0791666666,
          "standard_error": 303.2067917109883
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.1783877338164,
            "upper_bound": 1458.8165991012308
          },
          "point_estimate": 633.3921909423336,
          "standard_error": 373.3559172561261
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1549999.1103801169,
            "upper_bound": 1550509.8087863985
          },
          "point_estimate": 1550267.3155844156,
          "standard_error": 130.19833952128013
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300.4044470596666,
            "upper_bound": 2003.227357544864
          },
          "point_estimate": 1407.3854432247413,
          "standard_error": 485.1928158322616
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1409022.461953602,
            "upper_bound": 1410257.5012629733
          },
          "point_estimate": 1409647.2899877902,
          "standard_error": 315.4091363003987
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1409061.25,
            "upper_bound": 1410457.0576923075
          },
          "point_estimate": 1409505.875,
          "standard_error": 388.2637619062264
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194.57192205859465,
            "upper_bound": 1809.027214037168
          },
          "point_estimate": 976.3572518970484,
          "standard_error": 406.02740914214303
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1409184.2925069062,
            "upper_bound": 1410090.8069845191
          },
          "point_estimate": 1409586.0195804196,
          "standard_error": 231.66703800279944
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 521.6181734578969,
            "upper_bound": 1389.82301646253
          },
          "point_estimate": 1048.1046388620616,
          "standard_error": 225.59832036480677
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376212.8622492227,
            "upper_bound": 376944.7411006381
          },
          "point_estimate": 376559.1880833742,
          "standard_error": 188.62488077541585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376079.3082474227,
            "upper_bound": 377030.0798969072
          },
          "point_estimate": 376363.0431701031,
          "standard_error": 237.12282104598148
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.0917838255113,
            "upper_bound": 1022.0454914771456
          },
          "point_estimate": 457.8788665875259,
          "standard_error": 250.40899950613291
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376138.852335328,
            "upper_bound": 376477.1085295681
          },
          "point_estimate": 376302.6194135761,
          "standard_error": 85.95018709352615
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 250.6029304565616,
            "upper_bound": 778.9142256315504
          },
          "point_estimate": 632.8447982149773,
          "standard_error": 125.67384347132167
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527927.06599353,
            "upper_bound": 529184.4821189182
          },
          "point_estimate": 528503.6428048079,
          "standard_error": 323.14161357226965
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527715.1182712215,
            "upper_bound": 529106.8786231884
          },
          "point_estimate": 528359.8762479872,
          "standard_error": 344.23353173584877
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218.6753106104534,
            "upper_bound": 1662.9373490639937
          },
          "point_estimate": 1010.0404808000368,
          "standard_error": 357.4120956337892
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527979.3688750863,
            "upper_bound": 528684.0582163463
          },
          "point_estimate": 528362.9782796913,
          "standard_error": 176.28591503789585
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 478.9798324302928,
            "upper_bound": 1480.5053809952788
          },
          "point_estimate": 1077.602035159474,
          "standard_error": 286.2809043590694
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 375918.3427217416,
            "upper_bound": 376170.2627826767
          },
          "point_estimate": 376038.16783873347,
          "standard_error": 64.61534754806141
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 375847.46649484534,
            "upper_bound": 376188.63041237113
          },
          "point_estimate": 376034.12076583214,
          "standard_error": 92.55366406724724
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.64908740030942,
            "upper_bound": 380.4191044833429
          },
          "point_estimate": 234.08441275141675,
          "standard_error": 84.73574942887743
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 375888.7109134592,
            "upper_bound": 376084.4904231955
          },
          "point_estimate": 375998.39828624984,
          "standard_error": 50.280785413019416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.49013446980145,
            "upper_bound": 276.32284889513346
          },
          "point_estimate": 215.3776064886091,
          "standard_error": 42.82391923905103
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464147.00304905063,
            "upper_bound": 464723.9481745781
          },
          "point_estimate": 464396.7885021096,
          "standard_error": 149.1818146625484
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464098.211814346,
            "upper_bound": 464505.3670886076
          },
          "point_estimate": 464321.4303797468,
          "standard_error": 106.7056922615623
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.97347686906302,
            "upper_bound": 587.1915391955118
          },
          "point_estimate": 259.36819761045876,
          "standard_error": 134.11240529068962
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 464141.7316163021,
            "upper_bound": 464496.1582105243
          },
          "point_estimate": 464369.6383034687,
          "standard_error": 91.05816002217026
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154.78963395114366,
            "upper_bound": 726.1091066818026
          },
          "point_estimate": 498.648190585685,
          "standard_error": 171.51021252832524
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 980371.8191277412,
            "upper_bound": 981383.5811111112
          },
          "point_estimate": 980896.4230336256,
          "standard_error": 259.2262926577413
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 980256.6200657894,
            "upper_bound": 981759.6871345028
          },
          "point_estimate": 980852.8171052632,
          "standard_error": 308.63184678982014
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122.86266966086777,
            "upper_bound": 1505.9899390529258
          },
          "point_estimate": 893.1886693180388,
          "standard_error": 432.13327104820223
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 980695.8910648716,
            "upper_bound": 981533.576684808
          },
          "point_estimate": 981111.7392344498,
          "standard_error": 214.5942696945002
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463.6693469764816,
            "upper_bound": 1099.2720196888692
          },
          "point_estimate": 860.3937886852024,
          "standard_error": 168.51172282362612
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 281987.67879468895,
            "upper_bound": 282694.0025879015
          },
          "point_estimate": 282303.5474187892,
          "standard_error": 182.56592104870217
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 281864.67926356586,
            "upper_bound": 282541.8333333333
          },
          "point_estimate": 282215.3625968992,
          "standard_error": 179.71472685462524
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.82145918105283,
            "upper_bound": 836.948824094721
          },
          "point_estimate": 466.7535233995485,
          "standard_error": 183.13210568903023
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 281783.15470914973,
            "upper_bound": 282318.1007022806
          },
          "point_estimate": 281967.30160072487,
          "standard_error": 133.73962413803872
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 245.9877742792073,
            "upper_bound": 855.2363393618267
          },
          "point_estimate": 608.7402270048519,
          "standard_error": 179.17095606281302
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 794596.5784367884,
            "upper_bound": 795174.5817399931
          },
          "point_estimate": 794869.523865597,
          "standard_error": 148.06357229074473
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 794462.0135869565,
            "upper_bound": 795276.0263285025
          },
          "point_estimate": 794699.4048913043,
          "standard_error": 208.65096559805392
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.83548304572668,
            "upper_bound": 790.551671472149
          },
          "point_estimate": 427.66885458130616,
          "standard_error": 192.3575001704053
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 794595.266289458,
            "upper_bound": 795323.6817621033
          },
          "point_estimate": 794978.8803500847,
          "standard_error": 184.39085305672356
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236.6171121160807,
            "upper_bound": 621.2143930539384
          },
          "point_estimate": 494.6643626855148,
          "standard_error": 98.12169157606928
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389094.7964661854,
            "upper_bound": 389495.3527806273
          },
          "point_estimate": 389296.70895558933,
          "standard_error": 101.78965033121496
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389142.18297872343,
            "upper_bound": 389419.1896656535
          },
          "point_estimate": 389301.41504137113,
          "standard_error": 61.074161296576165
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.568405202306316,
            "upper_bound": 530.1042339575549
          },
          "point_estimate": 141.59044427707997,
          "standard_error": 130.88842787805962
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389225.58816911874,
            "upper_bound": 389376.21497693023
          },
          "point_estimate": 389316.911052777,
          "standard_error": 38.33011427795936
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.33371854860594,
            "upper_bound": 472.3481909582474
          },
          "point_estimate": 339.6080571522105,
          "standard_error": 95.5223448614373
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158700.7512524563,
            "upper_bound": 158954.3182112445
          },
          "point_estimate": 158822.90601528384,
          "standard_error": 64.83415538771068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158672.70305676857,
            "upper_bound": 158941.5283842795
          },
          "point_estimate": 158804.66681222708,
          "standard_error": 62.55407818585505
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.84627462683088,
            "upper_bound": 360.7109625917396
          },
          "point_estimate": 164.18553813317823,
          "standard_error": 84.78393340639417
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158664.92642376275,
            "upper_bound": 158966.78162992478
          },
          "point_estimate": 158804.4041172801,
          "standard_error": 75.51311910387997
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.15882940350868,
            "upper_bound": 290.39767083859266
          },
          "point_estimate": 215.8272412059128,
          "standard_error": 49.26115437122768
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230083.96048377536,
            "upper_bound": 230245.29655766525
          },
          "point_estimate": 230158.0040692184,
          "standard_error": 41.32852229500715
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230059.1223628692,
            "upper_bound": 230231.64477848104
          },
          "point_estimate": 230139.80932288527,
          "standard_error": 42.54531187073768
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.020197358366847,
            "upper_bound": 216.4864062831641
          },
          "point_estimate": 107.81535821246185,
          "standard_error": 47.80003109823757
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230088.1054785133,
            "upper_bound": 230163.14524181755
          },
          "point_estimate": 230122.85592635212,
          "standard_error": 19.23201499178933
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.203120059368885,
            "upper_bound": 186.00428846642623
          },
          "point_estimate": 137.60590279505735,
          "standard_error": 35.008116057610835
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171928.70598831985,
            "upper_bound": 172224.96515049416
          },
          "point_estimate": 172041.72714398027,
          "standard_error": 82.96663194479235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171903.34119496855,
            "upper_bound": 172023.77016509435
          },
          "point_estimate": 171978.99747304583,
          "standard_error": 41.04913081472137
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.15949009655849,
            "upper_bound": 158.07464601437093
          },
          "point_estimate": 89.82073184874335,
          "standard_error": 38.94358365225678
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171926.199854523,
            "upper_bound": 172019.310769061
          },
          "point_estimate": 171979.3077432002,
          "standard_error": 24.39279146495194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.94803355358591,
            "upper_bound": 425.68561245044464
          },
          "point_estimate": 277.69684884728696,
          "standard_error": 132.11621679732
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 651119.1062058709,
            "upper_bound": 651497.3830125957
          },
          "point_estimate": 651292.9062662984,
          "standard_error": 97.46075048373358
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 651071.8023809524,
            "upper_bound": 651505.6575963718
          },
          "point_estimate": 651241.6997023809,
          "standard_error": 98.46853172904474
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.52079101828206,
            "upper_bound": 513.7839265928006
          },
          "point_estimate": 209.60312284118152,
          "standard_error": 119.6288511085358
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 651094.8625313416,
            "upper_bound": 651407.2136837881
          },
          "point_estimate": 651235.4365955474,
          "standard_error": 78.94229213951478
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121.55928379722134,
            "upper_bound": 426.9523761356192
          },
          "point_estimate": 323.87992932171267,
          "standard_error": 80.60087310864857
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1307.2478729544382,
            "upper_bound": 1308.446612367451
          },
          "point_estimate": 1307.7630235694996,
          "standard_error": 0.31290220431825616
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1306.9968541854184,
            "upper_bound": 1308.1545341391284
          },
          "point_estimate": 1307.5391113111311,
          "standard_error": 0.2336231863251559
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13389570637176118,
            "upper_bound": 1.2288765416392995
          },
          "point_estimate": 0.5632334134929515,
          "standard_error": 0.32633915598500046
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1307.2891920771024,
            "upper_bound": 1307.8174233773743
          },
          "point_estimate": 1307.5383366804213,
          "standard_error": 0.13143442404158948
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.31003232044313284,
            "upper_bound": 1.4992375602239638
          },
          "point_estimate": 1.0427207968171013,
          "standard_error": 0.3517833999030514
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355304.9183811727,
            "upper_bound": 1356433.7442298648
          },
          "point_estimate": 1355867.8290755437,
          "standard_error": 288.9912349270317
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1354971.3333333333,
            "upper_bound": 1356546.7984126983
          },
          "point_estimate": 1356036.585648148,
          "standard_error": 451.2611499928579
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.38972304435387,
            "upper_bound": 1698.5360076536113
          },
          "point_estimate": 952.7822830847084,
          "standard_error": 393.998775839869
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355618.9568045798,
            "upper_bound": 1356652.087631329
          },
          "point_estimate": 1356130.5108225108,
          "standard_error": 268.7951981080209
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 581.4637463569537,
            "upper_bound": 1203.749645826944
          },
          "point_estimate": 965.0406386132202,
          "standard_error": 159.92369958177244
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1524033.086809028,
            "upper_bound": 1525137.4713458994
          },
          "point_estimate": 1524563.1256382274,
          "standard_error": 283.8723597480647
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523690.775,
            "upper_bound": 1525274.961574074
          },
          "point_estimate": 1524459.1614583335,
          "standard_error": 481.198892897025
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.04337194573463,
            "upper_bound": 1703.3778138564635
          },
          "point_estimate": 1154.87477949694,
          "standard_error": 400.0959921252497
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1524064.8892205458,
            "upper_bound": 1525223.4797908864
          },
          "point_estimate": 1524765.913852814,
          "standard_error": 296.806147947316
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 558.4616641975375,
            "upper_bound": 1172.1432086569268
          },
          "point_estimate": 947.303070321406,
          "standard_error": 160.40621749484075
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1426044.7720879118,
            "upper_bound": 1427486.3972222223
          },
          "point_estimate": 1426786.41995116,
          "standard_error": 368.64033105911193
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1426113.6923076925,
            "upper_bound": 1427706.9653846154
          },
          "point_estimate": 1426815.1903846154,
          "standard_error": 353.1883937036371
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189.73573163148288,
            "upper_bound": 2110.647379836308
          },
          "point_estimate": 954.010315755199,
          "standard_error": 528.853101314163
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1426209.3585951754,
            "upper_bound": 1427380.8411425708
          },
          "point_estimate": 1426811.324075924,
          "standard_error": 291.98686583321586
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 585.8834043191428,
            "upper_bound": 1637.1113080943887
          },
          "point_estimate": 1230.75524732711,
          "standard_error": 271.48352648219776
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789716.881945922,
            "upper_bound": 790310.8135933806
          },
          "point_estimate": 790005.726209051,
          "standard_error": 152.81119275718552
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789610.5543313071,
            "upper_bound": 790498.3510638297
          },
          "point_estimate": 789808.6276595744,
          "standard_error": 267.52729688029376
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.21504979880731,
            "upper_bound": 749.7399412993717
          },
          "point_estimate": 541.2995035814087,
          "standard_error": 199.13061504214505
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789654.0721617157,
            "upper_bound": 790363.6545428407
          },
          "point_estimate": 790085.4833932025,
          "standard_error": 179.24399606990693
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 305.26424884721143,
            "upper_bound": 615.6886655833322
          },
          "point_estimate": 509.442939710956,
          "standard_error": 79.28016709348428
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278420.9763528414,
            "upper_bound": 279382.73321811005
          },
          "point_estimate": 278944.15221343754,
          "standard_error": 246.0656395320816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278775.02374893974,
            "upper_bound": 279186.106870229
          },
          "point_estimate": 279076.69058524177,
          "standard_error": 105.89387872801431
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.11704670835737,
            "upper_bound": 1029.126480965993
          },
          "point_estimate": 203.44278875459736,
          "standard_error": 251.49123346503217
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277974.99004944245,
            "upper_bound": 279090.76365745947
          },
          "point_estimate": 278704.79222761974,
          "standard_error": 321.83713888958505
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.7220884165871,
            "upper_bound": 1209.9623561451815
          },
          "point_estimate": 819.4138916485955,
          "standard_error": 279.60709149147965
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492587.17912483914,
            "upper_bound": 493611.4655126153
          },
          "point_estimate": 493013.91822983697,
          "standard_error": 269.13681386812743
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492569.78571428574,
            "upper_bound": 493133.85945945943
          },
          "point_estimate": 492867.51022897894,
          "standard_error": 165.2432655590368
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.15092072934767,
            "upper_bound": 814.899054451571
          },
          "point_estimate": 396.6330866633574,
          "standard_error": 188.06655135734377
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492704.6952827681,
            "upper_bound": 493034.8442587275
          },
          "point_estimate": 492842.7056160056,
          "standard_error": 83.87659180040009
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 210.7287746056618,
            "upper_bound": 1342.1759701798096
          },
          "point_estimate": 898.814363686715,
          "standard_error": 354.83014159644284
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253101.927409612,
            "upper_bound": 253396.02731977517
          },
          "point_estimate": 253245.66126846345,
          "standard_error": 75.20267653872793
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253007.07083333333,
            "upper_bound": 253404.65228174604
          },
          "point_estimate": 253251.54933449073,
          "standard_error": 89.7802695406324
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.093150002358094,
            "upper_bound": 465.38219120541214
          },
          "point_estimate": 252.84708677843727,
          "standard_error": 114.26691314496009
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253031.71270235468,
            "upper_bound": 253318.80911155793
          },
          "point_estimate": 253196.89431818185,
          "standard_error": 76.39094946775539
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139.85568334784762,
            "upper_bound": 324.3531797950731
          },
          "point_estimate": 251.05167749527143,
          "standard_error": 49.0409492963915
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632815.2781458334,
            "upper_bound": 633356.9435221674
          },
          "point_estimate": 633079.3801169952,
          "standard_error": 138.87337668864032
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632604.6810344828,
            "upper_bound": 633516.2025862068
          },
          "point_estimate": 633034.0747126436,
          "standard_error": 231.5915100387074
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.28416657434435,
            "upper_bound": 796.5055341349689
          },
          "point_estimate": 549.6973721374716,
          "standard_error": 192.26096188686248
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632686.8988726791,
            "upper_bound": 633232.7687411493
          },
          "point_estimate": 632900.470174653,
          "standard_error": 140.77298526756078
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 273.61795899517176,
            "upper_bound": 559.6539326395111
          },
          "point_estimate": 463.5235061287493,
          "standard_error": 72.70390385273203
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2050.658966223775,
            "upper_bound": 2069.0363176031897
          },
          "point_estimate": 2059.177249524065,
          "standard_error": 4.681529921976077
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2047.9614198417717,
            "upper_bound": 2071.945783767216
          },
          "point_estimate": 2049.352433768174,
          "standard_error": 7.127726420587969
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.26183994133154115,
            "upper_bound": 26.179686698790405
          },
          "point_estimate": 2.397423263281099,
          "standard_error": 8.151393717818234
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2048.4266421021134,
            "upper_bound": 2053.6912219779592
          },
          "point_estimate": 2049.843211876911,
          "standard_error": 1.4221392660398202
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.700829311935758,
            "upper_bound": 20.21584742854748
          },
          "point_estimate": 15.586411272537228,
          "standard_error": 3.605150829832206
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 650.6968024546286,
            "upper_bound": 651.1161832237159
          },
          "point_estimate": 650.8936637067072,
          "standard_error": 0.10735728459282629
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 650.671728584772,
            "upper_bound": 651.0308751178317
          },
          "point_estimate": 650.8884491849709,
          "standard_error": 0.10294572482954414
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05826896199536097,
            "upper_bound": 0.5469697517163296
          },
          "point_estimate": 0.26584546630989275,
          "standard_error": 0.1187301901541632
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 650.5616689899697,
            "upper_bound": 650.9680862319892
          },
          "point_estimate": 650.7580392815175,
          "standard_error": 0.10578412988393832
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14082235596403722,
            "upper_bound": 0.4977835451282868
          },
          "point_estimate": 0.3554171219853522,
          "standard_error": 0.09605259404160828
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126.53460976076016,
            "upper_bound": 126.60275400289852
          },
          "point_estimate": 126.56305178070616,
          "standard_error": 0.0179568501663735
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126.53023126826818,
            "upper_bound": 126.57456591511152
          },
          "point_estimate": 126.55182484483294,
          "standard_error": 0.009521908824471194
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0036472647827353135,
            "upper_bound": 0.062047424935183546
          },
          "point_estimate": 0.019378928089382824,
          "standard_error": 0.01732210231377463
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126.5279275905472,
            "upper_bound": 126.5701570356944
          },
          "point_estimate": 126.5482290558076,
          "standard_error": 0.010789056570328855
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015472575867587478,
            "upper_bound": 0.0888357495760975
          },
          "point_estimate": 0.05992912706422872,
          "standard_error": 0.022847722711526703
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.852431735375845,
            "upper_bound": 49.88268204509784
          },
          "point_estimate": 49.86591736409877,
          "standard_error": 0.007825110206523265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.847927428110815,
            "upper_bound": 49.8819085240038
          },
          "point_estimate": 49.858954595387985,
          "standard_error": 0.0068912432227551884
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004367683881466579,
            "upper_bound": 0.03494533275130481
          },
          "point_estimate": 0.013728821999595724,
          "standard_error": 0.008232945086294222
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.851602545088056,
            "upper_bound": 49.87290506415464
          },
          "point_estimate": 49.860635312078855,
          "standard_error": 0.005387944460130096
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007560768922451048,
            "upper_bound": 0.03485900546132206
          },
          "point_estimate": 0.02595207592591593,
          "standard_error": 0.007270419413578364
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.70098174724754,
            "upper_bound": 78.7822844835692
          },
          "point_estimate": 78.74016123719299,
          "standard_error": 0.020854260886275625
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.68859513983217,
            "upper_bound": 78.79663543533638
          },
          "point_estimate": 78.73603052540673,
          "standard_error": 0.022274037880889944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007293133888957737,
            "upper_bound": 0.13407457732036318
          },
          "point_estimate": 0.05687039142886791,
          "standard_error": 0.03310271874536057
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.67411536638251,
            "upper_bound": 78.73670645572706
          },
          "point_estimate": 78.70325461823741,
          "standard_error": 0.015466928882747994
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03471907805102781,
            "upper_bound": 0.08775718066325275
          },
          "point_estimate": 0.06953101803237735,
          "standard_error": 0.013643074864247874
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299261.11020719487,
            "upper_bound": 299474.87845823576
          },
          "point_estimate": 299350.9962896825,
          "standard_error": 56.23552496246849
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299243.2291666666,
            "upper_bound": 299427.99153005466
          },
          "point_estimate": 299284.12059426226,
          "standard_error": 38.61447082464499
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.391963509498142,
            "upper_bound": 208.2846949906903
          },
          "point_estimate": 50.11830978642599,
          "standard_error": 45.91939004095671
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299251.96320140356,
            "upper_bound": 299352.3403698288
          },
          "point_estimate": 299289.2443900362,
          "standard_error": 25.62432171110293
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.86288372050949,
            "upper_bound": 265.6497371973051
          },
          "point_estimate": 187.72212550280304,
          "standard_error": 67.08253458928002
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.38508295045087,
            "upper_bound": 59.43751691684802
          },
          "point_estimate": 59.406683725188465,
          "standard_error": 0.01390753600764916
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.38127982641693,
            "upper_bound": 59.41453200610432
          },
          "point_estimate": 59.39601137912883,
          "standard_error": 0.008096153731252609
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003622073745553847,
            "upper_bound": 0.042310893409259503
          },
          "point_estimate": 0.017080539686360547,
          "standard_error": 0.010120465572579049
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.38605208455214,
            "upper_bound": 59.41420641303789
          },
          "point_estimate": 59.4013945625038,
          "standard_error": 0.007457231744882727
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01065253510331844,
            "upper_bound": 0.06964001287741417
          },
          "point_estimate": 0.046588751175049126,
          "standard_error": 0.018658912553844414
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.39232154453444,
            "upper_bound": 99.45690941249292
          },
          "point_estimate": 99.42250336012668,
          "standard_error": 0.016594118471454304
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.37972842391407,
            "upper_bound": 99.4693488815865
          },
          "point_estimate": 99.3972976002362,
          "standard_error": 0.024215930830746787
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0020913782277660058,
            "upper_bound": 0.0892364965047811
          },
          "point_estimate": 0.03500036251445517,
          "standard_error": 0.023819391457995588
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.37893839077756,
            "upper_bound": 99.45701212924956
          },
          "point_estimate": 99.40588212070062,
          "standard_error": 0.02179201249196968
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02146141684604133,
            "upper_bound": 0.06719649359203032
          },
          "point_estimate": 0.05539095742019199,
          "standard_error": 0.011195740960548876
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132489.1152055961,
            "upper_bound": 132863.5933521029
          },
          "point_estimate": 132715.19288408064,
          "standard_error": 101.32817075659538
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132703.88686131386,
            "upper_bound": 132865.22749391728
          },
          "point_estimate": 132798.45125130343,
          "standard_error": 44.81858942647559
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.786981876966937,
            "upper_bound": 238.9582311225968
          },
          "point_estimate": 119.60180882555603,
          "standard_error": 57.25476189596964
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132278.5821595575,
            "upper_bound": 132866.52384810636
          },
          "point_estimate": 132668.78835908615,
          "standard_error": 167.55147312684662
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.64535952599878,
            "upper_bound": 511.72191681367786
          },
          "point_estimate": 336.889268537826,
          "standard_error": 148.51112592899273
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.677561027685215,
            "upper_bound": 58.710332078428664
          },
          "point_estimate": 58.69206473438864,
          "standard_error": 0.00848499651844579
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.67636335512334,
            "upper_bound": 58.70466959890243
          },
          "point_estimate": 58.678031847231665,
          "standard_error": 0.008327645920537564
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008216458735561712,
            "upper_bound": 0.03772259750638508
          },
          "point_estimate": 0.011514050154716257,
          "standard_error": 0.010841603899853109
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.671793776431066,
            "upper_bound": 58.69007970587265
          },
          "point_estimate": 58.67839867436252,
          "standard_error": 0.004638001997128508
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008534302301974413,
            "upper_bound": 0.039476969841647135
          },
          "point_estimate": 0.028275074395631355,
          "standard_error": 0.00873158338613728
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.28308423770412,
            "upper_bound": 93.34001699120267
          },
          "point_estimate": 93.30768197242546,
          "standard_error": 0.014763565882032112
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.28363141058536,
            "upper_bound": 93.31841495888789
          },
          "point_estimate": 93.3013551395874,
          "standard_error": 0.00959046999502388
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017353651608703186,
            "upper_bound": 0.05652146100773861
          },
          "point_estimate": 0.024642722863861523,
          "standard_error": 0.013236111303890123
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.29229672043463,
            "upper_bound": 93.31716712946934
          },
          "point_estimate": 93.30638267100856,
          "standard_error": 0.0063004598563807515
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011848461126463697,
            "upper_bound": 0.07178382504161128
          },
          "point_estimate": 0.04910606246345077,
          "standard_error": 0.017564339408629354
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.58161865507137,
            "upper_bound": 237.83118229837413
          },
          "point_estimate": 237.67549189082507,
          "standard_error": 0.07061722024387748
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.56814452601503,
            "upper_bound": 237.68188886076825
          },
          "point_estimate": 237.5914599799088,
          "standard_error": 0.03195671000633874
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006387133227350258,
            "upper_bound": 0.1293858929137361
          },
          "point_estimate": 0.03780564187732884,
          "standard_error": 0.03477556499194887
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.5769506759164,
            "upper_bound": 237.63945152285757
          },
          "point_estimate": 237.6010029194439,
          "standard_error": 0.016122758084631796
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016691043981523896,
            "upper_bound": 0.3598984549059989
          },
          "point_estimate": 0.23490872702560744,
          "standard_error": 0.11343243614665643
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486.3360921711863,
            "upper_bound": 486.9591484295576
          },
          "point_estimate": 486.62544103271,
          "standard_error": 0.16000202520751838
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486.234106069524,
            "upper_bound": 487.0151726951683
          },
          "point_estimate": 486.4455461768955,
          "standard_error": 0.19304037888231876
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.093433763078055,
            "upper_bound": 0.8480506584686687
          },
          "point_estimate": 0.4032227062913228,
          "standard_error": 0.2025491038663608
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486.2151023603694,
            "upper_bound": 486.525164134722
          },
          "point_estimate": 486.3378880763947,
          "standard_error": 0.07832146091625443
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.22219016248199172,
            "upper_bound": 0.6990367915017651
          },
          "point_estimate": 0.5323247712682148,
          "standard_error": 0.12514139156515447
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.71083943200013,
            "upper_bound": 32.744393013598796
          },
          "point_estimate": 32.724324158730845,
          "standard_error": 0.009065484778169782
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.708922757110216,
            "upper_bound": 32.72771352646108
          },
          "point_estimate": 32.71627369997822,
          "standard_error": 0.005030622028520847
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003107279119373272,
            "upper_bound": 0.02378185031407016
          },
          "point_estimate": 0.009896999905006911,
          "standard_error": 0.00570051043874623
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.71014437122215,
            "upper_bound": 32.72731186325518
          },
          "point_estimate": 32.71831670066417,
          "standard_error": 0.004743063897193605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006094235321613303,
            "upper_bound": 0.045703526323121385
          },
          "point_estimate": 0.03027768528985247,
          "standard_error": 0.01284792086971887
        }
      }
    },
    "memrmem/krate/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33791.22737881336,
            "upper_bound": 33819.8796866868
          },
          "point_estimate": 33803.888402224584,
          "standard_error": 7.384949041347021
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33791.02975010326,
            "upper_bound": 33812.04542286246
          },
          "point_estimate": 33799.12356722429,
          "standard_error": 5.551547174146722
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3378047912106927,
            "upper_bound": 32.33150782129757
          },
          "point_estimate": 10.87801467211328,
          "standard_error": 7.155823914720143
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33785.46074205772,
            "upper_bound": 33804.921948340285
          },
          "point_estimate": 33796.117877661374,
          "standard_error": 4.993053257035193
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.354835052060361,
            "upper_bound": 35.31554817950539
          },
          "point_estimate": 24.655320343260197,
          "standard_error": 8.0079831077242
        }
      }
    },
    "memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.05394610498688,
            "upper_bound": 124.18383437607176
          },
          "point_estimate": 124.10494552295344,
          "standard_error": 0.035616493352933605
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.0442701931219,
            "upper_bound": 124.11116392144008
          },
          "point_estimate": 124.05981604402076,
          "standard_error": 0.020450609013219004
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006502392781463301,
            "upper_bound": 0.08513306545129964
          },
          "point_estimate": 0.029174761494118028,
          "standard_error": 0.022097600182750792
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.04759149396124,
            "upper_bound": 124.09753117365932
          },
          "point_estimate": 124.06498739044017,
          "standard_error": 0.013136607036160656
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01788756930301607,
            "upper_bound": 0.18005683199146713
          },
          "point_estimate": 0.11869437148813512,
          "standard_error": 0.052456440772710694
        }
      }
    },
    "memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1105025.19270202,
            "upper_bound": 1105866.9602994225
          },
          "point_estimate": 1105401.9959800388,
          "standard_error": 217.17974866196548
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104986.2015993265,
            "upper_bound": 1105860.0833333333
          },
          "point_estimate": 1105073.374621212,
          "standard_error": 213.32357023351844
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.244590281287778,
            "upper_bound": 1047.521464357348
          },
          "point_estimate": 224.518425559372,
          "standard_error": 254.85916251564285
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1105029.0803928955,
            "upper_bound": 1105330.7219460865
          },
          "point_estimate": 1105170.2523415978,
          "standard_error": 76.20191973574195
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175.73568307621358,
            "upper_bound": 939.5824506973896
          },
          "point_estimate": 721.0972805050884,
          "standard_error": 197.9499952737647
        }
      }
    },
    "memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2233.5790460575736,
            "upper_bound": 2235.294090030661
          },
          "point_estimate": 2234.2725579592866,
          "standard_error": 0.4577667852767429
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2233.502974788137,
            "upper_bound": 2234.3843853071926
          },
          "point_estimate": 2233.881455759736,
          "standard_error": 0.192925236104952
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04761988934778432,
            "upper_bound": 1.3319192437617342
          },
          "point_estimate": 0.3135911707586994,
          "standard_error": 0.3797946057861823
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2233.7499726540564,
            "upper_bound": 2234.456865102345
          },
          "point_estimate": 2234.0865333670695,
          "standard_error": 0.1988435636782564
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.309083438057052,
            "upper_bound": 2.3030277388998885
          },
          "point_estimate": 1.5281573883748758,
          "standard_error": 0.6379318980453788
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.90091018570184,
            "upper_bound": 24.925099927787397
          },
          "point_estimate": 24.911666181067847,
          "standard_error": 0.006235261631931542
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.89882731351203,
            "upper_bound": 24.921309971916557
          },
          "point_estimate": 24.90386381202312,
          "standard_error": 0.005641460001237201
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006786981633620686,
            "upper_bound": 0.029521226403311764
          },
          "point_estimate": 0.008227215000682886,
          "standard_error": 0.0067041863747392075
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.900343435310965,
            "upper_bound": 24.91105734683281
          },
          "point_estimate": 24.90408281847293,
          "standard_error": 0.0027509113274627995
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004043709213416963,
            "upper_bound": 0.02805447756672934
          },
          "point_estimate": 0.020782830653414647,
          "standard_error": 0.006175886068901321
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.385290905313116,
            "upper_bound": 25.402585048085477
          },
          "point_estimate": 25.393346344527632,
          "standard_error": 0.004449800746223411
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.381885783139808,
            "upper_bound": 25.40583459949316
          },
          "point_estimate": 25.390996311843992,
          "standard_error": 0.004520902868328415
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016661548501624142,
            "upper_bound": 0.023727613784894727
          },
          "point_estimate": 0.007478225347138606,
          "standard_error": 0.005870637797196568
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.38475550540972,
            "upper_bound": 25.39691676131478
          },
          "point_estimate": 25.390580971489424,
          "standard_error": 0.003048885611206434
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005617188749044138,
            "upper_bound": 0.018882864787548765
          },
          "point_estimate": 0.014839144478892886,
          "standard_error": 0.0033863835422491963
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.991958628345483,
            "upper_bound": 25.010639582277648
          },
          "point_estimate": 25.000292206293125,
          "standard_error": 0.004778002200084581
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.990309726792997,
            "upper_bound": 25.012175428631885
          },
          "point_estimate": 24.99494119710663,
          "standard_error": 0.004710430525590524
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011432208580715432,
            "upper_bound": 0.021290804152507115
          },
          "point_estimate": 0.007905471647067711,
          "standard_error": 0.004505243234472053
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.989935153490627,
            "upper_bound": 24.99747880429167
          },
          "point_estimate": 24.99341299587913,
          "standard_error": 0.0019092072969206935
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003642137529222352,
            "upper_bound": 0.019947518730134735
          },
          "point_estimate": 0.015894149365524975,
          "standard_error": 0.004299131020449716
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.55459705468998,
            "upper_bound": 31.18080648271003
          },
          "point_estimate": 30.870608142666725,
          "standard_error": 0.1602647245964498
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.46789073508135,
            "upper_bound": 31.463869810395995
          },
          "point_estimate": 30.807111898539265,
          "standard_error": 0.2964853116190424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02833066999581875,
            "upper_bound": 0.8882138984827836
          },
          "point_estimate": 0.6267478310192744,
          "standard_error": 0.23792053921638032
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.406322329608535,
            "upper_bound": 30.897000604373577
          },
          "point_estimate": 30.65300689910342,
          "standard_error": 0.12701509691514157
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3416664138041538,
            "upper_bound": 0.6467333350317211
          },
          "point_estimate": 0.5330880639094553,
          "standard_error": 0.07937981843971659
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.653209390656933,
            "upper_bound": 19.664283069236426
          },
          "point_estimate": 19.65817010358751,
          "standard_error": 0.002852575358228393
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.652467000789805,
            "upper_bound": 19.66338508079448
          },
          "point_estimate": 19.65612715564116,
          "standard_error": 0.0028317994973643896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009364947663703994,
            "upper_bound": 0.012870962218049843
          },
          "point_estimate": 0.0056862265203845765,
          "standard_error": 0.0031455664668360228
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.650083003629984,
            "upper_bound": 19.660293320939505
          },
          "point_estimate": 19.65447534441613,
          "standard_error": 0.0026017425582768976
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003601258128437483,
            "upper_bound": 0.013333861149178458
          },
          "point_estimate": 0.00949055519352141,
          "standard_error": 0.002823614301253516
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.861392009660307,
            "upper_bound": 21.91007904027948
          },
          "point_estimate": 21.88148847937894,
          "standard_error": 0.012827561459985556
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.858959481244437,
            "upper_bound": 21.89362869487019
          },
          "point_estimate": 21.86466077018939,
          "standard_error": 0.007951176767116519
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014013243036196843,
            "upper_bound": 0.042783406614037955
          },
          "point_estimate": 0.009986039792071728,
          "standard_error": 0.010304430429861074
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.86323286855167,
            "upper_bound": 21.894040026705344
          },
          "point_estimate": 21.874212346927393,
          "standard_error": 0.008692016083268207
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005477196633275416,
            "upper_bound": 0.06183635707895904
          },
          "point_estimate": 0.04273791553092342,
          "standard_error": 0.016136819974618834
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.511477634284276,
            "upper_bound": 35.55952716681119
          },
          "point_estimate": 35.53075188710033,
          "standard_error": 0.013084750963597902
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.507820730579866,
            "upper_bound": 35.53865632741598
          },
          "point_estimate": 35.51485991862838,
          "standard_error": 0.006908589950987478
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002664933614521318,
            "upper_bound": 0.03074667610541719
          },
          "point_estimate": 0.011037541411989924,
          "standard_error": 0.00792504109365341
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.51203186931495,
            "upper_bound": 35.539760113621035
          },
          "point_estimate": 35.52050766982875,
          "standard_error": 0.007338084177024074
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005531232731231448,
            "upper_bound": 0.06533275497318673
          },
          "point_estimate": 0.04361503556478032,
          "standard_error": 0.01852709634984604
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.10438908477863,
            "upper_bound": 26.12748246992391
          },
          "point_estimate": 26.115431127119713,
          "standard_error": 0.005938673132717523
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.095414199883667,
            "upper_bound": 26.130616908620276
          },
          "point_estimate": 26.11467505355199,
          "standard_error": 0.00849040783354561
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011458246108531204,
            "upper_bound": 0.034505568070646836
          },
          "point_estimate": 0.02231403166719259,
          "standard_error": 0.008920038293491169
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.097193242608224,
            "upper_bound": 26.11785124221496
          },
          "point_estimate": 26.10528481365957,
          "standard_error": 0.005272440248808257
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010385452138647324,
            "upper_bound": 0.02486922095313168
          },
          "point_estimate": 0.019807789589263283,
          "standard_error": 0.0037571641466434304
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.939257466875507,
            "upper_bound": 29.950060175168915
          },
          "point_estimate": 29.320606752694225,
          "standard_error": 0.2853981754727611
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.876371306497106,
            "upper_bound": 29.284339441680352
          },
          "point_estimate": 29.03033542881293,
          "standard_error": 0.11896631332395712
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06741466483909105,
            "upper_bound": 0.46884758053407366
          },
          "point_estimate": 0.23209042658947293,
          "standard_error": 0.12492435842402824
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.94831947789872,
            "upper_bound": 30.94194399308236
          },
          "point_estimate": 29.84361548264471,
          "standard_error": 0.5891109428484137
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1273499303521414,
            "upper_bound": 1.4577676321893724
          },
          "point_estimate": 0.9497244139114156,
          "standard_error": 0.4573286253127647
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.020458093592136,
            "upper_bound": 28.051735015657925
          },
          "point_estimate": 28.03620322864172,
          "standard_error": 0.008001118270771275
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.01731728609985,
            "upper_bound": 28.059689864507504
          },
          "point_estimate": 28.03403268682642,
          "standard_error": 0.012744724190394269
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001608365519004527,
            "upper_bound": 0.04496394049217261
          },
          "point_estimate": 0.025162491220241275,
          "standard_error": 0.011105471870598573
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.022752604599297,
            "upper_bound": 28.05128219527692
          },
          "point_estimate": 28.036098276140805,
          "standard_error": 0.007301065846676808
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01593559697849388,
            "upper_bound": 0.03350120613886742
          },
          "point_estimate": 0.026686824305837152,
          "standard_error": 0.004527230686728603
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.670717739716537,
            "upper_bound": 24.687353224668435
          },
          "point_estimate": 24.67830300654976,
          "standard_error": 0.00425487944944928
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.667470865664264,
            "upper_bound": 24.686093405067425
          },
          "point_estimate": 24.67646198502801,
          "standard_error": 0.00436361186549749
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015262794085597812,
            "upper_bound": 0.020979330580151404
          },
          "point_estimate": 0.01228468675331097,
          "standard_error": 0.004947433783587173
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.671950271416936,
            "upper_bound": 24.68221161541508
          },
          "point_estimate": 24.67717833545896,
          "standard_error": 0.002626863651061736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0060359163419241545,
            "upper_bound": 0.01944969971214343
          },
          "point_estimate": 0.014204878419226862,
          "standard_error": 0.003777349780271239
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.94446076469327,
            "upper_bound": 27.96965438158186
          },
          "point_estimate": 27.955824671758137,
          "standard_error": 0.006512866312573189
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.942480666541556,
            "upper_bound": 27.96691302020361
          },
          "point_estimate": 27.949031793846228,
          "standard_error": 0.006442238142392522
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035494525348881874,
            "upper_bound": 0.032329788616728965
          },
          "point_estimate": 0.011102292273306816,
          "standard_error": 0.007906863799051481
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.94420130637158,
            "upper_bound": 27.96454126578087
          },
          "point_estimate": 27.95433468592993,
          "standard_error": 0.005356443305318123
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007823813413838246,
            "upper_bound": 0.029587934947223997
          },
          "point_estimate": 0.02169198527876751,
          "standard_error": 0.005904763123536873
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1186713.7371214158,
            "upper_bound": 1187277.9167741935
          },
          "point_estimate": 1186952.633607271,
          "standard_error": 147.23904086938663
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1186719.5751152071,
            "upper_bound": 1187027.3838709677
          },
          "point_estimate": 1186886.498655914,
          "standard_error": 78.54368726561653
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.03489764085971,
            "upper_bound": 515.499745149022
          },
          "point_estimate": 219.8568225483514,
          "standard_error": 119.99465490351
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1186543.431322401,
            "upper_bound": 1186994.187574671
          },
          "point_estimate": 1186789.373607038,
          "standard_error": 119.38013645894726
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.673936569779,
            "upper_bound": 724.0455012058982
          },
          "point_estimate": 490.5273501726946,
          "standard_error": 183.07640984516928
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1644255.5848725843,
            "upper_bound": 1645897.2475827297
          },
          "point_estimate": 1644994.7561352656,
          "standard_error": 423.0525504176128
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1644001.8212560387,
            "upper_bound": 1645910.3278985508
          },
          "point_estimate": 1644602.9293478262,
          "standard_error": 388.62317459916926
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224.0516539933008,
            "upper_bound": 2054.2620533120507
          },
          "point_estimate": 757.6343708971588,
          "standard_error": 469.0093923729046
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1644262.2142248205,
            "upper_bound": 1645028.260676698
          },
          "point_estimate": 1644674.994805195,
          "standard_error": 196.85396066963935
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 448.19284695183535,
            "upper_bound": 1865.618834897574
          },
          "point_estimate": 1413.35669850414,
          "standard_error": 373.87517179792206
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1458213.8395457144,
            "upper_bound": 1459267.0750190474
          },
          "point_estimate": 1458734.2479206354,
          "standard_error": 269.1256005418569
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1458003.417777778,
            "upper_bound": 1459220.732
          },
          "point_estimate": 1458726.7346666667,
          "standard_error": 258.42500460185624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.12662301664831,
            "upper_bound": 1470.7179938895904
          },
          "point_estimate": 868.4727059148374,
          "standard_error": 420.66624198841333
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1458222.241253012,
            "upper_bound": 1459322.8301060072
          },
          "point_estimate": 1458796.362701299,
          "standard_error": 291.9863787146279
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 428.12445428896865,
            "upper_bound": 1196.4766149134375
          },
          "point_estimate": 898.6595352374129,
          "standard_error": 195.39467126380904
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398273.22617753624,
            "upper_bound": 399013.825202672
          },
          "point_estimate": 398607.84448110766,
          "standard_error": 190.94605207692064
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398123.4673913043,
            "upper_bound": 398846.5292443064
          },
          "point_estimate": 398580.7400362319,
          "standard_error": 205.52363255083057
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.0461569303305,
            "upper_bound": 941.9653272985992
          },
          "point_estimate": 540.7756545297409,
          "standard_error": 199.89111232814375
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398161.5642886824,
            "upper_bound": 398728.5968858132
          },
          "point_estimate": 398443.098136646,
          "standard_error": 147.92664756426828
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 275.99048368956215,
            "upper_bound": 895.6154975606745
          },
          "point_estimate": 638.8764712609945,
          "standard_error": 183.9240840599587
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 624312.4054973936,
            "upper_bound": 625941.0853719397
          },
          "point_estimate": 625082.8936272531,
          "standard_error": 418.8747801596445
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 623810.2360169492,
            "upper_bound": 626127.5211864407
          },
          "point_estimate": 624978.1196798494,
          "standard_error": 609.9889754983351
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.10177348272336,
            "upper_bound": 2328.616212048655
          },
          "point_estimate": 1635.6643408482082,
          "standard_error": 659.5052238473248
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 624120.2157637517,
            "upper_bound": 625237.601347045
          },
          "point_estimate": 624767.1644287915,
          "standard_error": 279.14667616936623
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 687.174881415541,
            "upper_bound": 1752.6984252922466
          },
          "point_estimate": 1399.9474261098856,
          "standard_error": 275.21240567769587
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392885.8644892474,
            "upper_bound": 393135.7184628776
          },
          "point_estimate": 392989.6562711214,
          "standard_error": 65.89407663301034
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392845.8820788531,
            "upper_bound": 393026.72795698926
          },
          "point_estimate": 392955.5539874552,
          "standard_error": 57.82888143962836
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.530831372646468,
            "upper_bound": 238.84571704997316
          },
          "point_estimate": 125.00223218936844,
          "standard_error": 54.428651760745474
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392868.16500254074,
            "upper_bound": 392997.3329864724
          },
          "point_estimate": 392938.0261136713,
          "standard_error": 33.38969364390867
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.44926922158784,
            "upper_bound": 328.0824873947778
          },
          "point_estimate": 220.231910439366,
          "standard_error": 85.19307510945843
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482429.5379010025,
            "upper_bound": 482846.23623590224
          },
          "point_estimate": 482634.26202380954,
          "standard_error": 107.01012137343908
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482311.8552631579,
            "upper_bound": 483000.7763157895
          },
          "point_estimate": 482612.25595238095,
          "standard_error": 202.2230744852049
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.98142955332746,
            "upper_bound": 579.2163239274149
          },
          "point_estimate": 463.7830036082907,
          "standard_error": 145.3482201050553
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 482351.8956965944,
            "upper_bound": 482853.8177930608
          },
          "point_estimate": 482597.40933014354,
          "standard_error": 134.43193891788468
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230.80680218666663,
            "upper_bound": 415.4014520835008
          },
          "point_estimate": 357.54415892760034,
          "standard_error": 47.15170468273933
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1014894.2220703814,
            "upper_bound": 1015717.3556163196
          },
          "point_estimate": 1015240.3966324956,
          "standard_error": 215.63209258946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1014779.9,
            "upper_bound": 1015504.0555555556
          },
          "point_estimate": 1015043.7831790124,
          "standard_error": 168.9538168516842
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.54606633483341,
            "upper_bound": 808.1094765560345
          },
          "point_estimate": 458.4577776478077,
          "standard_error": 198.33646461999592
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1014803.4930401522,
            "upper_bound": 1015301.7247723064
          },
          "point_estimate": 1015014.8562770564,
          "standard_error": 129.50272075303104
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.39223994834492,
            "upper_bound": 1052.3653415348315
          },
          "point_estimate": 718.7125992684875,
          "standard_error": 258.31959579238236
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 329375.71154466976,
            "upper_bound": 329821.0326447877
          },
          "point_estimate": 329556.04446911206,
          "standard_error": 119.5110512316548
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 329338.99245495495,
            "upper_bound": 329618.76833976834
          },
          "point_estimate": 329445.8201951952,
          "standard_error": 83.70769369717652
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.1874362767154,
            "upper_bound": 346.46619441204234
          },
          "point_estimate": 169.79030973338018,
          "standard_error": 80.19609562130665
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 329361.7978536305,
            "upper_bound": 329564.3213213213
          },
          "point_estimate": 329451.5560781561,
          "standard_error": 52.26973831352557
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.10405671748312,
            "upper_bound": 595.2872522407332
          },
          "point_estimate": 396.848296706031,
          "standard_error": 161.94835538941572
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 832694.937310606,
            "upper_bound": 833084.4513011364
          },
          "point_estimate": 832880.3637310605,
          "standard_error": 100.31322713547316
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 832590.3809659091,
            "upper_bound": 833083.4090909091
          },
          "point_estimate": 832854.6676136365,
          "standard_error": 120.52547331418964
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.60315759959462,
            "upper_bound": 569.6637682954746
          },
          "point_estimate": 292.0452384515148,
          "standard_error": 122.53209139290328
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 832649.9808584928,
            "upper_bound": 833009.0693506494
          },
          "point_estimate": 832816.8056080283,
          "standard_error": 90.53363632274672
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167.44486336633088,
            "upper_bound": 441.54661909503943
          },
          "point_estimate": 334.2707253764911,
          "standard_error": 73.93525337928705
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399758.1189764521,
            "upper_bound": 400180.385248561
          },
          "point_estimate": 399982.1727332984,
          "standard_error": 106.58746275328122
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399899.4148351649,
            "upper_bound": 400119.1773940345
          },
          "point_estimate": 399996.05128205125,
          "standard_error": 61.256023872981416
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.37493386303965,
            "upper_bound": 520.3339384545345
          },
          "point_estimate": 158.31188554105222,
          "standard_error": 105.06741496908889
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399901.23476026463,
            "upper_bound": 400069.9284364758
          },
          "point_estimate": 399980.3664906522,
          "standard_error": 43.14280379094559
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.09931176910914,
            "upper_bound": 511.9491546992512
          },
          "point_estimate": 355.05772726925096,
          "standard_error": 112.95044663188538
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174306.57874534823,
            "upper_bound": 174960.29360645937
          },
          "point_estimate": 174590.47301169593,
          "standard_error": 169.71512619578408
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174215.58133971292,
            "upper_bound": 174855.372169059
          },
          "point_estimate": 174445.85406698566,
          "standard_error": 143.54475353509503
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.86951000825366,
            "upper_bound": 702.369323415623
          },
          "point_estimate": 273.5635381257375,
          "standard_error": 167.84763486067243
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174260.63282619274,
            "upper_bound": 174542.6126623475
          },
          "point_estimate": 174411.36382278008,
          "standard_error": 72.1631907479203
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157.49090745286406,
            "upper_bound": 797.4988996941244
          },
          "point_estimate": 567.5947242933163,
          "standard_error": 180.79349512219017
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233013.67935236063,
            "upper_bound": 233155.75271768164
          },
          "point_estimate": 233076.13529583844,
          "standard_error": 36.7843122133423
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232988.9544159544,
            "upper_bound": 233120.3088942308
          },
          "point_estimate": 233052.3295940171,
          "standard_error": 42.23248869162256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.81859206236713,
            "upper_bound": 171.36204094408444
          },
          "point_estimate": 88.32927257713672,
          "standard_error": 38.155970679338914
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232988.08103615188,
            "upper_bound": 233071.27580331065
          },
          "point_estimate": 233019.65178155177,
          "standard_error": 21.587872115816285
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.03685227481838,
            "upper_bound": 175.8783900645982
          },
          "point_estimate": 122.7032672149465,
          "standard_error": 38.614997978589514
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175385.6726407967,
            "upper_bound": 175620.12843068907
          },
          "point_estimate": 175476.66340163306,
          "standard_error": 64.51357690969014
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175359.51976495725,
            "upper_bound": 175483.54807692306
          },
          "point_estimate": 175419.61658653844,
          "standard_error": 38.24793489263639
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.79944766474889,
            "upper_bound": 151.98711725363464
          },
          "point_estimate": 91.94218602795668,
          "standard_error": 34.82766033028137
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175378.1093445287,
            "upper_bound": 175454.20149219548
          },
          "point_estimate": 175415.36985514485,
          "standard_error": 19.79816941031745
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.29551336063465,
            "upper_bound": 329.1973297468459
          },
          "point_estimate": 215.71989662566875,
          "standard_error": 97.35017599264398
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 676158.3421523919,
            "upper_bound": 676591.3153148883
          },
          "point_estimate": 676351.4041453557,
          "standard_error": 111.69957436446865
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 676073.6666666666,
            "upper_bound": 676592.0540123456
          },
          "point_estimate": 676241.6430041152,
          "standard_error": 114.02609719546346
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.597980760009133,
            "upper_bound": 473.900922697728
          },
          "point_estimate": 226.11434209680849,
          "standard_error": 124.72518407863912
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 676122.982131254,
            "upper_bound": 676336.2585374179
          },
          "point_estimate": 676231.8567580568,
          "standard_error": 55.38298597700045
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.92961615172295,
            "upper_bound": 495.2820839064527
          },
          "point_estimate": 372.7501280160744,
          "standard_error": 103.80747166916797
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1402.446831161369,
            "upper_bound": 1403.4023992737962
          },
          "point_estimate": 1402.8078284212197,
          "standard_error": 0.2692090597414628
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1402.451005364517,
            "upper_bound": 1402.7634338054622
          },
          "point_estimate": 1402.5012124837583,
          "standard_error": 0.09369320991390742
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024675668196841204,
            "upper_bound": 0.4637722300692006
          },
          "point_estimate": 0.09105758653768574,
          "standard_error": 0.13399125828718492
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1402.4582120930677,
            "upper_bound": 1402.582770806886
          },
          "point_estimate": 1402.5077830355358,
          "standard_error": 0.03181318956250872
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07868462686361562,
            "upper_bound": 1.378021677383105
          },
          "point_estimate": 0.9006280270693808,
          "standard_error": 0.433623592104252
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1354790.604728395,
            "upper_bound": 1355415.531111111
          },
          "point_estimate": 1355103.112191358,
          "standard_error": 160.56246352111563
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1354667.4037037038,
            "upper_bound": 1355574.874074074
          },
          "point_estimate": 1355137.018518519,
          "standard_error": 222.49118082192643
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.10979510628324,
            "upper_bound": 995.6640359344436
          },
          "point_estimate": 521.3523912071823,
          "standard_error": 220.09206726855064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1354795.5308958192,
            "upper_bound": 1355326.563225658
          },
          "point_estimate": 1355109.477344877,
          "standard_error": 136.8606037596638
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 314.8670660754727,
            "upper_bound": 655.7794264538409
          },
          "point_estimate": 532.5613231487921,
          "standard_error": 87.63867986247902
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1524051.647295139,
            "upper_bound": 1524993.6873576392
          },
          "point_estimate": 1524505.3056944446,
          "standard_error": 242.10568956217145
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523991.036111111,
            "upper_bound": 1525104.5868055555
          },
          "point_estimate": 1524263.2701388889,
          "standard_error": 297.2360021174918
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.8420574118617,
            "upper_bound": 1323.6683452502468
          },
          "point_estimate": 523.6151865373247,
          "standard_error": 350.0855547893895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1524084.7978443743,
            "upper_bound": 1524951.822879921
          },
          "point_estimate": 1524453.9435064937,
          "standard_error": 229.3914058816394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 382.1240316385559,
            "upper_bound": 1020.8705287619964
          },
          "point_estimate": 805.5456084654679,
          "standard_error": 157.2977021623568
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415774.0115079363,
            "upper_bound": 1424833.9736248471
          },
          "point_estimate": 1420520.113655372,
          "standard_error": 2336.8433909117907
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1412689.9455128205,
            "upper_bound": 1426064.1568986569
          },
          "point_estimate": 1425612.1774038463,
          "standard_error": 3962.736453232536
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 196.66761061118,
            "upper_bound": 12839.30626821047
          },
          "point_estimate": 1858.7666442116397,
          "standard_error": 4185.675669475934
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1422635.2359580668,
            "upper_bound": 1426017.2576923077
          },
          "point_estimate": 1425060.245154845,
          "standard_error": 917.6311038487952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3596.4026671269726,
            "upper_bound": 9272.15613606682
          },
          "point_estimate": 7791.963896441005,
          "standard_error": 1423.2388856704454
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789272.4236899696,
            "upper_bound": 789667.5719842957
          },
          "point_estimate": 789477.7507632556,
          "standard_error": 100.95230744243682
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789274.4361702128,
            "upper_bound": 789813.3191489362
          },
          "point_estimate": 789443.3693009119,
          "standard_error": 149.0832409032705
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.85987140196893,
            "upper_bound": 643.3565084362372
          },
          "point_estimate": 305.3398873450717,
          "standard_error": 144.55446742083322
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789344.7770113032,
            "upper_bound": 789722.8045621648
          },
          "point_estimate": 789569.9993368334,
          "standard_error": 95.69320664716176
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 191.20971686116837,
            "upper_bound": 438.4710458156188
          },
          "point_estimate": 336.90459660800934,
          "standard_error": 68.01541553672477
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278698.92290230823,
            "upper_bound": 279017.1698364231
          },
          "point_estimate": 278850.56332242826,
          "standard_error": 81.37152335738904
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278583.3562340967,
            "upper_bound": 279080.70229007635
          },
          "point_estimate": 278782.25292620866,
          "standard_error": 114.97050905616597
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.223061190776303,
            "upper_bound": 436.7558441544234
          },
          "point_estimate": 314.23027388694146,
          "standard_error": 108.16916180466596
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278621.2323139282,
            "upper_bound": 278939.010301439
          },
          "point_estimate": 278762.4860116982,
          "standard_error": 82.0847835465362
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.5185258308347,
            "upper_bound": 346.2282954315065
          },
          "point_estimate": 271.7951112081927,
          "standard_error": 52.8319162286994
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492048.30622538616,
            "upper_bound": 492477.87524683616
          },
          "point_estimate": 492245.2666125054,
          "standard_error": 110.833605705326
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 491969.76833976834,
            "upper_bound": 492515.1111111111
          },
          "point_estimate": 492107.1439189189,
          "standard_error": 145.36713675154635
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.416247316816396,
            "upper_bound": 554.3755185362634
          },
          "point_estimate": 249.33357922209711,
          "standard_error": 161.3576233065376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492043.8538105709,
            "upper_bound": 492383.95345345343
          },
          "point_estimate": 492228.8149175149,
          "standard_error": 86.10765755069751
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.7012307519659,
            "upper_bound": 484.8421786997504
          },
          "point_estimate": 370.37016766731614,
          "standard_error": 90.43016297933583
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252931.40356812163,
            "upper_bound": 253063.6332792659
          },
          "point_estimate": 252991.50721009696,
          "standard_error": 33.91630304834735
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252905.75902777776,
            "upper_bound": 253049.36574074073
          },
          "point_estimate": 252963.57561728393,
          "standard_error": 38.142173704959006
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.706745025126885,
            "upper_bound": 168.58586774776933
          },
          "point_estimate": 102.12499821007503,
          "standard_error": 35.997519805649944
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252915.0214884611,
            "upper_bound": 252997.8206417625
          },
          "point_estimate": 252948.8352272727,
          "standard_error": 21.112518471234587
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.62611292107631,
            "upper_bound": 156.3397906966005
          },
          "point_estimate": 113.1097620125594,
          "standard_error": 31.071048060393377
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632451.466133005,
            "upper_bound": 633066.2487370006
          },
          "point_estimate": 632721.9970128626,
          "standard_error": 159.12210972937916
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632330.1280788177,
            "upper_bound": 632943.4906609196
          },
          "point_estimate": 632550.8120689655,
          "standard_error": 158.29268713206153
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.15276402319914,
            "upper_bound": 712.1702853448264
          },
          "point_estimate": 348.2333983003388,
          "standard_error": 165.49241692651236
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632469.6477699983,
            "upper_bound": 632794.7908148742
          },
          "point_estimate": 632630.3851321093,
          "standard_error": 82.86214776605013
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.68226917136693,
            "upper_bound": 744.7957670589203
          },
          "point_estimate": 530.2331844194129,
          "standard_error": 161.44501981050277
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.0186212507694,
            "upper_bound": 378.3425417052305
          },
          "point_estimate": 378.15036429397594,
          "standard_error": 0.08662052196730076
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.0033576501909,
            "upper_bound": 378.204173787248
          },
          "point_estimate": 378.023839359685,
          "standard_error": 0.05464975292186306
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004707853516157412,
            "upper_bound": 0.24815538707326715
          },
          "point_estimate": 0.06875309428240574,
          "standard_error": 0.06872245092440933
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.0221276627928,
            "upper_bound": 378.13975773454297
          },
          "point_estimate": 378.0688520348112,
          "standard_error": 0.02987150318553351
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04405704772693551,
            "upper_bound": 0.4293601432631239
          },
          "point_estimate": 0.2888695793457999,
          "standard_error": 0.11682062136391563
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.7261726631824,
            "upper_bound": 188.81679984223192
          },
          "point_estimate": 188.7684616400811,
          "standard_error": 0.02324083646143592
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.72392056658023,
            "upper_bound": 188.802890796978
          },
          "point_estimate": 188.75472487813943,
          "standard_error": 0.017032868566167703
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002705330460026024,
            "upper_bound": 0.11512513196665836
          },
          "point_estimate": 0.04395982332310222,
          "standard_error": 0.03021560933879241
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.69843983579295,
            "upper_bound": 188.8253264814633
          },
          "point_estimate": 188.74948528337052,
          "standard_error": 0.03228073273746498
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027602766222500127,
            "upper_bound": 0.1085770640224594
          },
          "point_estimate": 0.07746679230860665,
          "standard_error": 0.021294150883996145
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.505446918329675,
            "upper_bound": 26.52205923446906
          },
          "point_estimate": 26.513184491389573,
          "standard_error": 0.004266255678461841
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.504133504573165,
            "upper_bound": 26.52395753585464
          },
          "point_estimate": 26.508059037119672,
          "standard_error": 0.004876844508147023
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001543683584088733,
            "upper_bound": 0.02296481789474706
          },
          "point_estimate": 0.005903376095515071,
          "standard_error": 0.0060483329075838475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.50590437408995,
            "upper_bound": 26.520483313135266
          },
          "point_estimate": 26.51210985862866,
          "standard_error": 0.003956062290988244
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005840338585971713,
            "upper_bound": 0.01866603458460901
          },
          "point_estimate": 0.014201163389764716,
          "standard_error": 0.003333556765277639
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.686158226744297,
            "upper_bound": 17.694659679226135
          },
          "point_estimate": 17.690037251138072,
          "standard_error": 0.002184073135564104
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.685473721078196,
            "upper_bound": 17.69317830618985
          },
          "point_estimate": 17.689147583421807,
          "standard_error": 0.002008477334672972
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016444392488235244,
            "upper_bound": 0.010724032443281144
          },
          "point_estimate": 0.0047559971486695815,
          "standard_error": 0.0023179793592955833
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.68534714150977,
            "upper_bound": 17.692040266972512
          },
          "point_estimate": 17.68919462785282,
          "standard_error": 0.001662317602626809
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002849634457561341,
            "upper_bound": 0.010093542926701704
          },
          "point_estimate": 0.007253777763483347,
          "standard_error": 0.002024773183691567
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.259404347207457,
            "upper_bound": 26.275496408324088
          },
          "point_estimate": 26.266993926640595,
          "standard_error": 0.0041433334032500175
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.254574791411468,
            "upper_bound": 26.27730220503405
          },
          "point_estimate": 26.263328094544725,
          "standard_error": 0.006217483376475828
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024126062557027297,
            "upper_bound": 0.02492676959871775
          },
          "point_estimate": 0.014655602532136635,
          "standard_error": 0.005379367509212407
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.258732889578805,
            "upper_bound": 26.269608050254323
          },
          "point_estimate": 26.26337524339206,
          "standard_error": 0.002783836738582207
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007238661618894982,
            "upper_bound": 0.017743860971978456
          },
          "point_estimate": 0.013825553836458594,
          "standard_error": 0.0028173714901252
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299125.8495225572,
            "upper_bound": 299404.0077608477
          },
          "point_estimate": 299256.1568741868,
          "standard_error": 71.40752420592902
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299090.39446721313,
            "upper_bound": 299468.4049180328
          },
          "point_estimate": 299156.8857923497,
          "standard_error": 100.40841625413726
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.28470959207896,
            "upper_bound": 372.5636874020522
          },
          "point_estimate": 122.8839755505736,
          "standard_error": 96.0302272537132
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299129.16197501915,
            "upper_bound": 299298.81652057753
          },
          "point_estimate": 299191.91068767296,
          "standard_error": 43.83053896342409
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.07925001745036,
            "upper_bound": 290.8473841922329
          },
          "point_estimate": 238.55031859199883,
          "standard_error": 46.266333211512666
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.744548575708876,
            "upper_bound": 15.763612243087154
          },
          "point_estimate": 15.754065379583976,
          "standard_error": 0.004887148784667587
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.73673354366652,
            "upper_bound": 15.769684664982384
          },
          "point_estimate": 15.75558034467576,
          "standard_error": 0.009747816425474272
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012915812690637116,
            "upper_bound": 0.02671504395028294
          },
          "point_estimate": 0.025320792951736948,
          "standard_error": 0.007487365815747862
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.738892821421464,
            "upper_bound": 15.75961577441443
          },
          "point_estimate": 15.746672835415172,
          "standard_error": 0.005279373446348297
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011005398998576886,
            "upper_bound": 0.01858226441122417
          },
          "point_estimate": 0.016251191734211842,
          "standard_error": 0.001985138012888929
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.34778846118935,
            "upper_bound": 23.367351683417024
          },
          "point_estimate": 23.358185481417372,
          "standard_error": 0.005034168117356172
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.34041633907856,
            "upper_bound": 23.36996287156514
          },
          "point_estimate": 23.36526704982441,
          "standard_error": 0.007114277897479873
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00199557068670015,
            "upper_bound": 0.024953831808338545
          },
          "point_estimate": 0.007630720258857017,
          "standard_error": 0.006327325112548952
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.354135150281408,
            "upper_bound": 23.368888229830677
          },
          "point_estimate": 23.36358868667346,
          "standard_error": 0.00377146638015393
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005385149593505164,
            "upper_bound": 0.020374527251639175
          },
          "point_estimate": 0.01680416376703496,
          "standard_error": 0.0032839624561203156
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132641.01004562044,
            "upper_bound": 132755.55272723324
          },
          "point_estimate": 132691.11010355115,
          "standard_error": 29.653943544733668
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132621.7688564477,
            "upper_bound": 132741.74713242962
          },
          "point_estimate": 132658.8083941606,
          "standard_error": 31.323259135175046
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.20071891857515,
            "upper_bound": 135.5431189135845
          },
          "point_estimate": 68.5326727492268,
          "standard_error": 29.732889105790328
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132643.40659910615,
            "upper_bound": 132726.23172456084
          },
          "point_estimate": 132692.35138875723,
          "standard_error": 21.10645751781243
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.74282266935633,
            "upper_bound": 140.24650842338255
          },
          "point_estimate": 98.67284355932595,
          "standard_error": 30.80510077035394
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.05000633884645,
            "upper_bound": 24.063956533299436
          },
          "point_estimate": 24.05659984746939,
          "standard_error": 0.00358732570566994
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.046767492552124,
            "upper_bound": 24.0643725561142
          },
          "point_estimate": 24.054644778895508,
          "standard_error": 0.0049785830625516826
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011431156425107305,
            "upper_bound": 0.019919629585934556
          },
          "point_estimate": 0.011776610929325262,
          "standard_error": 0.004469763750576941
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.04783018436396,
            "upper_bound": 24.05824187771003
          },
          "point_estimate": 24.05183216295887,
          "standard_error": 0.002678298587695015
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006193229606426796,
            "upper_bound": 0.015665127129417807
          },
          "point_estimate": 0.011988256147611738,
          "standard_error": 0.002562467642395015
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.88257316872471,
            "upper_bound": 20.892225935632183
          },
          "point_estimate": 20.8870673950455,
          "standard_error": 0.002496740153044919
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.881740219662294,
            "upper_bound": 20.893341291529445
          },
          "point_estimate": 20.88405561102749,
          "standard_error": 0.002785852503646662
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010806646807203126,
            "upper_bound": 0.013613013145749782
          },
          "point_estimate": 0.005137493360419598,
          "standard_error": 0.003224524486956183
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.8795219138843,
            "upper_bound": 20.887837836564916
          },
          "point_estimate": 20.882987920411168,
          "standard_error": 0.0021524137791836023
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0031485679545284755,
            "upper_bound": 0.010723764444972122
          },
          "point_estimate": 0.008284840073536144,
          "standard_error": 0.001929823041890529
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.86545059083296,
            "upper_bound": 49.05202620550555
          },
          "point_estimate": 48.968824909473135,
          "standard_error": 0.0481689256984854
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.874220582835434,
            "upper_bound": 49.07704422565963
          },
          "point_estimate": 49.04638919588034,
          "standard_error": 0.058586106389158434
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01347247369353658,
            "upper_bound": 0.2369785932264808
          },
          "point_estimate": 0.07794653546255749,
          "standard_error": 0.06332622768444301
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.793248466072214,
            "upper_bound": 48.993118013179576
          },
          "point_estimate": 48.88790401138582,
          "standard_error": 0.04994378556774599
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05907400238154002,
            "upper_bound": 0.21807074773140775
          },
          "point_estimate": 0.159733961442236,
          "standard_error": 0.04406813968591063
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.79688241338344,
            "upper_bound": 90.87489662381772
          },
          "point_estimate": 90.83073874397788,
          "standard_error": 0.020233000747101455
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.7898106940386,
            "upper_bound": 90.862883044879
          },
          "point_estimate": 90.80498031184705,
          "standard_error": 0.017227146646839196
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0074740856793607474,
            "upper_bound": 0.08549031975294488
          },
          "point_estimate": 0.03450883717861071,
          "standard_error": 0.02048959242792668
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.78428822191948,
            "upper_bound": 90.8333858716192
          },
          "point_estimate": 90.80789388928297,
          "standard_error": 0.012921586743263866
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01789519941268432,
            "upper_bound": 0.0952124623905477
          },
          "point_estimate": 0.0674943956895476,
          "standard_error": 0.021968624028217143
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.81543194832318,
            "upper_bound": 10.836096658768987
          },
          "point_estimate": 10.82519924064807,
          "standard_error": 0.00531865635373001
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.81246515093001,
            "upper_bound": 10.840101975096472
          },
          "point_estimate": 10.818832709212115,
          "standard_error": 0.006870141539510769
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002410927718869991,
            "upper_bound": 0.02774588135947427
          },
          "point_estimate": 0.012321997059120468,
          "standard_error": 0.007038346557229199
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.813041805712846,
            "upper_bound": 10.831701700040576
          },
          "point_estimate": 10.820062482625762,
          "standard_error": 0.004798903919626367
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007506205916807891,
            "upper_bound": 0.022020228033979333
          },
          "point_estimate": 0.01775006911246696,
          "standard_error": 0.003471539115219986
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33635.59378101736,
            "upper_bound": 33668.72007271155
          },
          "point_estimate": 33650.461406913055,
          "standard_error": 8.582247032688272
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33631.43835073345,
            "upper_bound": 33671.49699352452
          },
          "point_estimate": 33640.82336314113,
          "standard_error": 8.592018944254109
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7391741496852708,
            "upper_bound": 45.15847282455216
          },
          "point_estimate": 14.45726005785489,
          "standard_error": 12.082759871375607
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33634.528354064445,
            "upper_bound": 33650.04107332561
          },
          "point_estimate": 33640.97389141848,
          "standard_error": 3.876543206837488
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.181365977236585,
            "upper_bound": 39.21600962905367
          },
          "point_estimate": 28.57577960020382,
          "standard_error": 7.860102287533105
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.79487812117768,
            "upper_bound": 24.818635093599113
          },
          "point_estimate": 24.804626025824756,
          "standard_error": 0.006257290137591588
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.79247012116909,
            "upper_bound": 24.80684615393877
          },
          "point_estimate": 24.801049769159977,
          "standard_error": 0.004390609761979081
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002020591564637339,
            "upper_bound": 0.01911681348053013
          },
          "point_estimate": 0.010900582049316944,
          "standard_error": 0.004232283341740197
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.7970982583529,
            "upper_bound": 24.8039405687402
          },
          "point_estimate": 24.800893803826984,
          "standard_error": 0.0017215705817312818
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005509621675349884,
            "upper_bound": 0.031205729245182556
          },
          "point_estimate": 0.02090053367871402,
          "standard_error": 0.008314294505388588
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1105035.586271044,
            "upper_bound": 1105632.6613552186
          },
          "point_estimate": 1105312.470446128,
          "standard_error": 153.2932705317785
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104893.121212121,
            "upper_bound": 1105643.9494949495
          },
          "point_estimate": 1105259.112121212,
          "standard_error": 182.82017222731136
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147.06493193440036,
            "upper_bound": 847.7192158590957
          },
          "point_estimate": 556.5889961792947,
          "standard_error": 174.62083005925598
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104938.511382114,
            "upper_bound": 1105384.94637224
          },
          "point_estimate": 1105157.7569460843,
          "standard_error": 115.4321010492625
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 242.86599115939885,
            "upper_bound": 691.1329831308922
          },
          "point_estimate": 509.998277563304,
          "standard_error": 125.48978363400792
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2206.604136051066,
            "upper_bound": 2207.6567360129225
          },
          "point_estimate": 2207.064910461888,
          "standard_error": 0.2704188453781772
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2206.4808926728138,
            "upper_bound": 2207.2570384763294
          },
          "point_estimate": 2207.0251472463415,
          "standard_error": 0.2312642237487883
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06559875468028967,
            "upper_bound": 1.217506673792177
          },
          "point_estimate": 0.3672361668968752,
          "standard_error": 0.2833363385368753
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2206.8345348214766,
            "upper_bound": 2207.1616992888735
          },
          "point_estimate": 2207.0366850588234,
          "standard_error": 0.08390500838013171
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.30983586173506883,
            "upper_bound": 1.3027365101879236
          },
          "point_estimate": 0.9005177725239613,
          "standard_error": 0.3004392575017381
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.368789063644709,
            "upper_bound": 8.391832833872032
          },
          "point_estimate": 8.379509632441776,
          "standard_error": 0.005900461146061319
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.364322814019669,
            "upper_bound": 8.394468444622785
          },
          "point_estimate": 8.373074767969303,
          "standard_error": 0.007029685863699428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0021751040300742545,
            "upper_bound": 0.03357840049730899
          },
          "point_estimate": 0.01979151735268472,
          "standard_error": 0.007920626989416774
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.362684021841513,
            "upper_bound": 8.383404334584498
          },
          "point_estimate": 8.373938727874982,
          "standard_error": 0.005399950487647289
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008977507141620386,
            "upper_bound": 0.025492597046420738
          },
          "point_estimate": 0.01962903873202312,
          "standard_error": 0.004335001157068927
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.439349932601899,
            "upper_bound": 7.475718339375132
          },
          "point_estimate": 7.455708105755882,
          "standard_error": 0.009368227786500464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.435294110824353,
            "upper_bound": 7.472591596217597
          },
          "point_estimate": 7.441361096425236,
          "standard_error": 0.010888105704137703
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001933138297205035,
            "upper_bound": 0.04794183815196582
          },
          "point_estimate": 0.01997681271431653,
          "standard_error": 0.012007080031958052
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.44072109693439,
            "upper_bound": 7.464919046004099
          },
          "point_estimate": 7.451199922559274,
          "standard_error": 0.006130750871779629
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01216205149286314,
            "upper_bound": 0.04258608949743794
          },
          "point_estimate": 0.031179935413393572,
          "standard_error": 0.008424263355313277
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.910853181086594,
            "upper_bound": 8.961036797339585
          },
          "point_estimate": 8.937443780900349,
          "standard_error": 0.012873317867312448
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.905152450727,
            "upper_bound": 8.970329348018197
          },
          "point_estimate": 8.946550522344868,
          "standard_error": 0.015690069510745093
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007894556754705283,
            "upper_bound": 0.07067991495330293
          },
          "point_estimate": 0.036556948546611776,
          "standard_error": 0.015741377317702435
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.929741086635188,
            "upper_bound": 8.973974532552836
          },
          "point_estimate": 8.954816680598531,
          "standard_error": 0.011201502148204995
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02035836127234345,
            "upper_bound": 0.05723691568252132
          },
          "point_estimate": 0.04288685931679911,
          "standard_error": 0.01009057333627863
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.388528999553284,
            "upper_bound": 19.402774612417435
          },
          "point_estimate": 19.39492460968471,
          "standard_error": 0.003675349648738444
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.38857971588024,
            "upper_bound": 19.40231457975108
          },
          "point_estimate": 19.39084570855005,
          "standard_error": 0.0030591902195890213
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00041509199874995793,
            "upper_bound": 0.0181668632364866
          },
          "point_estimate": 0.003532405064477941,
          "standard_error": 0.004235988223975378
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.38849416223777,
            "upper_bound": 19.39347946979463
          },
          "point_estimate": 19.390915794342916,
          "standard_error": 0.0012840646402362595
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028178102085076907,
            "upper_bound": 0.01606866612592463
          },
          "point_estimate": 0.012288981199736036,
          "standard_error": 0.0033965296675165743
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.263454898434652,
            "upper_bound": 13.276414314633495
          },
          "point_estimate": 13.2685666510453,
          "standard_error": 0.0035239608047590624
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.262117782919203,
            "upper_bound": 13.268557787379322
          },
          "point_estimate": 13.266626034687466,
          "standard_error": 0.0019666165016608245
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007387060942196774,
            "upper_bound": 0.008630611538272197
          },
          "point_estimate": 0.0047473416234537396,
          "standard_error": 0.0020644225206085196
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.262507247408642,
            "upper_bound": 13.267431219860583
          },
          "point_estimate": 13.264923344643565,
          "standard_error": 0.001252338836514713
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002541209017280576,
            "upper_bound": 0.017884396644847932
          },
          "point_estimate": 0.011756368728400673,
          "standard_error": 0.005166812341177086
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.14515034563011,
            "upper_bound": 19.154220407891664
          },
          "point_estimate": 19.148913265577548,
          "standard_error": 0.0023924542390397893
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.144499569051867,
            "upper_bound": 19.1506682988092
          },
          "point_estimate": 19.14670825818422,
          "standard_error": 0.0017665751971132806
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008139430055854194,
            "upper_bound": 0.007791524085452879
          },
          "point_estimate": 0.0035752571095845434,
          "standard_error": 0.0018126725833199895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.14467817147618,
            "upper_bound": 19.15008203307975
          },
          "point_estimate": 19.14739610535657,
          "standard_error": 0.0014398065642293298
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0021930421632150947,
            "upper_bound": 0.011816658226401893
          },
          "point_estimate": 0.007974910480893676,
          "standard_error": 0.0030451496496862775
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.853592258522902,
            "upper_bound": 10.92444344836876
          },
          "point_estimate": 10.887290216483157,
          "standard_error": 0.018117090070112724
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.84532843499643,
            "upper_bound": 10.92009550083527
          },
          "point_estimate": 10.881802579525186,
          "standard_error": 0.015187526037448074
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008347616287108256,
            "upper_bound": 0.10276225962271562
          },
          "point_estimate": 0.027506362806414945,
          "standard_error": 0.02598321811829548
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.87084377851969,
            "upper_bound": 10.91773465426302
          },
          "point_estimate": 10.88997499289564,
          "standard_error": 0.011910912577336976
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02579568409367711,
            "upper_bound": 0.08175063970667516
          },
          "point_estimate": 0.06017273720368606,
          "standard_error": 0.01468123336663668
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.751176980781157,
            "upper_bound": 15.80834748539232
          },
          "point_estimate": 15.775526856037873,
          "standard_error": 0.0149505766713982
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.740174585118838,
            "upper_bound": 15.78899828890616
          },
          "point_estimate": 15.766148571740755,
          "standard_error": 0.015812760565188017
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023397508474158374,
            "upper_bound": 0.06760966941291707
          },
          "point_estimate": 0.03648923694482622,
          "standard_error": 0.015091728883860242
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.744549669384988,
            "upper_bound": 15.778462762773769
          },
          "point_estimate": 15.759529734659008,
          "standard_error": 0.008797360184187324
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01760241911804154,
            "upper_bound": 0.07299821845363558
          },
          "point_estimate": 0.04974727285898676,
          "standard_error": 0.01759356690447511
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.334404820562177,
            "upper_bound": 23.35784253070788
          },
          "point_estimate": 23.34617130255971,
          "standard_error": 0.006018917212598493
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.32759455673629,
            "upper_bound": 23.361518404599256
          },
          "point_estimate": 23.35086961940259,
          "standard_error": 0.009902104633413029
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0054676337377224625,
            "upper_bound": 0.03291152159976622
          },
          "point_estimate": 0.026182797811983485,
          "standard_error": 0.007551438381239269
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.330152504298976,
            "upper_bound": 23.35890737758248
          },
          "point_estimate": 23.346907169576887,
          "standard_error": 0.007302276159749043
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012496100436243303,
            "upper_bound": 0.024382253575618896
          },
          "point_estimate": 0.02009997708652657,
          "standard_error": 0.003044397372015033
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.95009754494886,
            "upper_bound": 11.003333362446376
          },
          "point_estimate": 10.972629736196684,
          "standard_error": 0.013923470652204516
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.945369659431776,
            "upper_bound": 10.981982435121523
          },
          "point_estimate": 10.963585117481188,
          "standard_error": 0.008286547307817495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00572135773886144,
            "upper_bound": 0.050452746093091055
          },
          "point_estimate": 0.016969161610023636,
          "standard_error": 0.012794070162397291
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.95700716692332,
            "upper_bound": 11.032884775950947
          },
          "point_estimate": 10.988898868409125,
          "standard_error": 0.02023369557840763
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01303002864750283,
            "upper_bound": 0.06842559936990833
          },
          "point_estimate": 0.046515940278832345,
          "standard_error": 0.017018595506336248
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.05032343635355,
            "upper_bound": 24.075131896678077
          },
          "point_estimate": 24.06021721842107,
          "standard_error": 0.006688314754556301
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.04656817834531,
            "upper_bound": 24.063132742703285
          },
          "point_estimate": 24.054979857525172,
          "standard_error": 0.004142872577981319
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000379055161591478,
            "upper_bound": 0.0167224295462288
          },
          "point_estimate": 0.010220950511099294,
          "standard_error": 0.004948764945119932
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.04907296093134,
            "upper_bound": 24.059193640825953
          },
          "point_estimate": 24.05337131858741,
          "standard_error": 0.0025714996984917585
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004619615903016791,
            "upper_bound": 0.03367631104474733
          },
          "point_estimate": 0.02233595936338676,
          "standard_error": 0.009435614810844426
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.842636615302787,
            "upper_bound": 20.87061293870992
          },
          "point_estimate": 20.859726458219832,
          "standard_error": 0.007675494678601676
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.861423595642368,
            "upper_bound": 20.869191100740355
          },
          "point_estimate": 20.865525772008223,
          "standard_error": 0.002676231888694392
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012479382210147214,
            "upper_bound": 0.016038277918959384
          },
          "point_estimate": 0.005487232731898108,
          "standard_error": 0.003825388182222803
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.86427132987463,
            "upper_bound": 20.873669672469997
          },
          "point_estimate": 20.868371784851945,
          "standard_error": 0.0024093939789336894
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002774226909579004,
            "upper_bound": 0.038993430536122496
          },
          "point_estimate": 0.0254777649445502,
          "standard_error": 0.011845021591834166
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1186678.6463696875,
            "upper_bound": 1187460.1423627753
          },
          "point_estimate": 1187043.762686892,
          "standard_error": 200.593703040122
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1186593.9623655914,
            "upper_bound": 1187503.0129032258
          },
          "point_estimate": 1186813.3614631337,
          "standard_error": 242.83219341992145
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.43762462044849,
            "upper_bound": 1061.3789734149034
          },
          "point_estimate": 391.6354160309981,
          "standard_error": 266.8609033890667
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1186738.9574226842,
            "upper_bound": 1187131.1017381847
          },
          "point_estimate": 1186881.8688730623,
          "standard_error": 100.46743003924324
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.1215671050965,
            "upper_bound": 823.6583762411423
          },
          "point_estimate": 669.5450345136452,
          "standard_error": 140.27387585090133
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1644532.5978329882,
            "upper_bound": 1645745.1496985853
          },
          "point_estimate": 1645089.9935541751,
          "standard_error": 312.05739063105017
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1644269.9142512078,
            "upper_bound": 1645766.822463768
          },
          "point_estimate": 1644682.20931677,
          "standard_error": 476.6135618908012
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118.6456000675426,
            "upper_bound": 1912.1607547480064
          },
          "point_estimate": 817.3178192577664,
          "standard_error": 438.1915216216123
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1644279.4114805516,
            "upper_bound": 1645405.7259913394
          },
          "point_estimate": 1644723.160022586,
          "standard_error": 289.92008859380076
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 493.0858421408765,
            "upper_bound": 1389.3749374506992
          },
          "point_estimate": 1040.4221320830106,
          "standard_error": 247.30267451536645
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1456169.8101655554,
            "upper_bound": 1458477.5757111113
          },
          "point_estimate": 1457480.578349206,
          "standard_error": 598.9483709162675
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1456812.3111111112,
            "upper_bound": 1458781.35
          },
          "point_estimate": 1457977.8895238095,
          "standard_error": 471.14595495831344
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206.46934333451009,
            "upper_bound": 2368.1791769565048
          },
          "point_estimate": 1300.271476248795,
          "standard_error": 629.6399696678218
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1457375.813051643,
            "upper_bound": 1458499.8787975514
          },
          "point_estimate": 1457931.6794805194,
          "standard_error": 290.9721399860893
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 640.4645024258579,
            "upper_bound": 2855.4756755669114
          },
          "point_estimate": 1989.4776369329436,
          "standard_error": 656.9677236717563
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397740.4848416796,
            "upper_bound": 398801.0716301113
          },
          "point_estimate": 398263.74489561765,
          "standard_error": 272.0537100702564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397405.0996376811,
            "upper_bound": 399064.5407608696
          },
          "point_estimate": 398349.2880434783,
          "standard_error": 394.6773492084827
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.7903470436097,
            "upper_bound": 1601.6741617820012
          },
          "point_estimate": 1143.369729520003,
          "standard_error": 402.63800865305825
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397382.81272032904,
            "upper_bound": 398404.26463539136
          },
          "point_estimate": 397886.89028797287,
          "standard_error": 266.42367748812654
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 528.8568413294738,
            "upper_bound": 1121.111039094055
          },
          "point_estimate": 907.283054674954,
          "standard_error": 152.3734494076855
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 624897.7265361177,
            "upper_bound": 625855.6964591404
          },
          "point_estimate": 625287.2963411353,
          "standard_error": 257.1771339378697
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 624807.9482109228,
            "upper_bound": 625468.5610169491
          },
          "point_estimate": 624982.9194915255,
          "standard_error": 157.39301015818475
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.65588754147461,
            "upper_bound": 720.0121128103749
          },
          "point_estimate": 287.13996721873525,
          "standard_error": 178.52084510968373
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 624800.4860251333,
            "upper_bound": 625148.8857086274
          },
          "point_estimate": 624924.481972265,
          "standard_error": 89.95737845429149
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148.03033719386815,
            "upper_bound": 1280.7251480589357
          },
          "point_estimate": 859.7362594641615,
          "standard_error": 349.64609719331395
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392689.9486210957,
            "upper_bound": 392989.1422819594
          },
          "point_estimate": 392826.32044632186,
          "standard_error": 76.88301609140989
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392622.164874552,
            "upper_bound": 392965.37001194747
          },
          "point_estimate": 392761.63410138246,
          "standard_error": 93.55466101962728
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.196837706005702,
            "upper_bound": 386.0416661571427
          },
          "point_estimate": 211.88911924891912,
          "standard_error": 102.6376959206574
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392698.83691006433,
            "upper_bound": 392979.540517811
          },
          "point_estimate": 392836.68968021224,
          "standard_error": 73.19894189863466
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.93059107098672,
            "upper_bound": 339.3793069917045
          },
          "point_estimate": 255.89972864557112,
          "standard_error": 63.62723776267523
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 480877.7511736372,
            "upper_bound": 482067.6520906432
          },
          "point_estimate": 481482.04082811193,
          "standard_error": 300.08953285189443
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481160.0802631579,
            "upper_bound": 481809.79495614034
          },
          "point_estimate": 481504.91353383457,
          "standard_error": 158.98463821102138
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.10922411240576,
            "upper_bound": 1620.5110330722855
          },
          "point_estimate": 324.6590486221474,
          "standard_error": 331.8042409399748
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481085.1725357469,
            "upper_bound": 481615.91079850285
          },
          "point_estimate": 481417.2186602871,
          "standard_error": 135.16828123457793
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232.77934574269443,
            "upper_bound": 1414.2839404045635
          },
          "point_estimate": 1001.9149021148226,
          "standard_error": 308.7715776968989
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1014776.7855555556,
            "upper_bound": 1016491.979126984
          },
          "point_estimate": 1015400.9091269842,
          "standard_error": 500.492015844549
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1014745.5805555556,
            "upper_bound": 1015197.7567460318
          },
          "point_estimate": 1014864.398148148,
          "standard_error": 154.30230310860514
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.688446677434936,
            "upper_bound": 565.7876055108151
          },
          "point_estimate": 149.11523790837217,
          "standard_error": 190.17566898689947
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1014753.1267180432,
            "upper_bound": 1015146.4745467224
          },
          "point_estimate": 1014903.2575757576,
          "standard_error": 101.57391827240151
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.36944830534468,
            "upper_bound": 2553.8015203670648
          },
          "point_estimate": 1662.5815793132376,
          "standard_error": 852.1922116538777
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 330172.6196580087,
            "upper_bound": 330717.9125540765
          },
          "point_estimate": 330417.9103069986,
          "standard_error": 140.65312650974354
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 330083.77045454545,
            "upper_bound": 330618.7138961039
          },
          "point_estimate": 330332.3928787878,
          "standard_error": 145.45795804575954
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.80808681045072,
            "upper_bound": 679.7129336903486
          },
          "point_estimate": 371.0010813300818,
          "standard_error": 148.9617530382099
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 330197.0502901354,
            "upper_bound": 330535.07428276085
          },
          "point_estimate": 330349.7899409681,
          "standard_error": 86.6542680503814
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.36489164196055,
            "upper_bound": 648.9752967422174
          },
          "point_estimate": 468.42438969527075,
          "standard_error": 131.0514040661341
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 832309.9943025117,
            "upper_bound": 832770.4970576073
          },
          "point_estimate": 832538.7496311327,
          "standard_error": 118.26268698849904
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 832137.7017045454,
            "upper_bound": 832845.3515151516
          },
          "point_estimate": 832623.8640151515,
          "standard_error": 188.74643095979405
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.714066290865865,
            "upper_bound": 691.7758527186321
          },
          "point_estimate": 493.12622942704775,
          "standard_error": 167.87046517867643
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 832154.7529289058,
            "upper_bound": 832658.3475342387
          },
          "point_estimate": 832384.209622196,
          "standard_error": 129.16941034622317
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243.0705723737164,
            "upper_bound": 480.94238462925847
          },
          "point_estimate": 394.39864694031695,
          "standard_error": 61.48211904146857
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400055.0792183411,
            "upper_bound": 400644.5884249084
          },
          "point_estimate": 400334.3916954474,
          "standard_error": 150.36134324617296
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400084.42673992674,
            "upper_bound": 400495.87545787543
          },
          "point_estimate": 400336.069466248,
          "standard_error": 109.14567291913188
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.88635859705784,
            "upper_bound": 739.0082022645873
          },
          "point_estimate": 307.59856981375117,
          "standard_error": 163.80857616794043
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400117.7409995862,
            "upper_bound": 400365.4124869544
          },
          "point_estimate": 400260.72693021264,
          "standard_error": 63.64274495531601
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.43430114869133,
            "upper_bound": 722.5692833710988
          },
          "point_estimate": 501.2575587496564,
          "standard_error": 148.5258056595505
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174350.6357893296,
            "upper_bound": 174808.08939369657
          },
          "point_estimate": 174581.91370917278,
          "standard_error": 116.8788539077922
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174355.56370192306,
            "upper_bound": 174911.78165064103
          },
          "point_estimate": 174519.12587759463,
          "standard_error": 131.46390638736784
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.496223810924924,
            "upper_bound": 668.8762849039181
          },
          "point_estimate": 368.3512506719915,
          "standard_error": 161.4773196144007
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174492.2738547969,
            "upper_bound": 174817.06440002928
          },
          "point_estimate": 174626.6983141858,
          "standard_error": 82.5767385692733
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199.2443566936172,
            "upper_bound": 510.1615532711385
          },
          "point_estimate": 388.54648321376993,
          "standard_error": 80.02812631698151
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232963.7759396231,
            "upper_bound": 233090.43257756424
          },
          "point_estimate": 233021.3467012941,
          "standard_error": 32.560005785061634
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232935.24267515924,
            "upper_bound": 233088.9337579618
          },
          "point_estimate": 233004.14397558384,
          "standard_error": 42.28732049252411
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.9587765643611,
            "upper_bound": 178.33745551753964
          },
          "point_estimate": 102.54386192147427,
          "standard_error": 39.6012159039157
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232945.1953975196,
            "upper_bound": 233023.26322407747
          },
          "point_estimate": 232980.30043841508,
          "standard_error": 20.298890939155022
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.736322926034745,
            "upper_bound": 148.05450161581004
          },
          "point_estimate": 108.04119859917644,
          "standard_error": 28.203404099444754
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175347.5156856494,
            "upper_bound": 175550.98229076518
          },
          "point_estimate": 175439.54867540445,
          "standard_error": 52.34173595345702
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175315.7295673077,
            "upper_bound": 175521.33713942306
          },
          "point_estimate": 175415.66710164837,
          "standard_error": 60.64104808657399
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.22363129889502,
            "upper_bound": 264.119432089778
          },
          "point_estimate": 152.41689050318726,
          "standard_error": 55.99664526411693
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175392.5364188362,
            "upper_bound": 175486.3798105651
          },
          "point_estimate": 175450.8267107892,
          "standard_error": 23.39745856884856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.61669590662088,
            "upper_bound": 243.77532053689373
          },
          "point_estimate": 174.33979086169785,
          "standard_error": 49.326007654810496
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 675920.5491982473,
            "upper_bound": 676753.2555289903
          },
          "point_estimate": 676258.2539116696,
          "standard_error": 224.38218839311145
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 675859.237962963,
            "upper_bound": 676426.4349794239
          },
          "point_estimate": 676020.830026455,
          "standard_error": 125.9931541385317
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.045705098696622,
            "upper_bound": 594.7979294402111
          },
          "point_estimate": 198.22603518446667,
          "standard_error": 151.34514081220755
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 675934.0969366848,
            "upper_bound": 676217.8112419632
          },
          "point_estimate": 676039.9361231361,
          "standard_error": 72.66813904411987
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.86191954424298,
            "upper_bound": 1105.2264557124263
          },
          "point_estimate": 746.0347404985434,
          "standard_error": 304.38304058865134
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1365.7803379559746,
            "upper_bound": 1368.8180898748194
          },
          "point_estimate": 1366.987086074434,
          "standard_error": 0.8271602798477756
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1365.6076897044445,
            "upper_bound": 1367.3161364718858
          },
          "point_estimate": 1365.6665554260358,
          "standard_error": 0.5882888413133409
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016977245818893833,
            "upper_bound": 2.3140929145956632
          },
          "point_estimate": 0.2360878267100444,
          "standard_error": 0.6649680856618699
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1365.6258539382202,
            "upper_bound": 1366.1400232124136
          },
          "point_estimate": 1365.774357950089,
          "standard_error": 0.13720518394175038
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5445542200143465,
            "upper_bound": 4.201961848124855
          },
          "point_estimate": 2.7629475426871646,
          "standard_error": 1.2084773670535176
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1356570.0501234569,
            "upper_bound": 1358166.9339418726
          },
          "point_estimate": 1357274.130946502,
          "standard_error": 413.53483760165034
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1356306.5555555555,
            "upper_bound": 1357867.411522634
          },
          "point_estimate": 1356768.2120370371,
          "standard_error": 394.73440893834464
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154.99576021102507,
            "upper_bound": 1877.777539748904
          },
          "point_estimate": 761.9497673987166,
          "standard_error": 449.6245796453286
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1356301.6830734245,
            "upper_bound": 1356967.75761152
          },
          "point_estimate": 1356578.818085618,
          "standard_error": 174.22365569548606
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 371.9831002726756,
            "upper_bound": 1912.0628855310467
          },
          "point_estimate": 1380.04944199709,
          "standard_error": 413.2184896356103
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1524068.4764218754,
            "upper_bound": 1525178.669074074
          },
          "point_estimate": 1524578.5091782408,
          "standard_error": 285.8007516654137
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1524047.9609375,
            "upper_bound": 1525287.380787037
          },
          "point_estimate": 1524267.433333333,
          "standard_error": 289.5108537463176
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.30681218542111,
            "upper_bound": 1431.8672558292749
          },
          "point_estimate": 351.1962740775428,
          "standard_error": 363.8373174537039
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1524034.202591284,
            "upper_bound": 1524632.0549517393
          },
          "point_estimate": 1524348.165909091,
          "standard_error": 158.6269417014527
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295.561632561937,
            "upper_bound": 1209.1727742494402
          },
          "point_estimate": 949.178551740089,
          "standard_error": 227.5464889438675
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1425502.1465975277,
            "upper_bound": 1428664.9273076926
          },
          "point_estimate": 1426841.6568406592,
          "standard_error": 828.6744707234285
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1424883.2788461538,
            "upper_bound": 1427806.2384615385
          },
          "point_estimate": 1426129.8942307692,
          "standard_error": 785.1701869219015
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.49484512705538,
            "upper_bound": 3335.8733355120653
          },
          "point_estimate": 1881.728716784974,
          "standard_error": 799.4198389847818
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1425061.0966195215,
            "upper_bound": 1426534.120289969
          },
          "point_estimate": 1425605.0651348652,
          "standard_error": 379.99971867537295
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 845.842096987314,
            "upper_bound": 4055.332831448369
          },
          "point_estimate": 2772.241591255134,
          "standard_error": 987.7788572425302
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789255.1780124112,
            "upper_bound": 790398.497391844
          },
          "point_estimate": 789779.2561229315,
          "standard_error": 293.7421041783113
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789065.0,
            "upper_bound": 790423.5861702128
          },
          "point_estimate": 789458.1223404255,
          "standard_error": 337.76775192245776
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.284672122172825,
            "upper_bound": 1451.4827238056396
          },
          "point_estimate": 620.8860559983144,
          "standard_error": 357.70176911667727
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 789141.2099875704,
            "upper_bound": 790212.5814696485
          },
          "point_estimate": 789610.1351754628,
          "standard_error": 274.94055756467736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.8103623670001,
            "upper_bound": 1216.687178914005
          },
          "point_estimate": 976.604579671649,
          "standard_error": 238.01850824421908
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 276810.0288676844,
            "upper_bound": 278603.43973282434
          },
          "point_estimate": 277733.6298409669,
          "standard_error": 455.89243626577615
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 275951.53721374046,
            "upper_bound": 278936.2786259542
          },
          "point_estimate": 278710.9557251908,
          "standard_error": 956.5242015434616
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.73397132367312,
            "upper_bound": 2338.170504672353
          },
          "point_estimate": 614.2029723400232,
          "standard_error": 700.100385516123
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 276020.26015246805,
            "upper_bound": 277849.77287140343
          },
          "point_estimate": 276596.28157033806,
          "standard_error": 473.2690805597011
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 964.0890199796828,
            "upper_bound": 1668.8177102320997
          },
          "point_estimate": 1521.5531707462442,
          "standard_error": 194.22415287337853
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492625.1571651652,
            "upper_bound": 494069.9594901867
          },
          "point_estimate": 493382.2090513728,
          "standard_error": 366.82399416037464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492943.1959459459,
            "upper_bound": 494647.6739864865
          },
          "point_estimate": 493206.2077702703,
          "standard_error": 410.320575241905
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.52973490360074,
            "upper_bound": 2345.211866810085
          },
          "point_estimate": 492.81061028242215,
          "standard_error": 587.1548957466985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 491842.1955452006,
            "upper_bound": 494505.48318042816
          },
          "point_estimate": 493134.5910494911,
          "standard_error": 717.213526920117
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 580.1743491236981,
            "upper_bound": 1663.7286904041473
          },
          "point_estimate": 1227.6502112004364,
          "standard_error": 297.3417167327509
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253053.28048794364,
            "upper_bound": 253292.30408730157
          },
          "point_estimate": 253164.72887511025,
          "standard_error": 61.17794314602309
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253015.96064814815,
            "upper_bound": 253358.7337962963
          },
          "point_estimate": 253084.55590277776,
          "standard_error": 84.39337813635038
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.9702790429434485,
            "upper_bound": 314.5691785819315
          },
          "point_estimate": 101.77082154966368,
          "standard_error": 79.3300539267397
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253038.1417107584,
            "upper_bound": 253258.20924723137
          },
          "point_estimate": 253117.8786075036,
          "standard_error": 58.33584596525658
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.31458948029944,
            "upper_bound": 242.8543470453648
          },
          "point_estimate": 203.70866453767036,
          "standard_error": 40.06923574657252
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632794.8254789272,
            "upper_bound": 633256.2204980842
          },
          "point_estimate": 633009.9502777777,
          "standard_error": 118.72332002280729
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632654.8137931034,
            "upper_bound": 633287.3881704981
          },
          "point_estimate": 632922.0948275862,
          "standard_error": 158.77734650306192
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.29017453821434,
            "upper_bound": 641.6258130916817
          },
          "point_estimate": 384.93663351086064,
          "standard_error": 153.60320197695276
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632790.5342274966,
            "upper_bound": 633324.8968866884
          },
          "point_estimate": 633051.8260635916,
          "standard_error": 140.40322691683718
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 186.1977228607992,
            "upper_bound": 516.016154101185
          },
          "point_estimate": 396.4742265126003,
          "standard_error": 88.1623800536983
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2057.174901912832,
            "upper_bound": 2071.5398939665333
          },
          "point_estimate": 2064.5819532316664,
          "standard_error": 3.6564985302445296
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2049.629871910127,
            "upper_bound": 2073.9728868996385
          },
          "point_estimate": 2071.906103555785,
          "standard_error": 7.641612776141273
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6113310891644289,
            "upper_bound": 18.492467887679755
          },
          "point_estimate": 5.977091825944874,
          "standard_error": 5.557291468074353
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2050.9622930625965,
            "upper_bound": 2065.2373803356336
          },
          "point_estimate": 2055.48589105277,
          "standard_error": 3.721798780415786
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.76359714796165,
            "upper_bound": 13.49660529707568
          },
          "point_estimate": 12.21039486400844,
          "standard_error": 1.5724885314605617
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649.8140900507957,
            "upper_bound": 650.1191808613404
          },
          "point_estimate": 649.9599596991039,
          "standard_error": 0.07825293322272604
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649.7414582353013,
            "upper_bound": 650.1832084554965
          },
          "point_estimate": 649.8900992085038,
          "standard_error": 0.11904043256351188
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06407699916999597,
            "upper_bound": 0.4256852568578698
          },
          "point_estimate": 0.29001434041893787,
          "standard_error": 0.09506866608922192
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649.7357207798258,
            "upper_bound": 649.9717451768198
          },
          "point_estimate": 649.8273042229064,
          "standard_error": 0.06019885519294907
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1475505859728783,
            "upper_bound": 0.32590973266612416
          },
          "point_estimate": 0.26098016086743275,
          "standard_error": 0.04647928443149124
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.69118832219404,
            "upper_bound": 127.87658594215782
          },
          "point_estimate": 127.76906020726132,
          "standard_error": 0.04863734216216056
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.67441187977091,
            "upper_bound": 127.83866518611704
          },
          "point_estimate": 127.7045417919492,
          "standard_error": 0.041238918579767914
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01289454703797999,
            "upper_bound": 0.1988513406629284
          },
          "point_estimate": 0.056381110977894215,
          "standard_error": 0.04698002366395092
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.67901778594702,
            "upper_bound": 127.745149571072
          },
          "point_estimate": 127.70230983722914,
          "standard_error": 0.01711488014573318
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027080717318780135,
            "upper_bound": 0.2352584616560136
          },
          "point_estimate": 0.1618155192984875,
          "standard_error": 0.05749033196267866
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.325810275723576,
            "upper_bound": 50.35292245047509
          },
          "point_estimate": 50.33846697749648,
          "standard_error": 0.006987810194037827
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.319452354616274,
            "upper_bound": 50.35648275690215
          },
          "point_estimate": 50.329974111505706,
          "standard_error": 0.011201137755530385
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017531078242408074,
            "upper_bound": 0.04407427373517755
          },
          "point_estimate": 0.020446319226718905,
          "standard_error": 0.010384947995389786
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.32103358187563,
            "upper_bound": 50.34132508744575
          },
          "point_estimate": 50.327763853610016,
          "standard_error": 0.0052187158563075185
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012238310553918774,
            "upper_bound": 0.030369872492680597
          },
          "point_estimate": 0.023314217981450103,
          "standard_error": 0.005006538184915673
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.30803319541847,
            "upper_bound": 79.42797892795555
          },
          "point_estimate": 79.35996699368167,
          "standard_error": 0.03120520604968126
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.28826324513267,
            "upper_bound": 79.40070791361883
          },
          "point_estimate": 79.33235125957997,
          "standard_error": 0.03500189765919883
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002993252727011934,
            "upper_bound": 0.1400127722563417
          },
          "point_estimate": 0.06580642663935937,
          "standard_error": 0.03296100401824894
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.28892875263135,
            "upper_bound": 79.34793816929775
          },
          "point_estimate": 79.31569113486329,
          "standard_error": 0.015515137120326305
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03833904436949869,
            "upper_bound": 0.15098083211304408
          },
          "point_estimate": 0.1043029927829725,
          "standard_error": 0.03490438734070363
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299293.87439825654,
            "upper_bound": 299423.85195038054
          },
          "point_estimate": 299354.4720029925,
          "standard_error": 33.32181604629612
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299280.14936247724,
            "upper_bound": 299445.4405737705
          },
          "point_estimate": 299315.5925058548,
          "standard_error": 39.58237342670116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.59538382287792,
            "upper_bound": 181.76432628124124
          },
          "point_estimate": 84.33350690443021,
          "standard_error": 43.39814468222489
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299295.38937665673,
            "upper_bound": 299370.28965179774
          },
          "point_estimate": 299324.6420906962,
          "standard_error": 19.247040086811175
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.58102337530691,
            "upper_bound": 142.051182659018
          },
          "point_estimate": 110.85608944678889,
          "standard_error": 24.751960158975457
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.936678141730134,
            "upper_bound": 59.98639520464479
          },
          "point_estimate": 59.95744671181613,
          "standard_error": 0.013059297903692929
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.93492083721023,
            "upper_bound": 59.969855724708935
          },
          "point_estimate": 59.9432673684662,
          "standard_error": 0.008287404053644466
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003171543828433649,
            "upper_bound": 0.045041494368269944
          },
          "point_estimate": 0.01924303506509514,
          "standard_error": 0.0111896138887822
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.94102819657903,
            "upper_bound": 59.96029151860812
          },
          "point_estimate": 59.949476496685136,
          "standard_error": 0.004880294671205587
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010392752305212217,
            "upper_bound": 0.06393101683143118
          },
          "point_estimate": 0.0435048142921796,
          "standard_error": 0.016246083754206952
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.31468654370327,
            "upper_bound": 101.38417380305891
          },
          "point_estimate": 101.3463496479594,
          "standard_error": 0.017911577435923243
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.30216718783969,
            "upper_bound": 101.38707496928303
          },
          "point_estimate": 101.32047024250802,
          "standard_error": 0.023389435936516177
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008788276356918031,
            "upper_bound": 0.09430949420924654
          },
          "point_estimate": 0.04379266285552233,
          "standard_error": 0.02315822951745612
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.3101097654836,
            "upper_bound": 101.3664624190473
          },
          "point_estimate": 101.33676274506696,
          "standard_error": 0.014575678364836852
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025352680983868683,
            "upper_bound": 0.08008662369816465
          },
          "point_estimate": 0.05979250068771504,
          "standard_error": 0.014922383974274572
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132840.18988124203,
            "upper_bound": 132983.28096289543
          },
          "point_estimate": 132906.6104302804,
          "standard_error": 36.79292003185056
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132815.92554744525,
            "upper_bound": 132990.03163017033
          },
          "point_estimate": 132873.59210983664,
          "standard_error": 42.32031668804353
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.4580603449994,
            "upper_bound": 195.72904883656855
          },
          "point_estimate": 92.39798884624464,
          "standard_error": 46.15606821759576
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132836.05023862084,
            "upper_bound": 132939.0326101649
          },
          "point_estimate": 132875.86109583848,
          "standard_error": 26.63015466997877
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.63973586405881,
            "upper_bound": 153.70781500123329
          },
          "point_estimate": 122.37736604537018,
          "standard_error": 26.70754136326688
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.413577941001606,
            "upper_bound": 59.48661140489637
          },
          "point_estimate": 59.44217994318734,
          "standard_error": 0.02002651250177768
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.40469395492339,
            "upper_bound": 59.44471468421534
          },
          "point_estimate": 59.42479503575537,
          "standard_error": 0.011431218514955864
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006330378718086618,
            "upper_bound": 0.046475013653546886
          },
          "point_estimate": 0.026572528861378744,
          "standard_error": 0.010746329289599132
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.41887588708437,
            "upper_bound": 59.44213425436926
          },
          "point_estimate": 59.432325377715415,
          "standard_error": 0.006005127625402415
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013647180030931478,
            "upper_bound": 0.1018230708570114
          },
          "point_estimate": 0.06691110291937682,
          "standard_error": 0.02990681741612426
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.53361245166084,
            "upper_bound": 93.56651145137252
          },
          "point_estimate": 93.54924108009878,
          "standard_error": 0.008435667739527884
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.52886697059952,
            "upper_bound": 93.57461916762804
          },
          "point_estimate": 93.53186635535432,
          "standard_error": 0.01458744911861742
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005116867109895108,
            "upper_bound": 0.04887746324081217
          },
          "point_estimate": 0.010001601364932366,
          "standard_error": 0.014272572366017852
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.5338814116793,
            "upper_bound": 93.5712502511209
          },
          "point_estimate": 93.55490816605209,
          "standard_error": 0.009495912444676496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014329081789755446,
            "upper_bound": 0.03425308345919247
          },
          "point_estimate": 0.02822420791580283,
          "standard_error": 0.005043189959667901
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241.5789453206674,
            "upper_bound": 242.12423098894175
          },
          "point_estimate": 241.83063734666365,
          "standard_error": 0.14135517098603526
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241.57322658490065,
            "upper_bound": 242.15527592702296
          },
          "point_estimate": 241.6366712648957,
          "standard_error": 0.14975323032018345
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015688750245621823,
            "upper_bound": 0.767985450858811
          },
          "point_estimate": 0.31378761437693864,
          "standard_error": 0.2078305602815804
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241.459056277648,
            "upper_bound": 241.7279485583709
          },
          "point_estimate": 241.58894907101697,
          "standard_error": 0.06727589710562859
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.18085365585973184,
            "upper_bound": 0.6305353949675431
          },
          "point_estimate": 0.47003302704080807,
          "standard_error": 0.11831957630093816
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 487.56086502028626,
            "upper_bound": 487.9594801978139
          },
          "point_estimate": 487.753473062283,
          "standard_error": 0.10232709900040342
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 487.52278865404514,
            "upper_bound": 488.12849522325087
          },
          "point_estimate": 487.6645734997437,
          "standard_error": 0.1349880965695597
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005927642223062176,
            "upper_bound": 0.5434904072920979
          },
          "point_estimate": 0.3018141408733472,
          "standard_error": 0.16419929995509755
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 487.6268904507668,
            "upper_bound": 487.95382345934615
          },
          "point_estimate": 487.76782899993736,
          "standard_error": 0.08911381244090641
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1642455835142298,
            "upper_bound": 0.4136597842776168
          },
          "point_estimate": 0.34138925722366303,
          "standard_error": 0.058630907702186055
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.437918198810706,
            "upper_bound": 33.45692978686926
          },
          "point_estimate": 33.44653316732833,
          "standard_error": 0.004885088801039867
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.43576362978905,
            "upper_bound": 33.45513041018543
          },
          "point_estimate": 33.439775561135505,
          "standard_error": 0.005323030328634629
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007412802066320564,
            "upper_bound": 0.024441852746284305
          },
          "point_estimate": 0.006169422973594994,
          "standard_error": 0.005788174527525519
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.43738080580299,
            "upper_bound": 33.44879151936712
          },
          "point_estimate": 33.442655843552394,
          "standard_error": 0.0032252882749714907
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00314748785274991,
            "upper_bound": 0.021117766328992423
          },
          "point_estimate": 0.01627824117050822,
          "standard_error": 0.004309016052072551
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33786.3275211148,
            "upper_bound": 33811.27836718346
          },
          "point_estimate": 33797.83966064969,
          "standard_error": 6.423271554266422
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33780.33878552972,
            "upper_bound": 33810.67596899225
          },
          "point_estimate": 33792.39655813953,
          "standard_error": 9.332172576935143
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.091987420225859,
            "upper_bound": 37.400005639898126
          },
          "point_estimate": 19.3144849594287,
          "standard_error": 8.561774122082173
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33781.764053315484,
            "upper_bound": 33795.13430934915
          },
          "point_estimate": 33786.78377771066,
          "standard_error": 3.408770898573823
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.497784026402629,
            "upper_bound": 28.717385289425344
          },
          "point_estimate": 21.44663541689463,
          "standard_error": 5.165338067014351
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.68414422962964,
            "upper_bound": 124.91559647026295
          },
          "point_estimate": 124.7787318646075,
          "standard_error": 0.06152495887652518
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.66720170297025,
            "upper_bound": 124.83521867894048
          },
          "point_estimate": 124.68669926043808,
          "standard_error": 0.0421852271410803
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005304965911582651,
            "upper_bound": 0.19134470420219565
          },
          "point_estimate": 0.036860995873581194,
          "standard_error": 0.051137110576737504
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.67392956328872,
            "upper_bound": 124.72738934532744
          },
          "point_estimate": 124.69457005410004,
          "standard_error": 0.013761990822525449
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03155081192106468,
            "upper_bound": 0.3007787439946528
          },
          "point_estimate": 0.2043731296478092,
          "standard_error": 0.08029352021803203
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104928.2526249096,
            "upper_bound": 1105794.0089046718
          },
          "point_estimate": 1105326.7678583453,
          "standard_error": 222.9711648816925
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104788.9675324676,
            "upper_bound": 1105909.880681818
          },
          "point_estimate": 1105128.0067340066,
          "standard_error": 259.1923952008751
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.05395335504029,
            "upper_bound": 1221.627934069342
          },
          "point_estimate": 518.5618044300304,
          "standard_error": 270.31091945358435
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104981.2572476068,
            "upper_bound": 1106210.298505186
          },
          "point_estimate": 1105562.9723730814,
          "standard_error": 351.09175593954416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280.53338480730537,
            "upper_bound": 953.7781257285388
          },
          "point_estimate": 744.6090820550548,
          "standard_error": 174.5991794007135
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2233.2786353726137,
            "upper_bound": 2234.876791993862
          },
          "point_estimate": 2234.014722787653,
          "standard_error": 0.4126446357686985
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2233.0686836471477,
            "upper_bound": 2235.0517381157106
          },
          "point_estimate": 2233.5391971707,
          "standard_error": 0.4678689529322124
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2558908488537192,
            "upper_bound": 2.273303265745683
          },
          "point_estimate": 0.9131211402803918,
          "standard_error": 0.49214804346096896
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2233.0700649475793,
            "upper_bound": 2234.0611483061925
          },
          "point_estimate": 2233.5024134691007,
          "standard_error": 0.2622594824847375
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5081736197616706,
            "upper_bound": 1.7715365197894015
          },
          "point_estimate": 1.3766801214434137,
          "standard_error": 0.32644008752319925
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.452434321761935,
            "upper_bound": 43.487445075027765
          },
          "point_estimate": 43.46945908201705,
          "standard_error": 0.008934483971449744
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.444463579530584,
            "upper_bound": 43.50110667458628
          },
          "point_estimate": 43.45218027774159,
          "standard_error": 0.018306002414531192
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001845407207944016,
            "upper_bound": 0.04611761332947385
          },
          "point_estimate": 0.016532941146343755,
          "standard_error": 0.012700089989676066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.44477397084984,
            "upper_bound": 43.47660058667736
          },
          "point_estimate": 43.456908782632034,
          "standard_error": 0.008445112508044542
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017038286730108394,
            "upper_bound": 0.03327541164344203
          },
          "point_estimate": 0.029665962060351356,
          "standard_error": 0.003929566258294461
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.39765224263804,
            "upper_bound": 47.47729163786169
          },
          "point_estimate": 47.42677343280755,
          "standard_error": 0.02306829253096477
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.392174674604725,
            "upper_bound": 47.42029452814192
          },
          "point_estimate": 47.40161905693602,
          "standard_error": 0.008547108159864878
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0029383514119937083,
            "upper_bound": 0.029918956150499887
          },
          "point_estimate": 0.014307032420623289,
          "standard_error": 0.00910271892922563
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.39363867437332,
            "upper_bound": 47.4138359041618
          },
          "point_estimate": 47.40311482999489,
          "standard_error": 0.005450034027156416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007552735871529951,
            "upper_bound": 0.11838951102282122
          },
          "point_estimate": 0.07709108567908629,
          "standard_error": 0.03911224420609513
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.58908100015987,
            "upper_bound": 35.624044824639924
          },
          "point_estimate": 35.602709468372055,
          "standard_error": 0.009631201464143516
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.586047087688996,
            "upper_bound": 35.60246679463192
          },
          "point_estimate": 35.59373184250805,
          "standard_error": 0.004710284707609557
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024521673406315853,
            "upper_bound": 0.019626104645082545
          },
          "point_estimate": 0.011545672302861423,
          "standard_error": 0.004718622405838427
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.59085945143531,
            "upper_bound": 35.59986369898929
          },
          "point_estimate": 35.594899130169665,
          "standard_error": 0.002292235088169181
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00568830011432016,
            "upper_bound": 0.04918334645821993
          },
          "point_estimate": 0.03222965419626934,
          "standard_error": 0.01473900195174962
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.77543227996859,
            "upper_bound": 39.806462365919415
          },
          "point_estimate": 39.78920263495105,
          "standard_error": 0.008037161122038903
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.77064648749888,
            "upper_bound": 39.80193599452126
          },
          "point_estimate": 39.77827954324832,
          "standard_error": 0.00893128254206388
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030910995376422987,
            "upper_bound": 0.03773456831231447
          },
          "point_estimate": 0.015852276630386748,
          "standard_error": 0.009398311264778088
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.77206252730438,
            "upper_bound": 39.789908131816944
          },
          "point_estimate": 39.77866752568758,
          "standard_error": 0.004765310615341154
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00927973692150985,
            "upper_bound": 0.03725255400610539
          },
          "point_estimate": 0.026777081664269307,
          "standard_error": 0.007895242780102231
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.922473108136025,
            "upper_bound": 45.94812332942053
          },
          "point_estimate": 45.93278311773822,
          "standard_error": 0.007016927016864755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.92078611627125,
            "upper_bound": 45.93697210981783
          },
          "point_estimate": 45.92384226632516,
          "standard_error": 0.0035584534491905112
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003762905283615047,
            "upper_bound": 0.017687839393527482
          },
          "point_estimate": 0.004686664480612525,
          "standard_error": 0.0043269825626720574
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.92283372981484,
            "upper_bound": 45.93648254998451
          },
          "point_estimate": 45.92865483056444,
          "standard_error": 0.003607471290675464
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002477973840079478,
            "upper_bound": 0.03497350103723025
          },
          "point_estimate": 0.02344381648818909,
          "standard_error": 0.009956235930362734
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.8336808904692,
            "upper_bound": 71.92305565524939
          },
          "point_estimate": 71.86703181758227,
          "standard_error": 0.02539663472423407
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.83530440834696,
            "upper_bound": 71.85718213541658
          },
          "point_estimate": 71.84294941926889,
          "standard_error": 0.00760300886478969
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0026817349113015644,
            "upper_bound": 0.03709271734297455
          },
          "point_estimate": 0.010104127152540016,
          "standard_error": 0.01062143889441322
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.83903236578227,
            "upper_bound": 71.85046050417188
          },
          "point_estimate": 71.84346360597502,
          "standard_error": 0.0028940083247102405
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007515916396307124,
            "upper_bound": 0.13042597610968368
          },
          "point_estimate": 0.08485799147852864,
          "standard_error": 0.04208389817435091
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.53669261753058,
            "upper_bound": 80.60130214605881
          },
          "point_estimate": 80.5622931908237,
          "standard_error": 0.01763357772809614
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.52910178906741,
            "upper_bound": 80.56664588072933
          },
          "point_estimate": 80.54820222475458,
          "standard_error": 0.008823405782512493
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008163711817343599,
            "upper_bound": 0.04233696256689918
          },
          "point_estimate": 0.021544262255765927,
          "standard_error": 0.011727427477217788
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.52844793213218,
            "upper_bound": 80.55010581611201
          },
          "point_estimate": 80.53655520492462,
          "standard_error": 0.005545811864980871
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010275732969452053,
            "upper_bound": 0.08892280716204222
          },
          "point_estimate": 0.05881148474994795,
          "standard_error": 0.02552880980744759
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.809863260955105,
            "upper_bound": 59.84690653167898
          },
          "point_estimate": 59.82628854487982,
          "standard_error": 0.009605195935597272
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.8077777997704,
            "upper_bound": 59.84280339446653
          },
          "point_estimate": 59.81519116351219,
          "standard_error": 0.008330259189686717
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023398249851845857,
            "upper_bound": 0.04447355864575924
          },
          "point_estimate": 0.012152773059037846,
          "standard_error": 0.010122033197238516
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.81050386697482,
            "upper_bound": 59.82696289214109
          },
          "point_estimate": 59.81698716910605,
          "standard_error": 0.004280999403994193
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008131195587186968,
            "upper_bound": 0.043546344914202464
          },
          "point_estimate": 0.032036705156912,
          "standard_error": 0.00945912070992355
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.99688407216348,
            "upper_bound": 101.06049335637732
          },
          "point_estimate": 101.0256013936626,
          "standard_error": 0.01637929025326489
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.98632485925783,
            "upper_bound": 101.05763703672837
          },
          "point_estimate": 101.00469758848698,
          "standard_error": 0.023687646951491677
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005230605897026358,
            "upper_bound": 0.09571407414669338
          },
          "point_estimate": 0.042853434932187,
          "standard_error": 0.02220670713423722
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.9867700182782,
            "upper_bound": 101.03026682367448
          },
          "point_estimate": 101.0075607262105,
          "standard_error": 0.011052367939804046
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025385326984242156,
            "upper_bound": 0.07429341639700253
          },
          "point_estimate": 0.05453059338522086,
          "standard_error": 0.01399236760073434
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.96066546636177,
            "upper_bound": 60.994945645519735
          },
          "point_estimate": 60.97620905145934,
          "standard_error": 0.008812007378725062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.95392430849651,
            "upper_bound": 60.99084331777912
          },
          "point_estimate": 60.96814477196808,
          "standard_error": 0.010011832386827749
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004306593016199142,
            "upper_bound": 0.04435955201551566
          },
          "point_estimate": 0.02910367054708627,
          "standard_error": 0.009957398380515197
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.95633613090398,
            "upper_bound": 60.97766769680933
          },
          "point_estimate": 60.966008300056686,
          "standard_error": 0.005357322391827094
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01274785119194748,
            "upper_bound": 0.04083413358415704
          },
          "point_estimate": 0.029405939484763294,
          "standard_error": 0.008167302329684354
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.40409336551931,
            "upper_bound": 59.48490943594281
          },
          "point_estimate": 59.43784154734136,
          "standard_error": 0.021393691129060543
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.39926411815954,
            "upper_bound": 59.45850244285964
          },
          "point_estimate": 59.41456183537845,
          "standard_error": 0.013243873627743988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004260755617146001,
            "upper_bound": 0.06944114799819201
          },
          "point_estimate": 0.02367989958908143,
          "standard_error": 0.017136451890050666
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.40537121166469,
            "upper_bound": 59.425797899272
          },
          "point_estimate": 59.41370564059818,
          "standard_error": 0.005192381469449448
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01216882605712242,
            "upper_bound": 0.10373454683414864
          },
          "point_estimate": 0.07131547726256977,
          "standard_error": 0.026613892566578052
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.53222072189448,
            "upper_bound": 93.6074512003827
          },
          "point_estimate": 93.56190957400496,
          "standard_error": 0.020447208442235444
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.52738518402217,
            "upper_bound": 93.56969634864213
          },
          "point_estimate": 93.53758285712814,
          "standard_error": 0.01183871048015014
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003262331618708595,
            "upper_bound": 0.04998780128388308
          },
          "point_estimate": 0.01513793831588075,
          "standard_error": 0.013687330354349755
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.5287281483696,
            "upper_bound": 93.54989068986632
          },
          "point_estimate": 93.53627258323368,
          "standard_error": 0.005454688178967581
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01050034180809535,
            "upper_bound": 0.10255927680504053
          },
          "point_estimate": 0.06795833392766858,
          "standard_error": 0.029181475619244137
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1209058.420998464,
            "upper_bound": 1209997.3359267793
          },
          "point_estimate": 1209427.6470289298,
          "standard_error": 256.64299126943524
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1209085.6701612903,
            "upper_bound": 1209440.7867383512
          },
          "point_estimate": 1209173.6166666667,
          "standard_error": 106.1782004151244
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.143011077921862,
            "upper_bound": 632.3936528860428
          },
          "point_estimate": 141.13654289757503,
          "standard_error": 159.60623659374468
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1209134.611414392,
            "upper_bound": 1209274.1682303045
          },
          "point_estimate": 1209203.6067029743,
          "standard_error": 35.115153904162106
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.18335877514504,
            "upper_bound": 1301.6716925438766
          },
          "point_estimate": 855.4628013098059,
          "standard_error": 383.37486439164707
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1535662.132175926,
            "upper_bound": 1536652.0880954862
          },
          "point_estimate": 1536121.7773842593,
          "standard_error": 255.56828770036404
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1535574.5208333333,
            "upper_bound": 1536731.2826388888
          },
          "point_estimate": 1535888.3697916667,
          "standard_error": 250.18439558924808
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.5951613749508,
            "upper_bound": 1389.2708201272176
          },
          "point_estimate": 497.8215505370158,
          "standard_error": 312.3524622401483
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1535715.2016451303,
            "upper_bound": 1536405.6982199545
          },
          "point_estimate": 1536010.3212121213,
          "standard_error": 174.65252785558795
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 294.97819590333336,
            "upper_bound": 1115.4527821507188
          },
          "point_estimate": 853.4289069854177,
          "standard_error": 207.07429321753855
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1518278.3930523314,
            "upper_bound": 1519659.4126488096
          },
          "point_estimate": 1518999.9055886243,
          "standard_error": 350.9449385398171
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1518345.771577381,
            "upper_bound": 1519699.0208333333
          },
          "point_estimate": 1519138.323958333,
          "standard_error": 290.57144745437074
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121.57422742489796,
            "upper_bound": 1945.062568940488
          },
          "point_estimate": 719.8623462475961,
          "standard_error": 442.8271086016214
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1518455.9647093213,
            "upper_bound": 1519204.2169731995
          },
          "point_estimate": 1518878.9953463203,
          "standard_error": 189.52667330113923
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 440.6629983488743,
            "upper_bound": 1619.2646330802254
          },
          "point_estimate": 1171.6205969411724,
          "standard_error": 299.24492487724154
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 660447.7197270021,
            "upper_bound": 661301.2808294912
          },
          "point_estimate": 660837.3056760461,
          "standard_error": 219.93164593276225
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 660246.4383838384,
            "upper_bound": 661234.0681818182
          },
          "point_estimate": 660637.8651515152,
          "standard_error": 242.45459202053647
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107.25776445943816,
            "upper_bound": 1112.1902609366157
          },
          "point_estimate": 658.2032534661378,
          "standard_error": 256.11704564422024
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 660297.6333087964,
            "upper_bound": 661193.7157443903
          },
          "point_estimate": 660711.8434946871,
          "standard_error": 245.84708277992272
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300.3441511438916,
            "upper_bound": 992.1818772347268
          },
          "point_estimate": 733.6003020982232,
          "standard_error": 189.82542505131968
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1430174.6186752133,
            "upper_bound": 1431065.8193952988
          },
          "point_estimate": 1430600.2117094018,
          "standard_error": 227.63487476378936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1430018.8923076922,
            "upper_bound": 1430998.266025641
          },
          "point_estimate": 1430583.9636752135,
          "standard_error": 270.5749991237593
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.03862002250753,
            "upper_bound": 1287.3098776583877
          },
          "point_estimate": 610.098442630134,
          "standard_error": 268.97317768386205
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1430212.9035695302,
            "upper_bound": 1430792.3523480266
          },
          "point_estimate": 1430476.9883116884,
          "standard_error": 147.5606701158964
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 372.0343025220928,
            "upper_bound": 1017.2626858141658
          },
          "point_estimate": 758.9037187581368,
          "standard_error": 172.80664234427402
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393201.7338445661,
            "upper_bound": 393485.3819031618
          },
          "point_estimate": 393319.5414170507,
          "standard_error": 75.44963995871696
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393197.4802867384,
            "upper_bound": 393408.3501344086
          },
          "point_estimate": 393219.65277777775,
          "standard_error": 47.990553602881235
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9578446066216972,
            "upper_bound": 283.8112832678071
          },
          "point_estimate": 33.1240201108153,
          "standard_error": 61.035790731088206
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393197.6206171032,
            "upper_bound": 393254.7249359959
          },
          "point_estimate": 393219.8671414607,
          "standard_error": 14.615646872092729
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.6149216304079,
            "upper_bound": 355.9549386810487
          },
          "point_estimate": 251.02743894729295,
          "standard_error": 93.6985069161228
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 581102.9426398338,
            "upper_bound": 584047.8081349207
          },
          "point_estimate": 582674.99888322,
          "standard_error": 754.2337156791272
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579994.0476190476,
            "upper_bound": 584116.2962962964
          },
          "point_estimate": 583926.4482142858,
          "standard_error": 1027.2329902532867
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.48309764387382,
            "upper_bound": 3381.2869233034603
          },
          "point_estimate": 420.6544036430624,
          "standard_error": 1065.8710318452436
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 582691.2276404152,
            "upper_bound": 583968.3629585446
          },
          "point_estimate": 583607.806596578,
          "standard_error": 342.8556351545585
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 739.7940345795053,
            "upper_bound": 3092.863093850619
          },
          "point_estimate": 2523.7695928244766,
          "standard_error": 527.9622876554254
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1040476.0168095236,
            "upper_bound": 1045295.7075884354
          },
          "point_estimate": 1043220.7251712016,
          "standard_error": 1270.642144166874
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1041153.4128968256,
            "upper_bound": 1045537.4761904762
          },
          "point_estimate": 1045180.0729591836,
          "standard_error": 973.6942979312612
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160.44152286588388,
            "upper_bound": 5682.627236600374
          },
          "point_estimate": 676.1759004954616,
          "standard_error": 1085.520066615663
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1036747.1397057276,
            "upper_bound": 1045282.660630642
          },
          "point_estimate": 1040509.6634508348,
          "standard_error": 2312.026657633343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 370.57614297822454,
            "upper_bound": 5733.4009664594505
          },
          "point_estimate": 4226.137809557852,
          "standard_error": 1442.5521042853154
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 720281.9747292249,
            "upper_bound": 721309.5848806022
          },
          "point_estimate": 720770.8141409897,
          "standard_error": 262.28707994887384
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 720111.1185807656,
            "upper_bound": 721267.9852941176
          },
          "point_estimate": 720751.437254902,
          "standard_error": 260.43829986433633
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.73867445133496,
            "upper_bound": 1472.834678753977
          },
          "point_estimate": 752.2462743900254,
          "standard_error": 339.908924417861
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 720275.1849010135,
            "upper_bound": 721107.7778317939
          },
          "point_estimate": 720706.6411510059,
          "standard_error": 209.45892550032
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401.6051723184023,
            "upper_bound": 1178.3004707243872
          },
          "point_estimate": 872.8692511107618,
          "standard_error": 208.7696940948424
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 820630.6368148148,
            "upper_bound": 821195.3070507934
          },
          "point_estimate": 820895.5326684302,
          "standard_error": 145.27768536319857
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 820491.3022222222,
            "upper_bound": 821270.4101851851
          },
          "point_estimate": 820867.074074074,
          "standard_error": 171.20618995862216
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.76646484705907,
            "upper_bound": 822.8759320576333
          },
          "point_estimate": 448.5913722581415,
          "standard_error": 205.90803198957792
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 820558.2079369097,
            "upper_bound": 820943.1420435511
          },
          "point_estimate": 820725.1585569986,
          "standard_error": 97.42483978620966
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218.60184390541016,
            "upper_bound": 614.2366225625202
          },
          "point_estimate": 482.8924000555374,
          "standard_error": 102.91413114620296
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 449776.23360082303,
            "upper_bound": 450122.62432441703
          },
          "point_estimate": 449944.0151533412,
          "standard_error": 88.84750691997012
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 449716.39711934153,
            "upper_bound": 450160.96481481486
          },
          "point_estimate": 449925.06216931215,
          "standard_error": 100.71057712860042
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.21225208926487,
            "upper_bound": 538.4171626634196
          },
          "point_estimate": 235.2741508127527,
          "standard_error": 120.78017142743178
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 449778.86016628874,
            "upper_bound": 450005.33112381585
          },
          "point_estimate": 449912.9000160333,
          "standard_error": 57.08476430663459
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149.09943561737626,
            "upper_bound": 377.4700783011616
          },
          "point_estimate": 296.59460325802615,
          "standard_error": 58.03378085422847
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 324892.02988888894,
            "upper_bound": 325518.97787934914
          },
          "point_estimate": 325196.4634839144,
          "standard_error": 161.05709846561086
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 324676.7955357143,
            "upper_bound": 325863.7551020408
          },
          "point_estimate": 325079.2142857143,
          "standard_error": 296.28991005124635
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.52856450896223,
            "upper_bound": 887.8863860226098
          },
          "point_estimate": 604.6868565563514,
          "standard_error": 246.17492878586015
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 324731.97511225514,
            "upper_bound": 325572.8194474598
          },
          "point_estimate": 325032.55547309836,
          "standard_error": 214.48264167420368
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297.8647628349314,
            "upper_bound": 617.3504157533334
          },
          "point_estimate": 536.4399262779385,
          "standard_error": 76.56497773597667
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253735.88529523535,
            "upper_bound": 253850.70685404268
          },
          "point_estimate": 253785.11817598104,
          "standard_error": 29.885947135572124
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253730.475,
            "upper_bound": 253804.328125
          },
          "point_estimate": 253765.77551118823,
          "standard_error": 22.4660928207666
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.632234653813471,
            "upper_bound": 110.54727572257691
          },
          "point_estimate": 54.747320590536454,
          "standard_error": 24.479200713304103
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253739.117496313,
            "upper_bound": 253783.90957166516
          },
          "point_estimate": 253758.1961399711,
          "standard_error": 11.455840658954328
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.13143813466916,
            "upper_bound": 146.10093985982348
          },
          "point_estimate": 99.58145658582652,
          "standard_error": 35.79338186157706
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182104.71350401783,
            "upper_bound": 182222.63490357145
          },
          "point_estimate": 182158.4257119047,
          "standard_error": 30.401623996814376
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182086.76401785715,
            "upper_bound": 182229.47083333333
          },
          "point_estimate": 182124.924375,
          "standard_error": 34.50834495434858
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.454991886128363,
            "upper_bound": 149.22016617583077
          },
          "point_estimate": 76.37829976902246,
          "standard_error": 37.00088008418445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182094.01445640525,
            "upper_bound": 182169.86389802632
          },
          "point_estimate": 182138.49762337664,
          "standard_error": 19.57308900281403
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.59364316057665,
            "upper_bound": 135.3074374434842
          },
          "point_estimate": 101.69866936988096,
          "standard_error": 25.875368951782328
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-repeated-rare-huge_common-m"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2987422.5064468863,
            "upper_bound": 2989284.187965964
          },
          "point_estimate": 2988275.5506166047,
          "standard_error": 481.8311312851131
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2987174.7115384615,
            "upper_bound": 2989141.9076923076
          },
          "point_estimate": 2988157.8376068375,
          "standard_error": 482.3546048343473
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.3946890463083,
            "upper_bound": 2460.296091192658
          },
          "point_estimate": 1402.0279513908522,
          "standard_error": 622.5311935024325
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2987202.785329838,
            "upper_bound": 2988208.6258992804
          },
          "point_estimate": 2987748.4823176824,
          "standard_error": 257.0474498116448
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 591.7800247440086,
            "upper_bound": 2199.2057536145644
          },
          "point_estimate": 1611.683402694755,
          "standard_error": 432.95638913074674
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-repeated-rare-small_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6022.042877444253,
            "upper_bound": 6026.168526390171
          },
          "point_estimate": 6023.915947635086,
          "standard_error": 1.067428716344888
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6021.039810060184,
            "upper_bound": 6026.02970110246
          },
          "point_estimate": 6022.665843962233,
          "standard_error": 1.1844317675139933
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.218688086228287,
            "upper_bound": 5.375683530535998
          },
          "point_estimate": 2.4929822018900896,
          "standard_error": 1.2925426380248164
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6021.92355873863,
            "upper_bound": 6024.112760661108
          },
          "point_estimate": 6023.072461810179,
          "standard_error": 0.549647400164596
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1435325223496249,
            "upper_bound": 4.6611579560150584
          },
          "point_estimate": 3.5446519088439143,
          "standard_error": 0.9027986881215172
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355491.4832993825,
            "upper_bound": 1356296.2553436214
          },
          "point_estimate": 1355868.33473251,
          "standard_error": 206.31682133383
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355325.9814814816,
            "upper_bound": 1356364.5712962963
          },
          "point_estimate": 1355749.1419753088,
          "standard_error": 227.15905749360667
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.24702730446555,
            "upper_bound": 1149.130597993557
          },
          "point_estimate": 609.5575564620292,
          "standard_error": 269.8878408783411
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355391.7682526554,
            "upper_bound": 1356036.991251988
          },
          "point_estimate": 1355662.4245310244,
          "standard_error": 167.45457384565427
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290.2254571636755,
            "upper_bound": 865.617126229601
          },
          "point_estimate": 686.8447914839968,
          "standard_error": 145.92517825905605
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523833.1666402118,
            "upper_bound": 1525214.997108217
          },
          "point_estimate": 1524454.1015707671,
          "standard_error": 355.80589237210137
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523670.535416667,
            "upper_bound": 1525244.625
          },
          "point_estimate": 1523856.7873677248,
          "standard_error": 454.6981575880188
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.16295479129953,
            "upper_bound": 1935.6825256345585
          },
          "point_estimate": 592.30673003999,
          "standard_error": 479.75734706079805
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1523755.7289678436,
            "upper_bound": 1524812.888638639
          },
          "point_estimate": 1524220.0323593074,
          "standard_error": 305.7902126288961
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 489.4362996113361,
            "upper_bound": 1632.738141737488
          },
          "point_estimate": 1184.7176377757828,
          "standard_error": 328.8795390517087
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1424850.333580586,
            "upper_bound": 1426213.4193223445
          },
          "point_estimate": 1425560.637783883,
          "standard_error": 348.615282700308
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1424655.923076923,
            "upper_bound": 1426491.1564102564
          },
          "point_estimate": 1425787.728021978,
          "standard_error": 421.26798514090626
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252.0460686024859,
            "upper_bound": 2038.1187792006945
          },
          "point_estimate": 973.4314250258122,
          "standard_error": 455.95960432516625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1424233.5388607932,
            "upper_bound": 1426135.1451969084
          },
          "point_estimate": 1425036.4839160838,
          "standard_error": 486.9503345143377
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 572.7363246088316,
            "upper_bound": 1472.733535811864
          },
          "point_estimate": 1163.0367484171957,
          "standard_error": 228.2660950228095
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 788914.7927583586,
            "upper_bound": 789266.5762580209
          },
          "point_estimate": 789055.33801334,
          "standard_error": 95.22743475617004
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 788897.537993921,
            "upper_bound": 789117.3617021276
          },
          "point_estimate": 788932.4683510638,
          "standard_error": 60.50170440086543
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.04022109839655,
            "upper_bound": 266.0478335745736
          },
          "point_estimate": 57.97912237493115,
          "standard_error": 67.86307062853517
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 788905.6366612111,
            "upper_bound": 789074.513810119
          },
          "point_estimate": 788983.2138712351,
          "standard_error": 44.43216942045531
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.79290802202146,
            "upper_bound": 479.9879640606508
          },
          "point_estimate": 318.3930480585553,
          "standard_error": 135.12255662119202
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278615.3998091603,
            "upper_bound": 278872.5694124864
          },
          "point_estimate": 278741.8130061796,
          "standard_error": 65.86919204320952
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278527.57506361324,
            "upper_bound": 278900.5928753181
          },
          "point_estimate": 278764.6892039259,
          "standard_error": 93.0966359893634
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.6401265641226,
            "upper_bound": 403.57675497248306
          },
          "point_estimate": 217.73053086726745,
          "standard_error": 88.58481952120161
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278562.97512544,
            "upper_bound": 278822.07175585034
          },
          "point_estimate": 278709.7252106672,
          "standard_error": 66.00542978824757
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.61503926471983,
            "upper_bound": 279.0843944876709
          },
          "point_estimate": 219.6532046239983,
          "standard_error": 39.777936834787894
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492035.59326898336,
            "upper_bound": 492462.36399423535
          },
          "point_estimate": 492241.6298305448,
          "standard_error": 106.60226896395731
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492118.5630630631,
            "upper_bound": 492353.5173745174
          },
          "point_estimate": 492236.2388513514,
          "standard_error": 69.07115242279636
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.0171425998457,
            "upper_bound": 560.8259971379114
          },
          "point_estimate": 174.17162798892247,
          "standard_error": 110.83386364321538
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492154.1731660232,
            "upper_bound": 492326.74774774775
          },
          "point_estimate": 492258.1478062478,
          "standard_error": 44.3845144250004
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.88778119330186,
            "upper_bound": 498.85553280501085
          },
          "point_estimate": 354.88336085882105,
          "standard_error": 110.79709910762593
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252895.05898776455,
            "upper_bound": 253086.8293995122
          },
          "point_estimate": 252976.77971119928,
          "standard_error": 50.12499144628629
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252879.87648809524,
            "upper_bound": 253022.23194444444
          },
          "point_estimate": 252920.06452546295,
          "standard_error": 41.29246031986595
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.765632153794342,
            "upper_bound": 192.8903708231853
          },
          "point_estimate": 60.52319718940972,
          "standard_error": 48.86145794705818
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252903.3810356673,
            "upper_bound": 252995.17727621464
          },
          "point_estimate": 252949.0173520923,
          "standard_error": 23.246524823472296
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.22889848431837,
            "upper_bound": 241.3284088697144
          },
          "point_estimate": 166.8811879818508,
          "standard_error": 57.732347392220056
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632481.828808087,
            "upper_bound": 632941.9931749281
          },
          "point_estimate": 632695.041299261,
          "standard_error": 118.3411514083652
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632406.0988505747,
            "upper_bound": 632994.073275862
          },
          "point_estimate": 632536.0554187193,
          "standard_error": 138.28048157951804
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.969798017670314,
            "upper_bound": 629.3343989565004
          },
          "point_estimate": 233.72464326441272,
          "standard_error": 152.08581457206932
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632463.3457854406,
            "upper_bound": 632776.0451731337
          },
          "point_estimate": 632591.760367219,
          "standard_error": 80.64518574450842
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.2918944668937,
            "upper_bound": 487.64618095580875
          },
          "point_estimate": 394.12676133912834,
          "standard_error": 87.54416991904438
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.0031042718453,
            "upper_bound": 378.3774812768533
          },
          "point_estimate": 378.1477146354914,
          "standard_error": 0.1037249848882542
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.0034360776892,
            "upper_bound": 378.12917785202495
          },
          "point_estimate": 378.0661185118938,
          "standard_error": 0.0339893561271529
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018994843726502632,
            "upper_bound": 0.2004500778672612
          },
          "point_estimate": 0.05418074457595574,
          "standard_error": 0.05847861908899688
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378.0330320646398,
            "upper_bound": 378.11400896952625
          },
          "point_estimate": 378.07244986998944,
          "standard_error": 0.020058672713560072
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04369648360310036,
            "upper_bound": 0.529983447548946
          },
          "point_estimate": 0.3466487807405025,
          "standard_error": 0.1625333854295654
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.5484307575412,
            "upper_bound": 188.76290904103527
          },
          "point_estimate": 188.64954961918693,
          "standard_error": 0.05511922221630776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.52694130948925,
            "upper_bound": 188.7916891415294
          },
          "point_estimate": 188.59463365109332,
          "standard_error": 0.06772142832389794
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030367129844451737,
            "upper_bound": 0.29654373611270224
          },
          "point_estimate": 0.11674894234349024,
          "standard_error": 0.07511884249424988
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.556903328604,
            "upper_bound": 188.8815240578569
          },
          "point_estimate": 188.74884912088035,
          "standard_error": 0.08561059111682322
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0775428344245552,
            "upper_bound": 0.23070123100292228
          },
          "point_estimate": 0.1844653862441986,
          "standard_error": 0.03731078067865179
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.50486008114035,
            "upper_bound": 26.52974465525073
          },
          "point_estimate": 26.516239446680327,
          "standard_error": 0.006495583447484589
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.503771053995784,
            "upper_bound": 26.53278667009835
          },
          "point_estimate": 26.505640195522613,
          "standard_error": 0.007308687400061808
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004432173117329086,
            "upper_bound": 0.03866817400561802
          },
          "point_estimate": 0.0037282285364033823,
          "standard_error": 0.008937685613937872
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.504262318318172,
            "upper_bound": 26.52472167060528
          },
          "point_estimate": 26.512081471144793,
          "standard_error": 0.00593018053730968
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00418285508460728,
            "upper_bound": 0.02924777491358661
          },
          "point_estimate": 0.021699774637732917,
          "standard_error": 0.005958008343134816
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.688533458560737,
            "upper_bound": 17.70554700858445
          },
          "point_estimate": 17.695737330388944,
          "standard_error": 0.004443448920570419
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.688345536454666,
            "upper_bound": 17.700695552768217
          },
          "point_estimate": 17.690364571798856,
          "standard_error": 0.003013822723664095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005870120614459723,
            "upper_bound": 0.017274962460929896
          },
          "point_estimate": 0.003393977875806063,
          "standard_error": 0.004358702415264566
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.68969585129024,
            "upper_bound": 17.699259388896007
          },
          "point_estimate": 17.693054443109855,
          "standard_error": 0.0025904873451700295
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002824263991668953,
            "upper_bound": 0.021047212285998936
          },
          "point_estimate": 0.014766775976599138,
          "standard_error": 0.005112861454313797
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.256997145319207,
            "upper_bound": 26.27034252954887
          },
          "point_estimate": 26.262463881595146,
          "standard_error": 0.003530253663682612
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.25482950173717,
            "upper_bound": 26.26624654490874
          },
          "point_estimate": 26.258280442832863,
          "standard_error": 0.0027776909031702233
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00034418895043130727,
            "upper_bound": 0.011975265763699772
          },
          "point_estimate": 0.0054066407136322235,
          "standard_error": 0.002812598117807891
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.257714176416144,
            "upper_bound": 26.264159784652364
          },
          "point_estimate": 26.26131399429982,
          "standard_error": 0.0016095779128399684
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002827650195953404,
            "upper_bound": 0.017618076365379525
          },
          "point_estimate": 0.011829587578640346,
          "standard_error": 0.0046187513230348
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299059.1485428051,
            "upper_bound": 299229.8914113323
          },
          "point_estimate": 299137.5996991283,
          "standard_error": 43.994673433725815
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299029.5860655738,
            "upper_bound": 299229.60909445747
          },
          "point_estimate": 299105.11731557373,
          "standard_error": 40.42566556805548
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.14158000846149,
            "upper_bound": 225.3468411836691
          },
          "point_estimate": 101.83456663469656,
          "standard_error": 55.121819867792475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299067.36363636365,
            "upper_bound": 299189.3271316741
          },
          "point_estimate": 299118.63612944435,
          "standard_error": 31.238471297762757
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.28258121726701,
            "upper_bound": 197.02483931965313
          },
          "point_estimate": 147.32314134658444,
          "standard_error": 37.17421132491997
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.75489261007765,
            "upper_bound": 15.788038989625315
          },
          "point_estimate": 15.772567287171256,
          "standard_error": 0.008498187743756012
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.75305014861141,
            "upper_bound": 15.795468288138188
          },
          "point_estimate": 15.774576995675556,
          "standard_error": 0.010203998077760972
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001305102374174299,
            "upper_bound": 0.04518898400466007
          },
          "point_estimate": 0.026364781218492176,
          "standard_error": 0.011666649088512865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.75807469046994,
            "upper_bound": 15.782451669778842
          },
          "point_estimate": 15.770396821408552,
          "standard_error": 0.006099250318343364
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012754350510614748,
            "upper_bound": 0.037199530458093856
          },
          "point_estimate": 0.028477380653367253,
          "standard_error": 0.006458771449664153
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.346820186283615,
            "upper_bound": 23.37964190885198
          },
          "point_estimate": 23.360069329274815,
          "standard_error": 0.008816885187772464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.34402597844584,
            "upper_bound": 23.36215693847886
          },
          "point_estimate": 23.35329019706807,
          "standard_error": 0.005108077512633753
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003888987937863774,
            "upper_bound": 0.023102455361427757
          },
          "point_estimate": 0.012235354114246371,
          "standard_error": 0.0051935771539550046
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.34578137702532,
            "upper_bound": 23.36035173027984
          },
          "point_estimate": 23.353464716086357,
          "standard_error": 0.003695298866762555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006762699315886438,
            "upper_bound": 0.04454886058205211
          },
          "point_estimate": 0.029534400801719252,
          "standard_error": 0.012471581048486736
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132737.0234535106,
            "upper_bound": 132900.2317940708
          },
          "point_estimate": 132807.89213474683,
          "standard_error": 42.514138556706314
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132719.41061290697,
            "upper_bound": 132861.474756691
          },
          "point_estimate": 132747.58059610706,
          "standard_error": 40.566996319761216
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.294507012611524,
            "upper_bound": 185.4869343833631
          },
          "point_estimate": 54.16926619158598,
          "standard_error": 46.79343315917356
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132717.28547397276,
            "upper_bound": 132787.0294536638
          },
          "point_estimate": 132739.22365153095,
          "standard_error": 18.174651084394934
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.60804186784887,
            "upper_bound": 198.89246225577952
          },
          "point_estimate": 141.13585979450332,
          "standard_error": 45.08266297220739
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.04817963889395,
            "upper_bound": 24.073072566243788
          },
          "point_estimate": 24.05781239416249,
          "standard_error": 0.006888267577194095
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.047535356863335,
            "upper_bound": 24.059491391760726
          },
          "point_estimate": 24.049839039557977,
          "standard_error": 0.0034038314656461544
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007306982656433796,
            "upper_bound": 0.01465468538345487
          },
          "point_estimate": 0.0036827757132991625,
          "standard_error": 0.004117758860675195
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.048502863507444,
            "upper_bound": 24.05382438942996
          },
          "point_estimate": 24.05033891426648,
          "standard_error": 0.0013682218813851818
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003439233377701225,
            "upper_bound": 0.03500649051806619
          },
          "point_estimate": 0.02295653185824586,
          "standard_error": 0.010501828006740516
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.864370799636227,
            "upper_bound": 20.877578362555365
          },
          "point_estimate": 20.87118334899592,
          "standard_error": 0.0033859998722781753
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.86246865057621,
            "upper_bound": 20.878594031058235
          },
          "point_estimate": 20.874511390792694,
          "standard_error": 0.004410400234595641
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014061202030520455,
            "upper_bound": 0.01844421043018666
          },
          "point_estimate": 0.011545506288682056,
          "standard_error": 0.004492701846344162
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.87029991231188,
            "upper_bound": 20.877573894087764
          },
          "point_estimate": 20.87457553282387,
          "standard_error": 0.001836755502449754
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005933011034234949,
            "upper_bound": 0.01447147362174792
          },
          "point_estimate": 0.011270732597842469,
          "standard_error": 0.002204179497437676
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-freq_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.870742160803715,
            "upper_bound": 49.03078138850848
          },
          "point_estimate": 48.94881993473721,
          "standard_error": 0.040901437952261095
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.82570245090433,
            "upper_bound": 49.05125929573886
          },
          "point_estimate": 48.93996889989273,
          "standard_error": 0.06767569595689482
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02956513962798477,
            "upper_bound": 0.2398522447315143
          },
          "point_estimate": 0.15984776375617002,
          "standard_error": 0.05226824513536171
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.82123701584948,
            "upper_bound": 48.94855991661663
          },
          "point_estimate": 48.8767566032203,
          "standard_error": 0.032477870322179164
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08189680624839013,
            "upper_bound": 0.17082630794555043
          },
          "point_estimate": 0.13653188196609667,
          "standard_error": 0.02349023385430688
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-repeated_"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.79375247735864,
            "upper_bound": 90.83969433711933
          },
          "point_estimate": 90.813247092429,
          "standard_error": 0.012036833985904966
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.79060948256053,
            "upper_bound": 90.82841177601937
          },
          "point_estimate": 90.79846127422783,
          "standard_error": 0.008587241646268645
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003682862911569876,
            "upper_bound": 0.0456604034712129
          },
          "point_estimate": 0.01643481971841305,
          "standard_error": 0.01065429638061162
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.78720996421292,
            "upper_bound": 90.82451829482248
          },
          "point_estimate": 90.80159876541047,
          "standard_error": 0.009904098817705102
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008835928184990328,
            "upper_bound": 0.05791404904800643
          },
          "point_estimate": 0.04029220884383136,
          "standard_error": 0.014119727382267055
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector_rare-alph"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.803945363337322,
            "upper_bound": 10.817696346163752
          },
          "point_estimate": 10.81074135888053,
          "standard_error": 0.003508885400921858
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.798111007844645,
            "upper_bound": 10.816977791224588
          },
          "point_estimate": 10.812734176162968,
          "standard_error": 0.004513723693802234
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001311038285142883,
            "upper_bound": 0.02335225779075548
          },
          "point_estimate": 0.006554718388541776,
          "standard_error": 0.005604244011051803
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.805723495294108,
            "upper_bound": 10.816100538119413
          },
          "point_estimate": 10.812114003574678,
          "standard_error": 0.002648175473316184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00611258188418014,
            "upper_bound": 0.015276783535788523
          },
          "point_estimate": 0.011689586043375527,
          "standard_error": 0.0023702142357814164
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33634.81454286155,
            "upper_bound": 33669.981929714006
          },
          "point_estimate": 33650.051431985376,
          "standard_error": 9.136696621665196
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33629.13845205057,
            "upper_bound": 33661.45531914894
          },
          "point_estimate": 33641.6271777675,
          "standard_error": 9.416777101402756
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6992498826977043,
            "upper_bound": 39.61294088785642
          },
          "point_estimate": 23.991149851591448,
          "standard_error": 8.737665686617293
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33630.944945594354,
            "upper_bound": 33652.19181989859
          },
          "point_estimate": 33638.82926102575,
          "standard_error": 5.4051802897536945
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.26474063525674,
            "upper_bound": 43.89058558724357
          },
          "point_estimate": 30.341871181669543,
          "standard_error": 10.082926588295171
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.799993210721755,
            "upper_bound": 24.824250037977844
          },
          "point_estimate": 24.810721261746693,
          "standard_error": 0.006241755780631513
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.79644309088689,
            "upper_bound": 24.82115074678763
          },
          "point_estimate": 24.80226376361318,
          "standard_error": 0.007594113504929528
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024546189715851582,
            "upper_bound": 0.03125477353267585
          },
          "point_estimate": 0.014792549958526474,
          "standard_error": 0.007051704963923039
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.794509103374292,
            "upper_bound": 24.809320003840057
          },
          "point_estimate": 24.799654933551523,
          "standard_error": 0.003808776247234051
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008485291447106618,
            "upper_bound": 0.02926811883929211
          },
          "point_estimate": 0.02075908383777756,
          "standard_error": 0.006168877676343572
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-repeated-rare-huge_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104904.4190656566,
            "upper_bound": 1105574.2791863575
          },
          "point_estimate": 1105222.7070574795,
          "standard_error": 172.01722625739208
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104778.281355219,
            "upper_bound": 1105637.5818181818
          },
          "point_estimate": 1105069.6616161617,
          "standard_error": 290.43678656527703
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.24918876810633,
            "upper_bound": 1039.068959734805
          },
          "point_estimate": 523.8997259261201,
          "standard_error": 267.83103038152626
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1104829.867892407,
            "upper_bound": 1105533.3286015615
          },
          "point_estimate": 1105195.768909878,
          "standard_error": 181.9500779203273
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322.92883065316124,
            "upper_bound": 728.8622089763554
          },
          "point_estimate": 573.9751378801438,
          "standard_error": 109.61341776207628
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-repeated-rare-small_never-tric"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2206.4096973047185,
            "upper_bound": 2207.6495356600913
          },
          "point_estimate": 2206.9851686249012,
          "standard_error": 0.3170783293977138
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2206.1462215477995,
            "upper_bound": 2207.6240237733937
          },
          "point_estimate": 2206.5932422862925,
          "standard_error": 0.42299139001696034
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.21047820142248913,
            "upper_bound": 1.7755915754572242
          },
          "point_estimate": 0.9117539883653716,
          "standard_error": 0.3934982002886667
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2206.240615753035,
            "upper_bound": 2206.9646393525545
          },
          "point_estimate": 2206.56562852019,
          "standard_error": 0.1839497196100259
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.49792326218903094,
            "upper_bound": 1.4100253883883327
          },
          "point_estimate": 1.055688920456209,
          "standard_error": 0.2531775052904886
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.36989867733716,
            "upper_bound": 8.403965736981684
          },
          "point_estimate": 8.384086198409111,
          "standard_error": 0.008996058337487291
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.366808912263345,
            "upper_bound": 8.392389119382162
          },
          "point_estimate": 8.373309676020925,
          "standard_error": 0.005847710792302049
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001694026261123613,
            "upper_bound": 0.0303487130472393
          },
          "point_estimate": 0.011038774427991465,
          "standard_error": 0.007139852469505055
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.366803811241882,
            "upper_bound": 8.381236081628911
          },
          "point_estimate": 8.371886215632541,
          "standard_error": 0.003715918186593757
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005804204318371209,
            "upper_bound": 0.04393407439050185
          },
          "point_estimate": 0.030046965527039338,
          "standard_error": 0.011336203178907402
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.484274597387953,
            "upper_bound": 7.514126793171977
          },
          "point_estimate": 7.498580172215202,
          "standard_error": 0.00764405278817247
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.478770997448949,
            "upper_bound": 7.519933174145159
          },
          "point_estimate": 7.494772080835492,
          "standard_error": 0.009921255636778169
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0042880867914504885,
            "upper_bound": 0.04129728871489623
          },
          "point_estimate": 0.026873211927516995,
          "standard_error": 0.009668943134337075
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.483309239009816,
            "upper_bound": 7.508492361571586
          },
          "point_estimate": 7.49410576327768,
          "standard_error": 0.006456585238643884
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01348211351186646,
            "upper_bound": 0.03272039068631422
          },
          "point_estimate": 0.025480681297981717,
          "standard_error": 0.004986041163700016
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.894999500565747,
            "upper_bound": 8.948692684714398
          },
          "point_estimate": 8.920030665058897,
          "standard_error": 0.013802597522646948
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.887900192787065,
            "upper_bound": 8.952079158396403
          },
          "point_estimate": 8.907598541371652,
          "standard_error": 0.018215522181661833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0038946241920452904,
            "upper_bound": 0.07060925242203468
          },
          "point_estimate": 0.03355503465930139,
          "standard_error": 0.01894584875454429
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.88233725061614,
            "upper_bound": 8.926864484502797
          },
          "point_estimate": 8.905736285203641,
          "standard_error": 0.01165212403118492
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02059714646568939,
            "upper_bound": 0.05986326248616535
          },
          "point_estimate": 0.04586313492518923,
          "standard_error": 0.010401766483369289
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.38987026305795,
            "upper_bound": 19.397099410290014
          },
          "point_estimate": 19.393387322586563,
          "standard_error": 0.001843914070484355
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.38665274349001,
            "upper_bound": 19.40101805036047
          },
          "point_estimate": 19.39214655534375,
          "standard_error": 0.0029381945543337307
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00010019168959960152,
            "upper_bound": 0.010439258401701347
          },
          "point_estimate": 0.008150982380325451,
          "standard_error": 0.0029632233309215784
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.389081746618867,
            "upper_bound": 19.397367077067248
          },
          "point_estimate": 19.392984700032997,
          "standard_error": 0.002161106807977904
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032514568897330396,
            "upper_bound": 0.007269293575860776
          },
          "point_estimate": 0.00616738235777372,
          "standard_error": 0.0009325714977908317
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.261824195551204,
            "upper_bound": 13.272379646405184
          },
          "point_estimate": 13.266530953110143,
          "standard_error": 0.00270749732448673
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.261627695996502,
            "upper_bound": 13.26854945970256
          },
          "point_estimate": 13.265568974524308,
          "standard_error": 0.001745949396314303
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009814449934724943,
            "upper_bound": 0.011494334079713711
          },
          "point_estimate": 0.0037816597330048176,
          "standard_error": 0.0026769219631653184
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.263805316706936,
            "upper_bound": 13.267438508056774
          },
          "point_estimate": 13.265668520420205,
          "standard_error": 0.000946872337920064
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023033471006602448,
            "upper_bound": 0.012995776212610164
          },
          "point_estimate": 0.008987868704859337,
          "standard_error": 0.0029577886400812973
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.14149726860175,
            "upper_bound": 19.151845883712365
          },
          "point_estimate": 19.145775800582378,
          "standard_error": 0.0027363938176863567
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.140864202713352,
            "upper_bound": 19.14784760862078
          },
          "point_estimate": 19.14182711532257,
          "standard_error": 0.0021047922997503195
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003877477805715651,
            "upper_bound": 0.009217460620743424
          },
          "point_estimate": 0.0029200287167485827,
          "standard_error": 0.0024894318637812447
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.1410053341523,
            "upper_bound": 19.14596267497075
          },
          "point_estimate": 19.14331103648051,
          "standard_error": 0.0012852720166818966
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002017883706885946,
            "upper_bound": 0.013420911004302555
          },
          "point_estimate": 0.009097399593255971,
          "standard_error": 0.00346016373573286
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.868701959675793,
            "upper_bound": 10.960459394090448
          },
          "point_estimate": 10.910648854052564,
          "standard_error": 0.023491077802641543
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.862554456555236,
            "upper_bound": 10.94776395120914
          },
          "point_estimate": 10.89763939572804,
          "standard_error": 0.02308335713444979
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012591175903019869,
            "upper_bound": 0.1118278355722829
          },
          "point_estimate": 0.06327457497574099,
          "standard_error": 0.025549565902512495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.883040143303276,
            "upper_bound": 10.930883294422348
          },
          "point_estimate": 10.907050553903808,
          "standard_error": 0.012283901497497833
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03167815233152285,
            "upper_bound": 0.10954251818342048
          },
          "point_estimate": 0.07823522075689045,
          "standard_error": 0.021865417891389103
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.746498213083246,
            "upper_bound": 15.77844912831866
          },
          "point_estimate": 15.761552968761844,
          "standard_error": 0.008166436413689969
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.743920874590932,
            "upper_bound": 15.77558947849332
          },
          "point_estimate": 15.761213046066104,
          "standard_error": 0.008551672912772102
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006638593601898018,
            "upper_bound": 0.04257630733098683
          },
          "point_estimate": 0.021882564239658155,
          "standard_error": 0.009327512016286376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.74951517053631,
            "upper_bound": 15.767175931969634
          },
          "point_estimate": 15.75945414248717,
          "standard_error": 0.004458173266561455
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011855454067214326,
            "upper_bound": 0.03762295424470707
          },
          "point_estimate": 0.02725434631744365,
          "standard_error": 0.006994532602940126
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.34529693892284,
            "upper_bound": 23.366245746647607
          },
          "point_estimate": 23.35555550228174,
          "standard_error": 0.005371787240944479
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.342109590489265,
            "upper_bound": 23.369917165517663
          },
          "point_estimate": 23.35447398287461,
          "standard_error": 0.006265259342066011
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0027426177194379013,
            "upper_bound": 0.03356883727063559
          },
          "point_estimate": 0.013322415809557153,
          "standard_error": 0.007821460863018924
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.34916445492783,
            "upper_bound": 23.365151952204627
          },
          "point_estimate": 23.35776654677951,
          "standard_error": 0.0039674272180012906
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009623854091844852,
            "upper_bound": 0.02286961133270938
          },
          "point_estimate": 0.017962159499034146,
          "standard_error": 0.0034209908444242103
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.02009389451252,
            "upper_bound": 11.111766702266006
          },
          "point_estimate": 11.067349273533422,
          "standard_error": 0.02346088031379258
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.0029132518359,
            "upper_bound": 11.13038890720878
          },
          "point_estimate": 11.06745376073466,
          "standard_error": 0.032538442012570945
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011466614479983765,
            "upper_bound": 0.13828580070886545
          },
          "point_estimate": 0.078344721100367,
          "standard_error": 0.03178934794129709
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.014217228812347,
            "upper_bound": 11.116610053174124
          },
          "point_estimate": 11.072103756433483,
          "standard_error": 0.027584916956438
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04324433419630603,
            "upper_bound": 0.09770878470836776
          },
          "point_estimate": 0.07808546234309835,
          "standard_error": 0.013948730873220168
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.04958988680057,
            "upper_bound": 24.077253692940435
          },
          "point_estimate": 24.05978969285662,
          "standard_error": 0.007950560977361028
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.048243963977797,
            "upper_bound": 24.05694937086021
          },
          "point_estimate": 24.05097222019981,
          "standard_error": 0.0029473768871082573
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006950961368778012,
            "upper_bound": 0.010546758162052012
          },
          "point_estimate": 0.005861311315170721,
          "standard_error": 0.0031349056744700617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.049747641319012,
            "upper_bound": 24.05581023531814
          },
          "point_estimate": 24.05226087229945,
          "standard_error": 0.001557497069287904
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00292420625585744,
            "upper_bound": 0.04077246176156155
          },
          "point_estimate": 0.026533556518592363,
          "standard_error": 0.013285265684686286
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.86208652412844,
            "upper_bound": 20.871356494037975
          },
          "point_estimate": 20.86624897008924,
          "standard_error": 0.0023836024116986707
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.859659710974025,
            "upper_bound": 20.87187036909398
          },
          "point_estimate": 20.863787287255256,
          "standard_error": 0.003080610913886766
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002716507516287797,
            "upper_bound": 0.013134319923965836
          },
          "point_estimate": 0.006212755891935227,
          "standard_error": 0.0032296482988122153
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.86144755065085,
            "upper_bound": 20.86970873665294
          },
          "point_estimate": 20.865877450254196,
          "standard_error": 0.0021666382289507783
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030470474234355425,
            "upper_bound": 0.010772342046235743
          },
          "point_estimate": 0.007951334078307365,
          "standard_error": 0.002086314912347149
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1186516.2360205457,
            "upper_bound": 1187289.0027553767
          },
          "point_estimate": 1186880.701728111,
          "standard_error": 198.59320311989023
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1186357.375,
            "upper_bound": 1187356.161290323
          },
          "point_estimate": 1186666.865591398,
          "standard_error": 305.9841922425597
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.58641772899396,
            "upper_bound": 1172.513392086987
          },
          "point_estimate": 675.3521863973453,
          "standard_error": 268.16374008046574
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1186282.1616479508,
            "upper_bound": 1186784.502611442
          },
          "point_estimate": 1186479.000502723,
          "standard_error": 129.55783870216456
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 350.60649373567117,
            "upper_bound": 854.6444572653098
          },
          "point_estimate": 663.5115876934993,
          "standard_error": 136.82836470004042
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1634488.929373706,
            "upper_bound": 1644071.9748559354
          },
          "point_estimate": 1639429.4500086266,
          "standard_error": 2442.050527252027
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1630789.1376811594,
            "upper_bound": 1645801.2862318845
          },
          "point_estimate": 1644183.2347826087,
          "standard_error": 4758.972313183978
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203.06140465589303,
            "upper_bound": 12919.189771290756
          },
          "point_estimate": 5306.640004049238,
          "standard_error": 3531.287907346246
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1631057.7899456522,
            "upper_bound": 1642252.5225351576
          },
          "point_estimate": 1636328.3437605873,
          "standard_error": 2950.0142784879276
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4989.765240563637,
            "upper_bound": 9178.43468874157
          },
          "point_estimate": 8126.883177682442,
          "standard_error": 1081.059903187148
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1457300.5874666665,
            "upper_bound": 1458757.427936508
          },
          "point_estimate": 1458015.492336508,
          "standard_error": 373.6668692792677
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1456960.92,
            "upper_bound": 1459078.5166666666
          },
          "point_estimate": 1457816.8085714283,
          "standard_error": 501.6518849777083
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280.8883078132349,
            "upper_bound": 2078.3559451020587
          },
          "point_estimate": 1270.199406782865,
          "standard_error": 458.6527084685449
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1457634.99522807,
            "upper_bound": 1458639.169124411
          },
          "point_estimate": 1458206.6439480518,
          "standard_error": 254.3175319100181
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 689.3420877651363,
            "upper_bound": 1562.927640652257
          },
          "point_estimate": 1242.6491728989538,
          "standard_error": 224.47255634838547
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398161.6568925984,
            "upper_bound": 399491.1357954193
          },
          "point_estimate": 398819.9088095239,
          "standard_error": 340.29070657085254
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397947.7612577639,
            "upper_bound": 399718.7605072464
          },
          "point_estimate": 398627.14456521743,
          "standard_error": 502.4756241946108
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 285.273721891904,
            "upper_bound": 1880.9461163890755
          },
          "point_estimate": 1372.9863772550389,
          "standard_error": 431.4795114580804
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397709.71570635657,
            "upper_bound": 399461.1919989175
          },
          "point_estimate": 398573.59223602485,
          "standard_error": 459.97771625619725
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 660.0303137949521,
            "upper_bound": 1431.9060151082847
          },
          "point_estimate": 1137.1451592271733,
          "standard_error": 198.45667357148457
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 625380.582692426,
            "upper_bound": 626619.7454022989
          },
          "point_estimate": 625975.8045600711,
          "standard_error": 316.6646720490772
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 625035.1402572524,
            "upper_bound": 627079.9252873564
          },
          "point_estimate": 625690.2586206896,
          "standard_error": 543.9733025149166
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.25527302925977,
            "upper_bound": 1682.5635149561206
          },
          "point_estimate": 1154.5144760549974,
          "standard_error": 417.62788543003217
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 625117.7068023365,
            "upper_bound": 626480.1021076002
          },
          "point_estimate": 625831.3981191223,
          "standard_error": 348.0210722180918
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 623.8629936002537,
            "upper_bound": 1263.279130185993
          },
          "point_estimate": 1056.838028873991,
          "standard_error": 161.37043174001263
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392647.80247980665,
            "upper_bound": 392882.25416026625
          },
          "point_estimate": 392744.7573054276,
          "standard_error": 62.02496712398997
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392641.53548387095,
            "upper_bound": 392798.7150537635
          },
          "point_estimate": 392667.273297491,
          "standard_error": 44.61564474107864
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.04838322703422,
            "upper_bound": 202.76281683041816
          },
          "point_estimate": 91.48578426287344,
          "standard_error": 54.01023584860263
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392619.7840245776,
            "upper_bound": 392733.69962863176
          },
          "point_estimate": 392678.7518223711,
          "standard_error": 29.36166842285779
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.014000131518394,
            "upper_bound": 306.55443246430156
          },
          "point_estimate": 206.67831133377035,
          "standard_error": 79.3196678681993
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481554.7920626566,
            "upper_bound": 481890.6033834587
          },
          "point_estimate": 481723.6350250627,
          "standard_error": 85.79208010333299
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481513.78289473685,
            "upper_bound": 481967.94868421054
          },
          "point_estimate": 481710.3615288221,
          "standard_error": 92.29212011990964
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.88688338650337,
            "upper_bound": 563.7358807812504
          },
          "point_estimate": 209.82073456442268,
          "standard_error": 139.5650920672109
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481526.24207111134,
            "upper_bound": 481816.1312008469
          },
          "point_estimate": 481653.321872864,
          "standard_error": 72.90913697293918
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153.91365307566713,
            "upper_bound": 361.1881161365802
          },
          "point_estimate": 285.50297130204575,
          "standard_error": 53.61010277150087
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1015042.233833168,
            "upper_bound": 1015984.9824922286
          },
          "point_estimate": 1015479.4736044972,
          "standard_error": 241.88724050277665
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1014823.4018518518,
            "upper_bound": 1015986.6888888888
          },
          "point_estimate": 1015417.0967261904,
          "standard_error": 252.97678782488683
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.485393190997875,
            "upper_bound": 1356.7848925789006
          },
          "point_estimate": 641.8840592988207,
          "standard_error": 322.51634516007215
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1015108.9333280076,
            "upper_bound": 1015688.9073517204
          },
          "point_estimate": 1015373.9945887446,
          "standard_error": 149.69510692841143
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.1050929815093,
            "upper_bound": 1049.9750487203637
          },
          "point_estimate": 805.9140156365503,
          "standard_error": 188.0996758789771
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 330039.06559259264,
            "upper_bound": 330485.63424503076
          },
          "point_estimate": 330212.18551086803,
          "standard_error": 123.00669072760432
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 330002.3412162162,
            "upper_bound": 330230.7630630631
          },
          "point_estimate": 330077.5793293293,
          "standard_error": 60.48585506156092
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.724601412050422,
            "upper_bound": 262.6581311176175
          },
          "point_estimate": 138.38282167980404,
          "standard_error": 68.33251332870132
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 330025.4159317212,
            "upper_bound": 330190.15455101023
          },
          "point_estimate": 330107.71470691473,
          "standard_error": 43.121912853519206
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.05129820322351,
            "upper_bound": 624.6967878713496
          },
          "point_estimate": 409.4800871899263,
          "standard_error": 185.38724403019432
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 827647.8988861834,
            "upper_bound": 832149.304761003
          },
          "point_estimate": 829976.9561462845,
          "standard_error": 1133.3821233860676
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 827221.803030303,
            "upper_bound": 832383.2215909091
          },
          "point_estimate": 832145.8683261183,
          "standard_error": 1495.0745261890702
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.7944495954413,
            "upper_bound": 5944.361044466438
          },
          "point_estimate": 415.3144408084774,
          "standard_error": 1497.0174353994116
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 831323.2425765189,
            "upper_bound": 832261.6633512168
          },
          "point_estimate": 832000.8322904368,
          "standard_error": 251.48391223187664
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246.72949918287844,
            "upper_bound": 4564.330386463557
          },
          "point_estimate": 3776.393406875351,
          "standard_error": 895.9806953945761
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400055.0743894993,
            "upper_bound": 400473.0558641635
          },
          "point_estimate": 400274.8521214896,
          "standard_error": 107.00308110093098
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 400052.78266178264,
            "upper_bound": 400538.2490842491
          },
          "point_estimate": 400324.8467032967,
          "standard_error": 146.0548027241597
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.02159308035537,
            "upper_bound": 628.5836946096157
          },
          "point_estimate": 335.2894409705232,
          "standard_error": 130.8958245276763
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399911.12948427233,
            "upper_bound": 400498.9697992699
          },
          "point_estimate": 400192.2556871699,
          "standard_error": 147.0549632567954
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184.6062915475191,
            "upper_bound": 470.4265236429174
          },
          "point_estimate": 355.8563042266955,
          "standard_error": 77.33410618288319
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174199.9326738152,
            "upper_bound": 174412.8904528366
          },
          "point_estimate": 174296.59696798815,
          "standard_error": 54.92103522507308
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174159.14928229665,
            "upper_bound": 174429.49491626793
          },
          "point_estimate": 174242.51116427433,
          "standard_error": 65.70606765546785
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.830276260117958,
            "upper_bound": 262.11356671019115
          },
          "point_estimate": 121.98658785823557,
          "standard_error": 72.07716020588391
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174157.7018316717,
            "upper_bound": 174269.593207618
          },
          "point_estimate": 174198.13045423475,
          "standard_error": 29.219155791871497
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.62150226700689,
            "upper_bound": 240.394702362462
          },
          "point_estimate": 183.7736772942101,
          "standard_error": 46.893451346503376
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233006.2035722753,
            "upper_bound": 233185.63834041049
          },
          "point_estimate": 233084.24053331316,
          "standard_error": 46.68809524065461
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232965.52957233848,
            "upper_bound": 233145.01326963905
          },
          "point_estimate": 233049.31802901623,
          "standard_error": 52.18465532204356
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.833548657348846,
            "upper_bound": 209.74697525714723
          },
          "point_estimate": 113.62597632669758,
          "standard_error": 50.97056458645166
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232996.34747937173,
            "upper_bound": 233095.03114541725
          },
          "point_estimate": 233052.5583092067,
          "standard_error": 25.246685756920304
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.88807646454344,
            "upper_bound": 223.1155847704351
          },
          "point_estimate": 155.50927589675837,
          "standard_error": 49.83039034810423
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175436.3218349359,
            "upper_bound": 175509.12303685895
          },
          "point_estimate": 175469.33982371795,
          "standard_error": 18.656917924517288
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175425.40745192306,
            "upper_bound": 175491.84555288462
          },
          "point_estimate": 175467.23517628206,
          "standard_error": 20.30705714406998
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.322713331543716,
            "upper_bound": 89.28655406390186
          },
          "point_estimate": 48.48624625458942,
          "standard_error": 20.285376850838055
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175427.0740203193,
            "upper_bound": 175475.47861576497
          },
          "point_estimate": 175455.3876998002,
          "standard_error": 12.273941847591614
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.080646527050785,
            "upper_bound": 87.0000670358704
          },
          "point_estimate": 62.15764066564085,
          "standard_error": 17.675969361356593
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-repeated-rare-huge_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 675811.1090410054,
            "upper_bound": 676217.194404762
          },
          "point_estimate": 675996.4561177249,
          "standard_error": 104.48837727035868
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 675736.7901234567,
            "upper_bound": 676186.6261574074
          },
          "point_estimate": 675957.5996472663,
          "standard_error": 116.55402633562578
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.17283007833181,
            "upper_bound": 530.2611468359828
          },
          "point_estimate": 297.63599767895215,
          "standard_error": 119.95483141301384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 675724.6974085686,
            "upper_bound": 675975.4561930264
          },
          "point_estimate": 675824.1208754209,
          "standard_error": 64.57998154227725
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150.7293622122115,
            "upper_bound": 473.9480345133883
          },
          "point_estimate": 347.9279326538752,
          "standard_error": 90.59199975909124
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-repeated-rare-small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1366.099153868813,
            "upper_bound": 1367.0494717270292
          },
          "point_estimate": 1366.5410604824351,
          "standard_error": 0.24512301888456076
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1365.9718984456215,
            "upper_bound": 1367.1164405866807
          },
          "point_estimate": 1366.3036512767658,
          "standard_error": 0.27193650292367744
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12262243272334648,
            "upper_bound": 1.333669603948217
          },
          "point_estimate": 0.6026387560691869,
          "standard_error": 0.2991466230272899
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1366.050915995839,
            "upper_bound": 1366.5186765732892
          },
          "point_estimate": 1366.2721222564307,
          "standard_error": 0.11918577912655362
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.332345172146619,
            "upper_bound": 1.0574593144583777
          },
          "point_estimate": 0.8150013700299289,
          "standard_error": 0.18712052844023172
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1194009.5874526368,
            "upper_bound": 1196263.3324731183
          },
          "point_estimate": 1194914.96312468,
          "standard_error": 606.461127192812
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1193754.0609318996,
            "upper_bound": 1194920.1059907833
          },
          "point_estimate": 1194628.4811827955,
          "standard_error": 329.5615229604123
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.90746914089324,
            "upper_bound": 1613.3450982392071
          },
          "point_estimate": 567.4860637154978,
          "standard_error": 402.95295135493274
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1194044.156045082,
            "upper_bound": 1194856.5118902335
          },
          "point_estimate": 1194473.4436531211,
          "standard_error": 213.16884706427737
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 410.14903072262246,
            "upper_bound": 3065.2202916435376
          },
          "point_estimate": 2022.930079272726,
          "standard_error": 865.5500379911756
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1329528.840768814,
            "upper_bound": 1334942.6427976186
          },
          "point_estimate": 1331906.0493324832,
          "standard_error": 1403.8715213976218
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1328201.0142857144,
            "upper_bound": 1333680.6322278911
          },
          "point_estimate": 1331254.6392857144,
          "standard_error": 1204.9226046911954
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 470.5136916467369,
            "upper_bound": 5728.462466156616
          },
          "point_estimate": 3329.0862402720377,
          "standard_error": 1580.0645704318054
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1330150.5395807896,
            "upper_bound": 1332639.6181818182
          },
          "point_estimate": 1331331.140538033,
          "standard_error": 624.5459652221906
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1588.83384132304,
            "upper_bound": 6661.582654266773
          },
          "point_estimate": 4681.235830916258,
          "standard_error": 1502.2675193607042
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1237320.335901389,
            "upper_bound": 1240565.2206084656
          },
          "point_estimate": 1238798.320572751,
          "standard_error": 835.0153779210181
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1236742.711111111,
            "upper_bound": 1240429.1232804232
          },
          "point_estimate": 1238242.021527778,
          "standard_error": 889.7007162033683
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 488.9775328191988,
            "upper_bound": 4176.235162801503
          },
          "point_estimate": 2118.9977757135757,
          "standard_error": 967.7070403054038
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1236694.110896025,
            "upper_bound": 1239814.672556391
          },
          "point_estimate": 1238089.0658874458,
          "standard_error": 805.2103821460569
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1066.5781335055042,
            "upper_bound": 3740.203312214472
          },
          "point_estimate": 2789.247307361328,
          "standard_error": 720.5560706620389
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326349.3491286671,
            "upper_bound": 327529.68557645084
          },
          "point_estimate": 326903.31276608555,
          "standard_error": 303.088543541062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326180.4243055555,
            "upper_bound": 328222.9910714286
          },
          "point_estimate": 326351.1434151786,
          "standard_error": 501.4686550717065
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.07245705143929,
            "upper_bound": 1540.6050429612192
          },
          "point_estimate": 413.3738952132311,
          "standard_error": 389.5139139275219
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326184.73934475007,
            "upper_bound": 327219.15312758053
          },
          "point_estimate": 326515.1995825603,
          "standard_error": 271.8007757801942
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243.23638158569028,
            "upper_bound": 1146.6659642147656
          },
          "point_estimate": 1009.6177682145194,
          "standard_error": 186.98863774706717
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246364.5294237988,
            "upper_bound": 247141.29319139456
          },
          "point_estimate": 246743.1985454204,
          "standard_error": 199.2203205809164
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246186.61283783783,
            "upper_bound": 247406.9069069069
          },
          "point_estimate": 246488.49169481985,
          "standard_error": 336.3006966371463
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.22927297583584,
            "upper_bound": 1057.5256496936288
          },
          "point_estimate": 780.0125838378233,
          "standard_error": 261.0567797004017
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246101.4632112833,
            "upper_bound": 247031.6435298935
          },
          "point_estimate": 246504.56686556689,
          "standard_error": 246.23536280405557
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401.0823747780983,
            "upper_bound": 796.9243179286025
          },
          "point_estimate": 663.41789419141,
          "standard_error": 101.6155555054372
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 429632.4832946779,
            "upper_bound": 431398.85818635626
          },
          "point_estimate": 430488.64707609714,
          "standard_error": 454.36861207703595
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 429276.0598739496,
            "upper_bound": 431799.7247058824
          },
          "point_estimate": 429988.7817647059,
          "standard_error": 811.2890469539406
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161.5899277586652,
            "upper_bound": 2344.876629899548
          },
          "point_estimate": 1685.793321335936,
          "standard_error": 615.1450899971843
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 429171.2948646175,
            "upper_bound": 430273.44323832623
          },
          "point_estimate": 429588.5248892284,
          "standard_error": 279.01764565505687
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 904.246872036712,
            "upper_bound": 1849.1453583002008
          },
          "point_estimate": 1513.2831764039863,
          "standard_error": 243.312584087618
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236901.12301329625,
            "upper_bound": 237361.73197143627
          },
          "point_estimate": 237105.2973639456,
          "standard_error": 118.7842223820914
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236857.79123376624,
            "upper_bound": 237241.82007575757
          },
          "point_estimate": 237036.96525974025,
          "standard_error": 100.98674090237138
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.04039772925468,
            "upper_bound": 529.4869937815712
          },
          "point_estimate": 234.05171759801712,
          "standard_error": 114.8322618536919
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236869.0480272109,
            "upper_bound": 237138.3115729129
          },
          "point_estimate": 236978.13506493508,
          "standard_error": 69.35475241703364
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.70717524540535,
            "upper_bound": 561.0256072207437
          },
          "point_estimate": 394.8631647586253,
          "standard_error": 122.36948784446402
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 528103.627715666,
            "upper_bound": 530930.7717132505
          },
          "point_estimate": 529674.0389032667,
          "standard_error": 723.235716607562
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 529183.7010869565,
            "upper_bound": 531013.7658730159
          },
          "point_estimate": 529924.5493558776,
          "standard_error": 408.2662657773338
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 212.6682227461995,
            "upper_bound": 3065.510535902356
          },
          "point_estimate": 757.0943320661586,
          "standard_error": 735.9645421630088
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 529789.5415663504,
            "upper_bound": 531748.5714061888
          },
          "point_estimate": 530652.0455110107,
          "standard_error": 514.5704154741188
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 626.479588317366,
            "upper_bound": 3492.4123282912474
          },
          "point_estimate": 2414.765133051082,
          "standard_error": 802.8300903182443
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112349.04244710215,
            "upper_bound": 112570.10112594308
          },
          "point_estimate": 112458.74217249655,
          "standard_error": 56.18832609301458
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112332.83055555556,
            "upper_bound": 112544.37924382716
          },
          "point_estimate": 112483.15389231824,
          "standard_error": 43.878577163467405
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.935954332604236,
            "upper_bound": 306.16192808306937
          },
          "point_estimate": 102.84672975846324,
          "standard_error": 79.24776941900114
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112294.9327237258,
            "upper_bound": 112486.5933054574
          },
          "point_estimate": 112403.15072150072,
          "standard_error": 48.77273858229496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.28666637448924,
            "upper_bound": 257.16257239489795
          },
          "point_estimate": 186.6429526371193,
          "standard_error": 46.20639943329516
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118507.81409673105,
            "upper_bound": 118923.43899862983
          },
          "point_estimate": 118696.8735824156,
          "standard_error": 106.82206181267443
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118443.04336319218,
            "upper_bound": 118922.63714389122
          },
          "point_estimate": 118535.2750271444,
          "standard_error": 134.80317296363162
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.38119638912594,
            "upper_bound": 544.7739285476547
          },
          "point_estimate": 204.554406905895,
          "standard_error": 143.07749767323278
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118491.34182788253,
            "upper_bound": 118811.33125574396
          },
          "point_estimate": 118637.22098227506,
          "standard_error": 80.31589789022475
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 133.59401693401475,
            "upper_bound": 469.7000928258579
          },
          "point_estimate": 356.6833283202872,
          "standard_error": 88.14170288026175
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193807.6153584515,
            "upper_bound": 194287.86044968967
          },
          "point_estimate": 194039.60694444444,
          "standard_error": 123.18182429821888
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193614.46276595743,
            "upper_bound": 194306.15638297872
          },
          "point_estimate": 194053.74361702127,
          "standard_error": 147.35164898322756
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.366505968245523,
            "upper_bound": 684.2941054754733
          },
          "point_estimate": 454.9770071619035,
          "standard_error": 191.0253897870403
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193990.49670814932,
            "upper_bound": 194389.06663783704
          },
          "point_estimate": 194205.76971539095,
          "standard_error": 100.59083807835206
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224.72082994510663,
            "upper_bound": 530.7031834482902
          },
          "point_estimate": 409.5722508870786,
          "standard_error": 82.1477322059481
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271832.93073558545,
            "upper_bound": 272318.6424377961
          },
          "point_estimate": 272053.8772728619,
          "standard_error": 125.20389030743804
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271757.7174840085,
            "upper_bound": 272257.43134328356
          },
          "point_estimate": 272025.6163349917,
          "standard_error": 112.35796114659324
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.838907945556596,
            "upper_bound": 625.6135468533764
          },
          "point_estimate": 306.1386474505213,
          "standard_error": 155.59752443356032
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271806.42461444315,
            "upper_bound": 272052.1887143708
          },
          "point_estimate": 271941.4122892033,
          "standard_error": 62.95915280625902
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.77179640511102,
            "upper_bound": 576.9678418103587
          },
          "point_estimate": 416.6220722591753,
          "standard_error": 116.46487320755143
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289737.5470250693,
            "upper_bound": 291198.0725605316
          },
          "point_estimate": 290414.9091717057,
          "standard_error": 374.02050894725414
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289437.89654195006,
            "upper_bound": 291270.0307539683
          },
          "point_estimate": 290149.5081128748,
          "standard_error": 451.7373354152916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 268.7237666578055,
            "upper_bound": 2089.0268995790193
          },
          "point_estimate": 1163.998848779345,
          "standard_error": 460.7021960712554
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289463.5628486629,
            "upper_bound": 290363.3417201396
          },
          "point_estimate": 289859.535394764,
          "standard_error": 227.79525916042655
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 515.6710185116011,
            "upper_bound": 1589.6828245607385
          },
          "point_estimate": 1244.732565608649,
          "standard_error": 272.49345164117113
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 315964.38430573663,
            "upper_bound": 316558.4518831522
          },
          "point_estimate": 316237.4502439613,
          "standard_error": 153.27144828090516
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 315870.59826086956,
            "upper_bound": 316585.40597826085
          },
          "point_estimate": 316007.74260869564,
          "standard_error": 203.10415276017517
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.81786701994458,
            "upper_bound": 817.7478013878718
          },
          "point_estimate": 300.3097208133346,
          "standard_error": 210.26957136007508
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 315919.5209342697,
            "upper_bound": 316203.52583120205
          },
          "point_estimate": 316035.48219085264,
          "standard_error": 72.81270852265123
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.3711603751623,
            "upper_bound": 660.3021995118373
          },
          "point_estimate": 508.7838089523564,
          "standard_error": 119.09996415592114
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395350.5679080615,
            "upper_bound": 396005.60167572467
          },
          "point_estimate": 395597.6189583334,
          "standard_error": 184.6812942657368
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395311.3923913044,
            "upper_bound": 395522.37703804346
          },
          "point_estimate": 395454.7826086957,
          "standard_error": 62.1248292257402
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.448635069576785,
            "upper_bound": 300.67960085755647
          },
          "point_estimate": 127.93803713075656,
          "standard_error": 90.32813914456452
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395347.9211246987,
            "upper_bound": 395492.0387048229
          },
          "point_estimate": 395421.1197910785,
          "standard_error": 38.04150598126889
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.87801792713182,
            "upper_bound": 945.34734354545
          },
          "point_estimate": 615.0887352779049,
          "standard_error": 298.4141891727687
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272515.29391376453,
            "upper_bound": 273044.480984971
          },
          "point_estimate": 272730.9251844942,
          "standard_error": 142.17801589809437
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272468.1098258706,
            "upper_bound": 272837.90671641787
          },
          "point_estimate": 272526.2867433665,
          "standard_error": 102.67127820598589
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.831128279665396,
            "upper_bound": 405.4449920556816
          },
          "point_estimate": 114.5432798010515,
          "standard_error": 119.96331688374164
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272480.32923574717,
            "upper_bound": 272602.5145127715
          },
          "point_estimate": 272518.7562899787,
          "standard_error": 32.039021388130294
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.38466792453632,
            "upper_bound": 709.1357688447699
          },
          "point_estimate": 475.583703732329,
          "standard_error": 191.89922734212485
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122812.4223396952,
            "upper_bound": 124324.82984619684
          },
          "point_estimate": 123550.00670861294,
          "standard_error": 388.5237942568155
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122558.86744966442,
            "upper_bound": 124901.7244966443
          },
          "point_estimate": 122993.59437919463,
          "standard_error": 734.023187351399
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.60855261371236,
            "upper_bound": 1995.834230967865
          },
          "point_estimate": 1201.2473995628611,
          "standard_error": 554.281037218439
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123534.50312616138,
            "upper_bound": 124883.61877112332
          },
          "point_estimate": 124424.9782358581,
          "standard_error": 342.3803568147481
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 813.5915232114307,
            "upper_bound": 1506.290421531397
          },
          "point_estimate": 1296.2948215293172,
          "standard_error": 175.98183792355815
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203144.4873973575,
            "upper_bound": 203680.303075286
          },
          "point_estimate": 203382.15406402416,
          "standard_error": 137.87390982384397
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203035.0027932961,
            "upper_bound": 203589.9233838787
          },
          "point_estimate": 203320.73432650528,
          "standard_error": 134.87840226260653
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.23711710001189,
            "upper_bound": 651.4244135746011
          },
          "point_estimate": 399.7242627172528,
          "standard_error": 137.94011979648766
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203165.2267476073,
            "upper_bound": 203499.32339385475
          },
          "point_estimate": 203365.7813248204,
          "standard_error": 85.07348135661594
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184.4511034525598,
            "upper_bound": 649.9992627765131
          },
          "point_estimate": 458.0416835238635,
          "standard_error": 139.41623020400343
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150256.72122343563,
            "upper_bound": 150981.97523858718
          },
          "point_estimate": 150631.51128837073,
          "standard_error": 184.21368560648295
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150398.91520316806,
            "upper_bound": 151131.1611570248
          },
          "point_estimate": 150541.21723730816,
          "standard_error": 177.8105106710884
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.456148485914929,
            "upper_bound": 1049.7227730475602
          },
          "point_estimate": 308.0020780442711,
          "standard_error": 269.4971566741364
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150548.8095247397,
            "upper_bound": 151062.04890245432
          },
          "point_estimate": 150796.04618439413,
          "standard_error": 131.69437365049666
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 275.259266816012,
            "upper_bound": 834.8520094264112
          },
          "point_estimate": 612.6167092672143,
          "standard_error": 148.41224248347586
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47618.33580657492,
            "upper_bound": 47691.23218354085
          },
          "point_estimate": 47652.99361220329,
          "standard_error": 18.633505286364016
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47595.68414154653,
            "upper_bound": 47689.83413426533
          },
          "point_estimate": 47658.14826343382,
          "standard_error": 25.857188767444143
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.093267692963767,
            "upper_bound": 115.7003628541613
          },
          "point_estimate": 59.369093441405184,
          "standard_error": 25.167628838535467
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47623.397344075245,
            "upper_bound": 47690.811736908
          },
          "point_estimate": 47660.664948681726,
          "standard_error": 16.862599812189284
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.67983253498242,
            "upper_bound": 82.28554490296266
          },
          "point_estimate": 62.20135345284216,
          "standard_error": 13.686202572600571
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1113261.8044312168,
            "upper_bound": 1116718.7537765752
          },
          "point_estimate": 1114893.0562469936,
          "standard_error": 887.3218764263091
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1112484.6505050506,
            "upper_bound": 1117373.787878788
          },
          "point_estimate": 1114203.9094696972,
          "standard_error": 1395.553845628874
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 384.0086185360169,
            "upper_bound": 4960.447103601264
          },
          "point_estimate": 2658.940718450797,
          "standard_error": 1419.9751423994778
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1112587.691584306,
            "upper_bound": 1114542.69008223
          },
          "point_estimate": 1113349.812908304,
          "standard_error": 505.3909497275134
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1511.5539636358192,
            "upper_bound": 3660.0981875732023
          },
          "point_estimate": 2944.4092510107207,
          "standard_error": 561.9468532492044
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125144.01100839676,
            "upper_bound": 125428.97292148376
          },
          "point_estimate": 125285.44618352152,
          "standard_error": 72.82845071018741
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125097.26280477826,
            "upper_bound": 125479.27641752578
          },
          "point_estimate": 125257.24037800689,
          "standard_error": 89.98276079440078
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.84455425196883,
            "upper_bound": 408.696599617029
          },
          "point_estimate": 244.8607474312068,
          "standard_error": 99.1588367612282
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125187.21387808652,
            "upper_bound": 125434.1299029791
          },
          "point_estimate": 125293.76403802384,
          "standard_error": 62.5045628410081
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.47934025881304,
            "upper_bound": 309.27669351895923
          },
          "point_estimate": 242.5983904447345,
          "standard_error": 45.30670007672724
        }
      }
    },
    "memrmem/stud/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/stud_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24142.91021539833,
            "upper_bound": 24270.05585934042
          },
          "point_estimate": 24207.70863374545,
          "standard_error": 32.74101257843442
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24102.405610889775,
            "upper_bound": 24300.25019920319
          },
          "point_estimate": 24233.263649107274,
          "standard_error": 59.91060759297215
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.436676182711063,
            "upper_bound": 175.59078279897457
          },
          "point_estimate": 120.1979534636687,
          "standard_error": 44.79012564081302
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24091.579298568173,
            "upper_bound": 24226.399467076055
          },
          "point_estimate": 24146.66164260706,
          "standard_error": 34.55684419198129
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.42261685110527,
            "upper_bound": 128.84867241873997
          },
          "point_estimate": 109.50994737538528,
          "standard_error": 15.56191068795247
        }
      }
    },
    "memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/stud_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24543.04656065143,
            "upper_bound": 24619.78980196497
          },
          "point_estimate": 24581.91936030451,
          "standard_error": 19.637878407721736
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24518.434624971676,
            "upper_bound": 24649.46108089735
          },
          "point_estimate": 24582.907545887152,
          "standard_error": 36.4786975134288
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.07050935478479,
            "upper_bound": 105.68119571725546
          },
          "point_estimate": 97.12991005330008,
          "standard_error": 25.14255726848297
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24550.13103559349,
            "upper_bound": 24634.0380156714
          },
          "point_estimate": 24590.63672384719,
          "standard_error": 21.143093280030588
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.77562269815889,
            "upper_bound": 76.39909628109193
          },
          "point_estimate": 65.55813359059933,
          "standard_error": 8.604129579363807
        }
      }
    },
    "memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/stud_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 606357.2574712961,
            "upper_bound": 607486.5589629628
          },
          "point_estimate": 606886.2771296295,
          "standard_error": 289.22038842477497
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 606118.9366666666,
            "upper_bound": 607641.1854166667
          },
          "point_estimate": 606586.2481481482,
          "standard_error": 362.547529937752
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.42418195147695,
            "upper_bound": 1539.342712532287
          },
          "point_estimate": 781.9664686173661,
          "standard_error": 362.8265245829346
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 606536.5572223503,
            "upper_bound": 608045.2227404193
          },
          "point_estimate": 607271.575887446,
          "standard_error": 422.09312644226645
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 440.4263618829803,
            "upper_bound": 1228.1471309340848
          },
          "point_estimate": 962.3724794579282,
          "standard_error": 201.91384294915665
        }
      }
    },
    "memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/stud_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1336.6095994865368,
            "upper_bound": 1340.2592002190029
          },
          "point_estimate": 1338.5462454111216,
          "standard_error": 0.934872431096756
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1337.122079525354,
            "upper_bound": 1339.6715556210693
          },
          "point_estimate": 1338.9371604405324,
          "standard_error": 0.5633874176959442
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.044965351501106525,
            "upper_bound": 5.195690364162773
          },
          "point_estimate": 1.0634996384009614,
          "standard_error": 1.259784628164794
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1337.3911391216811,
            "upper_bound": 1339.4498985827645
          },
          "point_estimate": 1338.749019273769,
          "standard_error": 0.5313882522097226
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4707706410750475,
            "upper_bound": 4.37160047068887
          },
          "point_estimate": 3.113316957153536,
          "standard_error": 0.8994790166246944
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.4432482155268,
            "upper_bound": 29.530353399408824
          },
          "point_estimate": 29.49140748479498,
          "standard_error": 0.02251617508323605
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.451603385832716,
            "upper_bound": 29.53671212891103
          },
          "point_estimate": 29.524661746145483,
          "standard_error": 0.019980545666501988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002581303738800349,
            "upper_bound": 0.09900571940845349
          },
          "point_estimate": 0.018830281757537606,
          "standard_error": 0.023899998885734245
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.39457652311393,
            "upper_bound": 29.5312709283506
          },
          "point_estimate": 29.461514491363825,
          "standard_error": 0.03517843460464442
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013529594927306628,
            "upper_bound": 0.09944225354845004
          },
          "point_estimate": 0.07514325962867391,
          "standard_error": 0.022214937347056343
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.37354706763991,
            "upper_bound": 32.43947387393075
          },
          "point_estimate": 32.41172423421843,
          "standard_error": 0.01725791279182551
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.395542492558064,
            "upper_bound": 32.442084662780815
          },
          "point_estimate": 32.43688278481865,
          "standard_error": 0.01273037668873251
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002748641142978148,
            "upper_bound": 0.061291116217409146
          },
          "point_estimate": 0.01126676613212372,
          "standard_error": 0.01690236393322113
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.39429857543365,
            "upper_bound": 32.438915858130926
          },
          "point_estimate": 32.421156816659746,
          "standard_error": 0.012003271536679022
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010848873945569971,
            "upper_bound": 0.08310319869832399
          },
          "point_estimate": 0.057573649357902816,
          "standard_error": 0.020659216432660728
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.077217718579774,
            "upper_bound": 23.087521400114117
          },
          "point_estimate": 23.08202086097597,
          "standard_error": 0.0026427289209916926
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.075548905553394,
            "upper_bound": 23.089376208918974
          },
          "point_estimate": 23.0785635493527,
          "standard_error": 0.003329399751883863
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008578581009286625,
            "upper_bound": 0.014251040496491276
          },
          "point_estimate": 0.005600110859265334,
          "standard_error": 0.0033015366498683183
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.07487177265418,
            "upper_bound": 23.081930774045905
          },
          "point_estimate": 23.077249674137697,
          "standard_error": 0.0018232768352984868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0033239863835713424,
            "upper_bound": 0.010837143855449384
          },
          "point_estimate": 0.008807767252654381,
          "standard_error": 0.0018613541097372345
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.48468267397573,
            "upper_bound": 27.52054438081063
          },
          "point_estimate": 27.49816281763503,
          "standard_error": 0.010147795324922406
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.484336124374774,
            "upper_bound": 27.494955143856775
          },
          "point_estimate": 27.48871224834118,
          "standard_error": 0.0035898556366304305
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008053997131494656,
            "upper_bound": 0.016793892092983567
          },
          "point_estimate": 0.005123177741137943,
          "standard_error": 0.004516347260402255
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.48708417027667,
            "upper_bound": 27.494882311329263
          },
          "point_estimate": 27.490800347070433,
          "standard_error": 0.0020503498160714824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003799432423836816,
            "upper_bound": 0.05183778084202546
          },
          "point_estimate": 0.03376041917429624,
          "standard_error": 0.016463541841178608
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.919039083358655,
            "upper_bound": 35.943461918909634
          },
          "point_estimate": 35.92859246891298,
          "standard_error": 0.006695016047147913
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.91491479672644,
            "upper_bound": 35.92954431926657
          },
          "point_estimate": 35.922972826214036,
          "standard_error": 0.003792311758285304
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003368450077422897,
            "upper_bound": 0.015561961034203448
          },
          "point_estimate": 0.009435542101150052,
          "standard_error": 0.0040210337906531135
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.91785709916436,
            "upper_bound": 35.927377384567556
          },
          "point_estimate": 35.92206633511177,
          "standard_error": 0.0024590141538680596
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004437673257549932,
            "upper_bound": 0.03391581996220355
          },
          "point_estimate": 0.022291804038180003,
          "standard_error": 0.009914754699055115
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.19554176210576,
            "upper_bound": 65.37846052967052
          },
          "point_estimate": 65.29715387415685,
          "standard_error": 0.04535065288974171
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.29734103403634,
            "upper_bound": 65.33235027440179
          },
          "point_estimate": 65.3161894669397,
          "standard_error": 0.01191688688689585
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017570436734699233,
            "upper_bound": 0.17593542904590606
          },
          "point_estimate": 0.016846856942646943,
          "standard_error": 0.03693391242858884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.1056815302346,
            "upper_bound": 65.32061584185097
          },
          "point_estimate": 65.24859111711207,
          "standard_error": 0.06245751848321168
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01138935448539704,
            "upper_bound": 0.22597337294899267
          },
          "point_estimate": 0.1511172073129828,
          "standard_error": 0.0593892675166648
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.439163650176106,
            "upper_bound": 54.570501432516906
          },
          "point_estimate": 54.51658340087863,
          "standard_error": 0.03507980931460516
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.48390007096002,
            "upper_bound": 54.57413836754995
          },
          "point_estimate": 54.56448656436971,
          "standard_error": 0.02114516398697529
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0050918342137005156,
            "upper_bound": 0.11389730718359628
          },
          "point_estimate": 0.019889988965425088,
          "standard_error": 0.026449580130562828
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.38477284472314,
            "upper_bound": 54.561358988193696
          },
          "point_estimate": 54.48849830492857,
          "standard_error": 0.04548480823935056
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01361396451151311,
            "upper_bound": 0.1703219464247007
          },
          "point_estimate": 0.11667769159987636,
          "standard_error": 0.04563308085963098
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.032864765044856,
            "upper_bound": 50.16238223880296
          },
          "point_estimate": 50.103889829974634,
          "standard_error": 0.03260589092490158
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.09188300104633,
            "upper_bound": 50.144323199847186
          },
          "point_estimate": 50.10780064688275,
          "standard_error": 0.013453995293256178
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003927068584432303,
            "upper_bound": 0.1448479910266458
          },
          "point_estimate": 0.02686226428310023,
          "standard_error": 0.03151580587660445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.97108536088805,
            "upper_bound": 50.117661430641256
          },
          "point_estimate": 50.064502270052294,
          "standard_error": 0.04179385541407343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01842466818258191,
            "upper_bound": 0.16079906922842008
          },
          "point_estimate": 0.1087956783529699,
          "standard_error": 0.03786521698812522
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.39290989841714,
            "upper_bound": 95.68561291983562
          },
          "point_estimate": 95.55057156731267,
          "standard_error": 0.07554635937801307
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.32835806777096,
            "upper_bound": 95.7409144818414
          },
          "point_estimate": 95.62937034915495,
          "standard_error": 0.1002610571942892
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019190819856836433,
            "upper_bound": 0.36281332603374006
          },
          "point_estimate": 0.16698124243720364,
          "standard_error": 0.10303469895051638
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.40702489744484,
            "upper_bound": 95.74820883605688
          },
          "point_estimate": 95.63784788689406,
          "standard_error": 0.08875582917804868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09775351572143416,
            "upper_bound": 0.3150683424745749
          },
          "point_estimate": 0.2527174890686501,
          "standard_error": 0.05750356781765388
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.01411001561435,
            "upper_bound": 39.04808142040078
          },
          "point_estimate": 39.02732851860444,
          "standard_error": 0.009348616494684896
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.01015165105053,
            "upper_bound": 39.02804717556411
          },
          "point_estimate": 39.01861476902799,
          "standard_error": 0.006029117550988062
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013320147736104535,
            "upper_bound": 0.02345824202497545
          },
          "point_estimate": 0.01306113578609818,
          "standard_error": 0.005657137407278048
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.01031063950624,
            "upper_bound": 39.02164961953033
          },
          "point_estimate": 39.014083728962554,
          "standard_error": 0.0029458818191497346
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006338375338052657,
            "upper_bound": 0.04758883454869264
          },
          "point_estimate": 0.03118943920347962,
          "standard_error": 0.014057932160635788
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.99554929503732,
            "upper_bound": 44.03164722793627
          },
          "point_estimate": 44.01646476956591,
          "standard_error": 0.009415175945750111
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.013066825920134,
            "upper_bound": 44.03998700869437
          },
          "point_estimate": 44.01921439932263,
          "standard_error": 0.006272487053686251
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014254209813793796,
            "upper_bound": 0.03586544079523092
          },
          "point_estimate": 0.010478760003908448,
          "standard_error": 0.009659831715437724
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.01444497018824,
            "upper_bound": 44.03485761405751
          },
          "point_estimate": 44.02374154854017,
          "standard_error": 0.005368109915482299
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009321581512977042,
            "upper_bound": 0.04684674084482512
          },
          "point_estimate": 0.03164197180647648,
          "standard_error": 0.01189098442211252
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.54496379281476,
            "upper_bound": 79.64170979970713
          },
          "point_estimate": 79.58586840993607,
          "standard_error": 0.0253640006609053
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.53774819555494,
            "upper_bound": 79.60677041724213
          },
          "point_estimate": 79.56402104705602,
          "standard_error": 0.017737022925353556
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007823806497837904,
            "upper_bound": 0.095358668689768
          },
          "point_estimate": 0.04102192760754076,
          "standard_error": 0.02242500431922536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.54071437553891,
            "upper_bound": 79.58470482050967
          },
          "point_estimate": 79.55807592854964,
          "standard_error": 0.011377605748424962
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021973346029567676,
            "upper_bound": 0.12354924774802006
          },
          "point_estimate": 0.08470841486628673,
          "standard_error": 0.03026565047003448
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 912551.088347222,
            "upper_bound": 913046.688321875
          },
          "point_estimate": 912762.7927222222,
          "standard_error": 129.1866987197236
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 912504.80625,
            "upper_bound": 912884.4625
          },
          "point_estimate": 912644.168125,
          "standard_error": 103.57726558754128
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.647880047556285,
            "upper_bound": 487.2703807242042
          },
          "point_estimate": 265.7514121569866,
          "standard_error": 108.55945747850753
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 912496.970395156,
            "upper_bound": 912865.6112956016
          },
          "point_estimate": 912720.007012987,
          "standard_error": 93.99257551587048
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139.86753645740887,
            "upper_bound": 629.8490598361951
          },
          "point_estimate": 431.4405617391648,
          "standard_error": 150.98934373715744
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1235402.3098376323,
            "upper_bound": 1239535.8490742394
          },
          "point_estimate": 1237496.6231415342,
          "standard_error": 1064.6409486888542
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1234232.9944444443,
            "upper_bound": 1240647.9675925926
          },
          "point_estimate": 1237885.2238095235,
          "standard_error": 1899.472144552559
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 433.19237508724706,
            "upper_bound": 5785.819922281015
          },
          "point_estimate": 4113.590705302598,
          "standard_error": 1332.017503634412
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1234824.7473036346,
            "upper_bound": 1239132.850243309
          },
          "point_estimate": 1236791.3681385282,
          "standard_error": 1110.2725298058624
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2257.656316065908,
            "upper_bound": 4192.529268327135
          },
          "point_estimate": 3548.0818085434244,
          "standard_error": 492.4553696494374
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1200462.532767569,
            "upper_bound": 1203847.4504451165
          },
          "point_estimate": 1202052.3093766,
          "standard_error": 868.9647781707972
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1199488.7465437788,
            "upper_bound": 1204326.366935484
          },
          "point_estimate": 1201539.2435483872,
          "standard_error": 1291.649864885533
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 510.8513425433899,
            "upper_bound": 4905.0274425690195
          },
          "point_estimate": 3133.497932433924,
          "standard_error": 1131.5485194610635
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1201348.9471544717,
            "upper_bound": 1204461.788869056
          },
          "point_estimate": 1202992.8199413489,
          "standard_error": 791.0346425295181
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1510.113697958359,
            "upper_bound": 3739.0231674761576
          },
          "point_estimate": 2903.323903261661,
          "standard_error": 593.6833255787188
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1275812.9692528737,
            "upper_bound": 1276354.0086075536
          },
          "point_estimate": 1276074.518452381,
          "standard_error": 138.73580909938292
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1275726.4245689656,
            "upper_bound": 1276439.8357963874
          },
          "point_estimate": 1276004.359482759,
          "standard_error": 157.16665520131323
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.81163786811988,
            "upper_bound": 830.7538369753037
          },
          "point_estimate": 362.83652114450945,
          "standard_error": 190.22846958742304
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1275715.375126503,
            "upper_bound": 1276283.7087125145
          },
          "point_estimate": 1276010.56677116,
          "standard_error": 142.15887772504465
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238.0176553919277,
            "upper_bound": 593.0507448436746
          },
          "point_estimate": 463.2906098292633,
          "standard_error": 92.12944306702356
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1091616.8770665266,
            "upper_bound": 1092339.412221989
          },
          "point_estimate": 1091970.3438515407,
          "standard_error": 185.0709947298752
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1091638.4779411764,
            "upper_bound": 1092385.2864145658
          },
          "point_estimate": 1091831.026960784,
          "standard_error": 168.68791444881975
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.95122900430571,
            "upper_bound": 1069.9824339451982
          },
          "point_estimate": 297.66101530351654,
          "standard_error": 320.1827409516074
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1091680.222942967,
            "upper_bound": 1092332.3815607051
          },
          "point_estimate": 1092022.463789152,
          "standard_error": 164.52276955448926
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263.8363975307029,
            "upper_bound": 810.1732026925011
          },
          "point_estimate": 618.4759030606118,
          "standard_error": 134.2919937696849
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 348889.5790963908,
            "upper_bound": 349627.03726421954
          },
          "point_estimate": 349192.942340514,
          "standard_error": 197.4793870654704
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 348831.5243386243,
            "upper_bound": 349362.44841269846
          },
          "point_estimate": 348961.7757142857,
          "standard_error": 116.02186505452714
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.71064590837291,
            "upper_bound": 571.7888015153637
          },
          "point_estimate": 174.76394289737962,
          "standard_error": 142.88930637887805
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 348802.9042155777,
            "upper_bound": 349003.8118881119
          },
          "point_estimate": 348874.6471490414,
          "standard_error": 51.29738046883475
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.31968577090738,
            "upper_bound": 969.208673211619
          },
          "point_estimate": 658.1933786695419,
          "standard_error": 258.6269733690353
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389468.73033096927,
            "upper_bound": 390122.2591877217
          },
          "point_estimate": 389848.30436465715,
          "standard_error": 170.99341428234294
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389782.0765957447,
            "upper_bound": 390152.55319148937
          },
          "point_estimate": 389929.18284574465,
          "standard_error": 108.83101295284396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.35248250995836,
            "upper_bound": 554.2910737587032
          },
          "point_estimate": 242.769239599506,
          "standard_error": 127.2662520390954
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389377.58936170215,
            "upper_bound": 390012.75097392866
          },
          "point_estimate": 389788.55670074606,
          "standard_error": 166.39842610897188
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.68809583661613,
            "upper_bound": 849.746815469524
          },
          "point_estimate": 570.0943353455888,
          "standard_error": 219.95853845357135
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 893327.6667789392,
            "upper_bound": 895442.4637804877
          },
          "point_estimate": 894310.2142179636,
          "standard_error": 541.9565506357719
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 892931.7909407666,
            "upper_bound": 895461.9203929539
          },
          "point_estimate": 893797.5341463415,
          "standard_error": 719.7506506723379
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 426.68142413218015,
            "upper_bound": 3048.189384908177
          },
          "point_estimate": 1784.496474009842,
          "standard_error": 638.3789454801058
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 893453.8545431568,
            "upper_bound": 895525.5353955978
          },
          "point_estimate": 894479.5578713969,
          "standard_error": 519.9040192190672
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 883.3935693896304,
            "upper_bound": 2381.025298172841
          },
          "point_estimate": 1805.9193925771797,
          "standard_error": 411.04336910719786
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697431.334169287,
            "upper_bound": 697983.1031159967
          },
          "point_estimate": 697673.4556611261,
          "standard_error": 143.14700081794282
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697303.7264150943,
            "upper_bound": 697909.9539083558
          },
          "point_estimate": 697516.1665356394,
          "standard_error": 139.8804718527947
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.06649583455987,
            "upper_bound": 614.8507383369672
          },
          "point_estimate": 319.8963925126,
          "standard_error": 155.1485405428294
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697442.695685838,
            "upper_bound": 697701.6196283808
          },
          "point_estimate": 697544.01141877,
          "standard_error": 66.0322666082041
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150.7062200823189,
            "upper_bound": 672.0989874852825
          },
          "point_estimate": 477.6209190606383,
          "standard_error": 148.07307939991597
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 720696.126030579,
            "upper_bound": 723085.7430090453
          },
          "point_estimate": 721838.8191853408,
          "standard_error": 613.4657208531472
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 720103.3825280112,
            "upper_bound": 723558.7091503269
          },
          "point_estimate": 721603.1441176471,
          "standard_error": 812.2906039277156
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 350.7482690670647,
            "upper_bound": 3689.108455093547
          },
          "point_estimate": 1587.4299665232954,
          "standard_error": 843.8024700774747
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 720279.7251289571,
            "upper_bound": 723341.9061724198
          },
          "point_estimate": 721586.6655462185,
          "standard_error": 820.5925517545957
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1031.5747507233502,
            "upper_bound": 2557.5820034429375
          },
          "point_estimate": 2044.7826323688255,
          "standard_error": 388.3847829058937
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 310865.6633190883,
            "upper_bound": 312222.15479548235
          },
          "point_estimate": 311481.6666564917,
          "standard_error": 349.16802289101844
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 310638.2863247863,
            "upper_bound": 312345.8717948718
          },
          "point_estimate": 311095.3827838828,
          "standard_error": 421.69136862286825
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161.64933238658452,
            "upper_bound": 1869.4809520664548
          },
          "point_estimate": 724.99138712883,
          "standard_error": 423.4005671227512
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 310661.83647169964,
            "upper_bound": 311814.0407212578
          },
          "point_estimate": 311140.66595626593,
          "standard_error": 310.29814585377744
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477.16922955305466,
            "upper_bound": 1587.13487645765
          },
          "point_estimate": 1164.7802975744846,
          "standard_error": 306.99752442551676
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 531108.7865332414,
            "upper_bound": 531754.5952754773
          },
          "point_estimate": 531369.14071026,
          "standard_error": 173.91790413315172
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 531060.7987117553,
            "upper_bound": 531450.125
          },
          "point_estimate": 531213.286231884,
          "standard_error": 97.09318752170682
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.062425692001504,
            "upper_bound": 463.5887526391875
          },
          "point_estimate": 195.63631837453235,
          "standard_error": 117.52079297983224
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 531032.2351943565,
            "upper_bound": 531287.5470127735
          },
          "point_estimate": 531150.4424242424,
          "standard_error": 65.03885124955103
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123.50557818159588,
            "upper_bound": 869.8879756780988
          },
          "point_estimate": 577.2046698119074,
          "standard_error": 240.8314748513632
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171518.36057914048,
            "upper_bound": 171652.8967085954
          },
          "point_estimate": 171578.9869601677,
          "standard_error": 34.71830685633187
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171493.179245283,
            "upper_bound": 171657.91273584907
          },
          "point_estimate": 171532.26749213837,
          "standard_error": 40.7285559714984
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.767835356846753,
            "upper_bound": 184.28472903964536
          },
          "point_estimate": 61.510124025909946,
          "standard_error": 42.33745303723256
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171497.19354192467,
            "upper_bound": 171568.19315474274
          },
          "point_estimate": 171522.52623131586,
          "standard_error": 18.172017303999535
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.44745602780315,
            "upper_bound": 157.97585092483027
          },
          "point_estimate": 116.17128581897072,
          "standard_error": 31.27155141846134
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/stud_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146871.33826526496,
            "upper_bound": 146987.44063220048
          },
          "point_estimate": 146915.45426603305,
          "standard_error": 32.53593455605361
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146866.59879032258,
            "upper_bound": 146908.71303763438
          },
          "point_estimate": 146880.9291762673,
          "standard_error": 13.233993862498936
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.057490567861569,
            "upper_bound": 54.61218778039958
          },
          "point_estimate": 25.96118170646199,
          "standard_error": 14.888557678407626
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146867.08278627944,
            "upper_bound": 146892.50461787262
          },
          "point_estimate": 146878.43076036868,
          "standard_error": 6.496225956120004
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.01830551705772,
            "upper_bound": 166.62307861515228
          },
          "point_estimate": 108.75322743322968,
          "standard_error": 51.8427080563583
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/stud_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 442406.26892474666,
            "upper_bound": 442611.2664400101
          },
          "point_estimate": 442499.1778432779,
          "standard_error": 52.819537689743775
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 442380.5798192771,
            "upper_bound": 442580.99585006695
          },
          "point_estimate": 442446.2443775101,
          "standard_error": 55.43714453636954
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.790338667055178,
            "upper_bound": 254.45408843031424
          },
          "point_estimate": 123.08214522452184,
          "standard_error": 57.48297484951324
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 442391.53587075573,
            "upper_bound": 442577.78094182466
          },
          "point_estimate": 442487.5859176968,
          "standard_error": 47.87500628133709
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.10861861807176,
            "upper_bound": 243.0195667722908
          },
          "point_estimate": 176.05399993846697,
          "standard_error": 49.08821346587839
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/stud_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 927.7304087345648,
            "upper_bound": 929.2021021475446
          },
          "point_estimate": 928.3004561557466,
          "standard_error": 0.405760309725576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 927.6116634335596,
            "upper_bound": 928.2961355862984
          },
          "point_estimate": 927.9383891044296,
          "standard_error": 0.2044144214784918
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09645401814575516,
            "upper_bound": 0.8381846004645644
          },
          "point_estimate": 0.48536057512922,
          "standard_error": 0.2015472781099044
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 927.7900354908628,
            "upper_bound": 928.1942257018084
          },
          "point_estimate": 927.9813208181374,
          "standard_error": 0.10310611776272637
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.243733135563191,
            "upper_bound": 2.060517403413745
          },
          "point_estimate": 1.3504124059369893,
          "standard_error": 0.6161839700735714
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1381165.2916776896,
            "upper_bound": 1384116.0675661375
          },
          "point_estimate": 1382591.6314550263,
          "standard_error": 742.8198934323132
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1381615.7932098764,
            "upper_bound": 1383309.6199294534
          },
          "point_estimate": 1382569.3814814817,
          "standard_error": 443.12248086474904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 222.83889437732088,
            "upper_bound": 3875.2074562012617
          },
          "point_estimate": 891.836507314932,
          "standard_error": 823.5274597471841
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1382385.350627267,
            "upper_bound": 1383632.3038178664
          },
          "point_estimate": 1382862.5057239055,
          "standard_error": 317.98336365565365
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 619.7466761617314,
            "upper_bound": 3526.901095835259
          },
          "point_estimate": 2479.819134148604,
          "standard_error": 752.9041951770408
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1513908.99703373,
            "upper_bound": 1518265.211166543
          },
          "point_estimate": 1516042.2015525794,
          "standard_error": 1120.932544105009
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1512655.3958333335,
            "upper_bound": 1520735.71875
          },
          "point_estimate": 1515660.3402777778,
          "standard_error": 1975.7618502814787
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182.84086956640263,
            "upper_bound": 6223.228833265718
          },
          "point_estimate": 4953.160595397001,
          "standard_error": 1616.7714606036668
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1514150.8984169257,
            "upper_bound": 1518929.3763812154
          },
          "point_estimate": 1516109.8767316018,
          "standard_error": 1228.404551015852
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2257.5172956625024,
            "upper_bound": 4373.026414498313
          },
          "point_estimate": 3745.607313621345,
          "standard_error": 522.366720259748
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1411308.302659951,
            "upper_bound": 1415685.7780219775
          },
          "point_estimate": 1413400.178840049,
          "standard_error": 1121.203650541227
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1410504.6085164836,
            "upper_bound": 1417848.4038461538
          },
          "point_estimate": 1412264.8423076924,
          "standard_error": 1641.078034573519
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 662.609092467116,
            "upper_bound": 5854.3073518344545
          },
          "point_estimate": 2676.0476508241018,
          "standard_error": 1483.290083532061
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1410868.8704643126,
            "upper_bound": 1416979.4642168416
          },
          "point_estimate": 1413630.5562437563,
          "standard_error": 1716.610173695325
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1683.045967717785,
            "upper_bound": 4497.475828125333
          },
          "point_estimate": 3750.4018791343246,
          "standard_error": 644.6244906418663
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 869947.8935496738,
            "upper_bound": 871943.6688548753
          },
          "point_estimate": 870937.2417715419,
          "standard_error": 512.0434284281566
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 869525.0833333333,
            "upper_bound": 872670.1553571429
          },
          "point_estimate": 870487.2295918367,
          "standard_error": 925.7025520979204
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.88138655471954,
            "upper_bound": 2781.4869839519474
          },
          "point_estimate": 2580.1496553835996,
          "standard_error": 697.2303701170872
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 869410.5122027053,
            "upper_bound": 871951.6331848559
          },
          "point_estimate": 870504.3652442795,
          "standard_error": 662.7960290104373
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1118.4505779407034,
            "upper_bound": 2013.494412911793
          },
          "point_estimate": 1713.4059890014623,
          "standard_error": 231.09121587217695
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308921.9738738398,
            "upper_bound": 309867.0451832796
          },
          "point_estimate": 309377.2175995427,
          "standard_error": 240.8586095293585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308726.4467312349,
            "upper_bound": 310024.49058380414
          },
          "point_estimate": 309275.71398305084,
          "standard_error": 290.24425180676656
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.27360761518621,
            "upper_bound": 1342.1893400132096
          },
          "point_estimate": 751.5989610914756,
          "standard_error": 334.9844661964277
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308965.98694144835,
            "upper_bound": 309864.4659246142
          },
          "point_estimate": 309384.6914153643,
          "standard_error": 230.12180057747273
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 416.22370446128355,
            "upper_bound": 1047.0252902803006
          },
          "point_estimate": 802.8813339004975,
          "standard_error": 165.93601540738663
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 503091.78386062186,
            "upper_bound": 504520.5263796478
          },
          "point_estimate": 503784.4113530115,
          "standard_error": 364.475731021719
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 502794.3082191781,
            "upper_bound": 504719.41476407915
          },
          "point_estimate": 503766.17162426614,
          "standard_error": 401.17571251188906
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134.70639336189498,
            "upper_bound": 2275.9128171287152
          },
          "point_estimate": 829.3426340433616,
          "standard_error": 549.404107680382
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 502752.214020232,
            "upper_bound": 504192.1796738855
          },
          "point_estimate": 503534.5524639744,
          "standard_error": 367.1177622923885
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 613.7473877055693,
            "upper_bound": 1554.017468570962
          },
          "point_estimate": 1212.895762414355,
          "standard_error": 242.90788068604243
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278314.02092941315,
            "upper_bound": 278813.71266495314
          },
          "point_estimate": 278575.7158128908,
          "standard_error": 128.3151476226805
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278301.7916666667,
            "upper_bound": 278990.0696969697
          },
          "point_estimate": 278623.5429292929,
          "standard_error": 209.44446940156325
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.72131281673326,
            "upper_bound": 822.9386430414827
          },
          "point_estimate": 510.22049480540113,
          "standard_error": 190.7662513799784
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278024.56265050056,
            "upper_bound": 278891.46604809514
          },
          "point_estimate": 278466.0803817395,
          "standard_error": 235.1968707605034
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238.6648836293077,
            "upper_bound": 553.5762663590408
          },
          "point_estimate": 427.0310085755345,
          "standard_error": 87.8247093093526
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 643595.3078362574,
            "upper_bound": 646279.7061842105
          },
          "point_estimate": 644996.7868226122,
          "standard_error": 689.619804663173
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 642838.0233918129,
            "upper_bound": 646531.1142300195
          },
          "point_estimate": 645509.0614035088,
          "standard_error": 828.2648093293313
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 234.7601686391894,
            "upper_bound": 3857.2179262575537
          },
          "point_estimate": 2023.914419916314,
          "standard_error": 1050.3234402743578
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 642548.369987749,
            "upper_bound": 646238.7817721153
          },
          "point_estimate": 644286.9084073821,
          "standard_error": 952.9191677251436
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1103.7047719301763,
            "upper_bound": 2915.887074105415
          },
          "point_estimate": 2295.350417431049,
          "standard_error": 452.8781284167541
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1743.1303363726695,
            "upper_bound": 1744.48783362292
          },
          "point_estimate": 1743.735415517617,
          "standard_error": 0.35133807531120553
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1742.6803237530487,
            "upper_bound": 1744.2116876285208
          },
          "point_estimate": 1743.6609200899047,
          "standard_error": 0.37879752044424825
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06451943838421262,
            "upper_bound": 1.685565010245606
          },
          "point_estimate": 0.8840793769687938,
          "standard_error": 0.3908717452398098
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1742.8833393233735,
            "upper_bound": 1743.90389371281
          },
          "point_estimate": 1743.3650732510755,
          "standard_error": 0.26094264189455957
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4897567105516822,
            "upper_bound": 1.6648115067164484
          },
          "point_estimate": 1.1769767249541618,
          "standard_error": 0.3573345961548472
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-long-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506.7858217216019,
            "upper_bound": 507.043170915071
          },
          "point_estimate": 506.90342125845063,
          "standard_error": 0.06638739719927848
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506.738524005906,
            "upper_bound": 507.04086274768457
          },
          "point_estimate": 506.807643036664,
          "standard_error": 0.08873366730670099
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014340425617066098,
            "upper_bound": 0.3703579181162893
          },
          "point_estimate": 0.13190997275663874,
          "standard_error": 0.09158296399058824
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 506.7460905459402,
            "upper_bound": 506.8857693018774
          },
          "point_estimate": 506.808786092894,
          "standard_error": 0.035498352653605895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09149681277474764,
            "upper_bound": 0.2939948054997444
          },
          "point_estimate": 0.2207640785262895,
          "standard_error": 0.05470798521879348
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.628959116811,
            "upper_bound": 99.69126078472952
          },
          "point_estimate": 99.65698600126498,
          "standard_error": 0.01602346127401613
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.62554231223297,
            "upper_bound": 99.6843324762333
          },
          "point_estimate": 99.64951467982888,
          "standard_error": 0.014564072131138313
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001011034806989834,
            "upper_bound": 0.0749477248460574
          },
          "point_estimate": 0.03655246637373079,
          "standard_error": 0.0185059304006583
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.63442973126354,
            "upper_bound": 99.67402247626627
          },
          "point_estimate": 99.65415153219242,
          "standard_error": 0.01069099504333071
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01976445801902854,
            "upper_bound": 0.07541103329832333
          },
          "point_estimate": 0.053486505884163114,
          "standard_error": 0.016025605546261756
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.684627222502115,
            "upper_bound": 38.79980488659091
          },
          "point_estimate": 38.74598880881534,
          "standard_error": 0.029624265476938617
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.68633913713619,
            "upper_bound": 38.82629374363193
          },
          "point_estimate": 38.768610684448106,
          "standard_error": 0.04418138028212786
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0020183132980068495,
            "upper_bound": 0.17399224677769157
          },
          "point_estimate": 0.08553292482457066,
          "standard_error": 0.04304802361665131
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.723235992554066,
            "upper_bound": 38.81413313292065
          },
          "point_estimate": 38.772122347352926,
          "standard_error": 0.02340136182459372
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0498116065286921,
            "upper_bound": 0.1280705328966451
          },
          "point_estimate": 0.09841586216144182,
          "standard_error": 0.021278173990046483
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.41999947579443,
            "upper_bound": 62.63618778312368
          },
          "point_estimate": 62.53528200152552,
          "standard_error": 0.05549421055143759
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.35290578190946,
            "upper_bound": 62.66083078838438
          },
          "point_estimate": 62.63017440569784,
          "standard_error": 0.08075643726266421
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009457086714194813,
            "upper_bound": 0.2905552718943201
          },
          "point_estimate": 0.07869495286563773,
          "standard_error": 0.07509866865296909
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.58312533776092,
            "upper_bound": 62.68492056100459
          },
          "point_estimate": 62.65016391845169,
          "standard_error": 0.026462385799008573
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05394480070558368,
            "upper_bound": 0.2198488351812642
          },
          "point_estimate": 0.184643482803635,
          "standard_error": 0.03614019869547542
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316918.04042872664,
            "upper_bound": 317935.1175934438
          },
          "point_estimate": 317405.71956866805,
          "standard_error": 260.7630268488885
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316596.9130434783,
            "upper_bound": 318191.9880434782
          },
          "point_estimate": 317218.3049689441,
          "standard_error": 426.8858382781931
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.71354620869866,
            "upper_bound": 1484.0325908994866
          },
          "point_estimate": 950.8051761633365,
          "standard_error": 395.3720279379232
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316757.5517043503,
            "upper_bound": 317935.7759828509
          },
          "point_estimate": 317255.42902315076,
          "standard_error": 304.8788140361829
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 467.9210070598327,
            "upper_bound": 1046.5004748350336
          },
          "point_estimate": 873.1420581828893,
          "standard_error": 146.03472244878242
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.71699485377616,
            "upper_bound": 46.74673864226618
          },
          "point_estimate": 46.7303248060244,
          "standard_error": 0.0076868844399489
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.713070709044445,
            "upper_bound": 46.73900022253875
          },
          "point_estimate": 46.724660198881814,
          "standard_error": 0.007056138694455008
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002750457074398926,
            "upper_bound": 0.03185338797685043
          },
          "point_estimate": 0.019534304201043903,
          "standard_error": 0.007508157603628232
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.7221803787963,
            "upper_bound": 46.73623042345977
          },
          "point_estimate": 46.730898693845994,
          "standard_error": 0.003574281813733543
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009446809969594214,
            "upper_bound": 0.03670752355570963
          },
          "point_estimate": 0.02570799893801999,
          "standard_error": 0.008116591174336536
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.57798620093406,
            "upper_bound": 77.76544153819181
          },
          "point_estimate": 77.66026292316893,
          "standard_error": 0.04858894424070053
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.55418254034538,
            "upper_bound": 77.72673181031676
          },
          "point_estimate": 77.60356974598443,
          "standard_error": 0.046280116304924304
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02752910807259345,
            "upper_bound": 0.210511715555533
          },
          "point_estimate": 0.08913248833837548,
          "standard_error": 0.05055852976853238
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.56290051397716,
            "upper_bound": 77.65252789151386
          },
          "point_estimate": 77.59735000612962,
          "standard_error": 0.02306120075952935
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.048090686364045225,
            "upper_bound": 0.22817390436121168
          },
          "point_estimate": 0.1619410239039908,
          "standard_error": 0.050148963689687576
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140944.43106389965,
            "upper_bound": 141403.19479455057
          },
          "point_estimate": 141164.52250984372,
          "standard_error": 117.52727593937873
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140863.75796726957,
            "upper_bound": 141522.26782945736
          },
          "point_estimate": 140918.35717054264,
          "standard_error": 210.5857125908952
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.67532638672705,
            "upper_bound": 609.7174673342796
          },
          "point_estimate": 165.23773045793388,
          "standard_error": 186.19331186175296
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140892.21133259506,
            "upper_bound": 141411.3797167079
          },
          "point_estimate": 141147.4947246552,
          "standard_error": 135.06128323016716
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 209.88231511579784,
            "upper_bound": 461.86066366396415
          },
          "point_estimate": 390.8731804562898,
          "standard_error": 64.02623244889163
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.15804217136148,
            "upper_bound": 51.26037622794185
          },
          "point_estimate": 51.21439411905384,
          "standard_error": 0.026234010997376447
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.144341216004776,
            "upper_bound": 51.26892226058841
          },
          "point_estimate": 51.23977631823533,
          "standard_error": 0.02429795095666019
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00498550096578669,
            "upper_bound": 0.12528989785887834
          },
          "point_estimate": 0.03486202776798645,
          "standard_error": 0.026910856963879234
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.09242817268143,
            "upper_bound": 51.2528370021931
          },
          "point_estimate": 51.15708647345197,
          "standard_error": 0.04232886884567345
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020673805194822308,
            "upper_bound": 0.1106958723047951
          },
          "point_estimate": 0.08736569455450734,
          "standard_error": 0.023123451879072148
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.37544927908375,
            "upper_bound": 73.42533154380746
          },
          "point_estimate": 73.39972892259907,
          "standard_error": 0.012739047396017965
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.36618650917833,
            "upper_bound": 73.4287713630995
          },
          "point_estimate": 73.3983268446266,
          "standard_error": 0.01681746622390059
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00982569091489836,
            "upper_bound": 0.06994389611947394
          },
          "point_estimate": 0.04180766994552311,
          "standard_error": 0.01565612499424378
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.37621382480017,
            "upper_bound": 73.41888821903405
          },
          "point_estimate": 73.39814604591662,
          "standard_error": 0.011083747197251618
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023113073346225595,
            "upper_bound": 0.05434218539108764
          },
          "point_estimate": 0.042539154463954766,
          "standard_error": 0.00802571663877924
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.05530066763637,
            "upper_bound": 187.29664258755764
          },
          "point_estimate": 187.1639142582051,
          "standard_error": 0.0622914579612616
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.00377847562365,
            "upper_bound": 187.29309697944663
          },
          "point_estimate": 187.10835898859176,
          "standard_error": 0.06775760456589122
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024536027770653536,
            "upper_bound": 0.3134784252579578
          },
          "point_estimate": 0.16780986190022917,
          "standard_error": 0.07369445406646691
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.02917973724732,
            "upper_bound": 187.18575920077973
          },
          "point_estimate": 187.09721214452952,
          "standard_error": 0.03918971263164182
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07055389658016727,
            "upper_bound": 0.2829562457529803
          },
          "point_estimate": 0.2072437820218233,
          "standard_error": 0.056290590974097186
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 416.9610788578049,
            "upper_bound": 418.1996779923969
          },
          "point_estimate": 417.587701090874,
          "standard_error": 0.31480778323461195
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 416.9578834190039,
            "upper_bound": 418.18617120236166
          },
          "point_estimate": 417.62427282217186,
          "standard_error": 0.2886161697132058
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13981077284627158,
            "upper_bound": 1.8077512429603135
          },
          "point_estimate": 0.7858831785133714,
          "standard_error": 0.3980733171391856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 416.3143800513262,
            "upper_bound": 418.11215356920377
          },
          "point_estimate": 417.2511574004588,
          "standard_error": 0.4925023557992766
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.42664440096149187,
            "upper_bound": 1.438187417340396
          },
          "point_estimate": 1.0506406035822458,
          "standard_error": 0.25451454914147564
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.66329115869488,
            "upper_bound": 22.7512795064821
          },
          "point_estimate": 22.705969704980017,
          "standard_error": 0.022559591645324645
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.634499460022973,
            "upper_bound": 22.787408651285123
          },
          "point_estimate": 22.681114485534025,
          "standard_error": 0.041898134975933456
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006375366231060217,
            "upper_bound": 0.1231228343633976
          },
          "point_estimate": 0.07271070403981249,
          "standard_error": 0.032219462102161624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.643011712695937,
            "upper_bound": 22.718478769072313
          },
          "point_estimate": 22.665989551174953,
          "standard_error": 0.01955960265323825
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04527675156626804,
            "upper_bound": 0.0862891301838245
          },
          "point_estimate": 0.07500103738291929,
          "standard_error": 0.01060118940278635
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/twoway_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40792.810358285235,
            "upper_bound": 40825.27021230827
          },
          "point_estimate": 40810.637711506606,
          "standard_error": 8.38443176499139
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40797.127665544336,
            "upper_bound": 40830.49382716049
          },
          "point_estimate": 40815.82208193041,
          "standard_error": 7.947963863052732
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.904580579797352,
            "upper_bound": 39.64875427083402
          },
          "point_estimate": 19.03335555434308,
          "standard_error": 8.607785957549144
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40808.386296025856,
            "upper_bound": 40832.26056605569
          },
          "point_estimate": 40821.32831926771,
          "standard_error": 6.259717511226677
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.035299367566031,
            "upper_bound": 39.02106032621528
          },
          "point_estimate": 27.966050445748714,
          "standard_error": 8.106820222763202
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/twoway_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.96290814179702,
            "upper_bound": 99.34365193775976
          },
          "point_estimate": 99.15558298186028,
          "standard_error": 0.0974514568500739
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.81682937691946,
            "upper_bound": 99.4237824737423
          },
          "point_estimate": 99.2283012987155,
          "standard_error": 0.1869982082216905
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01341679942008723,
            "upper_bound": 0.515011228793487
          },
          "point_estimate": 0.4080683033283633,
          "standard_error": 0.1339479736299342
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.02049782263684,
            "upper_bound": 99.3738835996063
          },
          "point_estimate": 99.25800909802024,
          "standard_error": 0.09185114494669962
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2125805262824592,
            "upper_bound": 0.3815838972151749
          },
          "point_estimate": 0.3256919584402583,
          "standard_error": 0.043255360727311704
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/twoway_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1341838.5394094388,
            "upper_bound": 1344418.6488151927
          },
          "point_estimate": 1342974.1704322563,
          "standard_error": 667.0252188985634
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1341256.8095238097,
            "upper_bound": 1344023.740752551
          },
          "point_estimate": 1342490.8767857142,
          "standard_error": 747.4631596637571
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201.8586339161696,
            "upper_bound": 3042.753499670778
          },
          "point_estimate": 1661.2184117573115,
          "standard_error": 755.2183906613731
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1342209.56144958,
            "upper_bound": 1343586.9844155023
          },
          "point_estimate": 1342983.1290352503,
          "standard_error": 344.9318723014636
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 839.417293383332,
            "upper_bound": 3131.2476014245635
          },
          "point_estimate": 2215.3100612324174,
          "standard_error": 674.5900971566667
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/twoway_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2760.750316723773,
            "upper_bound": 2769.8018728205293
          },
          "point_estimate": 2765.8863108301302,
          "standard_error": 2.297673325056069
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2760.090693184993,
            "upper_bound": 2770.1246829448837
          },
          "point_estimate": 2769.5202545149737,
          "standard_error": 2.1503684016446587
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.30044218700032105,
            "upper_bound": 10.6879645058239
          },
          "point_estimate": 1.8853637400554248,
          "standard_error": 2.1616165248522643
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2767.386036068768,
            "upper_bound": 2770.4466309272425
          },
          "point_estimate": 2769.480103041057,
          "standard_error": 0.8019261255425628
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.184858410103219,
            "upper_bound": 9.667893021922426
          },
          "point_estimate": 7.671456722748038,
          "standard_error": 2.294194633441572
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.041024295521424,
            "upper_bound": 29.153498753693437
          },
          "point_estimate": 29.096723286904012,
          "standard_error": 0.028735128479689342
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.019104684258,
            "upper_bound": 29.203352426536934
          },
          "point_estimate": 29.031702217549892,
          "standard_error": 0.06618271746174914
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014871175537287742,
            "upper_bound": 0.14201034360851783
          },
          "point_estimate": 0.02562231588111457,
          "standard_error": 0.04805323606965602
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.0229697577441,
            "upper_bound": 29.10020357156162
          },
          "point_estimate": 29.04525767220311,
          "standard_error": 0.020270270996593957
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05782041947224152,
            "upper_bound": 0.10204747114881402
          },
          "point_estimate": 0.09584089472751708,
          "standard_error": 0.01149818656176162
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.91280529368806,
            "upper_bound": 31.95933507952851
          },
          "point_estimate": 31.930649424016668,
          "standard_error": 0.013021793948715612
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.909947420587596,
            "upper_bound": 31.9286212980778
          },
          "point_estimate": 31.91999104241865,
          "standard_error": 0.00601051955905304
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001419621892426025,
            "upper_bound": 0.02366438860922022
          },
          "point_estimate": 0.012366955607373872,
          "standard_error": 0.006031207243318447
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.90998836156674,
            "upper_bound": 31.922065323254948
          },
          "point_estimate": 31.914684520427972,
          "standard_error": 0.003087924313412601
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0068909450382753845,
            "upper_bound": 0.06649905397973603
          },
          "point_estimate": 0.043398841245020865,
          "standard_error": 0.020517527334190606
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.304401150404512,
            "upper_bound": 24.330234683323688
          },
          "point_estimate": 24.318038487913014,
          "standard_error": 0.006582546351482003
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.311258232796543,
            "upper_bound": 24.331660976797643
          },
          "point_estimate": 24.31627158704864,
          "standard_error": 0.005786368937797831
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009761637866721016,
            "upper_bound": 0.0348432001484895
          },
          "point_estimate": 0.008934877072665923,
          "standard_error": 0.008692231382005887
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.289209292777,
            "upper_bound": 24.3207685852285
          },
          "point_estimate": 24.30640118410372,
          "standard_error": 0.008636911395432637
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00795587574701282,
            "upper_bound": 0.030935844461983907
          },
          "point_estimate": 0.02196681591749931,
          "standard_error": 0.006087526596109141
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-two-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.653097100762906,
            "upper_bound": 33.670358754939414
          },
          "point_estimate": 33.65971386388695,
          "standard_error": 0.004793090984606494
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.65319782992012,
            "upper_bound": 33.65826896324329
          },
          "point_estimate": 33.65619441712189,
          "standard_error": 0.0015316413806988382
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004771068106846254,
            "upper_bound": 0.008700072177769703
          },
          "point_estimate": 0.0029185630827408084,
          "standard_error": 0.002270882807300757
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.65352356634482,
            "upper_bound": 33.658372768874116
          },
          "point_estimate": 33.65598884570927,
          "standard_error": 0.00128350655435266
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017482080941438984,
            "upper_bound": 0.024482000177668817
          },
          "point_estimate": 0.015973346733003796,
          "standard_error": 0.007600986098409766
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.59398873997809,
            "upper_bound": 35.615149142185615
          },
          "point_estimate": 35.60451646109308,
          "standard_error": 0.005408273663672987
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.58742643172923,
            "upper_bound": 35.62091983469051
          },
          "point_estimate": 35.604397552729615,
          "standard_error": 0.007191920900206871
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0037956574057619505,
            "upper_bound": 0.03282117446201629
          },
          "point_estimate": 0.019549441808962092,
          "standard_error": 0.00782898215539224
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.5874441687673,
            "upper_bound": 35.61060606990476
          },
          "point_estimate": 35.597269920873,
          "standard_error": 0.005974198985067961
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010565054379020987,
            "upper_bound": 0.02233222082712541
          },
          "point_estimate": 0.018028399399780577,
          "standard_error": 0.0030643791545291975
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.48168864950727,
            "upper_bound": 55.64376775336174
          },
          "point_estimate": 55.56081834491181,
          "standard_error": 0.041483411688437985
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.44253969337971,
            "upper_bound": 55.69923646751473
          },
          "point_estimate": 55.50973914688993,
          "standard_error": 0.08142877015384976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02040266327950128,
            "upper_bound": 0.21532555364205463
          },
          "point_estimate": 0.1388746010852208,
          "standard_error": 0.054446220542580526
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.44555175180093,
            "upper_bound": 55.70272469645958
          },
          "point_estimate": 55.59138926947886,
          "standard_error": 0.0672430971705665
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08497630178230141,
            "upper_bound": 0.1604283648230783
          },
          "point_estimate": 0.1380215755359441,
          "standard_error": 0.01886883103378721
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.7413623129065,
            "upper_bound": 58.94505912272782
          },
          "point_estimate": 58.84287840579246,
          "standard_error": 0.05218264766745624
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.66791412158585,
            "upper_bound": 59.00693788216804
          },
          "point_estimate": 58.85304329965871,
          "standard_error": 0.0972961893528521
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.038065565476815,
            "upper_bound": 0.28110054802671186
          },
          "point_estimate": 0.22919500124032968,
          "standard_error": 0.06709927311601982
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.72563368484237,
            "upper_bound": 58.996612612615976
          },
          "point_estimate": 58.87062893559985,
          "standard_error": 0.06897359735834886
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11366426421894392,
            "upper_bound": 0.20402357674008792
          },
          "point_estimate": 0.17350383624591392,
          "standard_error": 0.0232562875461124
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.43742491193275,
            "upper_bound": 46.58954728192541
          },
          "point_estimate": 46.51029916436249,
          "standard_error": 0.03920589003143964
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.39963424550653,
            "upper_bound": 46.662806656373306
          },
          "point_estimate": 46.46083679219275,
          "standard_error": 0.06963555740911011
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01242992606145335,
            "upper_bound": 0.20258908921768948
          },
          "point_estimate": 0.10515731096419732,
          "standard_error": 0.05715073870398182
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.42328979985966,
            "upper_bound": 46.63576747142533
          },
          "point_estimate": 46.53307322595237,
          "standard_error": 0.05567489520350987
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.061664483935373014,
            "upper_bound": 0.15100247800940808
          },
          "point_estimate": 0.13105099110491317,
          "standard_error": 0.02051611274490659
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.57929296653406,
            "upper_bound": 77.75428728845957
          },
          "point_estimate": 77.65095338691363,
          "standard_error": 0.0466031117784483
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.56160303836742,
            "upper_bound": 77.68385254291307
          },
          "point_estimate": 77.59662934752993,
          "standard_error": 0.032136497071327916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008437121006507202,
            "upper_bound": 0.1470969890702642
          },
          "point_estimate": 0.07087643439646478,
          "standard_error": 0.035139658256750674
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.58431017897934,
            "upper_bound": 77.89962660496182
          },
          "point_estimate": 77.73120719020748,
          "standard_error": 0.0922804688754552
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.036696767438782224,
            "upper_bound": 0.2314907246369984
          },
          "point_estimate": 0.1550111747915036,
          "standard_error": 0.061400945314481294
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.67856621612799,
            "upper_bound": 43.732063579707656
          },
          "point_estimate": 43.701907145702,
          "standard_error": 0.01387216822696028
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.67276368909787,
            "upper_bound": 43.7250089155762
          },
          "point_estimate": 43.68306030340864,
          "standard_error": 0.012152260892620724
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001473110460158761,
            "upper_bound": 0.059880160919350015
          },
          "point_estimate": 0.02139631572140672,
          "standard_error": 0.014000545949739296
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.674210974524655,
            "upper_bound": 43.72520519011293
          },
          "point_estimate": 43.70030183421901,
          "standard_error": 0.013273078970562235
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01140423239983456,
            "upper_bound": 0.06440339851823319
          },
          "point_estimate": 0.046327410363141634,
          "standard_error": 0.014612629480823931
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.23591189977848,
            "upper_bound": 51.27540109883656
          },
          "point_estimate": 51.25441934420827,
          "standard_error": 0.01013318659834173
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.2258331630244,
            "upper_bound": 51.276187464323215
          },
          "point_estimate": 51.244537170804584,
          "standard_error": 0.013965491385411907
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007852402132634922,
            "upper_bound": 0.058768733746881595
          },
          "point_estimate": 0.03236997927013685,
          "standard_error": 0.012420817983294166
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.23436440906819,
            "upper_bound": 51.25755152626
          },
          "point_estimate": 51.2448235918304,
          "standard_error": 0.005821358013501178
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016870265355613333,
            "upper_bound": 0.04454112993405889
          },
          "point_estimate": 0.033775908151398734,
          "standard_error": 0.007628398358638418
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.3168019716211,
            "upper_bound": 73.41777600196477
          },
          "point_estimate": 73.37489323759209,
          "standard_error": 0.02627112490176636
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.36348573482464,
            "upper_bound": 73.41437707210171
          },
          "point_estimate": 73.39051175858074,
          "standard_error": 0.015107882321363775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0117330672334862,
            "upper_bound": 0.08714870669670047
          },
          "point_estimate": 0.034270665242794165,
          "standard_error": 0.019042047172194577
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.24661447523488,
            "upper_bound": 73.3995743384944
          },
          "point_estimate": 73.33880040686576,
          "standard_error": 0.04411199667014913
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019200454162265835,
            "upper_bound": 0.1294656566435465
          },
          "point_estimate": 0.08763281679218134,
          "standard_error": 0.033417329674212064
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1212433.4970762895,
            "upper_bound": 1213486.7280195437
          },
          "point_estimate": 1212862.064570106,
          "standard_error": 280.4547128699578
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1212358.9,
            "upper_bound": 1212927.086666667
          },
          "point_estimate": 1212669.156878307,
          "standard_error": 155.45471439909895
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.1481854850069,
            "upper_bound": 793.2288059450603
          },
          "point_estimate": 377.0986855551969,
          "standard_error": 184.13197798666835
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1212456.516981132,
            "upper_bound": 1212775.3891946992
          },
          "point_estimate": 1212639.076883117,
          "standard_error": 81.43691870723252
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.10026275102015,
            "upper_bound": 1406.9035092879046
          },
          "point_estimate": 935.910309638262,
          "standard_error": 382.53789738886
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1535180.255573082,
            "upper_bound": 1537392.9575012403
          },
          "point_estimate": 1536210.355214947,
          "standard_error": 567.0984138171622
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1535166.2099537037,
            "upper_bound": 1537395.3472222222
          },
          "point_estimate": 1535820.9636904765,
          "standard_error": 498.6622714976923
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 302.35464338214354,
            "upper_bound": 3226.5647131651244
          },
          "point_estimate": 1003.8437321780962,
          "standard_error": 740.1047685464488
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1535261.620593692,
            "upper_bound": 1536605.016539209
          },
          "point_estimate": 1535775.9846320346,
          "standard_error": 344.72472240368035
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 631.6447584952703,
            "upper_bound": 2515.431126748565
          },
          "point_estimate": 1890.2396895996555,
          "standard_error": 466.3919173451825
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-let": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1515997.5910085978,
            "upper_bound": 1520784.6647156083
          },
          "point_estimate": 1518154.0270271164,
          "standard_error": 1236.3453838815024
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1514693.90625,
            "upper_bound": 1520281.8416666666
          },
          "point_estimate": 1517222.945601852,
          "standard_error": 1403.8048821541422
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 513.2425758881541,
            "upper_bound": 6218.7104333456555
          },
          "point_estimate": 3827.799434473563,
          "standard_error": 1352.8862317951864
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1516692.6781736412,
            "upper_bound": 1520010.1698198195
          },
          "point_estimate": 1518441.6463203465,
          "standard_error": 853.3966161680381
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1706.538363329405,
            "upper_bound": 5728.262546174164
          },
          "point_estimate": 4121.449157693311,
          "standard_error": 1164.7950679012015
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 368290.02938832773,
            "upper_bound": 369872.9616488496
          },
          "point_estimate": 369009.8486018919,
          "standard_error": 407.6405942940964
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 368086.7366161616,
            "upper_bound": 370136.32196969696
          },
          "point_estimate": 368367.4130190797,
          "standard_error": 491.06681297317647
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.252289223860295,
            "upper_bound": 1914.8733364587456
          },
          "point_estimate": 637.7838915727536,
          "standard_error": 498.62843728128786
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 368185.2818592428,
            "upper_bound": 368836.2576489533
          },
          "point_estimate": 368440.23077528534,
          "standard_error": 167.7033549351361
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 418.7977977396107,
            "upper_bound": 1707.8888002678025
          },
          "point_estimate": 1358.047493435529,
          "standard_error": 338.1955100535048
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 583860.8786190476,
            "upper_bound": 584683.9007118293
          },
          "point_estimate": 584280.6587666919,
          "standard_error": 211.02523432011463
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 583521.067063492,
            "upper_bound": 584883.7936507936
          },
          "point_estimate": 584638.4777777778,
          "standard_error": 444.9229457216341
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.0373955652052,
            "upper_bound": 1050.1561546894789
          },
          "point_estimate": 553.5040648822479,
          "standard_error": 281.4002084727189
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 584214.2597233825,
            "upper_bound": 584816.8282290932
          },
          "point_estimate": 584600.8616367759,
          "standard_error": 154.6619854811344
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 442.9568196571508,
            "upper_bound": 785.2034572457804
          },
          "point_estimate": 703.9497536501663,
          "standard_error": 84.51767526163201
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 436104.73006746033,
            "upper_bound": 437545.2350598073
          },
          "point_estimate": 436745.08053051773,
          "standard_error": 372.5716802628903
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 435883.4511904762,
            "upper_bound": 437369.8698979592
          },
          "point_estimate": 436167.0686507936,
          "standard_error": 435.3404887481176
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.82613731870704,
            "upper_bound": 1809.3448924808124
          },
          "point_estimate": 623.1364747704066,
          "standard_error": 472.52285441252536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 435982.3883385471,
            "upper_bound": 437191.2857827039
          },
          "point_estimate": 436505.62195423624,
          "standard_error": 313.6493310923772
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 447.1800982275916,
            "upper_bound": 1705.3636669747557
          },
          "point_estimate": 1242.4721678383803,
          "standard_error": 347.45417605151295
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-you": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550579.3150954603,
            "upper_bound": 551801.5553425729
          },
          "point_estimate": 551181.821966951,
          "standard_error": 313.4599516223795
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550133.1074626866,
            "upper_bound": 552221.6567164179
          },
          "point_estimate": 551080.0684079602,
          "standard_error": 609.5906048858988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 181.98140184374984,
            "upper_bound": 1700.6514285760172
          },
          "point_estimate": 1421.2118522311746,
          "standard_error": 412.4977662140984
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550605.9921933915,
            "upper_bound": 552176.1414300757
          },
          "point_estimate": 551513.8881178523,
          "standard_error": 412.0391884968005
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 695.9072185317576,
            "upper_bound": 1199.5896060263003
          },
          "point_estimate": 1047.2601559305372,
          "standard_error": 129.6753496307458
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1039281.12445805,
            "upper_bound": 1041512.6254126984
          },
          "point_estimate": 1040384.8306655328,
          "standard_error": 571.7088327395479
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1038675.7714285714,
            "upper_bound": 1042188.8095238096
          },
          "point_estimate": 1040120.7160714286,
          "standard_error": 974.7382893201147
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 545.420290316846,
            "upper_bound": 3113.1933036584546
          },
          "point_estimate": 2306.523139051024,
          "standard_error": 673.5718668466567
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1040045.3272532125,
            "upper_bound": 1042282.6519290725
          },
          "point_estimate": 1041382.402671614,
          "standard_error": 566.5134054352662
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1204.5395131826638,
            "upper_bound": 2257.983435915368
          },
          "point_estimate": 1902.921093482266,
          "standard_error": 268.7929579153085
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 310982.4086619862,
            "upper_bound": 311444.13788461534
          },
          "point_estimate": 311216.2640174332,
          "standard_error": 117.68610499236904
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 310929.8512820513,
            "upper_bound": 311578.63675213675
          },
          "point_estimate": 311154.1803181387,
          "standard_error": 179.91539750049674
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.987858865663,
            "upper_bound": 687.2756006189305
          },
          "point_estimate": 444.3607668973341,
          "standard_error": 152.63552176659746
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 310848.6327119902,
            "upper_bound": 311373.6250680683
          },
          "point_estimate": 311090.6923964924,
          "standard_error": 133.35338050467047
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.68717563693116,
            "upper_bound": 491.1032094248473
          },
          "point_estimate": 392.88301573423985,
          "standard_error": 66.41380794660236
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 847088.5865155038,
            "upper_bound": 848355.044717608
          },
          "point_estimate": 847714.8503709857,
          "standard_error": 324.4258187500564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 846825.1860465116,
            "upper_bound": 848831.7441860465
          },
          "point_estimate": 847398.3459302326,
          "standard_error": 570.3515008714006
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 178.46078869217953,
            "upper_bound": 1677.6350150995966
          },
          "point_estimate": 1477.9228879477478,
          "standard_error": 421.93058439652634
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 847277.8008091041,
            "upper_bound": 848466.1936402468
          },
          "point_estimate": 847880.31307762,
          "standard_error": 303.73388619017754
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 692.2355055085864,
            "upper_bound": 1259.1874389062098
          },
          "point_estimate": 1077.8280420612969,
          "standard_error": 145.8588446464566
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 425809.8487218531,
            "upper_bound": 426176.8434883721
          },
          "point_estimate": 425981.1869744371,
          "standard_error": 93.8894196588666
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 425737.7466777408,
            "upper_bound": 426237.7112403101
          },
          "point_estimate": 425817.3385820413,
          "standard_error": 148.17811211802933
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.13979155619564,
            "upper_bound": 552.5724806549831
          },
          "point_estimate": 146.84469656932356,
          "standard_error": 149.1135814533024
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 425768.38665801706,
            "upper_bound": 426076.9127290656
          },
          "point_estimate": 425870.0433101782,
          "standard_error": 80.16409405515606
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.47619912428465,
            "upper_bound": 399.01274520641806
          },
          "point_estimate": 312.93511235054956,
          "standard_error": 65.9098263501335
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163002.3679412911,
            "upper_bound": 163281.90985809948
          },
          "point_estimate": 163140.2934819657,
          "standard_error": 71.54547578763176
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162918.79296875,
            "upper_bound": 163317.48370535712
          },
          "point_estimate": 163153.171875,
          "standard_error": 127.10355964836896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.35947029433335,
            "upper_bound": 408.8858023390424
          },
          "point_estimate": 279.8341262819573,
          "standard_error": 96.9672364776145
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162928.15309362632,
            "upper_bound": 163314.45068956545
          },
          "point_estimate": 163120.26972402597,
          "standard_error": 98.90423006138144
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.64193340785306,
            "upper_bound": 291.321704361864
          },
          "point_estimate": 238.93303481641783,
          "standard_error": 36.41453158991756
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-that": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 249113.6097070015,
            "upper_bound": 249285.74517060773
          },
          "point_estimate": 249196.42877391828,
          "standard_error": 44.245161840559355
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 249072.8022260274,
            "upper_bound": 249328.77168949772
          },
          "point_estimate": 249173.4702435312,
          "standard_error": 66.00858859626095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.865817540529832,
            "upper_bound": 237.13412275921175
          },
          "point_estimate": 157.68078342781163,
          "standard_error": 58.77730237242586
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 249079.70289279064,
            "upper_bound": 249171.68656053313
          },
          "point_estimate": 249117.8638142679,
          "standard_error": 23.494992340334285
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.13132260114541,
            "upper_bound": 178.9278065286442
          },
          "point_estimate": 147.41697323489927,
          "standard_error": 25.400026432000992
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179448.12282497654,
            "upper_bound": 179671.451302682
          },
          "point_estimate": 179536.43914125417,
          "standard_error": 60.92173027831703
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179445.9465311987,
            "upper_bound": 179550.35492610838
          },
          "point_estimate": 179477.57903667213,
          "standard_error": 26.1219728427013
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.737984486493364,
            "upper_bound": 158.1984078810953
          },
          "point_estimate": 43.93328370277271,
          "standard_error": 38.647188269715585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179429.3459711471,
            "upper_bound": 179499.14544271404
          },
          "point_estimate": 179465.92796366196,
          "standard_error": 17.762361189963727
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.12989758322676,
            "upper_bound": 307.29540206868666
          },
          "point_estimate": 203.02635466768965,
          "standard_error": 88.81902397889404
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2260143.9416001397,
            "upper_bound": 2261634.97635195
          },
          "point_estimate": 2260845.4044561153,
          "standard_error": 382.17906507644886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2259853.719747899,
            "upper_bound": 2261643.2352941176
          },
          "point_estimate": 2260842.7642973857,
          "standard_error": 440.90480676560134
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 225.43029501743135,
            "upper_bound": 2065.2617633342743
          },
          "point_estimate": 1250.5855539254112,
          "standard_error": 493.18744451275313
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2260074.6851003873,
            "upper_bound": 2260945.000834376
          },
          "point_estimate": 2260486.71657754,
          "standard_error": 222.1242044298036
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 613.1028958499955,
            "upper_bound": 1698.2464756146032
          },
          "point_estimate": 1275.0803642633457,
          "standard_error": 294.77765472495037
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2022-04-29",
      "fullname": "2022-04-29/memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4571.1467285368135,
            "upper_bound": 4573.979129780147
          },
          "point_estimate": 4572.720939806342,
          "standard_error": 0.7201209258967078
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4572.500240203466,
            "upper_bound": 4573.58970735996
          },
          "point_estimate": 4572.859833315391,
          "standard_error": 0.3253755828781383
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1440170810909785,
            "upper_bound": 2.905784932083471
          },
          "point_estimate": 0.7067550226193858,
          "standard_error": 0.6133749992089752
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4572.737317437596,
            "upper_bound": 4573.605330319016
          },
          "point_estimate": 4573.126158995488,
          "standard_error": 0.22645392044708712
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.421154994204891,
            "upper_bound": 3.530635551553165
          },
          "point_estimate": 2.3939107956825736,
          "standard_error": 0.8763615290472985
        }
      }
    }
  }
}
